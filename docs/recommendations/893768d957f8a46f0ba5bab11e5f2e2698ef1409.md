---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-05-18 14:36:41 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3>Recommendations for the article <i>Parsimony as the ultimate regularizer for physics-informed machine learning</i></h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="Machine learning (ML) is redefining what is possible in data-intensive fields of science and engineering. However, applying ML to problems in the physical sciences comes with a unique set of challenges: scientists want physically interpretable models that can (i) generalize to predict previously unobserved behaviors, (ii) provide effective forecasting predictions (extrapolation), and (iii) be certifiable. Autonomous systems will necessarily interact with changing and uncertain environments, motivating the need for models that can accurately extrapolate based on physical principles (e.g. Newtonâ€™s universal second law for classical mechanics, $F=ma$ ). Standard ML approaches have shown impressive performance for predicting dynamics in an interpolatory regime, but the resulting models often lack interpretability and fail to generalize. We build on a sparse regression framework that discovers governing dynamical systems models from data, selecting relevant terms in the dynamics from a library of possible functions. Our critically enabling innovation introduces a relaxed version of a sparse optimization framework that allows the use of non-convex sparsity promoting regularization functions and addresses three open challenges in scientific problems and data sets: (i) robust handling of outliers and corrupt data within noisy sensor measurements, (ii) parametric dependencies in candidate library functions, and (iii) the imposition of physical constraints. By explicitly addressing these open challenges, the integrated and unified algorithm developed provides a significant advancement over current state-of-the-art sparse model discovery methods. We show that the approach discovers parsimonious dynamical models on several example systems. This flexible approach can be tailored to the unique challenges associated with a wide range of applications and data sets, providing a powerful ML-based framework for learning governing models for physical systems from data.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/7a752ef2c71afa4566dd038b63929e75e41ab32f" target='_blank'>A Unified Sparse Optimization Framework to Learn Parsimonious Physics-Informed Models From Data</a></td>
          <td>
            
              Kathleen P. Champion,
            
              P. Zheng,
            
              A. Aravkin,
            
              S. Brunton,
            
              J. Kutz,
            
          </td>
          <td>2019-06-25</td>
          <td>IEEE Access</td>
          <td>91</td>
          <td>61</td>
        </tr>
    
        <tr id="Identifying governing equations for a dynamical system is a topic of critical interest across an array of disciplines, from mathematics to engineering to biology. Machine learning-specifically deep learning-techniques have shown their capabilities in approximating dynamics from data, but a shortcoming of traditional deep learning is that there is little insight into the underlying mapping beyond its numerical output for a given input. This limits their utility in analysis beyond simple prediction. Simultaneously, a number of strategies exist which identify models based on a fixed dictionary of basis functions, but most either require some intuition or insight about the system, or are susceptible to overfitting or a lack of parsimony. Here, we present a novel approach that combines the flexibility and accuracy of deep learning approaches with the utility of symbolic solutions: a deep neural network that generates a symbolic expression for the governing equations. We first describe the architecture for our model and then show the accuracy of our algorithm across a range of classical dynamical systems.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/eddffa1f2cf18c7ca4edbdaf01fd805f415aa4b8" target='_blank'>Symbolic regression via neural networks.</a></td>
          <td>
            
              N. Boddupalli,
            
              T. Matchen,
            
              J. Moehlis,
            
          </td>
          <td>2023-08-01</td>
          <td>Chaos</td>
          <td>2</td>
          <td>37</td>
        </tr>
    
        <tr id="Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/036441fb7af2f04ef5ee8def7e8e0461fb9fdc66" target='_blank'>Machine Learning for Partial Differential Equations</a></td>
          <td>
            
              S. Brunton,
            
              J. Kutz,
            
          </td>
          <td>2023-03-30</td>
          <td>ArXiv</td>
          <td>14</td>
          <td>61</td>
        </tr>
    
        <tr id="Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are sample efficient, and interpretable but often rely on rigid assumptions. Furthermore, direct numerical approximation is usually computationally intensive, requiring significant computational resources and expertise, and many real-world systems do not have fully-known governing laws. While deep learning (DL) provides novel alternatives for efficiently recognizing complex patterns and emulating nonlinear dynamics, its predictions do not necessarily obey the governing laws of physical systems, nor do they generalize well across different systems. Thus, the study of physics-guided DL emerged and has gained great progress. Physics-guided DL aims to take the best from both physics-based modeling and state-of-the-art DL models to better solve scientific problems. In this paper, we provide a structured overview of existing methodologies of integrating prior physical knowledge or physics-based modeling into DL, with a special emphasis on learning dynamical systems. We also discuss the fundamental challenges and emerging opportunities in the area.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/7ac3e74f927b01f2b54661b09d9014a4e3a77bf8" target='_blank'>Physics-Guided Deep Learning for Dynamical Systems: A survey</a></td>
          <td>
            
              Rui Wang,
            
          </td>
          <td>2021-07-02</td>
          <td>ArXiv</td>
          <td>42</td>
          <td>9</td>
        </tr>
    
        <tr id="Neural ordinary differential equations (ODEs) are an emerging class of deep learning models for dynamical systems. They are particularly useful for learning an ODE vector field from observed trajectories (i.e., inverse problems). We here consider aspects of these models relevant for their application in science and engineering. Scientific predictions generally require structured uncertainty estimates. As a first contribution, we show that basic and lightweight Bayesian deep learning techniques like the Laplace approximation can be applied to neural ODEs to yield structured and meaningful uncertainty quantification. But, in the scientific domain, available information often goes beyond raw trajectories, and also includes mechanistic knowledge, e.g., in the form of conservation laws. We explore how mechanistic knowledge and uncertainty quantification interact on two recently proposed neural ODE frameworks - symplectic neural ODEs and physical models augmented with neural ODEs. In particular, uncertainty reflects the effect of mechanistic information more directly than the predictive power of the trained model could. And vice versa, structure can improve the extrapolation abilities of neural ODEs, a fact that can be best assessed in practice through uncertainty estimates. Our experimental analysis demonstrates the effectiveness of the Laplace approach on both low dimensional ODE problems and a high dimensional partial differential equation.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/bd5a854fe7f97a373a15ebf31264bb795adae877" target='_blank'>Uncertainty and Structure in Neural Ordinary Differential Equations</a></td>
          <td>
            
              Katharina Ott,
            
              Michael Tiemann,
            
              Philipp Hennig,
            
          </td>
          <td>2023-05-22</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>38</td>
        </tr>
    
        <tr id="None">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/e596988b1df3a0bc78bf72c0bfdb21c85eaab6c9" target='_blank'>Physics-informed learning of governing equations from scarce data</a></td>
          <td>
            
              Zhao Chen,
            
              Yang Liu,
            
              ,
            
          </td>
          <td>2020-05-05</td>
          <td>Nature Communications</td>
          <td>204</td>
          <td>12</td>
        </tr>
    
        <tr id="Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. Recent advances in sparse identification show encouraging success in distilling closed-form governing equations from data for a wide range of nonlinear dynamical systems. However, the fundamental bottleneck of this approach lies in the robustness and scalability with respect to data scarcity and noise. This work introduces a novel physics-informed deep learning framework to discover governing partial differential equations (PDEs) from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this approach seamlessly integrates the strengths of deep neural networks for rich representation learning, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the PDEs. The efficacy and robustness of this method are demonstrated on discovering a variety of PDE systems with different levels of data scarcity and noise. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/e2acd4a2bc6af5deb02f542846766868e3f1b180" target='_blank'>Deep learning of physical laws from scarce data</a></td>
          <td>
            
              Zhao Chen,
            
              Yang Liu,
            
              ,
            
          </td>
          <td>2020-05-05</td>
          <td>ArXiv</td>
          <td>19</td>
          <td>12</td>
        </tr>
    
        <tr id="Physics-Informed Machine Learning (PIML) has gained momentum in the last 5 years with scientists and researchers aiming to utilize the benefits afforded by advances in machine learning, particularly in deep learning. With large scientific data sets with rich spatio-temporal data and high-performance computing providing large amounts of data to be inferred and interpreted, the task of PIML is to ensure that these predictions, categorizations, and inferences are enforced by, and conform to the limits imposed by physical laws. In this work a new approach to utilizing PIML is discussed that deals with the use of physics-based loss functions. While typical usage of physical equations in the loss function requires complex layers of derivatives and other functions to ensure that the known governing equation is satisfied, here we show that a similar level of enforcement can be found by implementing more simpler loss functions on specific kinds of output data. The generalizability that this approach affords is shown using examples of simple mechanical models that can be thought of as sufficiently simplified surrogate models for a wide class of problems.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/28d4ee0a9db172bd2c67b3e617d4ce465c3ba46e" target='_blank'>Applying physics-based loss functions to neural networks for improved generalizability in mechanics problems</a></td>
          <td>
            
              Samuel J. Raymond,
            
              David B. Camarillo,
            
          </td>
          <td>2021-04-30</td>
          <td>ArXiv</td>
          <td>8</td>
          <td>29</td>
        </tr>
    
        <tr id="None">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/1a2c2bd1496da55a109939766c3ff8512b4a6851" target='_blank'>Physical laws meet machine intelligence: current developments and future directions</a></td>
          <td>
            
              T. Muther,
            
              A. K. Dahaghi,
            
              F. I. Syed,
            
              Vuong Van Pham,
            
          </td>
          <td>2022-12-05</td>
          <td>Artificial Intelligence Review</td>
          <td>14</td>
          <td>15</td>
        </tr>
    
        <tr id="How many free variables do we really need to build a credible model of a physical system? Currently there is no systematic approach; we appeal to some physical principles, tune free variables by comparing with canonical cases, and hope our real-world applications interpolate between them. In this work we combine two pioneering and entirely disparate pieces of mathematics: the century-old techniques of Sophus Lie for solving differential equtions and recent work initiated by Field's medallist Terence Tao on converting NP-complete combinatorical problems into neighbouring convex optimisations. We present a novel and fully systematic procedure for designing models of physical systems with necessary and just-sufficient complexity, in marked contrast with the approach to function approximation taken by neural networks and other current approaches to machine learning. Our methodology replaces the ad-hoc development of models to recover structure and understanding from observational, experimental or simulated data. At its core, our method seeks to find invariant properties of differential equations known as Lie symmetries, and for this reason we have called our algorithm the Lie Detector.">
          <td id="tag"></td>
          <td><a href="https://www.semanticscholar.org/paper/73027ff397a011f8ffb5affb47337bfa6efbfef9" target='_blank'>The Lie Detector.</a></td>
          <td>
            
              A. Young,
            
              A. Lawrie,
            
          </td>
          <td>2019-12-13</td>
          <td>arXiv: Signal Processing</td>
          <td>0</td>
          <td>18</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content">Abstract: ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
</script>
</html>