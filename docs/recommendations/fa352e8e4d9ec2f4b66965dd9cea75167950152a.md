---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-22 06:07:00 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d86084808994ac54ef4840ae65295f3c0ec4decd" target='_blank'>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</a></td>
          <td>
            M. Raissi, P. Perdikaris, G. Karniadakis
          </td>
          <td>2019-02-01</td>
          <td>J. Comput. Phys.</td>
          <td>7319</td>
          <td>127</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/43bba6a3bb186e7370366e5c899c90ca9a573820" target='_blank'>Physics Informed Extreme Learning Machine (PIELM) - A rapid method for the numerical solution of partial differential equations</a></td>
          <td>
            Vikas Dwivedi, B. Srinivasan
          </td>
          <td>2019-07-08</td>
          <td>ArXiv</td>
          <td>130</td>
          <td>14</td>
        </tr>
    
        <tr id="This paper establishes a method for solving partial differential equations using a multi-step physics-informed deep operator neural network. The network is trained by embedding physics-informed constraints. Different from traditional neural networks for solving partial differential equations, the proposed method uses a deep neural operator network to indirectly construct the mapping relationship between the variable functions and solution functions. This approach makes full use of the hidden information between the variable functions and independent variables. The process whereby the model captures incredibly complex and highly nonlinear relationships is simplified, thereby making network learning easier and enhancing the extraction of information about the independent variables in partial differential systems. In terms of solving partial differential equations, we verify that the multi-step physics-informed deep operator neural network markedly improves the solution accuracy compared with a traditional physics-informed deep neural operator network, especially when the problem involves complex physical phenomena with large gradient changes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ac5d801ec2ae339a109eb310769fe243cbff7025" target='_blank'>Multi-Step Physics-Informed Deep Operator Neural Network for Directly Solving Partial Differential Equations</a></td>
          <td>
            Jing Wang, Yubo Li, Anping Wu, Zheng Chen, Jun Huang, Qingfeng Wang, Feng Liu
          </td>
          <td>2024-06-25</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>6</td>
        </tr>
    
        <tr id="Physics-informed neural networks (PINNs) [31] use automatic differentiation to solve partial differential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the {\em trial space} to be the space of neural networks and the {\em test space} to be the space of Legendre polynomials. We formulate the \textit{variational residual} of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a \textit{variational physics-informed neural network} (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence effectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the \textit{variational residual}. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d483f6ecc4685767344822d9e0f03c82b68531ba" target='_blank'>Variational Physics-Informed Neural Networks For Solving Partial Differential Equations</a></td>
          <td>
            E. Kharazmi, Z. Zhang, G. Karniadakis
          </td>
          <td>2019-11-27</td>
          <td>ArXiv</td>
          <td>185</td>
          <td>127</td>
        </tr>
    
        <tr id="A long-standing problem at the interface of artificial intelligence and applied mathematics is to devise an algorithm capable of achieving human level or even superhuman proficiency in transforming observed data into predictive mathematical models of the physical world. In the current era of abundance of data and advanced machine learning capabilities, the natural question arises: How can we automatically uncover the underlying laws of physics from high-dimensional data generated from experiments? In this work, we put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr\"{o}dinger, and Navier-Stokes equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ebcc0e71ef6a77d05e7ab064435bc2da87c55e91" target='_blank'>Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations</a></td>
          <td>
            M. Raissi
          </td>
          <td>2018-01-20</td>
          <td>J. Mach. Learn. Res.</td>
          <td>658</td>
          <td>24</td>
        </tr>
    
        <tr id="We revisit the original approach of using deep learning and neural networks to solve differential equations by incorporating the knowledge of the equation. This is done by adding a dedicated term to the loss function during the optimization procedure in the training process. The so-called physics-informed neural networks (PINNs) are tested on a variety of academic ordinary differential equations in order to highlight the benefits and drawbacks of this approach with respect to standard integration methods. We focus on the possibility to use the least possible amount of data into the training process. The principles of PINNs for solving differential equations by enforcing physical laws via penalizing terms are reviewed. A tutorial on a simple equation model illustrates how to put into practice the method for ordinary differential equations. Benchmark tests show that a very small amount of training data is sufficient to predict the solution when the non linearity of the problem is weak. However, this is not the case in strongly non linear problems where a priori knowledge of training data over some partial or the whole time integration interval is necessary.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10db5d6f57cf7f9b3fdedbd9c578d79dd2e863fa" target='_blank'>Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests</a></td>
          <td>
            H. Baty, L. Baty
          </td>
          <td>2023-02-23</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>14</td>
        </tr>
    
        <tr id="Partial differential equations (PDEs) play a pivotal role in mathematical analysis and modeling of dynamic processes across various disciplines of science and engineering. Machine learning (ML) techniques have emerged as a promising new approach to solving PDEs. Among them, Physics-Informed Neural Networks (PINNs) have garnered significant attention in numerous scientific and engineering studies. PINNs employ a single deep neural network to assimilate observational data with PDEs across the entire space-time of a physical system, subsequently yielding rapid solutions. However, a PINN may entail intricate analyses or computations and can be cost-intensive, depending on initial or boundary conditions and other input parameters. To address the limitations of the PINN, especially concerning resolution for nonlinear problems, the Physical-Informed Deep Operator Network (DeepONet) is introduced in this paper. The Physics-Informed DeepONet is a deep learning framework crafted to discern solution operators for any given PDEs, even in scenarios lacking paired input/output training data. The proposed framework is able to predict solutions for various types of parameterized PDEs much faster than conventional PDE solvers. Several cases confirm that this approach is effective in establishing previously unexplored paradigms for modeling/simulating nonlinear and non-equilibrium processes in science and engineering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/54aa8e27e0e1c52997a21502817afcb71d9e8ccb" target='_blank'>Understanding on Physics-Informed DeepONet</a></td>
          <td>
            Sang-Min Lee
          </td>
          <td>2024-01-31</td>
          <td>Journal of the Korea Academia-Industrial cooperation Society</td>
          <td>0</td>
          <td>0</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3d6fd15fea21a3a5fff4bf294b67cb959b46008a" target='_blank'>Physics Informed RNN-DCT Networks for Time-Dependent Partial Differential Equations</a></td>
          <td>
            Benwei Wu, O. Hennigh, J. Kautz, S. Choudhry, Wonmin Byeon
          </td>
          <td>2022-02-24</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>91</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>