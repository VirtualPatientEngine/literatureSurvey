---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:05:10 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Discovery of Physics From Data: Universal Laws and Discrepancies</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="Machine learning (ML) is redefining what is possible in data-intensive fields of science and engineering. However, applying ML to problems in the physical sciences comes with a unique set of challenges: scientists want physically interpretable models that can (i) generalize to predict previously unobserved behaviors, (ii) provide effective forecasting predictions (extrapolation), and (iii) be certifiable. Autonomous systems will necessarily interact with changing and uncertain environments, motivating the need for models that can accurately extrapolate based on physical principles (e.g. Newton’s universal second law for classical mechanics, $F=ma$ ). Standard ML approaches have shown impressive performance for predicting dynamics in an interpolatory regime, but the resulting models often lack interpretability and fail to generalize. We build on a sparse regression framework that discovers governing dynamical systems models from data, selecting relevant terms in the dynamics from a library of possible functions. Our critically enabling innovation introduces a relaxed version of a sparse optimization framework that allows the use of non-convex sparsity promoting regularization functions and addresses three open challenges in scientific problems and data sets: (i) robust handling of outliers and corrupt data within noisy sensor measurements, (ii) parametric dependencies in candidate library functions, and (iii) the imposition of physical constraints. By explicitly addressing these open challenges, the integrated and unified algorithm developed provides a significant advancement over current state-of-the-art sparse model discovery methods. We show that the approach discovers parsimonious dynamical models on several example systems. This flexible approach can be tailored to the unique challenges associated with a wide range of applications and data sets, providing a powerful ML-based framework for learning governing models for physical systems from data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a752ef2c71afa4566dd038b63929e75e41ab32f" target='_blank'>A Unified Sparse Optimization Framework to Learn Parsimonious Physics-Informed Models From Data</a></td>
          <td>
            Kathleen P. Champion, P. Zheng, A. Aravkin, S. Brunton, J. Kutz
          </td>
          <td>2019-06-25</td>
          <td>IEEE Access</td>
          <td>94</td>
          <td>63</td>
        </tr>
    
        <tr id="Energy conservation is a basic physics principle, the breakdown of which often implies new physics. This paper presents a method for data-driven "new physics" discovery. Specifically, given a trajectory governed by unknown forces, our neural new-physics detector (NNPhD) aims to detect new physics by decomposing the force field into conservative and nonconservative components, which are represented by a Lagrangian neural network (LNN) and an unconstrained neural network, respectively, trained to minimize the force recovery error plus a constant λ times the magnitude of the predicted nonconservative force. We show that a phase transition occurs at λ=1, universally for arbitrary forces. We demonstrate that NNPhD successfully discovers new physics in toy numerical experiments, rediscovering friction (1493) from a damped double pendulum, Neptune from Uranus' orbit (1846), and gravitational waves (2017) from an inspiraling orbit. We also show how NNPhD coupled with an integrator outperforms both an LNN and an unconstrained neural network for predicting the future of a damped double pendulum.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb09f46438236f6e916d2ad23eca92330a41b9fb" target='_blank'>Machine-Learning Non-Conservative Dynamics for New-Physics Detection</a></td>
          <td>
            Ziming Liu, Bohan Wang, Qi Meng, Wei Chen, M. Tegmark, Tie-Yan Liu
          </td>
          <td>2021-05-31</td>
          <td>Physical review. E</td>
          <td>14</td>
          <td>82</td>
        </tr>
    
        <tr id="Invariants and conservation laws convey critical information about the underlying dynamics of a system, yet it is generally infeasible to find them from large-scale data without any prior knowledge or human insight. We propose ConservNet to achieve this goal, a neural network that spontaneously discovers a conserved quantity from grouped data where the members of each group share invariants, similar to a general experimental setting where trajectories from different trials are observed. As a neural network trained with a novel and intuitive loss function called noise-variance loss, ConservNet learns the hidden invariants in each group of multi-dimensional observables in a data-driven, end-to-end manner. Our model successfully discovers underlying invariants from the simulated systems having invariants as well as a real-world double pendulum trajectory. Since the model is robust to various noises and data conditions compared to baseline, our approach is directly applicable to experimental data for discovering hidden conservation laws and further, general relationships between variables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87648e8b1e3c94de06380ff58f39c150bc9379bb" target='_blank'>Discovering conservation laws from trajectories via machine learning</a></td>
          <td>
            Seungwoong Ha, Hawoong Jeong
          </td>
          <td>2021-02-08</td>
          <td>ArXiv</td>
          <td>9</td>
          <td>43</td>
        </tr>
    
        <tr id="How many free variables do we really need to build a credible model of a physical system? Currently there is no systematic approach; we appeal to some physical principles, tune free variables by comparing with canonical cases, and hope our real-world applications interpolate between them. In this work we combine two pioneering and entirely disparate pieces of mathematics: the century-old techniques of Sophus Lie for solving differential equtions and recent work initiated by Field's medallist Terence Tao on converting NP-complete combinatorical problems into neighbouring convex optimisations. We present a novel and fully systematic procedure for designing models of physical systems with necessary and just-sufficient complexity, in marked contrast with the approach to function approximation taken by neural networks and other current approaches to machine learning. Our methodology replaces the ad-hoc development of models to recover structure and understanding from observational, experimental or simulated data. At its core, our method seeks to find invariant properties of differential equations known as Lie symmetries, and for this reason we have called our algorithm the Lie Detector.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73027ff397a011f8ffb5affb47337bfa6efbfef9" target='_blank'>The Lie Detector.</a></td>
          <td>
            A. Young, A. Lawrie
          </td>
          <td>2019-12-13</td>
          <td>arXiv: Signal Processing</td>
          <td>0</td>
          <td>18</td>
        </tr>
    
        <tr id="Physics-informed machine learning (PIML) is a set of methods and tools that systematically integrate machine learning (ML) algorithms with physical constraints and abstract mathematical models developed in scientific and engineering domains. As opposed to purely data-driven methods, PIML models can be trained from additional information obtained by enforcing physical laws such as energy and mass conservation. More broadly, PIML models can include abstract properties and conditions such as stability, convexity, or invariance. The basic premise of PIML is that the integration of ML and physics can yield more effective, physically consistent, and data-efficient models. This paper aims to provide a tutorial-like overview of the recent advances in PIML for dynamical system modeling and control. Specifically, the paper covers an overview of the theory, fundamental concepts and methods, tools, and applications on topics of: 1) physics-informed learning for system identification; 2) physics-informed learning for control; 3) analysis and verification of PIML models; and 4) physics-informed digital twins. The paper is concluded with a perspective on open challenges and future research opportunities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d95c7f3a6ca2b98ba3c4422aabb33e3754cb1a47" target='_blank'>Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems</a></td>
          <td>
            Truong X. Nghiem, Ján Drgoňa, Colin N. Jones, Zoltán Nagy, Roland Schwan, Biswadip Dey, A. Chakrabarty, S. D. Cairano, J. Paulson, Andrea Carron, M. Zeilinger, Wenceslao Shaw-Cortez, D. Vrabie
          </td>
          <td>2023-05-31</td>
          <td>2023 American Control Conference (ACC)</td>
          <td>14</td>
          <td>37</td>
        </tr>
    
        <tr id="Solving inverse problems, such as parameter estimation and optimal control, is a vital part of science. Many experiments repeatedly collect data and rely on machine learning algorithms to quickly infer solutions to the associated inverse problems. We find that state-of-the-art training techniques are not well-suited to many problems that involve physical processes. The highly nonlinear behavior, common in physical processes, results in strongly varying gradients that lead first-order optimizers like SGD or Adam to compute suboptimal optimization directions. We propose a novel hybrid training approach that combines higher-order optimization methods with machine learning techniques. We take updates from a scale-invariant inverse problem solver and embed them into the gradient-descent-based learning pipeline, replacing the regular gradient of the physical process. We demonstrate the capabilities of our method on a variety of canonical physical systems, showing that it yields significant improvements on a wide range of optimization and learning problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8070e0a6ca3fdd46e7af5ec9090643ff60f13a78" target='_blank'>Scale-invariant Learning by Physics Inversion</a></td>
          <td>
            Philipp Holl, V. Koltun, Nils Thuerey
          </td>
          <td>2021-09-30</td>
          <td>ArXiv, DBLP</td>
          <td>5</td>
          <td>103</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0dfa7e909940fb1840831614e593714a5544ec02" target='_blank'>Learning continuous models for continuous physics</a></td>
          <td>
            A. Krishnapriyan, A. Queiruga, N. Benjamin Erichson, Michael W. Mahoney
          </td>
          <td>2022-02-17</td>
          <td>Communications Physics</td>
          <td>22</td>
          <td>30</td>
        </tr>
    
        <tr id="We present AI Poincaré, a machine learning algorithm for autodiscovering conserved quantities using trajectory data from unknown dynamical systems. We test it on five Hamiltonian systems, including the gravitational three-body problem, and find that it discovers not only all exactly conserved quantities, but also periodic orbits, phase transitions, and breakdown timescales for approximate conservation laws.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f3cf4756b2375cd6694267370a8d98543e781169" target='_blank'>Machine Learning Conservation Laws from Trajectories.</a></td>
          <td>
            Ziming Liu, Max Tegmark
          </td>
          <td>2021-05-06</td>
          <td>Physical review letters</td>
          <td>79</td>
          <td>81</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9865d777203b47ab6ad919ae979714a1dbac8eb9" target='_blank'>From data to conservation laws</a></td>
          <td>
            F. Chirigati
          </td>
          <td>2023-09-01</td>
          <td>Nature Computational Science</td>
          <td>0</td>
          <td>17</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2cd3a4931a71d74bcec53716855a3f465bca8e71" target='_blank'>Physics-informed regularization and structure preservation for learning stable reduced models from data with operator inference</a></td>
          <td>
            N. Sawant, B. Kramer, B. Peherstorfer
          </td>
          <td>2021-07-06</td>
          <td>ArXiv</td>
          <td>20</td>
          <td>27</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>