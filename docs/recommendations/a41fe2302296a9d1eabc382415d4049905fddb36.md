---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:06:29 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba76406526d5e39c738813118d5407cc05dfb8c6" target='_blank'>Data-driven sparse identification of nonlinear dynamical systems using linear multistep methods</a></td>
          <td>
            Hao-guang Chen
          </td>
          <td>2023-01-23</td>
          <td>Calcolo</td>
          <td>1</td>
          <td>7</td>
        </tr>
    
        <tr id="Identifying hidden dynamics from observed data is a significant and challenging task in a wide range of applications. Recently, the combination of linear multistep methods (LMMs) and deep learning has been successfully employed to discover dynamics, whereas a complete convergence analysis of this approach is still under development. In this work, we consider the deep network-based LMMs for the discovery of dynamics. We put forward error estimates for these methods using the approximation property of deep networks. It indicates, for certain families of LMMs, that the $\ell^2$ grid error is bounded by the sum of $O(h^p)$ and the network approximation error, where $h$ is the time step size and $p$ is the local truncation error order. Numerical results of several physically relevant examples are provided to demonstrate our theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a2d1f9d22766d54b81b390c48413e6a570e47ed1" target='_blank'>The Discovery of Dynamics via Linear Multistep Methods and Deep Learning: Error Estimation</a></td>
          <td>
            Q. Du, Yiqi Gu, Haizhao Yang, Chao Zhou
          </td>
          <td>2021-03-21</td>
          <td>ArXiv</td>
          <td>18</td>
          <td>68</td>
        </tr>
    
        <tr id="Nonlinear differential equations rarely admit closed-form solutions, thus requiring numerical time-stepping algorithms to approximate solutions. Further, many systems characterized by multiscale physics exhibit dynamics over a vast range of timescales, making numerical integration expensive. In this work, we develop a hierarchy of deep neural network time-steppers to approximate the dynamical system flow map over a range of time-scales. The model is purely data-driven, enabling accurate and efficient numerical integration and forecasting. Similar ideas can be used to couple neural network-based models with classical numerical time-steppers. Our hierarchical time-stepping scheme provides advantages over current time-stepping algorithms, including (i) capturing a range of timescales, (ii) improved accuracy in comparison with leading neural network architectures, (iii) efficiency in long-time forecasting due to explicit training of slow time-scale dynamics, and (iv) a flexible framework that is parallelizable and may be integrated with standard numerical time-stepping algorithms. The method is demonstrated on numerous nonlinear dynamical systems, including the Van der Pol oscillator, the Lorenz system, the Kuramoto–Sivashinsky equation, and fluid flow pass a cylinder; audio and video signals are also explored. On the sequence generation examples, we benchmark our algorithm against state-of-the-art methods, such as LSTM, reservoir computing and clockwork RNN. This article is part of the theme issue ‘Data-driven prediction in dynamical systems’.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6958b9125dfbea51cc33f9432c62e2b0ea2c8e4f" target='_blank'>Hierarchical deep learning of multiscale differential equation time-steppers</a></td>
          <td>
            Yuying Liu, N. Kutz, S. Brunton
          </td>
          <td>2020-08-22</td>
          <td>Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</td>
          <td>60</td>
          <td>63</td>
        </tr>
    
        <tr id="In many cases, the equations that dynamical systems are based on are unknown and hard to model and predict. On the other hand, machine learning algorithms are based on the data of a solution as it evolves and do not need equations. In the era of abundant data, using machine learning technology to discover accurate mathematical models of dynamical systems directly from time series data becomes increasingly important. Recently, a multi-step deep neural networks (multi-step DNN) model without need of direct access to temporal gradients is proposed, which can accurately learn the evolution from a given set of observed data, identify nonlinear dynamical systems, and forecast future states. However, the architecture lacks the capability to capture long term temporal dependencies from dynamical time-series data. In the paper, based on the multi-step time-stepping schemes, we proposed a new CLDNN model which combine convolutional layer, long short-term memory layer and fully connected layer, to address the aforementioned weakness. The effectiveness of our model is tested for several benchmark problems involving the identification and prediction of complex, nonlinear and chaotic dynamics. The experiment results show that the multi-step CLDNN has better identification and prediction performance than the multi-step DNN. The research provides possible corroboration for developing new deep learning based algorithms for nonlinear system identification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/766d4bcb8f8fbbbb47f89e5276f5ec24c8c0d460" target='_blank'>Data driven nonlinear dynamical systems identification using multi-step CLDNN</a></td>
          <td>
            Qi Teng, L. Zhang
          </td>
          <td>2019-08-19</td>
          <td>AIP Advances</td>
          <td>20</td>
          <td>37</td>
        </tr>
    
        <tr id="Discovering mathematical models that characterize the observed behavior of dynamical systems remains a major challenge, especially for systems in a chaotic regime. The challenge is even greater when the physics underlying such systems is not yet understood, and scientific inquiry must solely rely on empirical data. Driven by the need to fill this gap, we develop a framework that learns mathematical expressions modeling complex dynamical behaviors by identifying differential equations from noisy and sparse observable data. We train a small neural network to learn the dynamics of a system, its rate of change in time, and missing model terms, which are used as input for a symbolic regression algorithm to autonomously distill the explicit mathematical terms. This, in turn, enables us to predict the future evolution of the dynamical behavior. The performance of this framework is validated by recovering the right-hand sides and unknown terms of certain complex, chaotic systems such as the well-known Lorenz system, a six-dimensional hyperchaotic system, and the non-autonomous Sprott chaotic system, and comparing them with their known analytical expressions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f5cccd316e70bd2366e67260e1f691761bd512d3" target='_blank'>AI-Lorenz: A physics-data-driven framework for black-box and gray-box identification of chaotic systems with symbolic regression</a></td>
          <td>
            Mario De Florio, I. Kevrekidis, G. Karniadakis
          </td>
          <td>2023-12-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>127</td>
        </tr>
    
        <tr id="Many complex phenomena, from weather systems to heartbeat rhythm patterns, are effectively modeled as low-dimensional dynamical systems. Such systems may behave chaotically under certain conditions, and so the ability to detect chaos based on empirical measurement is an important step in characterizing and predicting these processes. Classifying a system as chaotic usually requires estimating its largest Lyapunov exponent, which quantifies the average rate of convergence or divergence of initially close trajectories in state space, and for which a positive value is generally accepted as an operational definition of chaos. Estimating the largest Lyapunov exponent from observations of a process is especially challenging in systems affected by dynamical noise, which is the case for many models of real-world processes, in particular models of biological systems. We describe a novel method for estimating the largest Lyapunov exponent from data, based on training Deep Learning models on synthetically generated trajectories, and demonstrate that this method yields accurate and noise-robust predictions given relatively short inputs and across a range of different dynamical systems. Our method is unique in that it can analyze tree-shaped data, a ubiquitous topology in biological settings, and specifically in dynamics over lineages of cells or organisms. We also characterize the types of input information extracted by our models for their predictions, allowing for a deeper understanding into the different ways by which chaos can be analyzed in different topologies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fb357e57624525f27224822502061cc8fb831e1" target='_blank'>Detecting chaos in lineage-trees: A deep learning approach</a></td>
          <td>
            H. Rappeport, Irit Levin Reisman, Naftali Tishby, N. Balaban
          </td>
          <td>2021-06-08</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>56</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b44db6efdb3999b65be446114afb04720e572cc" target='_blank'>Modeling of dynamical systems through deep learning</a></td>
          <td>
            P. Rajendra, V. Brahmajirao
          </td>
          <td>2020-11-22</td>
          <td>Biophysical Reviews</td>
          <td>31</td>
          <td>4</td>
        </tr>
    
        <tr id="The success of the current wave of artificial intelligence can be partly attributed to deep neural networks, which have proven to be very effective in learning complex patterns from large datasets with minimal human intervention. However, it is difficult to train these models on complex dynamical systems from data alone due to their low data efficiency and sensitivity to hyperparameters and initialisation. This work demonstrates that injection of partially known information at an intermediate layer in a DNN can improve model accuracy, reduce model uncertainty, and yield improved convergence during the training. The value of these physics-guided neural networks has been demonstrated by learning the dynamics of a wide variety of nonlinear dynamical systems represented by five well-known equations in nonlinear systems theory: the Lotka-Volterra, Duffing, Van der Pol, Lorenz, and Henon-Heiles systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/518a4b3c36c8a0e651caa603ea10d0875a1fbc27" target='_blank'>Physics guided neural networks for modelling of non-linear dynamics</a></td>
          <td>
            Haakon Robinson, Suraj Pawar, A. Rasheed, O. San
          </td>
          <td>2022-05-13</td>
          <td>Neural networks : the official journal of the International Neural Network Society</td>
          <td>27</td>
          <td>34</td>
        </tr>
    
        <tr id="Complex systems often show macroscopic coherent behavior due to the interactions of microscopic agents like molecules, cells, or individuals in a population with their environment. However, simulating such systems poses several computational challenges during simulation as the underlying dynamics vary and span wide spatiotemporal scales of interest. To capture the fast-evolving features, finer time steps are required while ensuring that the simulation time is long enough to capture the slow-scale behavior, making the analyses computationally unmanageable. This paper showcases how deep learning techniques can be used to develop a precise time-stepping approach for multiscale systems using the joint discovery of coordinates and flow maps. While the former allows us to represent the multiscale dynamics on a representative basis, the latter enables the iterative time-stepping estimation of the reduced variables. The resulting framework achieves state-of-the-art predictive accuracy while incurring lesser computational costs. We demonstrate this ability of the proposed scheme on the large-scale Fitzhugh Nagumo neuron model and the 1D Kuramoto-Sivashinsky equation in the chaotic regime.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22e6d5918a51fff97d4344a3b5b353d0a1439708" target='_blank'>Enhancing Computational Efficiency in Multiscale Systems Using Deep Learning of Coordinates and Flow Maps</a></td>
          <td>
            Asif Hamid, Danish Rafiq, S. A. Nahvi, M. A. Bazaz
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>
    
        <tr id="In this work, we blend machine learning and dictionary-based learning with numerical analysis tools to discover differential equations from noisy and sparsely sampled measurement data of time-dependent processes. We use the fact that given a dictionary containing large candidate nonlinear functions, dynamical models can often be described by a few appropriately chosen basis functions. As a result, we obtain parsimonious models that can be better interpreted by practitioners, and potentially generalize better beyond the sampling regime than black-box modelling. In this work, we integrate a numerical integration framework with dictionary learning that yields differential equations without requiring or approximating derivative information at any stage. Hence, it is utterly effective for corrupted and sparsely sampled data. We discuss its extension to governing equations, containing rational nonlinearities that typically appear in biological networks. Moreover, we generalized the method to governing equations subject to parameter variations and externally controlled inputs. We demonstrate the efficiency of the method to discover a number of diverse differential equations using noisy measurements, including a model describing neural dynamics, chaotic Lorenz model, Michaelis–Menten kinetics and a parameterized Hopf normal form.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5430c642bf5e56ce52dbfa94a9b3f4035b3222d1" target='_blank'>Discovery of nonlinear dynamical systems using a Runge–Kutta inspired dictionary-based sparse regression approach</a></td>
          <td>
            P. Goyal, P. Benner
          </td>
          <td>2021-05-11</td>
          <td>Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>37</td>
          <td>53</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>