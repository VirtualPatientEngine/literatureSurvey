<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Time-series%20forecasting/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Symbolic regression
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Time-series%20forecasting/" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Time-series%20forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Symbolic regression</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-08-05 09:00:19 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Symbolic regression</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Symbolic regression</a><br>
      <a href="#recommended_articles">3. Recommended articles on Symbolic regression</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Symbolic regression</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Symbolic regression</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5d150cec2775f9bc863760448f14104cc8f42368" target='_blank'>
                Discovering governing equations from data by sparse identification of nonlinear dynamical systems
                </a>
              </td>
          <td>
            S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2015-09-11</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America, Proceedings of the National Academy of Sciences</td>
          <td>3159</td>
          <td>63</td>

            <td><a href='../recommendations/5d150cec2775f9bc863760448f14104cc8f42368' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/60d0d998fa038182b3b69a57adb9b2f82d40589c" target='_blank'>
                Robust learning from noisy, incomplete, high-dimensional experimental data via physically constrained symbolic regression
                </a>
              </td>
          <td>
            Patrick A. K. Reinbold, Logan Kageorge, M. Schatz, R. Grigoriev
          </td>
          <td>2021-02-24</td>
          <td>Nature Communications</td>
          <td>84</td>
          <td>23</td>

            <td><a href='../recommendations/60d0d998fa038182b3b69a57adb9b2f82d40589c' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Significance Governing equations are essential to the study of physical systems, providing models that can generalize to predict previously unseen behaviors. There are many systems of interest across disciplines where large quantities of data have been collected, but the underlying governing equations remain unknown. This work introduces an approach to discover governing models from data. The proposed method addresses a key limitation of prior approaches by simultaneously discovering coordinates that admit a parsimonious dynamical model. Developing parsimonious and interpretable governing models has the potential to transform our understanding of complex systems, including in neuroscience, biology, and climate science. The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3c9961153493370500020c81527b3548c96f81e0" target='_blank'>
                Data-driven discovery of coordinates and governing equations
                </a>
              </td>
          <td>
            Kathleen P. Champion, Bethany Lusch, J. Kutz, S. Brunton
          </td>
          <td>2019-03-29</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>595</td>
          <td>63</td>

            <td><a href='../recommendations/3c9961153493370500020c81527b3548c96f81e0' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3df50e9b73cc2937dfd651f4c3344bc99b7ed3f2" target='_blank'>
                Chaos as an intermittently forced linear system
                </a>
              </td>
          <td>
            S. Brunton, Bingni W. Brunton, J. Proctor, E. Kaiser, J. Kutz
          </td>
          <td>2016-08-18</td>
          <td>Nature Communications</td>
          <td>446</td>
          <td>63</td>

            <td><a href='../recommendations/3df50e9b73cc2937dfd651f4c3344bc99b7ed3f2' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Data-driven discovery of dynamics via machine learning is pushing the frontiers of modelling and control efforts, providing a tremendous opportunity to extend the reach of model predictive control (MPC). However, many leading methods in machine learning, such as neural networks (NN), require large volumes of training data, may not be interpretable, do not easily include known constraints and symmetries, and may not generalize beyond the attractor where models are trained. These factors limit their use for the online identification of a model in the low-data limit, for example following an abrupt change to the system dynamics. In this work, we extend the recent sparse identification of nonlinear dynamics (SINDY) modelling procedure to include the effects of actuation and demonstrate the ability of these models to enhance the performance of MPC, based on limited, noisy data. SINDY models are parsimonious, identifying the fewest terms in the model needed to explain the data, making them interpretable and generalizable. We show that the resulting SINDY-MPC framework has higher performance, requires significantly less data, and is more computationally efficient and robust to noise than NN models, making it viable for online training and execution in response to rapid system changes. SINDY-MPC also shows improved performance over linear data-driven models, although linear models may provide a stopgap until enough data is available for SINDY. SINDY-MPC is demonstrated on a variety of dynamical systems with different challenges, including the chaotic Lorenz system, a simple model for flight control of an F8 aircraft, and an HIV model incorporating drug treatment.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b2eb064f432557c59ce99834d7dc7817e4687271" target='_blank'>
                Sparse identification of nonlinear dynamics for model predictive control in the low-data limit
                </a>
              </td>
          <td>
            E. Kaiser, J. Kutz, S. Brunton
          </td>
          <td>2017-11-15</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>425</td>
          <td>63</td>

            <td><a href='../recommendations/b2eb064f432557c59ce99834d7dc7817e4687271' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Inferring the structure and dynamics of network models is critical to understanding the functionality and control of complex systems, such as metabolic and regulatory biological networks. The increasing quality and quantity of experimental data enable statistical approaches based on information theory for model selection and goodness-of-fit metrics. We propose an alternative data-driven method to infer networked nonlinear dynamical systems by using sparsity-promoting optimization to select a subset of nonlinear interactions representing dynamics on a network. In contrast to standard model selection methods-based upon information content for a finite number of heuristic models (order 10 or less), our model selection procedure discovers a parsimonious model from a combinatorially large set of models, without an exhaustive search. Our particular innovation is appropriate for many biological networks, where the governing dynamical systems have rational function nonlinearities with cross terms, thus requiring an implicit formulation and the equations to be identified in the null-space of a library of mixed nonlinearities, including the state and derivative terms. This method, implicit-SINDy, succeeds in inferring three canonical biological models: 1) Michaelis-Menten enzyme kinetics; 2) the regulatory network for competence in bacteria; and 3) the metabolic network for yeast glycolysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06a0ba437d41a7c82c08a9636a4438c1b5031378" target='_blank'>
                Inferring Biological Networks by Sparse Identification of Nonlinear Dynamics
                </a>
              </td>
          <td>
            N. Mangan, S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2016-05-26</td>
          <td>IEEE Transactions on Molecular Biological and Multi-Scale Communications, IEEE Transactions on Molecular, Biological and Multi-Scale Communications</td>
          <td>314</td>
          <td>63</td>

            <td><a href='../recommendations/06a0ba437d41a7c82c08a9636a4438c1b5031378' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Accurately modelling the nonlinear dynamics of a system from measurement data is a challenging yet vital topic. The sparse identification of nonlinear dynamics (SINDy) algorithm is one approach to discover dynamical systems models from data. Although extensions have been developed to identify implicit dynamics, or dynamics described by rational functions, these extensions are extremely sensitive to noise. In this work, we develop SINDy-PI (parallel, implicit), a robust variant of the SINDy algorithm to identify implicit dynamics and rational nonlinearities. The SINDy-PI framework includes multiple optimization algorithms and a principled approach to model selection. We demonstrate the ability of this algorithm to learn implicit ordinary and partial differential equations and conservation laws from limited and noisy data. In particular, we show that the proposed approach is several orders of magnitude more noise robust than previous approaches, and may be used to identify a class of ODE and PDE dynamics that were previously unattainable with SINDy, including for the double pendulum dynamics and simplified model for the Belousov–Zhabotinsky (BZ) reaction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4971f9abd024e40fbbdff2e9492745b68a6bca01" target='_blank'>
                SINDy-PI: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics
                </a>
              </td>
          <td>
            Kadierdan Kaheman, J. Kutz, S. Brunton
          </td>
          <td>2020-04-05</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>190</td>
          <td>63</td>

            <td><a href='../recommendations/4971f9abd024e40fbbdff2e9492745b68a6bca01' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2b2aa13d4959073f61ad70555bc8c7da7d116196" target='_blank'>
                Multidimensional Approximation of Nonlinear Dynamical Systems
                </a>
              </td>
          <td>
            Patrick Gelß, Stefan Klus, J. Eisert, Christof Schutte
          </td>
          <td>2018-09-07</td>
          <td>Journal of Computational and Nonlinear Dynamics</td>
          <td>61</td>
          <td>76</td>

            <td><a href='../recommendations/2b2aa13d4959073f61ad70555bc8c7da7d116196' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="First principles modeling of physical systems has led to significant technological advances across all branches of science. For nonlinear systems, however, small modeling errors can lead to significant deviations from the true, measured behavior. Even in mechanical systems, where the equations are assumed to be well-known, there are often model discrepancies corresponding to nonlinear friction, wind resistance, etc. Discovering models for these discrepancies remains an open challenge for many complex systems. In this work, we use the sparse identification of nonlinear dynamics (SINDy) algorithm to discover a model for the discrepancy between a simplified model and measurement data. In particular, we assume that the model mismatch can be sparsely represented in a library of candidate model terms. We demonstrate the efficacy of our approach on several examples including experimental data from a double pendulum on a cart. We further design and implement a feed-forward controller in simulations, showing improvement with a discrepancy model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73dd9c49f205280991826b2ea4b50344203916b4" target='_blank'>
                Learning Discrepancy Models From Experimental Data
                </a>
              </td>
          <td>
            Kadierdan Kaheman, E. Kaiser, B. Strom, J. Kutz, S. Brunton
          </td>
          <td>2019-09-18</td>
          <td>ArXiv, arXiv.org</td>
          <td>31</td>
          <td>63</td>

            <td><a href='../recommendations/73dd9c49f205280991826b2ea4b50344203916b4' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of nuanced issues that must be addressed by modern data-driven methods for automated physics discovery. Specifically, we show that measurement noise and complex secondary physical mechanisms, like unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. We use the sparse identification of non-linear dynamics (SINDy) method to identify governing equations for real-world measurement data and simulated trajectories. Incorporating into SINDy the assumption that each falling object is governed by a similar physical law is shown to improve the robustness of the learned models, but discrepancies between the predictions and observations persist due to subtleties in drag dynamics. This work highlights the fact that the naive application of ML/AI will generally be insufficient to infer universal physical laws without further modification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/35e2571c17246577e0bc1b9de57a314c3b60e220" target='_blank'>
                Discovery of Physics From Data: Universal Laws and Discrepancies
                </a>
              </td>
          <td>
            Brian M. de Silva, D. Higdon, S. Brunton, J. Kutz
          </td>
          <td>2019-06-19</td>
          <td>Frontiers in Artificial Intelligence</td>
          <td>66</td>
          <td>63</td>

            <td><a href='../recommendations/35e2571c17246577e0bc1b9de57a314c3b60e220' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Researchers propose sparse regression for identifying governing partial differential equations for spatiotemporal systems. We propose a sparse regression method capable of discovering the governing partial differential equation(s) of a given system by time series measurements in the spatial domain. The regression framework relies on sparsity-promoting techniques to select the nonlinear and partial derivative terms of the governing equations that most accurately represent the data, bypassing a combinatorially large search through all possible candidate models. The method balances model complexity and regression accuracy by selecting a parsimonious model via Pareto analysis. Time series measurements can be made in an Eulerian framework, where the sensors are fixed spatially, or in a Lagrangian framework, where the sensors move with the dynamics. The method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems spanning a number of scientific domains including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation. Moreover, the method is capable of disambiguating between potentially nonunique dynamical terms by using multiple time series taken with different initial data. Thus, for a traveling wave, the method can distinguish between a linear wave equation and the Korteweg–de Vries equation, for instance. The method provides a promising new technique for discovering governing equations and physical laws in parameterized spatiotemporal systems, where first-principles derivations are intractable.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0acd117521ef5aafb09fed02ab415523b330b058" target='_blank'>
                Data-driven discovery of partial differential equations
                </a>
              </td>
          <td>
            S. Rudy, S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2016-09-21</td>
          <td>Science Advances</td>
          <td>1164</td>
          <td>63</td>

            <td><a href='../recommendations/0acd117521ef5aafb09fed02ab415523b330b058' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Sparse model identification enables the discovery of nonlinear dynamical systems purely from data; however, this approach is sensitive to noise, especially in the low-data limit. In this work, we leverage the statistical approach of bootstrap aggregating (bagging) to robustify the sparse identification of the nonlinear dynamics (SINDy) algorithm. First, an ensemble of SINDy models is identified from subsets of limited and noisy data. The aggregate model statistics are then used to produce inclusion probabilities of the candidate functions, which enables uncertainty quantification and probabilistic forecasts. We apply this ensemble-SINDy (E-SINDy) algorithm to several synthetic and real-world datasets and demonstrate substantial improvements to the accuracy and robustness of model discovery from extremely noisy and limited data. For example, E-SINDy uncovers partial differential equations models from data with more than twice as much measurement noise as has been previously reported. Similarly, E-SINDy learns the Lotka Volterra dynamics from remarkably limited data of yearly lynx and hare pelts collected from 1900 to 1920. E-SINDy is computationally efficient, with similar scaling as standard SINDy. Finally, we show that ensemble statistics from E-SINDy can be exploited for active learning and improved model predictive control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/883547fdbd88552328a6615ec620f96e39c57018" target='_blank'>
                Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control
                </a>
              </td>
          <td>
            Urban Fasel, J. Kutz, Bingni W. Brunton, S. Brunton
          </td>
          <td>2021-11-22</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>154</td>
          <td>63</td>

            <td><a href='../recommendations/883547fdbd88552328a6615ec620f96e39c57018' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6f0a85009481dcfd93aaa43ed3f980e5033b0d8" target='_blank'>
                Learning sparse nonlinear dynamics via mixed-integer optimization
                </a>
              </td>
          <td>
            D. Bertsimas, Wes Gurnee
          </td>
          <td>2022-06-01</td>
          <td>Nonlinear Dynamics</td>
          <td>27</td>
          <td>90</td>

            <td><a href='../recommendations/e6f0a85009481dcfd93aaa43ed3f980e5033b0d8' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Regularized regression problems are ubiquitous in statistical modeling, signal processing, and machine learning. Sparse regression, in particular, has been instrumental in scientific model discovery, including compressed sensing applications, variable selection, and high-dimensional analysis. We propose a broad framework for sparse relaxed regularized regression, called SR3. The key idea is to solve a relaxation of the regularized problem, which has three advantages over the state-of-the-art: 1) solutions of the relaxed problem are superior with respect to errors, false positives, and conditioning; 2) relaxation allows extremely fast algorithms for both convex and nonconvex formulations; and 3) the methods apply to composite regularizers, essential for total variation (TV) as well as sparsity-promoting formulations using tight frames. We demonstrate the advantages of SR3 (computational efficiency, higher accuracy, faster convergence rates, and greater flexibility) across a range of regularized regression problems with synthetic and real data, including applications in compressed sensing, LASSO, matrix completion, TV regularization, and group sparsity. Following standards of reproducible research, we also provide a companion MATLAB package that implements these examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c0fc3882a9976f6a9cdc3a724bce184b786503da" target='_blank'>
                A Unified Framework for Sparse Relaxed Regularized Regression: SR3
                </a>
              </td>
          <td>
            P. Zheng, T. Askham, S. Brunton, J. Kutz, A. Aravkin
          </td>
          <td>2018-07-14</td>
          <td>IEEE Access</td>
          <td>114</td>
          <td>63</td>

            <td><a href='../recommendations/c0fc3882a9976f6a9cdc3a724bce184b786503da' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Symbolic regression'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Symbolic regression</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="Over the past few years, equation discovery has gained popularity in different fields of science and engineering. However, existing equation discovery algorithms rely on the availability of noisy measurements of the state variables (i.e., displacement {and velocity}). This is a major bottleneck in structural dynamics, where we often only have access to acceleration measurements. To that end, this paper introduces a novel equation discovery algorithm for discovering governing equations of dynamical systems from acceleration-only measurements. The proposed algorithm employs a library-based approach for equation discovery. To enable equation discovery from acceleration-only measurements, we propose a novel Approximate Bayesian Computation (ABC) model that prioritizes parsimonious models. The efficacy of the proposed algorithm is illustrated using {four} structural dynamics examples that include both linear and nonlinear dynamical systems. The case studies presented illustrate the possible application of the proposed approach for equation discovery of dynamical systems from acceleration-only measurements.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f046e6be05e115bc55624ef726d92d6d06303346" target='_blank'>
              Discovering governing equation in structural dynamics from acceleration-only measurements
              </a>
            </td>
          <td>
            Calvin Alvares, Souvik Chakraborty
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper proposes a sparse regression strategy for discovery of ordinary and partial differential equations from incomplete and noisy data. Inference is performed over both equation parameters and state variables using a statistically motivated likelihood function. Sparsity is enforced by a selection algorithm which iteratively removes terms and compares models using statistical information criteria. Large scale optimization is performed using a second-order variant of the Levenberg-Marquardt method, where the gradient and Hessian are computed via automatic differentiation. Illustrations involving canonical systems of ordinary and partial differential equations are used to demonstrate the flexibility and robustness of the approach. Accurate reconstruction of systems is found to be possible even in extreme cases of limited data and large observation noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3d4419932eaa736ba7230568e3503746b37957c5" target='_blank'>
              Discovery of differential equations using sparse state and parameter regression
              </a>
            </td>
          <td>
            Teddy Meissner, Karl Glasner
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Enhancing the sparsity of data-driven reduced-order models (ROMs) has gained increasing attention in recent years. In this work, we analyze an efficient approach to identifying skillful ROMs with a sparse structure using an information-theoretic indicator called causation entropy. The causation entropy quantifies in a statistical way the additional contribution of each term to the underlying dynamics beyond the information already captured by all the other terms in the ansatz. By doing so, the causation entropy assesses the importance of each term to the dynamics before a parameter estimation procedure is performed. Thus, the approach can be utilized to eliminate terms with little dynamic impact, leading to a parsimonious structure that retains the essential physics. To circumvent the difficulty of estimating high-dimensional probability density functions (PDFs) involved in the causation entropy computation, we leverage Gaussian approximations for such PDFs, which are demonstrated to be sufficient even in the presence of highly non-Gaussian dynamics. The effectiveness of the approach is illustrated by the Kuramoto-Sivashinsky equation by building sparse causation-based ROMs for various purposes, such as recovering long-term statistics and inferring unobserved dynamics via data assimilation with partial observations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/51343d94516402e35ba4c9c52a62e0d4dd4986ba" target='_blank'>
              Minimum Reduced-Order Models via Causal Inference
              </a>
            </td>
          <td>
            Nan Chen, Honghu Liu
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper focuses on the application of experimental data-based system identification of unknown systems utilising sparse identification of nonlinear dynamics (SINDy). SINDy is used to detect the system dynamics in the three well-known nonlinear systems. Analyzed are SINDy’s abilities to accurately represent transient/steady-state behaviour, noise effect, and model structure. A sparse set of basis functions can effectively capture the dynamics of a system, according to the data-driven approach known as SINDy. The coefficients of these basis functions are determined via methods of sparse regression, and the final model is made up of a number of sparse ordinary differential equations. The findings demonstrate that SINDy, with sufficient time-series data, can capture both transient and steady-state phenomena. According to the analysis of the noise effect, SINDy’s performance declines as the system’s noise level rises. The feature library must contain the appropriate model structure in order for SINDy to function effectively. SINDy has the potential to extract unknown system dynamics from experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/315b7c145c5a86b92e656ad174cede9f67bf3f34" target='_blank'>
              Data-driven system identification of unknown systems utilising sparse identification of nonlinear dynamics (SINDy)
              </a>
            </td>
          <td>
            P. Pandey, H. Haddad Khodaparast, M. Friswell, T. Chatterjee, N. Jamia, T. Deighan
          </td>
          <td>2024-06-01</td>
          <td>Journal of Physics: Conference Series</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Sparse Identification of Nonlinear Dynamics (SINDy) framework is a robust method for identifying governing equations, successfully applied to ordinary, partial, and stochastic differential equations. In this work we extend SINDy to identify delay differential equations by using an augmented library that includes delayed samples and Bayesian optimization. To identify a possibly unknown delay we minimize the reconstruction error over a set of candidates. The resulting methodology improves the overall performance by remarkably reducing the number of calls to SINDy with respect to a brute force approach. We also address a multivariate setting to identify multiple unknown delays and (non-multiplicative) parameters. Several numerical tests on delay differential equations with different long-term behavior, number of variables, delays, and parameters support the use of Bayesian optimization highlighting both the efficacy of the proposed methodology and its computational advantages. As a consequence, the class of discoverable models is significantly expanded.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/827638609aa7a84d10096156bc7ead43d8060c5a" target='_blank'>
              Data-driven Discovery of Delay Differential Equations with Discrete Delays
              </a>
            </td>
          <td>
            Alessandro Pecile, N. Demo, M. Tezzele, G. Rozza, Dimitri Breda
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are first-principled, explainable, and sample-efficient. However, they often rely on strong modeling assumptions and expensive numerical integration, requiring significant computational resources and domain expertise. While deep learning (DL) provides efficient alternatives for modeling complex dynamics, they require a large amount of labeled training data. Furthermore, its predictions may disobey the governing physical laws and are difficult to interpret. Physics-guided DL aims to integrate first-principled physical knowledge into data-driven methods. It has the best of both worlds and is well equipped to better solve scientific problems. Recently, this field has gained great progress and has drawn considerable interest across discipline Here, we introduce the framework of physics-guided DL with a special emphasis on learning dynamical systems. We describe the learning pipeline and categorize state-of-the-art methods under this framework. We also offer our perspectives on the open challenges and emerging opportunities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/60d721e89c2f9c549241a4982b77f9c752b34460" target='_blank'>
              Learning dynamical systems from data: An introduction to physics-guided deep learning
              </a>
            </td>
          <td>
            Rose Yu, Rui Wang
          </td>
          <td>2024-06-24</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="We present a computational technique for modeling the evolution of dynamical systems in a reduced basis, with a focus on the challenging problem of modeling partially-observed partial differential equations (PDEs) on high-dimensional non-uniform grids. We address limitations of previous work on data-driven flow map learning in the sense that we focus on noisy and limited data to move toward data collection scenarios in real-world applications. Leveraging recent work on modeling PDEs in modal and nodal spaces, we present a neural network structure that is suitable for PDE modeling with noisy and limited data available only on a subset of the state variables or computational domain. In particular, spatial grid-point measurements are reduced using a learned linear transformation, after which the dynamics are learned in this reduced basis before being transformed back out to the nodal space. This approach yields a drastically reduced parameterization of the neural network compared with previous flow map models for nodal space learning. This primarily allows for smaller training data sets, but also enables reduced training times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33821b7c0acd5258ca0c60a09ffdc55439ac7ac2" target='_blank'>
              Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data
              </a>
            </td>
          <td>
            Victor Churchill
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The quasipotential function allows for comprehension and prediction of the escape mechanisms from metastable states in nonlinear dynamical systems. This function acts as a natural extension of the potential function for non-gradient systems and it unveils important properties such as the maximum likelihood transition paths, transition rates and expected exit times of the system. Here, we leverage on machine learning via the combination of two data-driven techniques, namely a neural network and a sparse regression algorithm, to obtain symbolic expressions of quasipotential functions. The key idea is first to determine an orthogonal decomposition of the vector field that governs the underlying dynamics using neural networks, then to interpret symbolically the downhill and circulatory components of the decomposition. These functions are regressed simultaneously with the addition of mathematical constraints. We show that our approach discovers a parsimonious quasipotential equation for an archetypal model with a known exact quasipotential and for the dynamics of a nanomechanical resonator. The analytical forms deliver direct access to the stability of the metastable states and predict rare events with significant computational advantages. Our data-driven approach is of interest for a wide range of applications in which to assess the fluctuating dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/174f3a662f5a30364246d8cf0f34af33eac8ccfd" target='_blank'>
              Sparse identification of quasipotentials via a combined data-driven method
              </a>
            </td>
          <td>
            Bo Lin, P. Belardinelli
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Kolmogorov-Arnold networks (KANs) as an alternative to multi-layer perceptrons (MLPs) are a recent development demonstrating strong potential for data-driven modeling. This work applies KANs as the backbone of a neural ordinary differential equation (ODE) framework, generalizing their use to the time-dependent and temporal grid-sensitive cases often seen in dynamical systems and scientific machine learning applications. The proposed KAN-ODEs retain the flexible dynamical system modeling framework of Neural ODEs while leveraging the many benefits of KANs compared to MLPs, including higher accuracy and faster neural scaling, stronger interpretability and generalizability, and lower parameter counts. First, we quantitatively demonstrated these improvements in a comprehensive study of the classical Lotka-Volterra predator-prey model. We then showcased the KAN-ODE framework's ability to learn symbolic source terms and complete solution profiles in higher-complexity and data-lean scenarios including wave propagation and shock formation, the complex Schr\"odinger equation, and the Allen-Cahn phase separation equation. The successful training of KAN-ODEs, and their improved performance compared to traditional Neural ODEs, implies significant potential in leveraging this novel network architecture in myriad scientific machine learning applications for discovering hidden physics and predicting dynamic evolution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/efeeddb4162e3eefbe1d974d6ce7cd1d32498a6b" target='_blank'>
              KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for Learning Dynamical Systems and Hidden Physics
              </a>
            </td>
          <td>
            Benjamin C. Koenig, Suyong Kim, Sili Deng
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>22</td>
        </tr>

        <tr id="The parametric greedy latent space dynamics identification (gLaSDI) framework has demonstrated promising potential for accurate and efficient modeling of high-dimensional nonlinear physical systems. However, it remains challenging to handle noisy data. To enhance robustness against noise, we incorporate the weak-form estimation of nonlinear dynamics (WENDy) into gLaSDI. In the proposed weak-form gLaSDI (WgLaSDI) framework, an autoencoder and WENDy are trained simultaneously to discover intrinsic nonlinear latent-space dynamics of high-dimensional data. Compared to the standard sparse identification of nonlinear dynamics (SINDy) employed in gLaSDI, WENDy enables variance reduction and robust latent space discovery, therefore leading to more accurate and efficient reduced-order modeling. Furthermore, the greedy physics-informed active learning in WgLaSDI enables adaptive sampling of optimal training data on the fly for enhanced modeling accuracy. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including viscous and inviscid Burgers' equations, time-dependent radial advection, and the Vlasov equation for plasma physics. With data that contains 5-10% Gaussian white noise, WgLaSDI outperforms gLaSDI by orders of magnitude, achieving 1-7% relative errors. Compared with the high-fidelity models, WgLaSDI achieves 121 to 1,779x speed-up.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15fe2e82d4161103b69fe83f184552b9e9774633" target='_blank'>
              Physics-informed active learning with simultaneous weak-form latent space dynamics identification
              </a>
            </td>
          <td>
            Xiaolong He, April Tran, David M. Bortz, Youngsoo Choi
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Dynamic Mode Decomposition (DMD) and its variants, such as extended DMD (EDMD), are broadly used to fit simple linear models to dynamical systems known from observable data. As DMD methods work well in several situations but perform poorly in others, a clarification of the assumptions under which DMD is applicable is desirable. Upon closer inspection, existing interpretations of DMD methods based on the Koopman operator are not quite satisfactory: they justify DMD under assumptions that hold only with probability zero for generic observables. Here, we give a justification for DMD as a local, leading-order reduced model for the dominant system dynamics under conditions that hold with probability one for generic observables and non-degenerate observational data. We achieve this for autonomous and for periodically forced systems of finite or infinite dimensions by constructing linearizing transformations for their dominant dynamics within attracting slow spectral submanifolds (SSMs). Our arguments also lead to a new algorithm, data-driven linearization (DDL), which is a higher-order, systematic linearization of the observable dynamics within slow SSMs. We show by examples how DDL outperforms DMD and EDMD on numerical and experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ce624ece7e2f2933c1731bee6e5b03bf7a57c90b" target='_blank'>
              Data-Driven Linearization of Dynamical Systems
              </a>
            </td>
          <td>
            George Haller, B. Kasz'as
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents a sequence of two approaches for the data-driven control-oriented modeling of networked systems, i.e., the systems that involve many interacting dynamical components. First, a novel deep learning approach named the weak Latent Dynamics Model (wLDM) is developed for learning generic nonlinear dynamics with control. Leveraging the weak form, the wLDM enables more numerically stable and computationally efficient training as well as more accurate prediction, when compared to conventional methods such as neural ordinary differential equations. Building upon the wLDM framework, we propose the weak Graph Koopman Bilinear Form (wGKBF) model, which integrates geometric deep learning and Koopman theory to learn latent space dynamics for networked systems, especially for the challenging cases having multiple timescales. The effectiveness of the wLDM framework and wGKBF model are demonstrated on three example systems of increasing complexity - a controlled double pendulum, the stiff Brusselator dynamics, and an electrified aircraft energy system. These numerical examples show that the wLDM and wGKBF achieve superior predictive accuracy and training efficiency as compared to baseline models. Parametric studies provide insights into the effects of hyperparameters in the weak form. The proposed framework shows the capability to efficiently capture control-dependent dynamics in these systems, including stiff dynamics and multi-physics interactions, offering a promising direction for learning control-oriented models of complex networked systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2168dc84b931e82498692c0f9277bed180e91df" target='_blank'>
              Learning Networked Dynamical System Models with Weak Form and Graph Neural Networks
              </a>
            </td>
          <td>
            Yin Yu, Daning Huang, Seho Park, H. Pangborn
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We introduce a method for constructing reduced-order models directly from videos of dynamical systems. The method uses a non-intrusive tracking to isolate the motion of a user-selected part in the video of an autonomous dynamical system. In the space of delayed observations of this motion, we reconstruct a low-dimensional attracting spectral submanifold (SSM) whose internal dynamics serves as a mathematically justified reduced-order model for nearby motions of the full system. We obtain this model in a simple polynomial form that allows explicit identification of important physical system parameters, such as natural frequencies, linear and nonlinear damping and nonlinear stiffness. Beyond faithfully reproducing attracting steady states and limit cycles, our SSM-reduced models can also uncover hidden motion not seen in the video, such as unstable fixed points and unstable limit cycles forming basin boundaries. We demonstrate all these features on experimental videos of five physical systems: a double pendulum, an inverted flag in counter-flow, water sloshing in tank, a wing exhibiting aeroelastic flutter and a shimmying wheel.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c0d9c3d71f8076148909574fb1589ce6d2d09d4" target='_blank'>
              Modeling Nonlinear Dynamics from Videos
              </a>
            </td>
          <td>
            Antony Yang, Joar Axaas, Fanni K'ad'ar, G'abor St'ep'an, George Haller
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="It has been found recently that more data can, counter-intuitively, hurt the performance of deep neural networks. Here, we show that a more extreme version of the phenomenon occurs in data-driven models of dynamical systems. To elucidate the underlying mechanism, we focus on next-generation reservoir computing (NGRC) -- a popular framework for learning dynamics from data. We find that, despite learning a better representation of the flow map with more training data, NGRC can adopt an ill-conditioned ``integrator'' and lose stability. We link this data-induced instability to the auxiliary dimensions created by the delayed states in NGRC. Based on these findings, we propose simple strategies to mitigate the instability, either by increasing regularization strength in tandem with data size, or by carefully introducing noise during training. Our results highlight the importance of proper regularization in data-driven modeling of dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b882f72ec6688e53edbf612e32d2b97c2a4c8236" target='_blank'>
              How more data can hurt: Instability and regularization in next-generation reservoir computing
              </a>
            </td>
          <td>
            Yuanzhao Zhang, Sean P. Cornelius
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cabb9ef9934cc450e8873d693c8dd8dfc73087ab" target='_blank'>
              MBD-NODE: physics-informed data-driven modeling and simulation of constrained multibody systems
              </a>
            </td>
          <td>
            Jingquan Wang, Shu Wang, H. Unjhawala, Jinlong Wu, D. Negrut
          </td>
          <td>2024-07-11</td>
          <td>Multibody System Dynamics</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="In a landscape where scientific discovery is increasingly driven by data, the integration of machine learning (ML) with traditional scientific methodologies has emerged as a transformative approach. This paper introduces a novel, data-driven framework that synergizes physics-based priors with advanced ML techniques to address the computational and practical limitations inherent in first-principle-based methods and brute-force machine learning methods. Our framework showcases four algorithms, each embedding a specific physics-based prior tailored to a particular class of nonlinear systems, including separable and nonseparable Hamiltonian systems, hyperbolic partial differential equations, and incompressible fluid dynamics. The intrinsic incorporation of physical laws preserves the system's intrinsic symmetries and conservation laws, ensuring solutions are physically plausible and computationally efficient. The integration of these priors also enhances the expressive power of neural networks, enabling them to capture complex patterns typical in physical phenomena that conventional methods often miss. As a result, our models outperform existing data-driven techniques in terms of prediction accuracy, robustness, and predictive capability, particularly in recognizing features absent from the training set, despite relying on small datasets, short training periods, and small sample sizes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6ba424643899decf4b48792c380cc0c66abe63c7" target='_blank'>
              Data-Driven Computing Methods for Nonlinear Physics Systems with Geometric Constraints
              </a>
            </td>
          <td>
            Yunjin Tong
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Machine learning techniques have recently been of great interest for solving differential equations. Training these models is classically a data-fitting task, but knowledge of the expression of the differential equation can be used to supplement the training objective, leading to the development of physics-informed scientific machine learning. In this article, we focus on one class of models called nonlinear vector autoregression (NVAR) to solve ordinary differential equations (ODEs). Motivated by connections to numerical integration and physics-informed neural networks, we explicitly derive the physics-informed NVAR (piNVAR) which enforces the right-hand side of the underlying differential equation regardless of NVAR construction. Because NVAR and piNVAR completely share their learned parameters, we propose an augmented procedure to jointly train the two models. Then, using both data-driven and ODE-driven metrics, we evaluate the ability of the piNVAR model to predict solutions to various ODE systems, such as the undamped spring, a Lotka-Volterra predator-prey nonlinear model, and the chaotic Lorenz system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59096640a40ed6b70acbbfdaca5a505cd1330bb4" target='_blank'>
              Physics-informed nonlinear vector autoregressive models for the prediction of dynamical systems
              </a>
            </td>
          <td>
            James H. Adler, Samuel Hocking, Xiaozhe Hu, Shafiqul Islam
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The identification of a mathematical dynamics model is a crucial step in the designing process of a controller. However, it is often very difficult to identify the system's governing equations, especially in complex environments that combine physical laws of different disciplines. In this paper, we present a new approach that allows identifying an ordinary differential equation by means of a physics-informed machine learning algorithm. Our method introduces a special neural network that allows exploiting prior human knowledge to a certain degree and extends it autonomously, so that the resulting differential equations describe the system as accurately as possible. We validate the method on a Duffing oscillator with simulation data and, additionally, on a cascaded tank example with real-world data. Subsequently, we use the developed algorithm in a model-based reinforcement learning framework by alternately identifying and controlling a system to a target state. We test the performance by swinging-up an inverted pendulum on a cart.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fb84f2c4de6cf8d43f1207b6c58095273d711f1" target='_blank'>
              Identifying Ordinary Differential Equations for Data-efficient Model-based Reinforcement Learning
              </a>
            </td>
          <td>
            Tobias Nagel, Marco F. Huber
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d487b041502645ead57bcc0782fa0256ccec2fc" target='_blank'>
              Development of data-driven modeling method for nonlinear coupling components
              </a>
            </td>
          <td>
            Taesan Ryu, Seunghun Baek
          </td>
          <td>2024-06-27</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents an identification method for the transient dynamics of microgrids that exploits the intrinsic geometric structure of the dynamics, i.e., the high-dimensional states reside on a relatively low-dimensional manifold. In terms of discrete-time dynamics, the increment in states is decomposed into the tangent and normal components using the local geometric information, inferred from the data set of dynamical responses. The sparse identification of nonlinear dynamical systems (SINDy) method and generalized moving least square (GMLS) algorithms are used to estimate the tangent and normal components of increments, respectively, at every time step to constrain the solution onto the manifold of dynamics; this reduces the sensitivity of the SINDy model to candidate function selection and improve the prediction performance. A ten-bus microgrid system with five loads is used to test and verify the effectiveness of the presented method in identifying the system’s nonlinear dynamics. Numerical tests show that the developed method can give a better estimation for the dynamic transients caused by load variation, when compared to the traditional SINDy model. The results imply that the proposed method is a useful tool to model the transient dynamics in power systems, especially when the state space lies on a low-dimensional manifold.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/296994d8631fcb9aaf6f02f26a1fe8d5ec69ee66" target='_blank'>
              Tangent and Normal Space-Based Method for Dynamics Identification in Microgrids
              </a>
            </td>
          <td>
            Hanyang He, John Harlim, Daning Huang, Yan Li
          </td>
          <td>2024-06-19</td>
          <td>2024 IEEE Transportation Electrification Conference and Expo (ITEC)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Structural dynamics models with nonlinear stiffness appear, for example, when analyzing systems with nonlinear material behavior or undergoing large deformations. For complex systems, these models become too large for real-time applications or multi-query workflows. Hence, model reduction is needed. However, the mathematical operators of these models are often not available since, as is common in industry practice, the models are constructed using commercial simulation software. In this work, we propose an operator inference-based approach aimed at inferring, from data generated by the simulation model, reduced-order models (ROMs) of structural dynamics systems with stiffness terms represented by polynomials of arbitrary degree. To ensure physically meaningful models, we impose constraints on the inference such that the model is guaranteed to exhibit stability properties. Convexity of the optimization problem associated with the inference is maintained by applying a sum-of-squares relaxation to the polynomial term. To further reduce the size of the ROM and improve numerical conditioning of the inference, we also propose a novel clustering-based sparsification of the polynomial term. We validate the proposed method on several numerical examples, including a representative 3D Finite Element Model (FEM) of a steel piston rod.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/12474e97123f0f45095cbb219da196a32731873d" target='_blank'>
              Stable Sparse Operator Inference for Nonlinear Structural Dynamics
              </a>
            </td>
          <td>
            P. D. Boef, Diana Manvelyan, Jos Maubach, W. Schilders, N. Wouw
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="In order to accurately model the dynamics of non-linear electro-mechanical systems, it is imperative to consider the contributions of coupling terms and dissipation. The Lagrangian formulation alone is insufficient to fully capture the holistic behavior of the system. Coupling and dissipation mechanisms play a pivotal role in shaping the system's response. Consequently, to effectively capture the dynamics of inter-coupled electro-mechanical systems with dissipation, we propose an extended Lagrangian-informed deep neural network framework in this paper. Our approach leverages the underlying physics-based knowledge of the system, incorporating it into the neural network architecture. By employing the Euler-Lagrange equations as constraints in the training process, we ensure that the learned dynamics conform to the true behavior of the system. To validate the theoretical framework, we conduct simulation experiments on a DC motor with a cart system, which serves as a representative model of dissipative nonlinear electro-mechanical systems. The experimental results demonstrate the efficacy of our approach in accurately capturing and integrating the dynamics to solve the reference tracking model predictive control design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fec77456fc41a426a57f697c4e29a12b83d9962" target='_blank'>
              Extended Lagrangian-Informed Deep Learning and Control for Electro-mechanical Systems
              </a>
            </td>
          <td>
            Nikhil Pagar, Pegah Ghaf-Ghanbari, Atul G. Kelkar, Javad Mohammadpour
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="
 We present a tensor-based method for model selection which identifies the unknown partial differential equation that governs a dynamical system using only spatiotemporal measurements. The method circumvents a disadvantage of standard matrix-based methods which typically have large storage consumption. Using a recently developed multidimensional approximation of nonlinear dynamical systems, we collect the nonlinear and partial derivative terms of the measured data and construct a low-rank dictionary tensor in the tensor-train format. A tensor-based linear regression problem is then built, which balances the learning accuracy, model complexity, and computational efficiency. An algebraic expression of the unknown equations can be extracted. Numerical results are demonstrated on datasets generated by the wave equation, the Burgers' equation, and a few parametric partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7f12b781a01ed3dd9a5f51e745426539b84443b" target='_blank'>
              Tensor-Based Data-Driven Identification of Partial Differential Equations
              </a>
            </td>
          <td>
            Wanting Lin, Xiaofan Lu, Linan Zhang
          </td>
          <td>2024-06-10</td>
          <td>Journal of Computational and Nonlinear Dynamics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Modeling the dynamics of flexible objects has become an emerging topic in the community as these objects become more present in many applications, e.g., soft robotics. Due to the properties of flexible materials, the movements of soft objects are often highly nonlinear and, thus, complex to predict. Data-driven approaches seem promising for modeling those complex dynamics but often neglect basic physical principles, which consequently makes them untrustworthy and limits generalization. To address this problem, we propose a physics-constrained learning method that combines powerful learning tools and reliable physical models. Our method leverages the data collected from observations by sending them into a Gaussian process that is physically constrained by a distributed Port-Hamiltonian model. Based on the Bayesian nature of the Gaussian process, we not only learn the dynamics of the system, but also enable uncertainty quantification. Furthermore, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/020e63b8b17cdec54c056d8a1edc98770c8fa7ab" target='_blank'>
              Physics-constrained learning for PDE systems with uncertainty quantified port-Hamiltonian models
              </a>
            </td>
          <td>
            Kaiyuan Tan, Peilun Li, Thomas Beckers
          </td>
          <td>2024-06-17</td>
          <td>DBLP, ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d69703a20ce4710f71900130dd935761bbf01fa" target='_blank'>
              Promising directions of machine learning for partial differential equations.
              </a>
            </td>
          <td>
            Steve Brunton, J. Kutz
          </td>
          <td>2024-06-28</td>
          <td>Nature computational science</td>
          <td>3</td>
          <td>1</td>
        </tr>

        <tr id="Neural simulators for modeling complex dynamical systems have been extensively studied for various real-world applications, such as weather forecasting, ocean current prediction, and computational fluid dynamics simulation. Although they have demonstrated powerful fitting and predicting, most existing models are only built to learn single-system dynamics. Several advanced researches have considered learning dynamics across environments, which can exploit the potential commonalities among the dynamics across environments and adapt to new environments. However, these methods still are prone to scarcity problems where per-environment data is sparse or limited. Therefore, we propose a novel CoNDP (Context-Informed Neural ODE Processes) to achieve learning system dynamics from sparse observations across environments. It can fully use contextual information of each environment to better capture the intrinsic commonalities across environments and distinguishable differences among environments while modeling uncertainty of system evolution, producing more accurate predictions. Intensive experiments are conducted on five complex dynamical systems in various fields. Results show that the proposed CoNDP can achieve optimal results compared with common neural simulators and state-of-the-art cross-environmental models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/40337006a651c2110229b8ae928a5d5fe9a01da1" target='_blank'>
              Stochastic Neural Simulator for Generalizing Dynamical Systems across Environments
              </a>
            </td>
          <td>
            Liu Jiaqi, Jiaxu Cui, Jiayi Yang, Bo Yang
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1491d337e12daab70edf38ee62d8f8ee586b8e98" target='_blank'>
              Learning nonlinear operators in latent spaces for real-time predictions of complex dynamics in physical systems
              </a>
            </td>
          <td>
            Katiana Kontolati, S. Goswami, G. Em Karniadakis, Michael D Shields
          </td>
          <td>2024-06-14</td>
          <td>Nature Communications</td>
          <td>5</td>
          <td>19</td>
        </tr>

        <tr id="When neural networks are trained from data to simulate the dynamics of physical systems, they encounter a persistent challenge: the long-time dynamics they produce are often unphysical or unstable. We analyze the origin of such instabilities when learning linear dynamical systems, focusing on the training dynamics. We make several analytical findings which empirical observations suggest extend to nonlinear dynamical systems. First, the rate of convergence of the training dynamics is uneven and depends on the distribution of energy in the data. As a special case, the dynamics in directions where the data have no energy cannot be learned. Second, in the unlearnable directions, the dynamics produced by the neural network depend on the weight initialization, and common weight initialization schemes can produce unstable dynamics. Third, injecting synthetic noise into the data during training adds damping to the training dynamics and can stabilize the learned simulator, though doing so undesirably biases the learned dynamics. For each contributor to instability, we suggest mitigative strategies. We also highlight important differences between learning discrete-time and continuous-time dynamics, and discuss extensions to nonlinear systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a7fe97785b117217a2f1064f6983b137fe02e06" target='_blank'>
              On instabilities in neural network-based physics simulators
              </a>
            </td>
          <td>
            Daniel Floryan
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="The identification of a nonlinear system model given only measurement data is a frequently encountered task. Uncovering the structure of differential equations as system model alongside with the determination of the optimal parameters is still problem under investigation. Assuming a differentially flat system under investigation, an algorithm for single input single output systems is proposed that faces this identification task. To find a proper state space model, an iterative strategy is presented expanding an initial solution in every step. Harnessing only the measurement data and not their time derivatives a criterion is proposed to select promising extensions to the model avoiding extensive simulation studies on possible model structures. Applying a subset selection reduces the complexity of the resulting models and avoids overfitting. To show the capability of the presented approach to identify a nonlinear state space model from data, a simulation example is presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ff4fcc76f9849634c1a641bd0787cff2f7c9419" target='_blank'>
              Flatness-Based Identification of Nonlinear Dynamics
              </a>
            </td>
          <td>
            Alexander M. Kopp, Lisa Fuchs, Christoph Ament
          </td>
          <td>2024-06-11</td>
          <td>2024 32nd Mediterranean Conference on Control and Automation (MED)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Reconstructing a nonlinear dynamical system from empirical time series is a fundamental task in data-driven analysis. One of the main challenges is the existence of hidden variables; we only have records for some variables, and those for hidden variables are unavailable. In this work, the techniques for Carleman linearization, phase-space embedding, and dynamic mode decomposition are integrated to rebuild an optimal dynamical system from time series for one specific variable. Using the Takens theorem, the embedding dimension is determined, which is adopted as the dynamical system's dimension. The Carleman linearization is then used to transform this finite nonlinear system into an infinite linear system, which is further truncated into a finite linear system using the dynamic mode decomposition technique. We illustrate the performance of this integrated technique using data generated by the well-known Lorenz model, the Duffing oscillator, and empirical records of electrocardiogram, electroencephalogram, and measles outbreaks. The results show that this solution accurately estimates the operators of the nonlinear dynamical systems. This work provides a new data-driven method to estimate the Carleman operator of nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dfecb35e8dbf1de60a32397c57f1dd761dcbe4ee" target='_blank'>
              Estimation of Carleman operator from a univariate time series.
              </a>
            </td>
          <td>
            Sherehe Semba, Huijie Yang, Xiaolu Chen, Huiyun Wan, C. Gu
          </td>
          <td>2024-08-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Neural manifolds are an attractive theoretical framework for characterizing the complex behaviors of neural populations. However, many of the tools for identifying these low-dimensional subspaces are correlational and provide limited insight into the underlying dynamics. The ability to precisely control this latent activity would allow researchers to investigate the structure and function of neural manifolds. Employing techniques from the field of optimal control, we simulate controlling the latent dynamics of a neural population using closed-loop, dynamically generated sensory inputs. Using a spiking neural network (SNN) as a model of a neural circuit, we find low-dimensional representations of both the network activity (the neural manifold) and a set of salient visual stimuli. With a data-driven latent dynamics model, we apply model predictive control (MPC) to provide anticipatory, optimal control over the trajectory of the circuit in a latent space. We are able to control the latent dynamics of the SNN to follow several reference trajectories despite observing only a subset of neurons and with a substantial amount of unknown noise injected into the network. These results provide a framework to experimentally test for causal relationships between manifold dynamics and other variables of interest such as organismal behavior and BCI performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2f3319a9926e88582124c34d6157bfd48e1d637" target='_blank'>
              Model Predictive Control of the Neural Manifold
              </a>
            </td>
          <td>
            Christof Fehrman, C. D. Meliza
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="System identification, as a rich and vital discipline, provides a practical and general methodology and tool for quantitatively modelling the input-output relationships of dynamical systems. Sparse nonlinear system identification (SNSI), especially parametric sparse nonlinear system identification (PSNSI), is an important and vital field of research with a wide range of applications. This work is concerned with PSNSI and particular attention is paid to the assessment of three well-known mainstream sparse learning methods, namely, orthogonal least squares (OLS), orthogonal matching pursuit (OMP) and least absolute shrinkage and selection operator (LASSO). The performances of these methods are tested and evaluated through three case studies relating to PSNSI problems. The research results and findings of this work provide practical useful information and guidance for researchers to better choose or adapt methods when solving PSNSI problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5faed1c3a0e751abb45b7883e4c109282e696556" target='_blank'>
              On Sparse Nonlinear System Identification Using Orthogonal Matching Pursuit, Orthogonal Least Squares and LASSO
              </a>
            </td>
          <td>
            Hua‐Liang Wei
          </td>
          <td>2024-06-11</td>
          <td>2024 32nd Mediterranean Conference on Control and Automation (MED)</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="We present a numerical method for learning unknown nonautonomous stochastic dynamical system, i.e., stochastic system subject to time dependent excitation or control signals. Our basic assumption is that the governing equations for the stochastic system are unavailable. However, short bursts of input/output (I/O) data consisting of certain known excitation signals and their corresponding system responses are available. When a sufficient amount of such I/O data are available, our method is capable of learning the unknown dynamics and producing an accurate predictive model for the stochastic responses of the system subject to arbitrary excitation signals not in the training data. Our method has two key components: (1) a local approximation of the training I/O data to transfer the learning into a parameterized form; and (2) a generative model to approximate the underlying unknown stochastic flow map in distribution. After presenting the method in detail, we present a comprehensive set of numerical examples to demonstrate the performance of the proposed method, especially for long-term system predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4edbecee7f2d1381939691be911b0026cab616bb" target='_blank'>
              Modeling Unknown Stochastic Dynamical System Subject to External Excitation
              </a>
            </td>
          <td>
            Yuan Chen, Dongbin Xiu
          </td>
          <td>2024-06-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Initial value problems -- a system of ordinary differential equations and corresponding initial conditions -- can be used to describe many physical phenomena including those arise in classical mechanics. We have developed a novel approach to solve physics-based initial value problems using unsupervised machine learning. We propose a deep learning framework that models the dynamics of a variety of mechanical systems through neural networks. Our framework is flexible, allowing us to solve non-linear, coupled, and chaotic dynamical systems. We demonstrate the effectiveness of our approach on systems including a free particle, a particle in a gravitational field, a classical pendulum, and the H\'enon--Heiles system (a pair of coupled harmonic oscillators with a non-linear perturbation, used in celestial mechanics). Our results show that deep neural networks can successfully approximate solutions to these problems, producing trajectories which conserve physical properties such as energy and those with stationary action. We note that probabilistic activation functions, as defined in this paper, are required to learn any solutions of initial value problems in their strictest sense, and we introduce coupled neural networks to learn solutions of coupled systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71b2f475e6dd93def3f335fbbfac10259c5dba45" target='_blank'>
              Solving physics-based initial value problems with unsupervised machine learning
              </a>
            </td>
          <td>
            Jack Griffiths, S. A. Wrathmall, Simon A. Gardiner
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Nonlinear and non-stationary processes are prevalent in various natural and physical phenomena, where system dynamics can change qualitatively due to bifurcation phenomena. Traditional machine learning methods have advanced our ability to learn and predict such systems from observed time series data. However, predicting the behavior of systems with temporal parameter variations without knowledge of true parameter values remains a significant challenge. This study leverages the reservoir computing framework to address this problem by unsupervised extraction of slowly varying system parameters from time series data. We propose a model architecture consisting of a slow reservoir with long timescale internal dynamics and a fast reservoir with short timescale dynamics. The slow reservoir extracts the temporal variation of system parameters, which are then used to predict unknown bifurcations in the fast dynamics. Through experiments using data generated from chaotic dynamical systems, we demonstrate the ability to predict bifurcations not present in the training data. Our approach shows potential for applications in fields such as neuroscience, material science, and weather prediction, where slow dynamics influencing qualitative changes are often unobservable.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1f7ba13092aa22ef11ae95641fe8961d09c8fcd7" target='_blank'>
              Prediction of Unobserved Bifurcation by Unsupervised Extraction of Slowly Time-Varying System Parameter Dynamics from Time Series Using Reservoir Computing
              </a>
            </td>
          <td>
            Keita Tokuda, Yuichi Katori
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="We introduce a method based on Gaussian process regression to identify discrete variational principles from observed solutions of a field theory. The method is based on the data-based identification of a discrete Lagrangian density. It is a geometric machine learning technique in the sense that the variational structure of the true field theory is reflected in the data-driven model by design. We provide a rigorous convergence statement of the method. The proof circumvents challenges posed by the ambiguity of discrete Lagrangian densities in the inverse problem of variational calculus. Moreover, our method can be used to quantify model uncertainty in the equations of motions and any linear observable of the discrete field theory. This is illustrated on the example of the discrete wave equation and Schr\"odinger equation. The article constitutes an extension of our previous article arXiv:2404.19626 for the data-driven identification of (discrete) Lagrangians for variational dynamics from an ode setting to the setting of discrete pdes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e38b7de6c7022b28628627bbf045123b665619b4" target='_blank'>
              Machine learning of discrete field theories with guaranteed convergence and uncertainty quantification
              </a>
            </td>
          <td>
            Christian Offen
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
 We introduce a novel neural network structure called Strongly Constrained Theory-Guided Neural Network (SCTgNN), to investigate the behaviours of the localized solutions of the generalized nonlinear Schrödinger (NLS) equation. This equation comprises four physically significant nonlinear evolution equations, namely, (i) NLS equation, Hirota equation Lakshmanan-Porsezian-Daniel (LPD) equation and fifth-order NLS equation. The generalized NLS equation demonstrates nonlinear effects up to quintic order, indicating rich and complex dynamics in various fields of physics. By combining concepts from the Physics-Informed Neural Network (PINN) and Theory-Guided Neural Network (TgNN) models, SCTgNN aims to enhance our understanding of complex phenomena, particularly within nonlinear systems that defy conventional patterns. To begin, we employ the TgNN method to predict the behaviours of localized waves, including solitons, rogue waves, and breathers, within the generalized NLS equation. We then use SCTgNN to predict the aforementioned localized solutions and calculate the mean square errors in both SCTgNN and TgNN in predicting these three localized solutions. Our findings reveal that both models excel in understanding complex behaviours and provide predictions across a wide variety of situations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e405cc588c01c3af8b8d5bde5f1526dd7d964ae3" target='_blank'>
              On examining the predictive capabilities of two variants of PINN in validating localised wave solutions in the generalized nonlinear Schrödinger equation
              </a>
            </td>
          <td>
            Thulasidharan K, Sinthuja N, Vishnu Priya N, Senthilvelan M
          </td>
          <td>2024-07-10</td>
          <td>Communications in Theoretical Physics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Before we attempt to learn a function between two (sets of) observables of a physical process, we must first decide what the inputs and what the outputs of the desired function are going to be. Here we demonstrate two distinct, data-driven ways of initially deciding ``the right quantities'' to relate through such a function, and then proceed to learn it. This is accomplished by processing multiple simultaneous heterogeneous data streams (ensembles of time series) from observations of a physical system: multiple observation processes of the system. We thus determine (a) what subsets of observables are common between the observation processes (and therefore observable from each other, relatable through a function); and (b) what information is unrelated to these common observables, and therefore particular to each observation process, and not contributing to the desired function. Any data-driven function approximation technique can subsequently be used to learn the input-output relation, from k-nearest neighbors and Geometric Harmonics to Gaussian Processes and Neural Networks. Two particular ``twists'' of the approach are discussed. The first has to do with the identifiability of particular quantities of interest from the measurements. We now construct mappings from a single set of observations of one process to entire level sets of measurements of the process, consistent with this single set. The second attempts to relate our framework to a form of causality: if one of the observation processes measures ``now'', while the second observation process measures ``in the future'', the function to be learned among what is common across observation processes constitutes a dynamical model for the system evolution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/575c35910c98644dde823648246e69b413bae054" target='_blank'>
              On Learning what to Learn: heterogeneous observations of dynamics and establishing (possibly causal) relations among them
              </a>
            </td>
          <td>
            David W. Sroczynski, Felix Dietrich, E. D. Koronaki, R. Talmon, Ronald R. Coifman, Erik Bollt, Ioannis G. Kevrekidis
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="Data assimilation is a central problem in many geophysical applications, such as weather forecasting. It aims to estimate the state of a potentially large system, such as the atmosphere, from sparse observations, supplemented by prior physical knowledge. The size of the systems involved and the complexity of the underlying physical equations make it a challenging task from a computational point of view. Neural networks represent a promising method of emulating the physics at low cost, and therefore have the potential to considerably improve and accelerate data assimilation. In this work, we introduce a deep learning approach where the physical system is modeled as a sequence of coarse-to-fine Gaussian prior distributions parametrized by a neural network. This allows us to define an assimilation operator, which is trained in an end-to-end fashion to minimize the reconstruction error on a dataset with different observation processes. We illustrate our approach on chaotic dynamical physical systems with sparse observations, and compare it to traditional variational data assimilation methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2950eebe3fe16f882003d5d9945c97a8517a5f89" target='_blank'>
              Neural Incremental Data Assimilation
              </a>
            </td>
          <td>
            Matthieu Blanke, R. Fablet, Marc Lelarge
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="In this study, we investigate the effect of reservoir computing training data on the reconstruction of chaotic dynamics. Our findings indicate that a training time series comprising a few periodic orbits of low periods can successfully reconstruct the Lorenz attractor. We also demonstrate that biased training data does not negatively impact reconstruction success. Our method's ability to reconstruct a physical measure is much better than the so-called cycle expansion approach, which relies on weighted averaging. Additionally, we demonstrate that fixed point attractors and chaotic transients can be accurately reconstructed by a model trained from a few periodic orbits, even when using different parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f833e966a95961819f625f1095e380b4cf40eb4f" target='_blank'>
              Data-driven modeling from biased small training data using periodic orbits
              </a>
            </td>
          <td>
            Kengo Nakai, Yoshitaka Saiki
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Stability is a basic requirement when studying the behavior of dynamical systems. However, stabilizing dynamical systems via reinforcement learning is challenging because only little data can be collected over short time horizons before instabilities are triggered and data become meaningless. This work introduces a reinforcement learning approach that is formulated over latent manifolds of unstable dynamics so that stabilizing policies can be trained from few data samples. The unstable manifolds are minimal in the sense that they contain the lowest dimensional dynamics that are necessary for learning policies that guarantee stabilization. This is in stark contrast to generic latent manifolds that aim to approximate all -- stable and unstable -- system dynamics and thus are higher dimensional and often require higher amounts of data. Experiments demonstrate that the proposed approach stabilizes even complex physical systems from few data samples for which other methods that operate either directly in the system state space or on generic latent manifolds fail.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89660d65895827cad089c32e3f1a9f0b892601ff" target='_blank'>
              System stabilization with policy optimization on unstable latent manifolds
              </a>
            </td>
          <td>
            Steffen W. R. Werner, B. Peherstorfer
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Learning linear and nonlinear dynamical systems from available data is a timely topic in scientific machine learning. Learning must be performed while enforcing the numerical stability of the learned model, the existing knowledge within an informed or augmented setting, or by taking into account the multiscale dynamics—for both linear and nonlinear dynamics. However, when the final objective of such a learned dynamical system is to be used for control purposes, learning transformed dynamics can be advantageous. Therefore, many alternatives exists, and the present paper focuses on two of them: the first based on the discovery and use of the so-called flat control and the second one based on the use of the Koopman theory. The main contributions when addressing the first is the discovery of the flat output transformation by using an original neural framework. Moreover, when using the Koopman theory, this paper proposes an original procedure for learning parametric dynamics in the latent space, which is of particular interest in control-based engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bde1af4107f9ee8f6740e26e9c5d03b288c163cb" target='_blank'>
              Learning Transformed Dynamics for Efficient Control Purposes
              </a>
            </td>
          <td>
            C. Ghnatios, Joel Mouterde, Jerome Tomezyk, Joaquim Da Silva, Francisco Chinesta
          </td>
          <td>2024-07-19</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Non-stationary systems are found throughout the world, from climate patterns under the influence of variation in carbon dioxide concentration, to brain dynamics driven by ascending neuromodulation. Accordingly, there is a need for methods to analyze non-stationary processes, and yet most time-series analysis methods that are used in practice, on important problems across science and industry, make the simplifying assumption of stationarity. One important problem in the analysis of non-stationary systems is the problem class that we refer to as Parameter Inference from a Non-stationary Unknown Process (PINUP). Given an observed time series, this involves inferring the parameters that drive non-stationarity of the time series, without requiring knowledge or inference of a mathematical model of the underlying system. Here we review and unify a diverse literature of algorithms for PINUP. We formulate the problem, and categorize the various algorithmic contributions. This synthesis will allow researchers to identify gaps in the literature and will enable systematic comparisons of different methods. We also demonstrate that the most common systems that existing methods are tested on - notably the non-stationary Lorenz process and logistic map - are surprisingly easy to perform well on using simple statistical features like windowed mean and variance, undermining the practice of using good performance on these systems as evidence of algorithmic performance. We then identify more challenging problems that many existing methods perform poorly on and which can be used to drive methodological advances in the field. Our results unify disjoint scientific contributions to analyzing non-stationary systems and suggest new directions for progress on the PINUP problem and the broader study of non-stationary phenomena.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/585c3c4f3b30a814bc05e6f9e38a5a28b702ee79" target='_blank'>
              Parameter inference from a non-stationary unknown process
              </a>
            </td>
          <td>
            Kieran S. Owens, Ben D. Fulcher
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This study aims to discover the governing mathematical expressions of car-following dynamics from trajectory data directly using deep learning techniques. We propose an expression exploration framework based on deep symbolic regression (DSR) integrated with a variable intersection selection (VIS) method to find variable combinations that encourage interpretable and parsimonious mathematical expressions. In the exploration learning process, two penalty terms are added to improve the reward function: (i) a complexity penalty to regulate the complexity of the explored expressions to be parsimonious, and (ii) a variable interaction penalty to encourage the expression exploration to focus on variable combinations that can best describe the data. We show the performance of the proposed method to learn several car-following dynamics models and discuss its limitations and future research directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f9bb309310be4265ecdbe08b6a11baf260a21d96" target='_blank'>
              Discovering Car-following Dynamics from Trajectory Data through Deep Learning
              </a>
            </td>
          <td>
            Ohay Angah, James Enouen, Xuegang (Jeff) Ban, Yan Liu
          </td>
          <td>2024-08-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Dynamical systems provide a comprehensive way to study complex and changing behaviors across various sciences. Many modern systems are too complicated to analyze directly or we do not have access to models, driving significant interest in learning methods. Koopman operators have emerged as a dominant approach because they allow the study of nonlinear dynamics using linear techniques by solving an infinite-dimensional spectral problem. However, current algorithms face challenges such as lack of convergence, hindering practical progress. This paper addresses a fundamental open question: \textit{When can we robustly learn the spectral properties of Koopman operators from trajectory data of dynamical systems, and when can we not?} Understanding these boundaries is crucial for analysis, applications, and designing algorithms. We establish a foundational approach that combines computational analysis and ergodic theory, revealing the first fundamental barriers -- universal for any algorithm -- associated with system geometry and complexity, regardless of data quality and quantity. For instance, we demonstrate well-behaved smooth dynamical systems on tori where non-trivial eigenfunctions of the Koopman operator cannot be determined by any sequence of (even randomized) algorithms, even with unlimited training data. Additionally, we identify when learning is possible and introduce optimal algorithms with verification that overcome issues in standard methods. These results pave the way for a sharp classification theory of data-driven dynamical systems based on how many limits are needed to solve a problem. These limits characterize all previous methods, presenting a unified view. Our framework systematically determines when and how Koopman spectral properties can be learned.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb7265ab4b5b6dce43f8b2a37e99d9fd03f2bef7" target='_blank'>
              Limits and Powers of Koopman Learning
              </a>
            </td>
          <td>
            Matthew J. Colbrook, Igor Mezi'c, Alexei Stepanenko
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="The manipulation of deformable linear objects (DLOs) via model-based control requires an accurate and computationally efficient dynamics model. Yet, data-driven DLO dynamics models require large training data sets while their predictions often do not generalize, whereas physics-based models rely on good approximations of physical phenomena and often lack accuracy. To address these challenges, we propose a physics-informed neural ODE capable of predicting agile movements with significantly less data and hyper-parameter tuning. In particular, we model DLOs as serial chains of rigid bodies interconnected by passive elastic joints in which interaction forces are predicted by neural networks. The proposed model accurately predicts the motion of an robotically-actuated aluminium rod and an elastic foam cylinder after being trained on only thirty seconds of data. The project code and data are available at: \url{https://tinyurl.com/neuralprba}">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/815ff430d40660092c67ed599422c4c980a0db8d" target='_blank'>
              Learning deformable linear object dynamics from a single trajectory
              </a>
            </td>
          <td>
            Shamil Mamedov, A. R. Geist, Ruan Viljoen, Sebastian Trimpe, Jan Swevers
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Physics-informed neural networks have proven to be a powerful tool for solving differential equations, leveraging the principles of physics to inform the learning process. However, traditional deep neural networks often face challenges in achieving high accuracy without incurring significant computational costs. In this work, we implement the Physics-Informed Kolmogorov-Arnold Neural Networks (PIKAN) through efficient-KAN and WAV-KAN, which utilize the Kolmogorov-Arnold representation theorem. PIKAN demonstrates superior performance compared to conventional deep neural networks, achieving the same level of accuracy with fewer layers and reduced computational overhead. We explore both B-spline and wavelet-based implementations of PIKAN and benchmark their performance across various ordinary and partial differential equations using unsupervised (data-free) and supervised (data-driven) techniques. For certain differential equations, the data-free approach suffices to find accurate solutions, while in more complex scenarios, the data-driven method enhances the PIKAN's ability to converge to the correct solution. We validate our results against numerical solutions and achieve $99 \%$ accuracy in most scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/785706911b444c085e5f732a9fef3bb0159349db" target='_blank'>
              Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN
              </a>
            </td>
          <td>
            Subhajit Patra, Sonali Panda, B. K. Parida, Mahima Arya, Kurt Jacobs, Denys I. Bondar, Abhijit Sen
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We introduce a general framework for solving partial differential equations (PDEs) using generative diffusion models. In particular, we focus on the scenarios where we do not have the full knowledge of the scene necessary to apply classical solvers. Most existing forward or inverse PDE approaches perform poorly when the observations on the data or the underlying coefficients are incomplete, which is a common assumption for real-world measurements. In this work, we propose DiffusionPDE that can simultaneously fill in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces. We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c821763f76272b6deee95f9729107969ce0ae44" target='_blank'>
              DiffusionPDE: Generative PDE-Solving Under Partial Observation
              </a>
            </td>
          <td>
            Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The goal of this paper is to make a strong point for the usage of dynamical models when using reinforcement learning (RL) for feedback control of dynamical systems governed by partial differential equations (PDEs). To breach the gap between the immense promises we see in RL and the applicability in complex engineering systems, the main challenges are the massive requirements in terms of the training data, as well as the lack of performance guarantees. We present a solution for the first issue using a data-driven surrogate model in the form of a convolutional Long-Short Term Memory network with actuation. We demonstrate that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system. Furthermore, we show that iteratively updating the model is of major importance to avoid biases in the RL training. Detailed ablation studies reveal the most important ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky equation do demonstrate our findings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73d91c3ba7ba5d34e084bcd3bd2c0c397052d50a" target='_blank'>
              Numerical Evidence for Sample Efficiency of Model-Based Over Model-Free Reinforcement Learning Control of Partial Differential Equations
              </a>
            </td>
          <td>
            Stefan Werner, Sebastian Peitz
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Estimating and controlling dynamical systems from observable time-series data are essential for understanding and manipulating nonlinear dynamics. This paper proposes a probabilistic framework for simultaneously estimating and controlling nonlinear dynamics under noisy observation conditions. Our proposed method utilizes the particle filter not only as a state estimator and a prior estimator for the dynamics but also as a controller. This approach allows us to handle the nonlinearity of the dynamics and uncertainty of the latent state. We apply two distinct dynamics to verify the effectiveness of our proposed framework: a chaotic system defined by the Lorenz equation and a nonlinear neuronal system defined by the Morris–Lecar neuron model. The results indicate that our proposed framework can simultaneously estimate and control complex nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/682bd11b89e6862f9721d25af1d528fadb6746f4" target='_blank'>
              Probabilistic Estimation and Control of Dynamical Systems Using Particle Filter with Adaptive Backward Sampling
              </a>
            </td>
          <td>
            Taketo Omi, Toshiaki Omori
          </td>
          <td>2024-07-30</td>
          <td>Entropy</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Accurately modeling the mechanical behavior of materials is crucial for numerous engineering applications. The quality of these models depends directly on the accuracy of the constitutive law that defines the stress-strain relation. Discovering these constitutive material laws remains a significant challenge, in particular when only material deformation data is available. To address this challenge, unsupervised machine learning methods have been proposed. However, existing approaches have several limitations: they either fail to ensure that the learned constitutive relations are consistent with physical principles, or they rely on a predefined library of constitutive relations or manually crafted input features. These dependencies require significant expertise and specialized domain knowledge. Here, we introduce a machine learning approach called uLED, which overcomes the limitations by using the input convex neural network (ICNN) as the surrogate constitutive model. We improve the optimization strategy for training ICNN, allowing it to be trained end-to-end using direct strain invariants as input across various materials. Furthermore, we utilize the nodal force equilibrium at the internal domain as the training objective, which enables us to learn the constitutive relation solely from temporal displacement recordings. We validate the effectiveness of the proposed method on a diverse range of material laws. We demonstrate that it is robust to a significant level of noise and that it converges to the ground truth with increasing data resolution. We also show that the model can be effectively trained using a displacement field from a subdomain of the test specimen and that the learned constitutive relation from one material sample is transferable to other samples with different geometries. The developed methodology provides an effective tool for discovering constitutive relations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/66917df8a8f69b2b557c7f74ef2c576d38a94797" target='_blank'>
              Learning Physics-Consistent Material Behavior Without Prior Knowledge
              </a>
            </td>
          <td>
            Zhichao Han, Mohit Pundir, Olga Fink, David S. Kammer
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Multiphysics problems that are characterized by complex interactions among fluid dynamics, heat transfer, structural mechanics, and electromagnetics, are inherently challenging due to their coupled nature. While experimental data on certain state variables may be available, integrating these data with numerical solvers remains a significant challenge. Physics-informed neural networks (PINNs) have shown promising results in various engineering disciplines, particularly in handling noisy data and solving inverse problems. However, their effectiveness in forecasting nonlinear phenomena in multiphysics regimes is yet to be fully established. This study introduces NeuroSEM, a hybrid framework integrating PINNs with the high-fidelity Spectral Element Method (SEM) solver, Nektar++. NeuroSEM leverages strengths of both PINNs and SEM, providing robust solutions for multiphysics problems. PINNs are trained to assimilate data and model physical phenomena in specific subdomains, which are then integrated into Nektar++. We demonstrate the efficiency and accuracy of NeuroSEM for thermal convection in cavity flow and flow past a cylinder. The framework effectively handles data assimilation by addressing those subdomains and state variables where data are available. We applied NeuroSEM to the Rayleigh-B\'enard convection system, including cases with missing thermal boundary conditions. Our results indicate that NeuroSEM accurately models the physical phenomena and assimilates the data within the specified subdomains. The framework's plug-and-play nature facilitates its extension to other multiphysics or multiscale problems. Furthermore, NeuroSEM is optimized for an efficient execution on emerging integrated GPU-CPU architectures. This hybrid approach enhances the accuracy and efficiency of simulations, making it a powerful tool for tackling complex engineering challenges in various scientific domains.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf355890cfebb6682f601bfaea0ed245cda232dc" target='_blank'>
              NeuroSEM: A hybrid framework for simulating multiphysics problems by coupling PINNs and spectral elements
              </a>
            </td>
          <td>
            K. Shukla, Zongren Zou, Chi Hin Chan, Additi Pandey, Zhicheng Wang, G. Karniadakis
          </td>
          <td>2024-07-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="Bifurcations mark qualitative changes of long-term behavior in dynamical systems and can often signal sudden ("hard") transitions or catastrophic events (divergences). Accurately locating them is critical not just for deeper understanding of observed dynamic behavior, but also for designing efficient interventions. When the dynamical system at hand is complex, possibly noisy, and expensive to sample, standard (e.g. continuation based) numerical methods may become impractical. We propose an active learning framework, where Bayesian Optimization is leveraged to discover saddle-node or Hopf bifurcations, from a judiciously chosen small number of vector field observations. Such an approach becomes especially attractive in systems whose state x parameter space exploration is resource-limited. It also naturally provides a framework for uncertainty quantification (aleatoric and epistemic), useful in systems with inherent stochasticity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8580056dd0caf69a81d8730639e0828479823e8a" target='_blank'>
              Active search for Bifurcations
              </a>
            </td>
          <td>
            Y. M. Psarellis, T. Sapsis, I. G. Kevrekidis
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="We consider solving complex spatiotemporal dynamical systems governed by partial differential equations (PDEs) using frequency domain-based discrete learning approaches, such as Fourier neural operators. Despite their widespread use for approximating nonlinear PDEs, the majority of these methods neglect fundamental physical laws and lack interpretability. We address these shortcomings by introducing Physics-embedded Fourier Neural Networks (PeFNN) with flexible and explainable error control. PeFNN is designed to enforce momentum conservation and yields interpretable nonlinear expressions by utilizing unique multi-scale momentum-conserving Fourier (MC-Fourier) layers and an element-wise product operation. The MC-Fourier layer is by design translation- and rotation-invariant in the frequency domain, serving as a plug-and-play module that adheres to the laws of momentum conservation. PeFNN establishes a new state-of-the-art in solving widely employed spatiotemporal PDEs and generalizes well across input resolutions. Further, we demonstrate its outstanding performance for challenging real-world applications such as large-scale flood simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b1987fcbdf3f85b47d4df2efa239c0c5050a329a" target='_blank'>
              Physics-embedded Fourier Neural Network for Partial Differential Equations
              </a>
            </td>
          <td>
            Qingsong Xu, Nils Thuerey, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang Zhu
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="This proposed work introduces a data-assimilation-assisted approach to train neural networks, aimed at effectively reducing epistemic uncertainty in state estimates of separated flows. This method, referred to as model-consistent training, ensures that input features are derived directly from physics-based models, such as Reynolds Averaged Navier Stokes (RANS) turbulence models, to accurately represent the current state of the flow. Autoencoders have been selected for this task due to their capability to capture essential information from large datasets, making them particularly suitable for handling high-dimensional data with numerous discretization points in both spatial and temporal dimensions. This innovative approach integrates the ensemble Kalman method to enhance the training process, providing a robust framework for improving model accuracy and performance in turbulent flow predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c44941f2e715c2b0202ef4e5d7ba0fe96659fe0" target='_blank'>
              Developing a Model-Consistent Reduced-Dimensionality training approach to quantify and reduce epistemic uncertainty in separated flows
              </a>
            </td>
          <td>
            Minghan Chu
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Physics informed neural networks have been gaining popularity due to their unique ability to incorporate physics laws into data-driven models, ensuring that the predictions are not only consistent with empirical data but also align with domain-specific knowledge in the form of physics equations. The integration of physics principles enables the method to require less data while maintaining the robustness of deep learning in modeling complex dynamical systems. However, current PINN frameworks are not sufficiently mature for real-world ODE systems, especially those with extreme multi-scale behavior such as mosquito population dynamical modelling. In this research, we propose a PINN framework with several improvements for forward and inverse problems for ODE systems with a case study application in modelling the dynamics of mosquito populations. The framework tackles the gradient imbalance and stiff problems posed by mosquito ordinary differential equations. The method offers a simple but effective way to resolve the time causality issue in PINNs by gradually expanding the training time domain until it covers entire domain of interest. As part of a robust evaluation, we conduct experiments using simulated data to evaluate the effectiveness of the approach. Preliminary results indicate that physics-informed machine learning holds significant potential for advancing the study of ecological systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf1c97b75409ff1ee6e5c4298dd05295c9b772df" target='_blank'>
              Adapting Physics-Informed Neural Networks To Optimize ODEs in Mosquito Population Dynamics
              </a>
            </td>
          <td>
            D. V. Cuong, Branislava Lali'c, Mina Petri'c, Binh Nguyen, M. Roantree
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="We propose CoNSAL (Combining Neural networks and Symbolic regression for Analytical Lyapunov function) to construct analytical Lyapunov functions for nonlinear dynamic systems. This framework contains a neural Lyapunov function and a symbolic regression component, where symbolic regression is applied to distill the neural network to precise analytical forms. Our approach utilizes symbolic regression not only as a tool for translation but also as a means to uncover counterexamples. This procedure terminates when no counterexamples are found in the analytical formulation. Compared with previous results, CoNSAL directly produces an analytical form of the Lyapunov function with improved interpretability in both the learning process and the final results. We apply CoNSAL to 2-D inverted pendulum, path following, Van Der Pol Oscillator, 3-D trig dynamics, 4-D rotating wheel pendulum, 6-D 3-bus power system, and demonstrate that our algorithm successfully finds their valid Lyapunov functions. Code examples are available at https://github.com/HaohanZou/CoNSAL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6829a0516285c595b5e8d5fcbc9c4858827f5784" target='_blank'>
              Combining Neural Networks and Symbolic Regression for Analytical Lyapunov Function Discovery
              </a>
            </td>
          <td>
            Jie Feng, Haohan Zou, Yuanyuan Shi
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The precise and efficient interpretation of Fisher's equation is paramount while comprehending different dynamics of biological population spread and colonization. In this context, Physics-informed neural networks (PINNs) are emerging as a promising alternative to conventional numerical methods. This paper highlights the distinct advantages of using PINN in solving Fisher's equations compared to traditional methods. This method involves numerically approximated partial differential equations through PINNs based on the sampling of Latin supermassives. The optimization was performed using Adam's optimization technique to minimize the root mean square loss function associated with the targeted partial differential equation. This study provides a powerful tool that not only accurately predicts population distribution, but also captures underlying dynamics of the system. The effectiveness of the mentioned method is highlighted with numerous numerical testing and comparative analysis with traditional numerical methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d6732656e841a9a913263f302a06cb3a1fbe2653" target='_blank'>
              A Novel Approach for Approximating the Dynamics of Fisher's Equation Based on Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Nimmagadda Greeshma, Soumyendra Singh
          </td>
          <td>2024-06-08</td>
          <td>2024 International Conference on Integrated Circuits, Communication, and Computing Systems (ICIC3S)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Port-Hamiltonian systems (pHS) allow for a structure-preserving modeling of dynamical systems. Coupling pHS via linear relations between input and output defines an overall pHS, which is structure preserving. However, in multiphysics applications, some subsystems do not allow for a physical pHS description, as (a) this is not available or (b) too expensive. Here, data-driven approaches can be used to deliver a pHS for such subsystems, which can then be coupled to the other subsystems in a structure-preserving way. In this work, we derive a data-driven identification approach for port-Hamiltonian differential algebraic equation (DAE) systems. The approach uses input and state space data to estimate nonlinear effort functions of pH-DAEs. As underlying technique, we us (multi-task) Gaussian processes. This work thereby extends over the current state of the art, in which only port-Hamiltonian ordinary differential equation systems could be identified via Gaussian processes. We apply this approach successfully to two applications from network design and constrained multibody system dynamics, based on pH-DAE system of index one and three, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f006ed4bbd09834cab7ba0f2c3258b36e07050c2" target='_blank'>
              Data-driven identification of port-Hamiltonian DAE systems by Gaussian processes
              </a>
            </td>
          <td>
            Peter Zaspel, Michael Günther
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Many systems in biology, physics, and engineering are modeled by nonlinear dynamical systems where the states are usually unknown and only a subset of the state variables can be physically measured. Can we understand the full system from what we measure? In the mathematics literature, this question is framed as the observability problem. It has to do with recovering information about the state variables from the observed states (the measurements). In this paper, we relate the observability problem to another structural feature of many models relevant in the physical and biological sciences: the conserved quantity. For models based on systems of differential equations, conserved quantities offer desirable properties such as dimension reduction which simplifies model analysis. Here, we use differential embeddings to show that conserved quantities involving a set of special variables provide more flexibility in what can be measured to address the observability problem for systems of interest in biology. Specifically, we provide conditions under which a collection of conserved quantities make the system observable. We apply our methods to provide alternate measurable variables in models where conserved quantities have been used for model analysis historically in biological contexts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ac75a2c31069a04d822b3532f120e93eba3378c7" target='_blank'>
              Observability of complex systems via conserved quantities
              </a>
            </td>
          <td>
            B. Karamched, Jack Schmidt, David Murrugarra
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed trial-to-trial variability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/579f56813a03517163b86fa89044dc78505bf2cb" target='_blank'>
              Inferring stochastic low-rank recurrent neural networks from neural data
              </a>
            </td>
          <td>
            Matthijs Pals, A. E. Saugtekin, Felix Pei, Manuel Gloeckler, J. H. Macke
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this paper, we present a generic physics-informed generative model called MPDM that integrates multi-fidelity physics simulations with diffusion models. MPDM categorizes multi-fidelity physics simulations into inexpensive and expensive simulations, depending on computational costs. The inexpensive simulations, which can be obtained with low latency, directly inject contextual information into DDMs. Furthermore, when results from expensive simulations are available, MPDM refines the quality of generated samples via a guided diffusion process. This design separates the training of a denoising diffusion model from physics-informed conditional probability models, thus lending flexibility to practitioners. MPDM builds on Bayesian probabilistic models and is equipped with a theoretical guarantee that provides upper bounds on the Wasserstein distance between the sample and underlying true distribution. The probabilistic nature of MPDM also provides a convenient approach for uncertainty quantification in prediction. Our models excel in cases where physics simulations are imperfect and sometimes inaccessible. We use a numerical simulation in fluid dynamics and a case study in heat dynamics within laser-based metal powder deposition additive manufacturing to demonstrate how MPDM seamlessly integrates multi-idelity physics simulations and observations to obtain surrogates with superior predictive performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e5e531479fd961311f387fb904eca9e6d2466a0" target='_blank'>
              Multi-physics Simulation Guided Generative Diffusion Models with Applications in Fluid and Heat Dynamics
              </a>
            </td>
          <td>
            Naichen Shi, Hao Yan, Shenghan Guo, R. Kontar
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="The effective inclusion of a priori knowledge when embedding known data in physics‐based models of dynamical systems can ensure that the reconstructed model respects physical principles, while simultaneously improving the accuracy of the solution in the previously unseen regions of state space. This paper presents a physics‐constrained data‐driven discrepancy modeling method that variationally embeds known data in the modeling framework. The hierarchical structure of the method yields fine scale variational equations that facilitate the derivation of residuals which are comprised of the first‐principles theory and sensor‐based data from the dynamical system. The embedding of the sensor data via residual terms leads to discrepancy‐informed closure models that yield a method which is driven not only by boundary and initial conditions, but also by measurements that are taken at only a few observation points in the target system. Specifically, the data‐embedding term serves as residual‐based least‐squares loss function, thus retaining variational consistency. Another important relation arises from the interpretation of the stabilization tensor as a kernel function, thereby incorporating a priori knowledge of the problem and adding computational intelligence to the modeling framework. Numerical test cases show that when known data is taken into account, the data driven variational (DDV) method can correctly predict the system response in the presence of several types of discrepancies. Specifically, the damped solution and correct energy time histories are recovered by including known data in the undamped situation. Morlet wavelet analyses reveal that the surrogate problem with embedded data recovers the fundamental frequency band of the target system. The enhanced stability and accuracy of the DDV method is manifested via reconstructed displacement and velocity fields that yield time histories of strain and kinetic energies which match the target systems. The proposed DDV method also serves as a procedure for restoring eigenvalues and eigenvectors of a deficient dynamical system when known data is taken into account, as shown in the numerical test cases presented here.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dda505dc42399a19a75eff263166e84931a95cf6" target='_blank'>
              Data‐driven variational method for discrepancy modeling: Dynamics with small‐strain nonlinear elasticity and viscoelasticity
              </a>
            </td>
          <td>
            Arif Masud, Shoaib A. Goraya
          </td>
          <td>2024-07-04</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this paper, we explore the application of Machine Learning techniques, specifically Support Vector Machines (SVM), to unveil the chaotic and regular nature of trajectories in Hamiltonian systems using Lagrangian descriptors. Traditional chaos indicators, while effective, are computationally expensive and require an exhaustive study of the parameter space to establish the classification thresholds. By using SVMs trained on a dataset obtained from the analysis of the dynamics of the double pendulum Hamiltonian system, we aim at reducing the complexity of this process. Our trained SVM models demonstrate high accuracy when it comes to classifying trajectories in diverse Hamiltonian systems, such as for example in the four-well Hamiltonian, the H\'enon-Heiles system and the Chirikov Standard Map. The results indicate that SVMs, when combined with Lagrangian descriptors, offer a robust and efficient method for chaos classification across different dynamical systems. Our approach not only simplifies the classification process but also is highlighting the potential of Machine Learning algorithms in the study of nonlinear dynamics and chaos.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3fc433a1690a469869c09f4b8c25ee5213c533c" target='_blank'>
              Learning the Chaotic and Regular Nature of Trajectories in Hamiltonian Systems with Lagrangian descriptors
              </a>
            </td>
          <td>
            Javier Jim'enez L'opez, V'ictor Jos'e Garc'ia Garrido
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The continuously increasing amount of noisy data demands the development of accurate and efficient models for analysis, modeling, and control. In this article, we propose a novel data-driven moment matching method which employs Tikhonov regularization in the Reproducing Kernel Hilbert Spaces (RKHSs). Specifically, considering a realistic scenario in which the system's plant is unknown and only noisy measured data are available, we provide an estimation of the moment of the unknown plant by solving a regularized optimization problem on RKHS. For, we first demonstrate that the estimation of the moment can be improved via tuning the regularization term, and further, we show under which condition the effect of the transient improves the performance of the estimation. Then, we construct a parameterized model characterized by a kernel-based output mapping. Finally, the proposed data-driven approach is validated and discussed by means of a DC-to-DC Cuk converter driven by a Van der Pol oscillator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/970212021b4134e841e09f425053018067ffcdbb" target='_blank'>
              Nonlinear Data-Driven Moment Matching in Reproducing Kernel Hilbert Spaces
              </a>
            </td>
          <td>
            Alessio Moreschini, Matteo Scandella, Thomas Parisini
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions -- e.g. symmetries or boundary conditions -- in favour of modelling flexibility. Instead, we propose a space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space - respects known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency. We validated that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail - and improve over baselines in a number of challenging geometries.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c8295fdfb6de078027cabc7d49b319df8abb0ee" target='_blank'>
              Space-Time Continuous PDE Forecasting using Equivariant Neural Fields
              </a>
            </td>
          <td>
            David M. Knigge, David R. Wessels, Riccardo Valperga, Samuele Papa, J. Sonke, E. Gavves, E. J. Bekkers
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>56</td>
        </tr>

        <tr id="We propose the Artificial Intelligence Velocimetry-Thermometry (AIVT) method to infer hidden temperature fields from experimental turbulent velocity data. This physics-informed machine learning method enables us to infer continuous temperature fields using only sparse velocity data, hence eliminating the need for direct temperature measurements. Specifically, AIVT is based on physics-informed Kolmogorov-Arnold Networks (not neural networks) and is trained by optimizing a combined loss function that minimizes the residuals of the velocity data, boundary conditions, and the governing equations. We apply AIVT to a unique set of experimental volumetric and simultaneous temperature and velocity data of Rayleigh-B\'enard convection (RBC) that we acquired by combining Particle Image Thermometry and Lagrangian Particle Tracking. This allows us to compare AIVT predictions and measurements directly. We demonstrate that we can reconstruct and infer continuous and instantaneous velocity and temperature fields from sparse experimental data at a fidelity comparable to direct numerical simulations (DNS) of turbulence. This, in turn, enables us to compute important quantities for quantifying turbulence, such as fluctuations, viscous and thermal dissipation, and QR distribution. This paradigm shift in processing experimental data using AIVT to infer turbulent fields at DNS-level fidelity is a promising avenue in breaking the current deadlock of quantitative understanding of turbulence at high Reynolds numbers, where DNS is computationally infeasible.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb6d8afffcc7bf5a551a6c5a5b9a454bde425d85" target='_blank'>
              Inferring turbulent velocity and temperature fields and their statistics from Lagrangian velocity measurements using physics-informed Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Juan Diego Toscano, Theo Kaufer, Zhibo Wang, Martin Maxey, Christian Cierpka, G. Karniadakis
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="Classical model reduction techniques project the governing equations onto a linear subspace of the original state space. More recent data-driven techniques use neural networks to enable nonlinear projections. Whilst those often enable stronger compression, they may have redundant parameters and lead to suboptimal latent dimensionality. To overcome these, we propose a multistep algorithm that induces sparsity in the encoder-decoder networks for effective reduction in the number of parameters and additional compression of the latent space. This algorithm starts with sparsely initialized a network and training it using linearized Bregman iterations. These iterations have been very successful in computer vision and compressed sensing tasks, but have not yet been used for reduced-order modelling. After the training, we further compress the latent space dimensionality by using a form of proper orthogonal decomposition. Last, we use a bias propagation technique to change the induced sparsity into an effective reduction of parameters. We apply this algorithm to three representative PDE models: 1D diffusion, 1D advection, and 2D reaction-diffusion. Compared to conventional training methods like Adam, the proposed method achieves similar accuracy with 30% less parameters and a significantly smaller latent space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6d0716bb98b1bad42ed563160283dce3e8413da6" target='_blank'>
              Sparsifying dimensionality reduction of PDE solution data with Bregman learning
              </a>
            </td>
          <td>
            T. J. Heeringa, Christoph Brune, Mengwu Guo
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="To improve the predictive capacity of system models in the input-output sense, this paper presents a framework for model updating via learning of modeling uncertainties in locally (and thus also in globally) Lipschitz nonlinear systems. First, we introduce a method to extend an existing known model with an uncertainty model so that stability of the extended model is guaranteed in the sense of set invariance and input-to-state stability. To achieve this, we provide two tractable semi-definite programs. These programs allow obtaining optimal uncertainty model parameters for both locally and globally Lipschitz nonlinear models, given uncertainty and state trajectories. Subsequently, in order to extract this data from the available input-output trajectories, we introduce a filter that incorporates an approximated internal model of the uncertainty and asymptotically estimates uncertainty and state realizations. This filter is also synthesized using semi-definite programs with guaranteed robustness with respect to uncertainty model mismatches, disturbances, and noise. Numerical simulations for a large data-set of a roll plane model of a vehicle illustrate the effectiveness and practicality of the proposed methodology in improving model accuracy, while guaranteeing stability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e7ee6ed031b0068e5df250ad046941ba05393280" target='_blank'>
              Model Updating for Nonlinear Systems with Stability Guarantees
              </a>
            </td>
          <td>
            Farhad Ghanipoor, C. Murguia, Peyman Mohajerin Esfahani, N. Wouw
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="
 The growing time-series data make it possible to glimpse the hidden dynamics in various fields. However, developing a computational toolbox with high interpretability to unveil the interaction dynamics from data remains a crucial challenge. Here, we propose a new computational approach called Automated Dynamical Model Inference based on Expression Trees (ADMIET), in which the machine learning algorithm, the numerical integration of ordinary differential equations and the interpretability from prior knowledge are embedded into the symbolic learning scheme to establish a general framework for revealing the hidden dynamics in time-series data. ADMIET takes full advantage of both machine learning algorithm and expression tree. Firstly, we translate the prior knowledge into constraints on the structure of expression tree, reducing the search space and enhancing the interpretability. Secondly, we utilize the proposed adaptive penalty function to ensure the convergence of gradient descent algorithm and the selection of the symbols. Compared to gene expression programming, ADMIET exhibits its remarkable capability in function fitting with higher accuracy and broader applicability. Moreover, ADMIET can better fit parameters in nonlinear forms compared to regression methods. Furthermore, we apply ADMIET to two typical biological systems and one real data with different prior knowledge to infer the dynamical equations. The results indicate that ADMIET can not only discover the interaction relationships but also provide accurate estimates of the parameters in the equations. These results demonstrate ADMIET's superiority in revealing interpretable dynamics from time-series biological data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9687aa0373c8e0262b0df16232ea9a08145ff46c" target='_blank'>
              Inferring dynamical models from time-series biological data using an interpretable machine learning method based on weighted expression trees
              </a>
            </td>
          <td>
            Yu Zhou, Xiufen Zou
          </td>
          <td>2024-07-09</td>
          <td>Inverse Problems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fd49cad114aa0d9be0df7a779ad44da18fbea25" target='_blank'>
              Stochastic modeling of stationary scalar Gaussian processes in continuous time from autocorrelation data
              </a>
            </td>
          <td>
            Martin Hanke
          </td>
          <td>2024-06-24</td>
          <td>Advances in Computational Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The development of data-driven approaches for solving differential equations has been followed by a plethora of applications in science and engineering across a multitude of disciplines and remains a central focus of active scientific inquiry. However, a large body of natural phenomena incorporates memory effects that are best described via fractional integro-differential equations (FIDEs), in which the integral or differential operators accept non-integer orders. Addressing the challenges posed by nonlinear FIDEs is a recognized difficulty, necessitating the application of generic methods with immediate practical relevance. This work introduces the Universal Fractional Integro-Differential Equation Solvers (UniFIDES), a comprehensive machine learning platform designed to expeditiously solve a variety of FIDEs in both forward and inverse directions, without the need for ad hoc manipulation of the equations. The effectiveness of UniFIDES is demonstrated through a collection of integer-order and fractional problems in science and engineering. Our results highlight UniFIDES' ability to accurately solve a wide spectrum of integro-differential equations and offer the prospect of using machine learning platforms universally for discovering and describing dynamical and complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41fa0a29da947de99741b68960e6591a9bfd2771" target='_blank'>
              UniFIDES: Universal Fractional Integro-Differential Equation Solvers
              </a>
            </td>
          <td>
            Milad Saadat, Deepak Mangal, Safa Jamali
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of interest are control affine. We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence. Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost. We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Casta\~neda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF). Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/29cde075fe82ea752c015715f01f8a198477efc2" target='_blank'>
              Random features approximation for control-affine systems
              </a>
            </td>
          <td>
            Kimia Kazemian, Yahya Sattar, Sarah Dean
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose a randomized physics-informed neural network (PINN) or rPINN method for uncertainty quantification in inverse partial differential equation (PDE) problems with noisy data. This method is used to quantify uncertainty in the inverse PDE PINN solutions. Recently, the Bayesian PINN (BPINN) method was proposed, where the posterior distribution of the PINN parameters was formulated using the Bayes' theorem and sampled using approximate inference methods such as the Hamiltonian Monte Carlo (HMC) and variational inference (VI) methods. In this work, we demonstrate that HMC fails to converge for non-linear inverse PDE problems. As an alternative to HMC, we sample the distribution by solving the stochastic optimization problem obtained by randomizing the PINN loss function. The effectiveness of the rPINN method is tested for linear and non-linear Poisson equations, and the diffusion equation with a high-dimensional space-dependent diffusion coefficient. The rPINN method provides informative distributions for all considered problems. For the linear Poisson equation, HMC and rPINN produce similar distributions, but rPINN is on average 27 times faster than HMC. For the non-linear Poison and diffusion equations, the HMC method fails to converge because a single HMC chain cannot sample multiple modes of the posterior distribution of the PINN parameters in a reasonable amount of time.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a72f5071131521363e3f04ad6b40fd9347a43b5" target='_blank'>
              Randomized Physics-Informed Neural Networks for Bayesian Data Assimilation
              </a>
            </td>
          <td>
            Yifei Zong, D. Barajas-Solano, A. Tartakovsky
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>41</td>
        </tr>

        <tr id="For mathematical and experimental ease, models with time varying parameters are often simplified to assume constant parameters. However, this simplification can potentially lead to identifiability issues (lack of uniqueness of parameter estimates). Methods have been developed to algebraically and numerically determine the identifiability of a model, as well as resolve identifiability issues. This specific type of simplification presents an alternate opportunity to instead use this information to resolve the unidentifiability. Given that re-parameterizing, collecting more data, and adding inputs can be potentially costly or impractical, this could present new alternatives. We present a method for resolving unidentifiability in a system by introducing a new data stream correlated with a parameter of interest. First, we demonstrate how and when non-constant input data can be introduced into any rational function ODE system without worsening the model identifiability. Then, we prove when these input functions improve structural and potentially also practical identifiability for a given model and relevant data. By utilizing pre-existing data streams, these methods can potentially reduce experimental costs, while still answering key questions. By connecting mathematical proofs to application, our framework removes guesswork from when, where, and how researchers can best introduce new data to improve model outcomes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fbc38cc84b188d260525dcc553f64371eca970c3" target='_blank'>
              Examining the impact of forcing function inputs on structural identifiability
              </a>
            </td>
          <td>
            Jessica R Conrad, Marisa C Eisenberg
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Deep Operator Networks (DeepONets) and their physics-informed variants have shown significant promise in learning mappings between function spaces of partial differential equations, enhancing the generalization of traditional neural networks. However, for highly nonlinear real-world applications like aerospace composites processing, existing models often fail to capture underlying solutions accurately and are typically limited to single input functions, constraining rapid process design development. This paper introduces an advanced physics-informed DeepONet tailored for such complex systems with multiple input functions. Equipped with architectural enhancements like nonlinear decoders and effective training strategies such as curriculum learning and domain decomposition, the proposed model handles high-dimensional design spaces with significantly improved accuracy, outperforming the vanilla physics-informed DeepONet by two orders of magnitude. Its zero-shot prediction capability across a broad design space makes it a powerful tool for accelerating composites process design and optimization, with potential applications in other engineering fields characterized by strong nonlinearity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f126a96d65c71da6172a1c8961580b6248f14d65" target='_blank'>
              An Advanced Physics-Informed Neural Operator for Comprehensive Design Optimization of Highly-Nonlinear Systems: An Aerospace Composites Processing Case Study
              </a>
            </td>
          <td>
            Milad Ramezankhani, A. Deodhar, Rishi Parekh, Dagnachew Birru
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="This paper presents the open-source stochastic model predictive control framework GRAMPC-S for nonlinear uncertain systems with chance constraints. It provides several uncertainty propagation methods to predict stochastic moments of the system state and can consider unknown parts of the system dynamics using Gaussian process regression. These methods are used to reformulate a stochastic MPC formulation as a deterministic one that is solved with GRAMPC. The performance of the presented framework is evaluated using examples from a wide range of technical areas. The experimental evaluation shows that GRAMPC-S can be used in practice for the control of nonlinear uncertain systems with sampling times in the millisecond range.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c62a7baf784d003dda62b0480d24a61f0d5beb4" target='_blank'>
              A software framework for stochastic model predictive control of nonlinear continuous-time systems (GRAMPC-S)
              </a>
            </td>
          <td>
            D. Landgraf, Andreas Volz, Knut Graichen
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this study, we present a novel non-intrusive reduced-order model (ROM) for solving time-dependent stochastic partial differential equations (SPDEs). Utilizing proper orthogonal decomposition (POD), we extract spatial modes from high-fidelity solutions. A dynamic mode decomposition (DMD) method is then applied to vertically stacked matrices of projection coefficients for future prediction of coefficient fields. Polynomial chaos expansion (PCE) is employed to construct a mapping from random parameter inputs to the DMD-predicted coefficient field. These lead to the POD-DMD-PCE method. The innovation lies in vertically stacking projection coefficients, ensuring time-dimensional consistency in the coefficient matrix for DMD and facilitating parameter integration for PCE analysis. This method combines the model reduction of POD with the time extrapolation strengths of DMD, effectively recovering field solutions both within and beyond the training time interval. The efficiency and time extrapolation capabilities of the proposed method are validated through various nonlinear SPDEs. These include a reaction-diffusion equation with 19 parameters, a two-dimensional heat equation with two parameters, and a one-dimensional Burgers equation with three parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f03b1ec02154fef08c7c119096855e273290c56c" target='_blank'>
              Non-intrusive reduced-order model for time-dependent stochastic partial differential equations utilizing dynamic mode decomposition and polynomial chaos expansion.
              </a>
            </td>
          <td>
            Shuman Wang, Afshan Batool, Xiang Sun, Xiaomin Pan
          </td>
          <td>2024-07-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Data-enabled predictive control (DeePC) for linear systems utilizes data matrices of recorded trajectories to directly predict new system trajectories, which is very appealing for real-life applications. In this paper we leverage the universal approximation properties of neural networks (NNs) to develop neural DeePC algorithms for nonlinear systems. Firstly, we point out that the outputs of the last hidden layer of a deep NN implicitly construct a basis in a so-called neural (feature) space, while the output linear layer performs affine interpolation in the neural space. As such, we can train off-line a deep NN using large data sets of trajectories to learn the neural basis and compute on-line a suitable affine interpolation using DeePC. Secondly, methods for guaranteeing consistency of neural DeePC and for reducing computational complexity are developed. Several neural DeePC formulations are illustrated on a nonlinear pendulum example.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9e9290726d4b4f1bdd4194482dd006f7ef5fa49f" target='_blank'>
              Neural Data-Enabled Predictive Control
              </a>
            </td>
          <td>
            Mircea Lazar
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We provide an algorithm for the simultaneous system identification and model predictive control of nonlinear systems. The algorithm has finite-time near-optimality guarantees and asymptotically converges to the optimal (non-causal) controller. Particularly, the algorithm enjoys sublinear dynamic regret, defined herein as the suboptimality against an optimal clairvoyant controller that knows how the unknown disturbances and system dynamics will adapt to its actions. The algorithm is self-supervised and applies to control-affine systems with unknown dynamics and disturbances that can be expressed in reproducing kernel Hilbert spaces. Such spaces can model external disturbances and modeling errors that can even be adaptive to the system's state and control input. For example, they can model wind and wave disturbances to aerial and marine vehicles, or inaccurate model parameters such as inertia of mechanical systems. The algorithm first generates random Fourier features that are used to approximate the unknown dynamics or disturbances. Then, it employs model predictive control based on the current learned model of the unknown dynamics (or disturbances). The model of the unknown dynamics is updated online using least squares based on the data collected while controlling the system. We validate our algorithm in both hardware experiments and physics-based simulations. The simulations include (i) a cart-pole aiming to maintain the pole upright despite inaccurate model parameters, and (ii) a quadrotor aiming to track reference trajectories despite unmodeled aerodynamic drag effects. The hardware experiments include a quadrotor aiming to track a circular trajectory despite unmodeled aerodynamic drag effects, ground effects, and wind disturbances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f6fdbd86334d255c0b4f1f232d3ee77ab3286d0f" target='_blank'>
              Simultaneous System Identification and Model Predictive Control with No Dynamic Regret
              </a>
            </td>
          <td>
            Hongyu Zhou, Vasileios Tzoumas
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/233d8ac0f20cf948548fc9238018c681a00d4cec" target='_blank'>
              Structure-preserving formulations for data-driven analysis of coupled multi-physics systems
              </a>
            </td>
          <td>
            Alba Muixí, David González, Francisco Chinesta, Elías Cueto
          </td>
          <td>2024-06-07</td>
          <td>Computational Mechanics</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="The contributions of this technical note are twofold. Firstly, we formulate an optimization problem to obtain a linear representation of a nonlinear vector field based on a system's trajectory. We also prove that its cost function is strictly convex, given the trajectory is persistently exciting. Under certain observability conditions, we provide results that guarantee the Hurwitz stability of the global minimizer. Secondly, we present a novel algorithm based on point-wise geometric flows to estimate the boundary of the region of attraction. We show that the algorithm converges to the exact boundary of the region of attraction under certain assumptions on the system dynamics. Finally, we validate the results using simulations on various nonlinear autonomous systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f60368ae370b28f9ac8a52aafa0723b5154ed62" target='_blank'>
              Analog Data-Driven Theory and Estimation of the Region of Attraction Using Sampled-Data
              </a>
            </td>
          <td>
            Karthik Shenoy, Arvind Ragghav, V. Chellaboina
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="This paper aims to construct a representation to approximate the behavior of linear time-invariant systems from a large data set whose input contains unmeasured uncertainty and output is subject to measurement noise. Using big data combined with the statistical properties of the input uncertainty and measurement noise, the covariance of the input uncertainty and the output can be approximated. This enables the construction of an approximate covariance of the trajectories in the data set, through which a representation for the approximation of system behavior is obtained. The behavior of this representation is shown to converge in probability to the true behavior. An illustrative example is provided to show that the proposed representation is able to predict the system trajectory to a satisfactory accuracy. The result of this paper provides a potential basis for the development of data-based trajectory estimation and predictive control algorithm when the system input uncertainty is unmeasured.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/963fe53a6065bfdb118e3128347e8c5bfc500a1d" target='_blank'>
              Approximating the System Behavior with Input Uncertainty Using Big Data
              </a>
            </td>
          <td>
            Yitao Yan, Jie Bao, Biao Huang
          </td>
          <td>2024-06-18</td>
          <td>2024 IEEE 18th International Conference on Control & Automation (ICCA)</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this paper, we introduce semi-autonomous neural ordinary differential equations (SA-NODEs), a variation of the vanilla NODEs, employing fewer parameters. We investigate the universal approximation properties of SA-NODEs for dynamical systems from both a theoretical and a numerical perspective. Within the assumption of a finite-time horizon, under general hypotheses we establish an asymptotic approximation result, demonstrating that the error vanishes as the number of parameters goes to infinity. Under additional regularity assumptions, we further specify this convergence rate in relation to the number of parameters, utilizing quantitative approximation results in the Barron space. Based on the previous result, we prove an approximation rate for transport equations by their neural counterparts. Our numerical experiments validate the effectiveness of SA-NODEs in capturing the dynamics of various ODE systems and transport equations. Additionally, we compare SA-NODEs with vanilla NODEs, highlighting the superior performance and reduced complexity of our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0c428906ea1c6ad6b5b45bc8a6ac5d654dad8749" target='_blank'>
              Universal Approximation of Dynamical Systems by Semi-Autonomous Neural ODEs and Applications
              </a>
            </td>
          <td>
            Ziqian Li, Kang Liu, Lorenzo Liverani, Enrique Zuazua
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Given the complexity and nonlinearity inherent in traffic dynamics within vehicular platoons, there exists a critical need for a modeling methodology with high accuracy while concurrently achieving physical analyzability. Currently, there are two predominant approaches: the physics model-based approach and the Artificial Intelligence (AI)--based approach. Knowing the facts that the physical-based model usually lacks sufficient modeling accuracy and potential function mismatches and the pure-AI-based method lacks analyzability, this paper innovatively proposes an AI-based Koopman approach to model the unknown nonlinear platoon dynamics harnessing the power of AI and simultaneously maintain physical analyzability, with a particular focus on periods of traffic oscillation. Specifically, this research first employs a deep learning framework to generate the embedding function that lifts the original space into the embedding space. Given the embedding space descriptiveness, the platoon dynamics can be expressed as a linear dynamical system founded by the Koopman theory. Based on that, the routine of linear dynamical system analysis can be conducted on the learned traffic linear dynamics in the embedding space. By that, the physical interpretability and analyzability of model-based methods with the heightened precision inherent in data-driven approaches can be synergized. Comparative experiments have been conducted with existing modeling approaches, which suggests our method's superiority in accuracy. Additionally, a phase plane analysis is performed, further evidencing our approach's effectiveness in replicating the complex dynamic patterns. Moreover, the proposed methodology is proven to feature the capability of analyzing the stability, attesting to the physical analyzability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/61de6c4570b16ba2cf34ebf49465dfa322f09fda" target='_blank'>
              Physically Analyzable AI-Based Nonlinear Platoon Dynamics Modeling During Traffic Oscillation: A Koopman Approach
              </a>
            </td>
          <td>
            Kexin Tian, Haotian Shi, Yang Zhou, Sixu Li
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="We report synthesis of an open source test data set and work in progress to expand it. Sophisticated data mining and Machine Learning (ML) techniques can discover statistical associations among variables that may or may not reflect actual causal dependencies. In many applications, systems must discriminate between associations that are mere coincidences and those that are at least plausibly causal. Further, a graph of causal relationships may be complex, with fan-in, fan-out, transitive, and various combinations of, dependencies. To test a system’s power to filter out non-causal associations and untangle the causal web, suitable synthetic data is needed. We report the development, in Wolfram Mathematica, of code that synthesizes data with subtle, complex, causal dependencies among some but not all of the generated observable variables. We implement several simple dissipative chaotic flows. Four (4) are autonomous, six (6) are driven. Among the resulting ten (10) observable state vectors, there are forty-five (45) potential pairwise (1:1) relationships, of which four (4) are strong, five (5) are moderate, three (3) are weak, for a total of twelve (12) that are actually causal, and any others are mere statistical artifacts that a tool under test should reject. Each system’s observables are corrupted by additive Gaussian noise. Each system’s hidden dynamics are disturbed by a normal Wiener process. The levels of these stochastic components are parameterized to make problem difficulty tunable. A set of generated data and code for generating more will be released openly on-line.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c21bbba8736b5afd19182b829dfd6049fad4e46" target='_blank'>
              Coupled dynamical systems data for testing detection of subtle causal dependencies
              </a>
            </td>
          <td>
            Stuart W. Card
          </td>
          <td>2024-06-07</td>
          <td>None</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="The advancement of scientific machine learning (ML) techniques has led to the development of methods for approximating solutions to nonlinear partial differential equations (PDE) with increased efficiency and accuracy. Automatic differentiation has played a pivotal role in this progress, enabling the creation of physics-informed neural networks (PINN) that integrate relevant physics into machine learning models. PINN have shown promise in approximating the solutions to the Navier–Stokes equations, overcoming the limitations of traditional numerical discretization methods. However, challenges such as local minima and long training times persist, motivating the exploration of domain decomposition techniques to improve it. Previous domain decomposition models have introduced spatial and temporal domain decompositions but have yet to fully address issues of smoothness and regularity of global solutions. In this study, we present a novel domain decomposition approach for PINN, termed domain-discretized PINN (DD-PINN), which incorporates complementary loss functions, subdomain-specific transformer networks (TRF), and independent optimization within each subdomain. By enforcing continuity and differentiability through interface constraints and leveraging the Sobolev (H 1) norm of the mean squared error (MSE), rather than the Euclidean norm (L 2), DD-PINN enhances solution regularity and accuracy. The inclusion of TRF in each subdomain facilitates feature extraction and improves convergence rates, as demonstrated through simulations of threetest problems: steady-state flow in a two-dimensional lid-driven cavity, the time-dependent cylinder wake, and the viscous Burgers equation. Numerical comparisons highlight the effectiveness of DD-PINN in preserving global solution regularity and accurately approximating complex phenomena, marking a significant advancement over previous domain decomposition methods within the PINN framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ea15c09f70a4fe243856a69a988cd4b37e7e4a11" target='_blank'>
              A novel discretized physics-informed neural network model applied to the Navier–Stokes equations
              </a>
            </td>
          <td>
            Amirhossein Khademi, Steven Dufour
          </td>
          <td>2024-06-07</td>
          <td>Physica Scripta</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper investigates the dynamics of cosmological models incorporating nonlinear electrodynamics (NLED), focusing on their stability and causality. We explore two specific NLED models: the Power-Law and Rational Lagrangians. We assess these models' viability in describing the universe's evolution using dynamical systems theory and Bayesian inference. We present the theoretical framework of NLED coupled with general relativity, followed by an analysis of the stability and causality through the squared sound speed of the NLED Lagrangians. We then conduct a detailed dynamical analysis to identify the universe's evolution with this matter content. Our results show that the Power-Law Lagrangian model transitions through various cosmological phases from a Maxwell radiation-dominated state to a matter-dominated state. For the Rational Lagrangian model, including the Maxwell term, stable and causal behavior is observed within specific parameter ranges, with critical points indicating the evolutionary pathways of the universe. To validate our theoretical findings, we perform Bayesian parameter estimation using a comprehensive set of observational data, including cosmic chronometers, Baryon Acoustic Oscillation (BAO) measurements, and Type Ia Supernovae (SNeIa). The estimated parameters for both models align with expected values for the current universe, particularly the matter density $\Omega_m$ and the Hubble parameter $h$. However, the parameters $\alpha$ and $b$ are not tightly constrained within the prior ranges. Our model comparison strongly favors the $\Lambda$CDM model over the NLED models for late-universe observations, as the NLED model does not exhibit a cosmological constant behavior. Our results highlight the need for further refinement and exploration of NLED-based cosmological models to fully integrate them into the standard cosmological framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/530617a7cd0b3388c0d7af097d46362991b5398b" target='_blank'>
              Exploring the Evolution of Nonlinear Electrodynamics in the Universe: A Dynamical Systems Approach
              </a>
            </td>
          <td>
            Ricardo Garc'ia-Salcedo, Isidro G'omez-Vargas, Tam'e Gonz'alez, Vicent Martinez-Badenes, Israel Quiros
          </td>
          <td>2024-06-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Matrix evolution equations occur in many applications, such as dynamical Lyapunov/Sylvester systems or Riccati equations in optimization and stochastic control, machine learning or data assimilation. In many cases, their tightest stability condition is coming from a linear term. Exponential time differencing (ETD) is known to produce highly stable numerical schemes by treating the linear term in an exact fashion. In particular, for stiff problems, ETD methods are a method of choice. We propose an extension of the class of ETD algorithms to matrix-valued dynamical equations. This allows us to produce highly efficient and stable integration schemes. We show their efficiency and applicability for a variety of real-world problems, from geophysical applications to dynamical problems in machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f314941ad68915d7af43429006a76cffcf6ab2db" target='_blank'>
              Exponential time differencing for matrix-valued dynamical systems
              </a>
            </td>
          <td>
            Nayef Shkeir, Tobias Schäfer, T. Grafke
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Predicting chaotic dynamical systems is critical in many scientific fields such as weather prediction, but challenging due to the characterizing sensitive dependence on initial conditions. Traditional modeling approaches require extensive domain knowledge, often leading to a shift towards data-driven methods using machine learning. However, existing research provides inconclusive results on which machine learning methods are best suited for predicting chaotic systems. In this paper, we compare different lightweight and heavyweight machine learning architectures using extensive existing databases, as well as a newly introduced one that allows for uncertainty quantification in the benchmark results. We perform hyperparameter tuning based on computational cost and introduce a novel error metric, the cumulative maximum error, which combines several desirable properties of traditional metrics, tailored for chaotic systems. Our results show that well-tuned simple methods, as well as untuned baseline methods, often outperform state-of-the-art deep learning models, but their performance can vary significantly with different experimental setups. These findings underscore the importance of matching prediction methods to data characteristics and available computational resources.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5acbbc502b5f063b4661a749a95f9833ab9d11c1" target='_blank'>
              Machine Learning for predicting chaotic systems
              </a>
            </td>
          <td>
            Christof Schötz, Alistair White, Maximilian Gelbrecht, Niklas Boers
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="With the rapid development of articial intelligence, especially deep learning technology, scientic researchers have begun to explore its application in the eld of traditional scientic computing. Traditional scientic computing relies on mathematical equations to describe and predict the scientic laws of nature, while deep learning provides a new perspective to solve complex mathematical problems by learning patterns in data. The introduction of the Physical Information Neural Network (PINN) and the Ordinary Dierential Equation (ODENet) network layer enables deep learning technology to more accurately simulate and predict scientic phenomena. This study shows that by embedding an ODE-Net network layer in a physical information neural network (PINN), the tting accuracy and generalization performance of the model can be signicantly improved. Experimental results show that compared with traditional numerical methods and fully connected neural networks, this model combined with deep learning technology not only shows higher accuracy when solving partial dierential equations, but also exhibits faster convergence speed and stronger adaptability. These ndings not only promote the integration of scientic computing and deep learning, but also provide new research directions and practical strategies for using deep learning technology to solve complex scientic problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dedb6a263e8acff267028f948e0fbc5cc501e50c" target='_blank'>
              A numerical method to solve PDE through PINN based on ODENet
              </a>
            </td>
          <td>
            Ziyi Wang
          </td>
          <td>2024-06-24</td>
          <td>Applied and Computational Engineering</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present ConDiff, a novel dataset for scientific machine learning. ConDiff focuses on the diffusion equation with varying coefficients, a fundamental problem in many applications of parametric partial differential equations (PDEs). The main novelty of the proposed dataset is that we consider discontinuous coefficients with high contrast. These coefficient functions are sampled from a selected set of distributions. This class of problems is not only of great academic interest, but is also the basis for describing various environmental and industrial problems. In this way, ConDiff shortens the gap with real-world problems while remaining fully synthetic and easy to use. ConDiff consists of a diverse set of diffusion equations with coefficients covering a wide range of contrast levels and heterogeneity with a measurable complexity metric for clearer comparison between different coefficient functions. We baseline ConDiff on standard deep learning models in the field of scientific machine learning. By providing a large number of problem instances, each with its own coefficient function and right-hand side, we hope to encourage the development of novel physics-based deep learning approaches, such as neural operators and physics-informed neural networks, ultimately driving progress towards more accurate and efficient solutions of complex PDE problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5b93a26257c0ccfa34050385c31006a18b0c2538" target='_blank'>
              ConDiff: A Challenging Dataset for Neural Solvers of Partial Differential Equations
              </a>
            </td>
          <td>
            Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, I. Oseledets, Ekaterina A. Muravleva
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Studies show that artificial intelligence (AI) with embedded physics solvers has improved the accuracy of predictions on various physics problems, especially those associated with fluid dynamics. The crucial element in optimizing weight training for estimating flow fields within the AI network lies in the choice of the loss function. In addressing regression-type problems, particularly those involving the temporal evolution of flow fields, the mean square error (MSE) loss function is commonly employed at the current and single time step. However, an issue arises in existing methodologies that utilize MSE-based loss functions with single-time step information for predicting unsteady flow. Most of these approaches overlook the significance of incorporating the temporal history of the flow, a factor that cannot be disregarded in the context of numerical solvers. Hence, in this work, a physics-based AI (PbAI) method with higher-order loss functions is applied to unsteady scenarios, in particular to two distinct turbulent flows where a multitude of fine structures is present, namely, forced and decaying turbulence. Direct numerical simulations on uniform Cartesian grids are conducted to simulate these scenarios, generating two distinct datasets for training and inference. Each dataset comprises 32 randomly initialized conditions spanning 4, 848 time steps for each turbulent flow type. Five distinct models are devised, incorporating features such as rollouts from coarse numerical solvers and temporal considerations in the loss function calculation. The constructed PbAI models demonstrate consistent improvements in predictive performance over the entire temporal domain. These findings are further corroborated through vorticity correlation analyses. The empirical result demonstrates that the accuracy of the baseline case improves by up to 48% and 30% for forced and decaying turbulence, respectively. These results significantly underscore the importance of the temporal histories of flow in the loss function in enhancing predictive capabilities for complex and unsteady turbulent flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6a508269bfb08f5c8c66413efa4d444da72838f" target='_blank'>
              Multi-Order Loss Functions For Accelerating Unsteady Flow Simulations with Physics-Based AI
              </a>
            </td>
          <td>
            Wei Xian Lim, Naheed Anjum Arafat, Wai Lee Chan, Adams Kong
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id=""AI for Science"aims to solve fundamental scientific problems using AI techniques. As most physical phenomena can be described as Partial Differential Equations (PDEs) , approximating their solutions using neural networks has evolved as a central component of scientific-ML. Physics-Informed Neural Networks (PINNs) is the general method that has evolved for this task but its training is well-known to be very unstable. In this work we explore the possibility of changing the model being trained from being just a neural network to being a non-linear transformation of it - one that algebraically includes the boundary/initial conditions. This reduces the number of terms in the loss function than the standard PINN losses. We demonstrate that our modification leads to significant performance gains across a range of benchmark tasks, in various dimensions and without having to tweak the training algorithm. Our conclusions are based on conducting hundreds of experiments, in the fully unsupervised setting, over multiple linear and non-linear PDEs set to exactly solvable scenarios, which lends to a concrete measurement of our performance gains in terms of order(s) of magnitude lower fractional errors being achieved, than by standard PINNs. The code accompanying this manuscript is publicly available at, https://github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/50ec057bbac038765a71bbcf35743418ac73a1d3" target='_blank'>
              Improving PINNs By Algebraic Inclusion of Boundary and Initial Conditions
              </a>
            </td>
          <td>
            Mohan Ren, Zhihao Fang, Keren Li, Anirbit Mukherjee
          </td>
          <td>2024-07-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We previously presented a novel loss formulation for efficient learning of complex dynamics from governing physics, typically described by partial differential equations (PDEs), using physics-informed neural networks (PINNs). In these experiments, the incorporation of a Boundary Connectivity (BCXN) loss function was shown to greatly improve physics-informed learning across many problems, especially those with complex geometries. However, this imposition may not always be ideal, as the previously presented BCXN-loss strongly enforces a linear local structure at the boundary. While this assumption helps facilitate faster learning with an order of magnitude sparser training samples, this can adversely impact convergence in other situations. Hence, we propose a modification of this BCXN-loss that reduces the imposed structure to a soft constraint, allowing for more flexible learning and convergence. We further demonstrate the potential for this method to improve the convergence and performance of LSA-PINN across additional numerical experiments, with much smaller errors than existing methods in terms of the standard L2-norm metric. In particular, we have applied this method to the modelling of flow past complex geometries such as airfoils, which serve as the basic building block for various applications in fluid dynamics and renewable energy (e.g., in wind and tidal turbine design).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/535c64c537da066482154d77b3dfa4334e26bb4e" target='_blank'>
              Soft Constraint in Local Structure Approximation-PINN
              </a>
            </td>
          <td>
            Jian Cheng Wong, P. Chiu, C. Ooi, M. Dao
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We present a neural operator architecture to simulate Lagrangian dynamics, such as fluid flow, granular flows, and elastoplasticity. Traditional numerical methods, such as the finite element method (FEM), suffer from long run times and large memory consumption. On the other hand, approaches based on graph neural networks are faster but still suffer from long computation times on dense graphs, which are often required for high-fidelity simulations. Our model, GIOROM or Graph Interaction Operator for Reduced-Order Modeling, learns temporal dynamics within a reduced-order setting, capturing spatial features from a highly sparse graph representation of the input and generalizing to arbitrary spatial locations during inference. The model is geometry-aware and discretization-agnostic and can generalize to different initial conditions, velocities, and geometries after training. We show that point clouds of the order of 100,000 points can be inferred from sparse graphs with $\sim$1000 points, with negligible change in computation time. We empirically evaluate our model on elastic solids, Newtonian fluids, Non-Newtonian fluids, Drucker-Prager granular flows, and von Mises elastoplasticity. On these benchmarks, our approach results in a 25$\times$ speedup compared to other neural network-based physics simulators while delivering high-fidelity predictions of complex physical systems and showing better performance on most benchmarks. The code and the demos are provided at https://github.com/HrishikeshVish/GIOROM.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b0931d26d7d6b637fece82e4653a754d6a28f5dc" target='_blank'>
              Reduced-Order Neural Operators: Learning Lagrangian Dynamics on Highly Sparse Graphs
              </a>
            </td>
          <td>
            Hrishikesh Viswanath, Yue Chang, Julius Berner, Peter Yichen Chen, Aniket Bera
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="We present a domain decomposition strategy for developing structure-preserving finite element discretizations from data when exact governing equations are unknown. On subdomains, trainable Whitney form elements are used to identify structure-preserving models from data, providing a Dirichlet-to-Neumann map which may be used to globally construct a mortar method. The reduced-order local elements may be trained offline to reproduce high-fidelity Dirichlet data in cases where first principles model derivation is either intractable, unknown, or computationally prohibitive. In such cases, particular care must be taken to preserve structure on both local and mortar levels without knowledge of the governing equations, as well as to ensure well-posedness and stability of the resulting monolithic data-driven system. This strategy provides a flexible means of both scaling to large systems and treating complex geometries, and is particularly attractive for multiscale problems with complex microstructure geometry. While consistency is traditionally obtained in finite element methods via quasi-optimality results and the Bramble-Hilbert lemma as the local element diameter $h\rightarrow0$, our analysis establishes notions of accuracy and stability for finite h with accuracy coming from matching data. Numerical experiments and analysis establish properties for $H(\operatorname{div})$ problems in small data limits ($\mathcal{O}(1)$ reference solutions).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5e40b79cf0d6051174cb4015baf4f92afc84afb1" target='_blank'>
              A Structure-Preserving Domain Decomposition Method for Data-Driven Modeling
              </a>
            </td>
          <td>
            Shuai Jiang, Jonas A. Actor, Scott A. Roberts, Nathaniel Trask
          </td>
          <td>2024-06-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In the context of proxy modeling for process systems, traditional data-driven deep learning approaches frequently encounter significant challenges, such as substantial training costs induced by large amounts of data, and limited generalization capabilities. As a promising alternative, physics-aware models incorporate partial physics knowledge to ameliorate these challenges. Although demonstrating efficacy, they fall short in terms of exploration depth and universality. To address these shortcomings, we introduce a physics-aware proxy model (PAPM) that fully incorporates partial prior physics of process systems, which includes multiple input conditions and the general form of conservation relations, resulting in better out-of-sample generalization. Additionally, PAPM contains a holistic temporal-spatial stepping module for flexible adaptation across various process systems. Through systematic comparisons with state-of-the-art pure data-driven and physics-aware models across five two-dimensional benchmarks in nine generalization tasks, PAPM notably achieves an average performance improvement of 6.7%, while requiring fewer FLOPs, and just 1% of the parameters compared to the prior leading method. The code is available at https://github.com/pengwei07/PAPM.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/168b93840ae25e4ba1a3d7b19572fb125f6bb72a" target='_blank'>
              PAPM: A Physics-aware Proxy Model for Process Systems
              </a>
            </td>
          <td>
            Pengwei Liu, Zhongkai Hao, Xingyu Ren, Hangjie Yuan, Jiayang Ren, Dong Ni
          </td>
          <td>2024-07-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Physics-informed neural networks (PINNs) have shown promising potential for solving partial differential equations (PDEs) using deep learning. 

However, PINNs face training difficulties for evolutionary PDEs, particularly for dynamical systems whose solutions exhibit multi-scale or turbulent behavior over time.

The reason is that PINNs may violate the temporal causality property since all the temporal features in the PINNs loss are trained simultaneously. 

This paper proposes to use implicit time differencing schemes to enforce temporal causality, and use transfer learning to sequentially update the PINNs in space as surrogates for PDE solutions in different time frames.

The evolving PINNs are better able to capture the varying complexities of the evolutionary equations, while only requiring minor updates between adjacent time frames.

Our method is theoretically proven to be convergent if the time step is small and each PINN in different time frames is well-trained.

In addition, we provide state-of-the-art (SOTA) numerical results for a variety of benchmarks for which existing PINNs formulations may fail or be inefficient.

We demonstrate that the proposed method improves the accuracy of PINNs approximation for evolutionary PDEs and improves efficiency by a factor of 4–40x.

The code is available at https://github.com/SiqiChen9/TL-DPINNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2baf92352b6cdc0a229e693600511b572a32085f" target='_blank'>
              Causality-enhanced Discreted Physics-informed Neural Networks for Predicting Evolutionary Equations
              </a>
            </td>
          <td>
            Ye Li, Siqi Chen, Bin Shan, Sheng-Jun Huang
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="One of the most promising applications of machine learning (ML) in computational physics is to accelerate the solution of partial differential equations (PDEs). The key objective of ML-based PDE solvers is to output a sufficiently accurate solution faster than standard numerical methods, which are used as a baseline comparison. We first perform a systematic review of the ML-for-PDE solving literature. Of articles that use ML to solve a fluid-related PDE and claim to outperform a standard numerical method, we determine that 79% (60/76) compare to a weak baseline. Second, we find evidence that reporting biases, especially outcome reporting bias and publication bias, are widespread. We conclude that ML-for-PDE solving research is overoptimistic: weak baselines lead to overly positive results, while reporting biases lead to underreporting of negative results. To a large extent, these issues appear to be caused by factors similar to those of past reproducibility crises: researcher degrees of freedom and a bias towards positive results. We call for bottom-up cultural changes to minimize biased reporting as well as top-down structural reforms intended to reduce perverse incentives for doing so.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fda0812099547cd3b91031851f644e1929b4b77c" target='_blank'>
              Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations
              </a>
            </td>
          <td>
            N. McGreivy, Ammar Hakim
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In this letter we investigate an information theoretic analogue of the classic two masses on spring system, arising from a physical interpretation of Friston's free energy principle in the theory of learning in a system of agents. Using methods from classical mechanics on manifolds, we define a kinetic energy term using the Fisher metric on distributions and a potential energy function defined in terms of stress on the agents' beliefs. The resulting Lagrangian (Hamiltonian) produces a variation of the classic DeGroot dynamics. In the two agent case, the potential function is defined using the Jeffrey's divergence and the resulting dynamics are characterized by a non-linear spring. These dynamics produce trajectories that resemble flows on tori but are shown numerically to produce chaos near the boundary of the space. We then investigate persuasion as an information theoretic control problem where analysis indicates that manipulating peer pressure with a fixed target is a more stable approach to altering an agent's belief than providing a slowly changing belief state that approaches the target.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/52c8e6266ddf04fc931ce8e2677ea1562dda36dc" target='_blank'>
              Dynamics of An Information Theoretic Analog of Two Masses on a Spring
              </a>
            </td>
          <td>
            Geoff Goehle, Christopher Griffin
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c78e8674f4b4b137f15f996d2bc27957f1fe2af7" target='_blank'>
              Optimal Reconstruction of Vector Fields from Data for Prediction and Uncertainty Quantification
              </a>
            </td>
          <td>
            Sean P. McGowan, William S. P. Robertson, Chantelle Blachut, Sanjeeva Balasuriya
          </td>
          <td>2024-06-13</td>
          <td>J. Nonlinear Sci.</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Deep neural networks (DNNs) have achieved exceptional performance across various fields by learning complex nonlinear mappings from large-scale datasets. However, they encounter challenges such as high computational costs and limited interpretability. To address these issues, hybrid approaches that integrate physics with AI are gaining interest. This paper introduces a novel physics-based AI model called the"Nonlinear Schr\"odinger Network", which treats the Nonlinear Schr\"odinger Equation (NLSE) as a general-purpose trainable model for learning complex patterns including nonlinear mappings and memory effects from data. Existing physics-informed machine learning methods use neural networks to approximate the solutions of partial differential equations (PDEs). In contrast, our approach directly treats the PDE as a trainable model to obtain general nonlinear mappings that would otherwise require neural networks. As a type of physics-AI symbiosis, it offers a more interpretable and parameter-efficient alternative to traditional black-box neural networks, achieving comparable or better accuracy in some time series classification tasks while significantly reducing the number of required parameters. Notably, the trained Nonlinear Schr\"odinger Network is interpretable, with all parameters having physical meanings as properties of a virtual physical system that transforms the data to a more separable space. This interpretability allows for insight into the underlying dynamics of the data transformation process. Applications to time series forecasting have also been explored. While our current implementation utilizes the NLSE, the proposed method of using physics equations as trainable models to learn nonlinear mappings from data is not limited to the NLSE and may be extended to other master equations of physics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/86e993b6260a5161e12c46644b62f8d53d34dba1" target='_blank'>
              Nonlinear Schr\"odinger Network
              </a>
            </td>
          <td>
            Yiming Zhou, Callen MacPhee, Tingyi Zhou, Bahram Jalali
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In dynamical systems reconstruction (DSR) we seek to infer from time series measurements a generative model of the underlying dynamical process. This is a prime objective in any scientific discipline, where we are particularly interested in parsimonious models with a low parameter load. A common strategy here is parameter pruning, removing all parameters with small weights. However, here we find this strategy does not work for DSR, where even low magnitude parameters can contribute considerably to the system dynamics. On the other hand, it is well known that many natural systems which generate complex dynamics, like the brain or ecological networks, have a sparse topology with comparatively few links. Inspired by this, we show that geometric pruning, where in contrast to magnitude-based pruning weights with a low contribution to an attractor's geometrical structure are removed, indeed manages to reduce parameter load substantially without significantly hampering DSR quality. We further find that the networks resulting from geometric pruning have a specific type of topology, and that this topology, and not the magnitude of weights, is what is most crucial to performance. We provide an algorithm that automatically generates such topologies which can be used as priors for generative modeling of dynamical systems by RNNs, and compare it to other well studied topologies like small-world or scale-free networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7262300a7dc61b5b9020753dc9d2bbd63a06cc2" target='_blank'>
              Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction
              </a>
            </td>
          <td>
            Christoph Jurgen Hemmer, Manuel Brenner, Florian Hess, Daniel Durstewitz
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This paper addresses the need for deep learning models to integrate well-defined constraints into their outputs, driven by their application in surrogate models, learning with limited data and partial information, and scenarios requiring flexible model behavior to incorporate non-data sample information. We introduce Bayesian Entropy Neural Networks (BENN), a framework grounded in Maximum Entropy (MaxEnt) principles, designed to impose constraints on Bayesian Neural Network (BNN) predictions. BENN is capable of constraining not only the predicted values but also their derivatives and variances, ensuring a more robust and reliable model output. To achieve simultaneous uncertainty quantification and constraint satisfaction, we employ the method of multipliers approach. This allows for the concurrent estimation of neural network parameters and the Lagrangian multipliers associated with the constraints. Our experiments, spanning diverse applications such as beam deflection modeling and microstructure generation, demonstrate the effectiveness of BENN. The results highlight significant improvements over traditional BNNs and showcase competitive performance relative to contemporary constrained deep learning methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4d74383a8ad720e106440e126defbad7231d316d" target='_blank'>
              Bayesian Entropy Neural Networks for Physics-Aware Prediction
              </a>
            </td>
          <td>
            R. Rathnakumar, Jiayu Huang, Hao Yan, Yongming Liu
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Many dynamical systems are subjected to stochastic influences, such as random excitations, noise, and unmodeled behavior. Tracking the system's state and parameters based on a physical model is a common task for which filtering algorithms, such as Kalman filters and their non-linear extensions, are typically used. However, many of these filters use assumptions on the transition probabilities or the covariance model, which can lead to inaccuracies in non-linear systems. We will show the application of a stochastic coupling filter that can approximate arbitrary transition densities under non-Gaussian noise. The filter is based on transport maps, which couple the approximation densities to a user-chosen reference density, allowing for straightforward sampling and evaluation of probabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/674a1134430bb9d26de4b9e60e39258373cc6b87" target='_blank'>
              Transport Map Coupling Filter for State-Parameter Estimation
              </a>
            </td>
          <td>
            J. Grashorn, M. Broggi, Ludovic Chamoin, Michael Beer
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Modeling real-world problems with partial differential equations (PDEs) is a prominent topic in scientific machine learning. Classic solvers for this task continue to play a central role, e.g. to generate training data for deep learning analogues. Any such numerical solution is subject to multiple sources of uncertainty, both from limited computational resources and limited data (including unknown parameters). Gaussian process analogues to classic PDE simulation methods have recently emerged as a framework to construct fully probabilistic estimates of all these types of uncertainty. So far, much of this work focused on theoretical foundations, and as such is not particularly data efficient or scalable. Here we propose a framework combining a discretization scheme based on the popular Finite Volume Method with complementary numerical linear algebra techniques. Practical experiments, including a spatiotemporal tsunami simulation, demonstrate substantially improved scaling behavior of this approach over previous collocation-based techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f405569ff0b7887d7f887ed428142bf13a634400" target='_blank'>
              Scaling up Probabilistic PDE Simulators with Structured Volumetric Information
              </a>
            </td>
          <td>
            Tim Weiland, Marvin Pförtner, Philipp Hennig
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="A dynamical system may be defined by a simple transition law - such as a map or a vector field. Most learning techniques primarily try to recreate the dynamic evolution law. This is a major shortcoming, as most dynamic properties of interest are asymptotic properties such as an attractor or invariant measure. One of the major theoretical challenges for numerical methods is that approximating the dynamical law may not be sufficient to approximate these asymptotic properties. This article presents a method of representing a discrete-time deterministic dynamical system as a Markov process. The procedure is completely data-driven. The technique is proved to be convergent -- the stationary density of the Markov process has a support that converges to the targeted invariant set. Thus invariant sets of arbitrary dynamical systems, even with complicated non-smooth topology, can be approximated by this technique. Under further assumptions of stochastic stability of the targeted system, the technique is also shown to provide a convergent statistical approximation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7a9c8ed7750a7d4d14152ec7fe7c869a1655699" target='_blank'>
              Reconstructing dynamical systems as zero-noise limits
              </a>
            </td>
          <td>
            Suddhasattwa Das
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this paper, we evaluate the effectiveness of deep operator networks (DeepONets) in solving both forward and inverse problems of partial differential equations (PDEs) on unknown manifolds. By unknown manifolds, we identify the manifold by a set of randomly sampled data point clouds that are assumed to lie on or close to the manifold. When the loss function incorporates the physics, resulting in the so-called physics-informed DeepONets (PI-DeepONets), we approximate the differentiation terms in the PDE by an appropriate operator approximation scheme. For the second-order elliptic PDE with a nontrivial diffusion coefficient, we approximate the differentiation term with one of these methods: the Diffusion Maps (DM), the Radial Basis Functions (RBF), and the Generalized Moving Least Squares (GMLS) methods. For the GMLS approximation, which is more flexible for problems with boundary conditions, we derive the theoretical error bound induced by the approximate differentiation. Numerically, we found that DeepONet is accurate for various types of diffusion coefficients, including linear, exponential, piecewise linear, and quadratic functions, for linear and semi-linear PDEs with/without boundaries. When the number of observations is small, PI-DeepONet trained with sufficiently large samples of PDE constraints produces more accurate approximations than DeepONet. For the inverse problem, we incorporate PI-DeepONet in a Bayesian Markov Chain Monte Carlo (MCMC) framework to estimate the diffusion coefficient from noisy solutions of the PDEs measured at a finite number of point cloud data. Numerically, we found that PI-DeepONet provides accurate approximations comparable to those obtained by a more expensive method that directly solves the PDE on the proposed diffusion coefficient in each MCMC iteration.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f1a2db1eb2d9fce75d3ec1e4b8ba8bfdab80a365" target='_blank'>
              Solving forward and inverse PDE problems on unknown manifolds via physics-informed neural operators
              </a>
            </td>
          <td>
            Anran Jiao, Qile Yan, Jhn Harlim, Lu Lu
          </td>
          <td>2024-07-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Echo state network (ESN) implements an alternative paradigm called reservoir computing to train recurrent neural networks (RNNs), where internal weights are randomly generated and kept fixed, and only readout weights need to be trained, which greatly reduces the training complexity of RNNs. ESN not only facilitates the practical implementation of RNNs but also shows superior performance over fully trained RNNs across a range of applications. However, the conventional ESN suffers from the drawbacks of stringent conditions for weight convergence and slow convergence speed. This paper proposes a memory regressor extended learning method to update the readout weights of ESNs. By constructing and incorporating a generalized prediction error based on regressor extension and filtering, the capacity of ESN to utilize historical data can be greatly improved. In the discrete-time domain, it is proven that exponential convergence of readout weights is achieved under a condition termed interval excitation that is strictly weaker than the classical condition of persistent excitation. Simulation results on modeling a 10th-order nonlinear autoregressive moving-average (NARMA) system have revealed that the proposed approach accelerates weight convergence speed almost ten times higher compared to the conventional ESN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e0fa7b7dfe357970b3e72acb8d86266f028d299" target='_blank'>
              Memory Regressor Extended Echo State Networks for Nonlinear Dynamics Modeling
              </a>
            </td>
          <td>
            Kai Hu, Qian Wang, Tian Shi, Kohei Nakajima, Yongping Pan
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This work proposes a data‐driven state observation algorithm for nonlinear dynamical systems, when the true state trajectory is not measurable and hence the states information needs to be reconstructed from input and output measurements. Such a reduction is formed by kernel canonical correlation analysis (KCCA), which (i) implicitly maps the available input–output data into a higher‐dimensional feature space, namely the reproducing kernel Hilbert space (RKHS); (ii) finds a projection of the past input–output data and a projection of the future input–output data with maximal correlation; and (iii) identifies the projected inputs and outputs, namely the canonical variates, as the observed states. We adopt a least squares support vector machine (LS‐SVM) formulation for KCCA, which imposes regularization on the vectors that specify the projections and is amenable to convex optimization. We prove theoretically that, based on the statistical consistency of KCCA, the observed states determined by the proposed state observer has a guaranteed correlativity with the actual states (when properly transformed). Furthermore, such observed states, when supplemented with the information of succeeding inputs, can be used to predict the succeeding outputs with guaranteed upper bound on the prediction error. Case studies are performed on two numerical examples and an exothermic continuously stirred tank reactor (CSTR).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afc53bd0e86a21cb48431210cc83dc9943ac1742" target='_blank'>
              Data‐driven nonlinear state observation for controlled systems: A kernel method and its analysis
              </a>
            </td>
          <td>
            Moritz Woelk, Wentao Tang
          </td>
          <td>2024-07-08</td>
          <td>The Canadian Journal of Chemical Engineering</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Koopman operator framework is a promising direction of analysis and synthesis of systems with nonlinear dynamics based on (linear) Koopman operators. In this paper, we address the resolvent of a Koopman operator for a nonlinear autonomous discrete-time system, which we call the Koopman resolvent, and its identification problem. First, we show that for the nonlinear system with a scalar-valued output, the $z$- transform of the output is represented by the action of Koop-man resolvent. Second, we describe an identification method of the Koopman resolvent directly from time-series data of the output, in which we estimate parameters of the resolvent as well as poles and residues of the z-transform of the output. By combining the so-called frequency-domain Prony method with the Vandermonde-Cauchy form in the Dynamic Mode Decomposition (DMD), we propose the method which we call the frequency-domain DMD, in which all the unknowns can be estimated in the frequency domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/df691892a89d4f0c1e7b43d373b04818f75bcc0b" target='_blank'>
              Koopman Resolvents of Nonlinear Discrete-Time Systems: Formulation and Identification
              </a>
            </td>
          <td>
            Yoshihiko Susuki, A. Mauroy, Z. Drmač
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="We propose a meta-learning method for modeling Hamiltonian dynamics from a limited number of data. Although Hamiltonian neural networks have been successfully used for modeling dynamics that obey the energy conservation law, they require many data to achieve high performance. The proposed method meta-learns our neural network-based model using datasets in various dynamical systems, such that our model can predict vector fields of unseen systems. In our model, a system representation is inferred from given small data using an encoder network. Then, the system-specific vector field is predicted by modeling the Hamiltonian using a Gaussian process (GP) with neural network-based mean and kernel functions that depend on the inferred system representation. This GP-based Hamiltonian allows us to analytically obtain predictions that are adapted to small data while imposing the constraint of the conservation law. The neural networks are shared across systems, which enables us to learn knowledge from multiple systems, and use it for unseen systems. In our experiments, we demonstrate that the proposed method outperforms existing methods for predicting dynamics from a small number of observations in target systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37ac3a5b751bd136ecb4369b5f0c68f06bf4a91f" target='_blank'>
              Symplectic Neural Gaussian Processes for Meta-learning Hamiltonian Dynamics
              </a>
            </td>
          <td>
            Tomoharu Iwata, Yusuke Tanaka
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Within the context of machine learning-based closure mappings for RANS turbulence modelling, physical realizability is often enforced using ad-hoc postprocessing of the predicted anisotropy tensor. In this study, we address the realizability issue via a new physics-based loss function that penalizes non-realizable results during training, thereby embedding a preference for realizable predictions into the model. Additionally, we propose a new framework for data-driven turbulence modelling which retains the stability and conditioning of optimal eddy viscosity-based approaches while embedding equivariance. Several modifications to the tensor basis neural network to enhance training and testing stability are proposed. We demonstrate the conditioning, stability, and generalization of the new framework and model architecture on three flows: flow over a flat plate, flow over periodic hills, and flow through a square duct. The realizability-informed loss function is demonstrated to significantly increase the number of realizable predictions made by the model when generalizing to a new flow configuration. Altogether, the proposed framework enables the training of stable and equivariant anisotropy mappings, with more physically realizable predictions on new data. We make our code available for use and modification by others.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d49d700d5797f8d7145ca83fa89f5c1491a5d703" target='_blank'>
              Realizability-Informed Machine Learning for Turbulence Anisotropy Mappings
              </a>
            </td>
          <td>
            R. McConkey, Eugene Yee, F. Lien
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Nonlinear optimization (NOPT) is a meaningful tool for solving complex tasks in fields like engineering, economics, and operations research, among others. However, NOPT has problems when it comes to dealing with data variability and noisy input measurements that lead to incorrect solutions. Furthermore, nonlinear constraints may result in outcomes that are either infeasible or suboptimal, such as nonconvex optimization. This paper introduces a novel regularized physics-informed neural network (RPINN) framework as a new NOPT tool for both supervised and unsupervised data-driven scenarios. Our RPINN is threefold: By using custom activation functions and regularization penalties in an artificial neural network (ANN), RPINN can handle data variability and noisy inputs. Furthermore, it employs physics principles to construct the network architecture, computing the optimization variables based on network weights and learned features. In addition, it uses automatic differentiation training to make the system scalable and cut down on computation time through batch-based back-propagation. The test results for both supervised and unsupervised NOPT tasks show that our RPINN can provide solutions that are competitive compared to state-of-the-art solvers. In turn, the robustness of RPINN against noisy input measurements makes it particularly valuable in environments with fluctuating information. Specifically, we test a uniform mixture model and a gas-powered system as NOPT scenarios. Overall, with RPINN, its ANN-based foundation offers significant flexibility and scalability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/516f48e222fdad35b92113ecdbf8a459d37c95aa" target='_blank'>
              A Regularized Physics-Informed Neural Network to Support Data-Driven Nonlinear Constrained Optimization
              </a>
            </td>
          <td>
            Diego Armando Perez-Rosero, A. Álvarez-Meza, C. Castellanos-Dominguez
          </td>
          <td>2024-07-18</td>
          <td>Computers</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="The task of sampling from a probability density can be approached as transporting a tractable density function to the target, known as dynamical measure transport. In this work, we tackle it through a principled unified framework using deterministic or stochastic evolutions described by partial differential equations (PDEs). This framework incorporates prior trajectory-based sampling methods, such as diffusion models or Schr\"odinger bridges, without relying on the concept of time-reversals. Moreover, it allows us to propose novel numerical methods for solving the transport task and thus sampling from complicated targets without the need for the normalization constant or data samples. We employ physics-informed neural networks (PINNs) to approximate the respective PDE solutions, implying both conceptional and computational advantages. In particular, PINNs allow for simulation- and discretization-free optimization and can be trained very efficiently, leading to significantly better mode coverage in the sampling task compared to alternative methods. Moreover, they can readily be fine-tuned with Gauss-Newton methods to achieve high accuracy in sampling.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be6b71f84d3c641029effa69aa23e0c32eb9de03" target='_blank'>
              Dynamical Measure Transport and Neural PDE Solvers for Sampling
              </a>
            </td>
          <td>
            Jingtong Sun, Julius Berner, Lorenz Richter, Marius Zeinhofer, Johannes Muller, K. Azizzadenesheli, A. Anandkumar
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Physics-Informed Neural Networks (PINNs) have emerged as a robust framework for solving Partial Differential Equations (PDEs) by approximating their solutions via neural networks and imposing physics-based constraints on the loss function. Traditionally, Multilayer Perceptrons (MLPs) are the neural network of choice, and significant progress has been made in optimizing their training. Recently, Kolmogorov-Arnold Networks (KANs) were introduced as a viable alternative, with the potential of offering better interpretability and efficiency while requiring fewer parameters. In this paper, we present a fast JAX-based implementation of grid-dependent Physics-Informed Kolmogorov-Arnold Networks (PIKANs) for solving PDEs. We propose an adaptive training scheme for PIKANs, incorporating known MLP-based PINN techniques, introducing an adaptive state transition scheme to avoid loss function peaks between grid updates, and proposing a methodology for designing PIKANs with alternative basis functions. Through comparative experiments we demonstrate that these adaptive features significantly enhance training efficiency and solution accuracy. Our results illustrate the effectiveness of PIKANs in improving performance for PDE solutions, highlighting their potential as a superior alternative in scientific and engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c1aaa25c69658448f0abd4e2430cf6925f39fdd5" target='_blank'>
              Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Spyros Rigas, M. Papachristou, Theofilos Papadopoulos, Fotios Anagnostopoulos, Georgios Alexandridis
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We introduce data to predictive control, D2PC, a framework to facilitate the design of robust and predictive controllers from data. The proposed framework is designed for discrete-time stochastic linear systems with output measurements and provides a principled design of a predictive controller based on data. The framework starts with a parameter identification method based on the Expectation-Maximization algorithm, which incorporates pre-defined structural constraints. Additionally, we provide an asymptotically correct method to quantify uncertainty in parameter estimates. Next, we develop a strategy to synthesize robust dynamic output-feedback controllers tailored to the derived uncertainty characterization. Finally, we introduce a predictive control scheme that guarantees recursive feasibility and satisfaction of chance constraints. This framework marks a significant advancement in integrating data into robust and predictive control schemes. We demonstrate the efficacy of D2PC through a numerical example involving a $10$-dimensional spring-mass-damper system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ea52a361e1eddc6fae9770357307a43f124b006" target='_blank'>
              From Data to Predictive Control: A Framework for Stochastic Linear Systems with Output Measurements
              </a>
            </td>
          <td>
            Haldun Balim, Andrea Carron, M. Zeilinger, Johannes Kohler
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="This paper proposes a fully data-driven approach for optimal control of nonlinear control-affine systems represented by a stochastic diffusion. The focus is on the scenario where both the nonlinear dynamics and stage cost functions are unknown, while only control penalty function and constraints are provided. Leveraging the theory of reproducing kernel Hilbert spaces, we introduce novel kernel mean embeddings (KMEs) to identify the Markov transition operators associated with controlled diffusion processes. The KME learning approach seamlessly integrates with modern convex operator-theoretic Hamilton-Jacobi-Bellman recursions. Thus, unlike traditional dynamic programming methods, our approach exploits the ``kernel trick'' to break the curse of dimensionality. We demonstrate the effectiveness of our method through numerical examples, highlighting its ability to solve a large class of nonlinear optimal control problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69ffcc674bf67738c50bad11d4a1a6996a25767b" target='_blank'>
              Data-Driven Optimal Feedback Laws via Kernel Mean Embeddings
              </a>
            </td>
          <td>
            Petar Bevanda, Nicolas Hoischen, Stefan Sosnowski, Sandra Hirche, Boris Houska
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="We study the model reduction by moment matching problem for linear systems in a data-driven framework. We show that reduced-order models can be directly computed from data without knowledge of the structure of the signal generator or of its internal state. The reduced-order models thus obtained match the moments of the unknown underlying system asymptotically. Our construction provides a simple way to enforce additional constraints in the reduced-order model. We demonstrate the applicability of the results using data from a high-dimensional model of a building.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83399652779edc533b785a20de6431d8d0060e66" target='_blank'>
              Data-Driven Model Reduction by Moment Matching for Linear Systems Driven by an Unknown Implicit Signal Generator
              </a>
            </td>
          <td>
            Debraj Bhattacharjee, Alessandro Astolfi
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Using equilibrium fluctuations to understand the response of a physical system to an externally imposed perturbation is the basis for linear response theory, which is widely used to interpret experiments and shed light on microscopic dynamics. For nonequilibrium systems, perturbations cannot be interpreted simply by monitoring fluctuations in a conjugate observable -- additional dynamical information is needed. The theory of linear response around nonequilibrium steady states relies on path ensemble averaging, which makes this theory inapplicable to perturbations that affect the diffusion constant or temperature in a stochastic system. Here, we show that a separate, ``effective'' physical process can be used to describe the perturbed dynamics and that this dynamics in turn allows us to accurately calculate the response to a change in the diffusion. Interestingly, the effective dynamics contains an additional drift that is proportional to the ``score'' of the instantaneous probability density of the system -- this object has also been studied extensively in recent years in the context of denoising diffusion models in the machine learning literature. Exploiting recently developed algorithms for learning the score, we show that we can carry out nonequilibrium response calculations on systems for which the exact score cannot be obtained.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f45af9914ab526c24b8c0bd202775dddc280a5c4" target='_blank'>
              Computing Nonequilibrium Responses with Score-shifted Stochastic Differential Equations
              </a>
            </td>
          <td>
            J'er'emie Klinger, Grant M. Rotskoff
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="We apply two data assimilation (DA) methods, a smoother and a filter, and a model-free machine learning (ML) shallow network to forecast two weakly turbulent systems. We analyse the effect of the spatial sparsity of observations on accuracy of the predictions obtained from these data-driven methods. Based on the results, we divide the spatial sparsity levels in three zones. First is the good-predictions zone in which both DA and ML methods work. We find that in the good-predictions zone the observations remain dense enough to accurately capture the fractal manifold of the system's dynamics, which is measured using the correlation dimension. The accuracy of the DA methods in this zone remains almost as good as for full-resolution observations. Second is the reasonable-predictions zone in which the DA methods still work but at reduced prediction accuracy. Third is the bad-predictions zone in which even the DA methods fail. We find that the sparsity level up to which the DA methods work is almost the same up to which chaos synchronisation of these systems can be achieved. The main implications of these results are that they (i) firmly establish the spatial resolution up to which the data-driven methods can be utilised, (ii) provide measures to determine if adding more sensors will improve the predictions, and (iii) quantify the advantage (in terms of the required measurement resolution) of using the governing equations within data-driven methods. We also discuss the applicability of these results to fully developed turbulence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f37955fcf738b892c9cedb2952e279d466fad9b8" target='_blank'>
              Predictability of weakly turbulent systems from spatially sparse observations using data assimilation and machine learning
              </a>
            </td>
          <td>
            Vikrant Gupta, Yuanqing Chen, Minping Wan
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics. Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation. To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly. We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process and the associated resolvent operator. We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data. In experiments, we highlight the advantages of our method over transfer operator approaches and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33ff38eef166593508576694dab15cbdbd79cb82" target='_blank'>
              From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach
              </a>
            </td>
          <td>
            Timothée Devergne, Vladimir Kostic, Michele Parrinello, Massimiliano Pontil
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="High-precision control for nonlinear systems is impeded by the low-fidelity dynamical model and external disturbance. Especially, the intricate coupling between internal uncertainty and external disturbance is usually difficult to be modeled explicitly. Here we show an effective and convergent algorithm enabling accurate estimation of the coupled disturbance via combining control and learning philosophies. Specifically, by resorting to Chebyshev series expansion, the coupled disturbance is firstly decomposed into an unknown parameter matrix and two known structures depending on system state and external disturbance respectively. A Regularized Least Squares (RLS) algorithm is subsequently formalized to learn the parameter matrix by using historical time-series data. Finally, a higher-order disturbance observer (HODO) is developed to achieve a high-precision estimation of the coupled disturbance by utilizing the learned portion. The efficiency of the proposed algorithm is evaluated through extensive simulations. We believe this work can offer a new option to merge learning schemes into the control framework for addressing existing intractable control problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c2b582f6ad335402a40cb9dffeaea1baf4781dd" target='_blank'>
              Disturbance Observer for Estimating Coupled Disturbances
              </a>
            </td>
          <td>
            Jindou Jia, Yuhang Liu, Kexin Guo, Xiang Yu, Lihua Xie, Lei Guo
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="We study the problem of estimating the states of a linear system based on measured data. We investigate the problem in both deterministic and stochastic settings. In the deterministic case, we develop data-driven conditions under which we can reconstruct state trajectories uniquely. Also, we discuss the case in which we have some missing data in the given input/output measurements. In the stochastic case, we develop a Kalman filter-like algorithm to recursively estimate both states and outputs. Finally, we consider a multi-input multi-output system to elucidate the developed results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/344c27f38393d70906627a195cc65ea2636e5a14" target='_blank'>
              Data-Driven State Estimation for Linear Systems
              </a>
            </td>
          <td>
            Vikas Kumar Mishra, S. Hiremath, N. Bajçinca
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Macroscopic features of dynamical systems such as almost-invariant sets and coherent sets provide crucial high-level information on how the dynamics organises phase space. We introduce a method to identify time-parameterised families of almost-invariant sets in time-dependent dynamical systems, as well as the families' emergence and disappearance. In contrast to coherent sets, which may freely move about in phase space over time, our technique focuses on families of metastable sets that are quasi-stationary in space. Our straightforward approach extends successful transfer operator methods for almost-invariant sets to time-dependent dynamics and utilises the Ulam scheme for the generator of the transfer operator on a time-expanded domain. The new methodology is illustrated with an idealised fluid flow and with atmospheric velocity data. We identify atmospheric blocking events in the 2003 European heatwave and compare our technique to existing geophysical methods of blocking diagnosis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85cdef684e76e594fd24e0e2863fe622d0a8314d" target='_blank'>
              Identifying the onset and decay of quasi-stationary families of almost-invariant sets with an application to atmospheric blocking events
              </a>
            </td>
          <td>
            Aleksandar Badza, Gary Froyland School of Mathematics, Statistics Unsw Sydney Nsw Australia
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="One of the core concepts in science, and something that happens intuitively in every-day dynamic systems modeling, is the combination of models or methods. Especially in dynamical systems modeling, often two or more structures are combined to obtain a more powerful or efficient architecture regarding a specific application (area). Further, even physical simulations are combined with machine learning architectures, to increase prediction accuracy or optimize the computational performance. In this work, we shortly discuss, which types of models are usually combined and propose a model interface that is capable of expressing a width variety of mixed algebraic, discrete and differential equation based models. Further, we examine different established, as well as new ways of combining these models from a system theoretical point of view and highlight two challenges - algebraic loops and local event affect functions in discontinuous models - that require a special approach. Finally, we propose a new wildcard topology, that is capable of describing the generic connection between two combined models in an easy to interpret fashion that can be learned as part of a gradient based optimization procedure. The contributions of this paper are highlighted at a proof of concept: Different connection topologies between two models are learned, interpreted and compared applying the proposed methodology and software implementation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe81ab9d1a688154b1e40c6ca68ccf95c122d0c7" target='_blank'>
              Learnable & Interpretable Model Combination in Dynamic Systems Modeling
              </a>
            </td>
          <td>
            Tobias Thummerer, Lars Mikelsons
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="A data-driven approach to calculating tight-binding models for discrete coupled-mode systems is presented. Specifically, spectral and topological data is used to build an appropriate discrete model that accurately replicates these properties. This work is motivated by topological insulator systems that are often described by tight-binding models. The problem is formulated as the minimization of an appropriate residual (objective) function. Given bulk spectral data and a topological index (e.g. winding number), an appropriate discrete model is obtained to arbitrary precision. A nonlinear least squares method is used to determine the coefficients. The effectiveness of the scheme is highlighted against a Schr\"odinger equation with a periodic potential that can be described by the Su-Schrieffer-Heeger model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b260d38c46caaea8a67dc40a10c56986fd078985" target='_blank'>
              Data-driven approximations of topological insulator systems
              </a>
            </td>
          <td>
            Justin T. Cole, Michael J. Nameika
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Although neural operator networks theoretically approximate any operator mapping, the limited generalization capability prevents them from learning correct physical dynamics when potential data biases exist, particularly in the practical PDE solving scenario where the available data amount is restricted or the resolution is extremely low. To address this issue, we propose and formulate the Physical Trajectory Residual Learning (DeltaPhi), which learns to predict the physical residuals between the pending solved trajectory and a known similar auxiliary trajectory. First, we transform the direct operator mapping between input-output function fields in original training data to residual operator mapping between input function pairs and output function residuals. Next, we learn the surrogate model for the residual operator mapping based on existing neural operator networks. Additionally, we design helpful customized auxiliary inputs for efficient optimization. Through extensive experiments, we conclude that, compared to direct learning, physical residual learning is preferred for PDE solving.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/75ea87f76067c9b62442cad18c16c45c61d35489" target='_blank'>
              DeltaPhi: Learning Physical Trajectory Residual for PDE Solving
              </a>
            </td>
          <td>
            Xihang Yue, Linchao Zhu, Yi Yang
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>41</td>
        </tr>

        <tr id="Fractional and tempered fractional partial differential equations (PDEs) are effective models of long-range interactions, anomalous diffusion, and non-local effects. Traditional numerical methods for these problems are mesh-based, thus struggling with the curse of dimensionality (CoD). Physics-informed neural networks (PINNs) offer a promising solution due to their universal approximation, generalization ability, and mesh-free training. In principle, Monte Carlo fractional PINN (MC-fPINN) estimates fractional derivatives using Monte Carlo methods and thus could lift CoD. However, this may cause significant variance and errors, hence affecting convergence; in addition, MC-fPINN is sensitive to hyperparameters. In general, numerical methods and specifically PINNs for tempered fractional PDEs are under-developed. Herein, we extend MC-fPINN to tempered fractional PDEs to address these issues, resulting in the Monte Carlo tempered fractional PINN (MC-tfPINN). To reduce possible high variance and errors from Monte Carlo sampling, we replace the one-dimensional (1D) Monte Carlo with 1D Gaussian quadrature, applicable to both MC-fPINN and MC-tfPINN. We validate our methods on various forward and inverse problems of fractional and tempered fractional PDEs, scaling up to 100,000 dimensions. Our improved MC-fPINN/MC-tfPINN using quadrature consistently outperforms the original versions in accuracy and convergence speed in very high dimensions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f554b1c8b8360b3842290f4ce5cc7ca4106693d" target='_blank'>
              Tackling the Curse of Dimensionality in Fractional and Tempered Fractional PDEs with Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Zheyuan Hu, Kenji Kawaguchi, Zhongqiang Zhang, G. Karniadakis
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="This paper introduces PDEformer-1, a versatile neural solver capable of simultaneously addressing various partial differential equations (PDEs). With the PDE represented as a computational graph, we facilitate the seamless integration of symbolic and numeric information inherent in a PDE. A graph Transformer and an implicit neural representation (INR) are employed subsequently to generate mesh-free predicted solutions. We generated a dataset with up to three million samples involving diverse one-dimensional PDEs to pretrain our model. Compared with baseline models trained specifically on benchmark datasets, our pretrained model achieves comparable accuracy via zero-shot inference, and the advantage expands after finetuning. For PDEs new or unseen in the pretraining stage, our model can adapt quickly by finetuning on a relatively small set of examples from the target equation. Additionally, PDEformer-1 demonstrates promising results in the inverse problem of PDE scalar coefficient recovery and coefficient field recovery.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/40c21c8b077c4c5aea793ff9f0cb0cf1332acd86" target='_blank'>
              PDEformer-1: A Foundation Model for One-Dimensional Partial Differential Equations
              </a>
            </td>
          <td>
            Zhanhong Ye, Xiang Huang, Leheng Chen, Zining Liu, Bingyang Wu, Hongsheng Liu, Zidong Wang, Bin Dong
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The study of structures involving vortices in one component and bright solitary waves in another has a time-honored history in two-component atomic Bose-Einstein condensates. In the present work, we revisit this topic extending considerations well-past the near-integrable regime of nearly equal scattering lengths. Instead, we focus on stationary states and spectral stability of such structures for large values of the inter-component interaction coefficient. We find that the state can manifest dynamical instabilities for suitable parameter values. We also explore a phenomenological, yet quantitatively accurate upon suitable tuning, particle model which, in line also with earlier works, offers the potential of accurately following the associated stability and dynamical features. Finally, we probe the dynamics of the unstable vortex-bright structure, observing an unprecedented, to our knowledge, instability scenario in which the oscillatory instability leads to a patch of vorticity that harbors and eventually ejects multiple vortex-bright structures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6484c395c329306744208c49808d9af38aca3bbf" target='_blank'>
              Stability and dynamics of massive vortices in two-component Bose-Einstein condensates
              </a>
            </td>
          <td>
            J. D’Ambroise, Wenlong Wang, Christopher Ticknor, R. Carretero-Gonz´alez, P. Kevrekidis
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>53</td>
        </tr>

        <tr id="We perform a comparative analysis of quintessence and $k$-essence scalar field models in the data analysis perspective. We study the quintessence field with an exponential potential and the $k$-essence field with an inverse square potential in the present work. Before delving into data analysis, we provide a brief perspective on dynamical evolution on both of the models and obtain the stability constraints on the model parameters. We adopt Bayesian inference procedure to estimate the model parameters that best-fit the data. A comprehensive analysis utilizing Observational Hubble data (OHD) and Pantheon+ compilation of Type Ia supernovae (SNIa) shows that $k$-essence model fits the data slightly better than the quintessence model while the evidence of these models in comparison with the $\Lambda$CDM model is weak. The value of the Hubble constant predicted by both the models is in close agreement with the value obtained by the Planck2018 collaboration assuming the $\Lambda$CDM model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/42380f8c1154a7b525a2b52752ab4cbca4ead1d4" target='_blank'>
              Comparative Analysis of $k$-essence and Quintessence Scalar Field Models: A Data Analysis Approach
              </a>
            </td>
          <td>
            Saddam Hussain, Sarath Nelleri, Kaushik Bhattacharya
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="
Fusing known physics into data-driven learners allows modelling practitioners to combine the expressive power of traditional machine learning with known mechanistic laws, where the objective is to enhance predictive performance, interpretability, and model generalisation. A core consideration that must be made when implementing a physics-informed learning architecture is how relevant knowledge will be embedded into the model structure, which, generally, is informed by the type of physics that is available. Frequently this knowledge may not be complete, with only a partial understanding of the governing physics available. 

In this work, possible paths for deriving Gaussian process kernels that are representative of partial knowledge will be considered. How the type of knowledge that is possessed influences the derivation will be explored, particularly when there is the potential for some aspect of misspecified physics. An example of deriving partially structured kernels will be investigated for modelling the decoupled response of a GARTEUR laboratory aircraft structure, where the derived kernels are used to decompose the dynamics of the aircraft into modal contributions. 

">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/da9bbf07470b8cc6d6606c4b51a040b8e45ce3d6" target='_blank'>
              Gaussian process kernels for partial physical insight
              </a>
            </td>
          <td>
            Matthew R. Jones, D. J. Pitchforth, E. J. Cross
          </td>
          <td>2024-07-01</td>
          <td>e-Journal of Nondestructive Testing</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In recent decades, the utilization of machine learning (ML) and artificial intelligence (AI) approaches have been explored for process modelling applications. However, different types of ML models may have contrasting advantages and disadvantages, which become critical during the optimal selection of a specific data‐driven model for a particular application as well as estimation of parameters during model training. This paper compares and contrasts two different types of data‐driven modelling approaches, namely the series/parallel all‐nonlinear static‐dynamic neural network models and models from a Bayesian ML approach. Both types of AI modelling approaches considered in this work have shown to significantly outperform several state‐of‐the‐art steady‐state and dynamic data‐driven modelling techniques for various performance measures, specifically, model sparsity, predictive capabilities, and computational expense. The performances of the proposed model structures and algorithms have been evaluated for two nonlinear dynamic chemical engineering systems—a plug‐flow reactor for vapour phase cracking of acetone for production of acetic anhydride and a pilot‐plant for post‐combustion CO2 capture using monoethanolamine as the solvent. For the validation data from the CO2 capture pilot plant, root mean squared error (RMSE) for flue gas outlet temperature, flowrate and CO2 concentration is 0.05%, 1.07%, and 5.0%, respectively, for the all‐nonlinear static‐dynamic neural networks and 0.1%, 1.75%, and 14.14%, respectively, for the Bayesian ML models. For the plug flow reactor data, the Bayesian ML models yield superior RMSE compared to the all‐nonlinear static‐dynamic neural networks when the measurement data are corrupted with Gaussian, auto‐correlated, or cross‐correlated noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/056ceb214531414b85db491a54767e185fb5c38a" target='_blank'>
              All‐nonlinear static‐dynamic neural networks versus Bayesian machine learning for data‐driven modelling of chemical processes
              </a>
            </td>
          <td>
            Angan Mukherjee, Samuel Adeyemo, D. Bhattacharyya
          </td>
          <td>2024-06-24</td>
          <td>The Canadian Journal of Chemical Engineering</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="The paper presents a framework for online learning of the Koopman operator using streaming data. Many complex systems for which data-driven modeling and control are sought provide streaming sensor data, the abundance of which can present computational challenges but cannot be ignored. Streaming data can intermittently sample dynamically different regimes or rare events which could be critical to model and control. Using ideas from subspace identification, we present a method where the Grassmannian distance between the subspace of an extended observability matrix and the streaming segment of data is used to assess the `novelty' of the data. If this distance is above a threshold, it is added to an archive and the Koopman operator is updated if not it is discarded. Therefore, our method identifies data from segments of trajectories of a dynamical system that are from different dynamical regimes, prioritizes minimizing the amount of data needed in updating the Koopman model and furthermore reduces the number of basis functions by learning them adaptively. Therefore, by dynamically adjusting the amount of data used and learning basis functions, our method optimizes the model's accuracy and the system order.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7ee04d648defa5ecd02e1e1b2368c710c5df6c2" target='_blank'>
              Online learning of Koopman operator using streaming data from different dynamical regimes
              </a>
            </td>
          <td>
            Kartik Loya, Phanindra Tallapragada
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Equations of State model relations between thermodynamic variables and are ubiquitous in scientific modelling, appearing in modern day applications ranging from Astrophysics to Climate Science. The three desired properties of a general Equation of State model are adherence to the Laws of Thermodynamics, incorporation of phase transitions, and multiscale accuracy. Analytic models that adhere to all three are hard to develop and cumbersome to work with, often resulting in sacrificing one of these elements for the sake of efficiency. In this work, two deep-learning methods are proposed that provably satisfy the first and second conditions on a large-enough region of thermodynamic variable space. The first is based on learning the generating function (thermodynamic potential) while the second is based on structure-preserving, symplectic neural networks, respectively allowing modifications near or on phase transition regions. They can be used either"from scratch"to learn a full Equation of State, or in conjunction with a pre-existing consistent model, functioning as a modification that better adheres to experimental data. We formulate the theory and provide several computational examples to justify both approaches, and highlight their advantages and shortcomings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6515ce69075cc8fe93c06073e12dcf389d87c37b" target='_blank'>
              Neural Network Representations of Multiphase Equations of State
              </a>
            </td>
          <td>
            George A. Kevrekidis, Daniel A. Serino, Alexander Kaltenborn, J. Gammel, J. Burby, Marc L. Klasky
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Sampling-based kinodynamic motion planners (SKMPs) are powerful in finding collision-free trajectories for high-dimensional systems under differential constraints. Time-informed set (TIS) can provide the heuristic search domain to accelerate their convergence to the time-optimal solution. However, existing TIS approximation methods suffer from the curse of dimensionality, computational burden, and limited system applicable scope, e.g., linear and polynomial nonlinear systems. To overcome these problems, we propose a method by leveraging deep learning technology, Koopman operator theory, and random set theory. Specifically, we propose a Deep Invertible Koopman operator with control U model named DIKU to predict states forward and backward over a long horizon by modifying the auxiliary network with an invertible neural network. A sampling-based approach, ASKU, performing reachability analysis for the DIKU is developed to approximate the TIS of nonlinear control systems online. Furthermore, we design an online time-informed SKMP using a direct sampling technique to draw uniform random samples in the TIS. Simulation experiment results demonstrate that our method outperforms other existing works, approximating TIS in near real-time and achieving superior planning performance in several time-optimal kinodynamic motion planning problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/47eec5e6ac343847f3ccf8a58bf04f8fa124b137" target='_blank'>
              Online Time-Informed Kinodynamic Motion Planning of Nonlinear Systems
              </a>
            </td>
          <td>
            Fei Meng, Jianbang Liu, Hao-bin Shi, Han Ma, Hongliang Ren, Max Q.-H. Meng
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We introduce a method that combines neural operators, physics-informed machine learning, and standard numerical methods for solving PDEs. The proposed approach extends each of the aforementioned methods and unifies them within a single framework. We can parametrically solve partial differential equations in a data-free manner and provide accurate sensitivities, meaning the derivatives of the solution space with respect to the design space. These capabilities enable gradient-based optimization without the typical sensitivity analysis costs, unlike adjoint methods that scale directly with the number of response functions. Our Finite Operator Learning (FOL) approach uses an uncomplicated feed-forward neural network model to directly map the discrete design space (i.e. parametric input space) to the discrete solution space (i.e. finite number of sensor points in the arbitrary shape domain) ensuring compliance with physical laws by designing them into loss functions. The discretized governing equations, as well as the design and solution spaces, can be derived from any well-established numerical techniques. In this work, we employ the Finite Element Method (FEM) to approximate fields and their spatial derivatives. Subsequently, we conduct Sobolev training to minimize a multi-objective loss function, which includes the discretized weak form of the energy functional, boundary conditions violations, and the stationarity of the residuals with respect to the design variables. Our study focuses on the steady-state heat equation within heterogeneous materials that exhibits significant phase contrast and possibly temperature-dependent conductivity. The network's tangent matrix is directly used for gradient-based optimization to improve the microstructure's heat transfer characteristics. ...">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6248ca7a394e7da69cd6d0cb638b974e5ed8f57e" target='_blank'>
              Finite Operator Learning: Bridging Neural Operators and Numerical Methods for Efficient Parametric Solution and Optimization of PDEs
              </a>
            </td>
          <td>
            Shahed Rezaei, Reza Najian Asl, Kianoosh Taghikhani, Ahmad Moeineddin, Michael Kaliske, Markus Apel
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Stochastic dynamics on sparse graphs and disordered systems often lead to complex behaviors characterized by heterogeneity in time and spatial scales, slow relaxation, localization, and aging phenomena. The mathematical tools and approximation techniques required to analyze these complex systems are still under development, posing significant technical challenges and resulting in a reliance on numerical simulations. We introduce a novel computational framework for investigating the dynamics of sparse disordered systems with continuous degrees of freedom. Starting with a graphical model representation of the dynamic partition function for a system of linearly-coupled stochastic differential equations, we use dynamic cavity equations on locally tree-like factor graphs to approximate the stochastic measure. Here, cavity marginals are identified with local functionals of single-site trajectories. Our primary approximation involves a second-order truncation of a small-coupling expansion, leading to a Gaussian form for the cavity marginals. For linear dynamics with additive noise, this method yields a closed set of causal integro-differential equations for cavity versions of one-time and two-time averages. These equations provide an exact dynamical description within the local tree-like approximation, retrieving classical results for the spectral density of sparse random matrices. Global constraints, non-linear forces, and state-dependent noise terms can be addressed using a self-consistent perturbative closure technique. The resulting equations resemble those of dynamical mean-field theory in the mode-coupling approximation used for fully-connected models. However, due to their cavity formulation, the present method can also be applied to ensembles of sparse random graphs and employed as a message-passing algorithm on specific graph instances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1409599021b5a3a43a70ea04c2d775f534dd885c" target='_blank'>
              Gaussian approximation of dynamic cavity equations for linearly-coupled stochastic dynamics
              </a>
            </td>
          <td>
            Mattia Tarabolo, Luca Dall'Asta
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Dynamic systems described by differential equations often involve feedback among system components. When there are time delays for components to sense and respond to feedback, delay differential equation (DDE) models are commonly used. This paper considers the problem of inferring unknown system parameters, including the time delays, from noisy and sparse experimental data observed from the system. We propose an extension of manifold-constrained Gaussian processes to conduct parameter inference for DDEs, whereas the time delay parameters have posed a challenge for existing methods that bypass numerical solvers. Our method uses a Bayesian framework to impose a Gaussian process model over the system trajectory, conditioned on the manifold constraint that satisfies the DDEs. For efficient computation, a linear interpolation scheme is developed to approximate the values of the time-delayed system outputs, along with corresponding theoretical error bounds on the approximated derivatives. Two simulation examples, based on Hutchinson's equation and the lac operon system, together with a real-world application using Ontario COVID-19 data, are used to illustrate the efficacy of our method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59505531dc6e56a864c5a883b0860d09309620d3" target='_blank'>
              Inference for Delay Differential Equations Using Manifold-Constrained Gaussian Processes
              </a>
            </td>
          <td>
            Yuxuan Zhao, Samuel W. K. Wong
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Diffusion regulates a phenomenal number of natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and properly model only the drift of the system. We propose a new simple model, JKOnet*, which bypasses altogether the complexity of existing architectures while presenting significantly enhanced representational capacity: JKOnet* recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss, runs at lightspeed, and drastically outperforms other baselines in practice. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals. Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions, in light of few-weeks-old advancements in optimization in the probability space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e136b8e7261e1835acc89bf0402e956bd4ee5225" target='_blank'>
              Learning Diffusion at Lightspeed
              </a>
            </td>
          <td>
            Antonio Terpin, Nicolas Lanzetti, Florian Dörfler
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="We introduce an innovative approach for solving high-dimensional Fokker-Planck-L\'evy (FPL) equations in modeling non-Brownian processes across disciplines such as physics, finance, and ecology. We utilize a fractional score function and Physical-informed neural networks (PINN) to lift the curse of dimensionality (CoD) and alleviate numerical overflow from exponentially decaying solutions with dimensions. The introduction of a fractional score function allows us to transform the FPL equation into a second-order partial differential equation without fractional Laplacian and thus can be readily solved with standard physics-informed neural networks (PINNs). We propose two methods to obtain a fractional score function: fractional score matching (FSM) and score-fPINN for fitting the fractional score function. While FSM is more cost-effective, it relies on known conditional distributions. On the other hand, score-fPINN is independent of specific stochastic differential equations (SDEs) but requires evaluating the PINN model's derivatives, which may be more costly. We conduct our experiments on various SDEs and demonstrate numerical stability and effectiveness of our method in dealing with high-dimensional problems, marking a significant advancement in addressing the CoD in FPL equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8627a1dd31e82891b19eb525d8d99ebc327fe094" target='_blank'>
              Score-fPINN: Fractional Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck-Levy Equations
              </a>
            </td>
          <td>
            Zheyuan Hu, Zhongqiang Zhang, G. Karniadakis, Kenji Kawaguchi
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>127</td>
        </tr>

        <tr id="Real-world problems, particularly in the sciences and engineering, are often analyzed using differential equations to understand physical phenomena. Many situations involve rates of change of independent variables, represented by derivatives, which lead to differential equations. Solving higher-order ordinary differential equations typically involves reducing them to systems of first-order equations, but this approach has challenges. To overcome these and enhance numerical methods, a novel one-step block method with eight partitions was developed for the direct solution of higher-order initial value problems. This method will target issues in physics, biology, chemistry and economics. The new method was formulated using the linear block approach and numerical analysis was ensure essential and sufficient conditions. The new method addresses second-order problems like simple harmonic motion, third-order issues such as oscillatory differential equations, and fourth-order problems like thin film dynamics. The new method demonstrates faster convergence and improved accuracy compared to existing solutions for second, third, and fourth-order oscillatory differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/104875937aeb83cd6f3ad125733de7fa030115ca" target='_blank'>
              The Numerical Application of Dynamic Problems Involving Mass in Motion Governed by Higher Order Oscillatory Differential Equations
              </a>
            </td>
          <td>
            Yusuf Skwame, D. J. Zirra, Sabo John
          </td>
          <td>2024-08-01</td>
          <td>Physical Science International Journal</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We consider a class of infinite-dimensional dynamical systems driven by non-linear parabolic partial differential equations with initial condition $\theta$ modelled by a Gaussian process `prior' probability measure. Given discrete samples of the state of the system evolving in space-time, one obtains updated `posterior' measures on a function space containing all possible trajectories. We give a general set of conditions under which these non-Gaussian posterior distributions are approximated, in Wasserstein distance for the supremum-norm metric, by the law of a Gaussian random function. We demonstrate the applicability of our results to periodic non-linear reaction diffusion equations \begin{align*} \frac{\partial}{\partial t} u - \Delta u&= f(u) \\ u(0)&= \theta \end{align*} where $f$ is any smooth and compactly supported reaction function. In this case the limiting Gaussian measure can be characterised as the solution of a time-dependent Schr\"odinger equation with `rough' Gaussian initial conditions whose covariance operator we describe.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db83d3bfd7b84fe9f13762bbba8065506f1c2480" target='_blank'>
              Bernstein-von Mises theorems for time evolution equations
              </a>
            </td>
          <td>
            Richard Nickl
          </td>
          <td>2024-07-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="We introduce mochi_class, an extension of the Einstein-Boltzmann solver hi_class, designed to unlock the full phenomenological potential of Horndeski gravity. This extension allows for general input functions of time without the need for hard-coded parametrisations or covariant Lagrangians. By replacing the traditional $\alpha$-parametrisation with a set of stable basis functions, mochi_class ensures that the resulting effective theories are inherently free from gradient and ghost instabilities. Additionally, mochi_class features a quasi-static approximation implemented at the level of modified metric potentials, enhancing prediction accuracy, especially for models transitioning between a super- and sub-Compton regime. mochi_class can robustly handle a wide range of models without fine-tuning, and introduces a new approximation scheme that activates modifications to the standard cosmology deep in the matter-dominated era. Furthermore, it incorporates viability conditions on the equation of motion for the scalar field fluctuations, aiding in the identification of numerical instabilities. Through comprehensive validation against other Einstein-Boltzmann solvers, mochi_class demonstrates excellent performance and accuracy, broadening the scope of hi_class by facilitating the study of specific modified gravity models and enabling exploration of previously inaccessible regions of the Horndeski landscape. The code is publicly available at https://github.com/mcataneo/mochi_class_public">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/45c44ba92da169b2eb987805f3b6ad66528aeee4" target='_blank'>
              mochi_class: Modelling Optimisation to Compute Horndeski In class
              </a>
            </td>
          <td>
            Matteo Cataneo, Emilio Bellini
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using function-valued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure--a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2fb48eb416a7b362eb444cb5d9c111a6939db5fe" target='_blank'>
              Linearization Turns Neural Operators into Function-Valued Gaussian Processes
              </a>
            </td>
          <td>
            Emilia Magnani, Marvin Pförtner, Tobias Weber, Philipp Hennig
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In recent years, surrogate models based on deep neural networks have been widely used to solve partial differential equations for fluid flow physics. This kind of model focuses on global interpolation of the training data and thus requires a large network structure. The process is both time consuming and computationally costly. In the present study, we develop a neural network with local converging input (NNLCI) for high-fidelity prediction using unstructured data. The framework uses the local domain of dependence with converging coarse solutions as input, thereby greatly reducing computational resource and training time. As a validation case, the NNLCI method is applied to study two-dimensional inviscid supersonic flows in channels with bumps. Different bump geometries and locations are examined to benchmark the effectiveness and versatility of this new approach. The NNLCI method can accurately and efficiently capture the structure and dynamics of the entire flowfield, including regions with shock discontinuities. For a new bump configuration, the method can perform prediction with only one neural network, eliminating the need for repeated training of multiple networks for different geometries. A saving of computing wall time is achieved by several orders of magnitude against the high-fidelity simulation with the same level of accuracy. The demand on training data is modest, and the training data can be allocated sparsely. These features are especially advantageous compared with conventional global-to-global deep learning methods and physics-informed methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/88415c3817a7058d9678f49a393b306a231a9953" target='_blank'>
              Neural Network with Local Converging Input for Unstructured-Grid Computational Fluid Dynamics
              </a>
            </td>
          <td>
            Weiming Ding, Haoxiang Huang, T. Lee, Yingjie Liu, Vigor Yang
          </td>
          <td>2024-07-01</td>
          <td>AIAA Journal</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This two-part tutorial dives into the field of data-driven power flow linearization (DPFL), a domain gaining increased attention. DPFL stands out for its higher approximation accuracy, wide adaptability, and better ability to implicitly incorporate the latest system attributes. This renders DPFL a potentially superior option for managing the significant fluctuations from renewable energy sources, a step towards realizing a more sustainable energy future, by translating the higher model accuracy into increased economic efficiency and less energy losses. To conduct a deep and rigorous reexamination, this tutorial first classifies existing DPFL methods into DPFL training algorithms and supportive techniques. Their mathematical models, analytical solutions, capabilities, limitations, and generalizability are systematically examined, discussed, and summarized. In addition, this tutorial reviews existing DPFL experiments, examining the settings of test systems, the fidelity of datasets, and the comparison made among a limited number of DPFL methods. Further, this tutorial implements extensive numerical comparisons of all existing DPFL methods (40 methods in total) and four classic physics-driven approaches, focusing on their generalizability, applicability, accuracy, and computational efficiency. Through these simulationmethodss, this tutorial aims to reveal the actual performance of all the methods (including the performances exposed to data noise or outliers), guiding the selection of appropriate linearization methods. Furthermore, this tutorial discusses future directions based on the theoretical and numerical insights gained. As the first part, this paper reexamines DPFL theories, covering all the training algorithms and supportive techniques. Capabilities, limitations, and aspects of generalizability, which were previously unmentioned in the literature, have been identified.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1bdfa4f227320698ee10d100d4da18f732e22c80" target='_blank'>
              Data-driven Power Flow Linearization: Theory
              </a>
            </td>
          <td>
            Mengshuo Jia, Gabriela Hug, Ning Zhang, Zhaojian Wang, Yi Wang, Chongqing Kang
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="We propose kernel-based approaches for the construction of a single-step and multi-step predictor of the velocity form of nonlinear (NL) systems, which describes the time-difference dynamics of the corresponding NL system and admits a highly structured representation. The predictors in turn allow to formulate completely data-driven representations of the velocity form. The kernel-based formulation that we derive, inherently respects the structured quasi-linear and specific time-dependent relationship of the velocity form. This results in an efficient multi-step predictor for the velocity form and hence for nonlinear systems. Moreover, by using the velocity form, our methods open the door for data-driven behavioral analysis and control of nonlinear systems with global stability and performance guarantees.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b4583a2f8fb5852420ac4a8c1d46a5e7cd4adf9" target='_blank'>
              Kernel-based multi-step predictors for data-driven analysis and control of nonlinear systems through the velocity form
              </a>
            </td>
          <td>
            C. Verhoek, Roland T'oth
          </td>
          <td>2024-08-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In a sequence of papers, the author examined several statistical affinity measures for selecting the coarse degrees of freedom (CDOFs) or coarse nodes (Cnodes) in algebraic multigrid (AMG) for systems of elliptic partial differential equations (PDEs). These measures were applied to a set of relaxed vectors that exposes the problematic error components. Once the CDOFs are determined using any one of these measures, the interpolation operator is constructed in a bootstrap AMG (BAMG) procedure. However, in a recent paper of Kahl and Rottmann, the statistical least angle regression (LARS) method was utilized in the coarsening procedure and shown to be promising in the CDOF selection. This method is generally used in the statistics community to select the most relevant variables in constructing a parsimonious model for a very complicated and high‐dimensional model or data set (i.e., variable selection for a “reduced” model). As pointed out by Kahl and Rottmann, the LARS procedure has the ability to detect group relations between variables, which can be more useful than binary relations that are derived from strength‐of‐connection, or affinity measures, between pairs of variables. Moreover, by using an updated Cholesky factorization approach in the regression computation, the LARS procedure can be performed efficiently even when the original set of variables is large; and due to the LARS formulation itself (i.e., its ‐norm constraint), sparse interpolation operators can be generated. In this article, we extend the LARS coarsening approach to systems of PDEs. Furthermore, we incorporate some modifications to the LARS approach based on the so‐called elastic net and relaxed lasso methods, which are well known and thoroughly analyzed in the statistics community for ameliorating several major issues with LARS as a variable selection procedure. We note that the original LARS coarsening approach may have addressed some of these issues in similar or other ways but due to the limited details provided there, it is difficult to determine the extent of their similarities. Incorporating these modifications (or effecting them in similar ways) leads to improved robustness in the LARS coarsening procedure, and numerical experiments indicate that the changes lead to faster convergence in the multigrid method. Moreover, the relaxed lasso modification permits an indirect BAMG (iBAMG) extension to the interpolation operator. This iBAMG extension applied in an intra‐ or inter‐variable interpolation setting (i.e., nodal‐based coarsening), as well as in variable‐based coarsening, which will not preserve the nodal structure of a finest‐level discretization on the lower levels of the multilevel hierarchy, will be examined. For the variable‐based coarsening, because of the parsimonious feature of LARS, the performance is reasonably good when applied to systems of PDEs albeit at a substantial additional cost over a nodal‐based procedure.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9cd541114204fbe8a6e43f7aea58c521464d1e1f" target='_blank'>
              Least angle regression, relaxed lasso, and elastic net for algebraic multigrid of systems of elliptic partial differential equations
              </a>
            </td>
          <td>
            Barry Lee
          </td>
          <td>2024-06-18</td>
          <td>Numerical Linear Algebra with Applications</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Reservoir computing has been shown to be a useful framework for predicting critical transitions of a dynamical system if the bifurcation parameter is also provided as an input. Its utility is significant because in real-world scenarios, the exact model equations are unknown. This Letter shows how the theory of dynamical system provides the underlying mechanism behind the prediction. Using numerical methods, by considering dynamical systems which show Hopf bifurcation, we demonstrate that the map produced by the reservoir after a successful training undergoes a Neimark-Sacker bifurcation such that the critical point of the map is in immediate proximity to that of the original dynamical system. In addition, we have compared and analyzed different structures in the phase space. Our findings provide insight into the functioning of machine learning algorithms for predicting critical transitions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e38c7638dd9a0b907d25cf396095bd4a2d25e12" target='_blank'>
              Dynamical analysis of a parameter-aware reservoir computer
              </a>
            </td>
          <td>
            Dishant Sisodia, S. Jalan
          </td>
          <td>2024-07-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3ef6261d72acb3b8f584e24594af1af686836c8" target='_blank'>
              Differentiable Programming for Differential Equations: A Review
              </a>
            </td>
          <td>
            Facundo Sapienza, Jordi Bolibar, Frank Schäfer, Brian Groenke, Avik Pal, Victor Boussange, Patrick Heimbach, Giles Hooker, Fernando P'erez, Per-Olof Persson, Christopher Rackauckas
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="We introduce and analyze generalised polynomial chaos (gPC), considering both intrusive and non-intrusive approaches, as an uncertainty quantification method in studies of probabilistic robustness. The considered gPC methods are complementary to Monte Carlo (MC) methods and are shown to be fast and scalable, allowing for comprehensive and efficient exploration of parameter spaces. These properties enable robustness analysis of a wider set of models, compared to computationally expensive MC methods, while retaining desired levels of accuracy. We discuss the application of gPC methods to systems in biology and neuroscience, notably subject to multiple parametric uncertainties, and we examine a well-known model of neural dynamics as a case study.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/abb89db7124ec1925a107b3eb959ec0287242359" target='_blank'>
              Efficient gPC-based quantification of probabilistic robustness for systems in neuroscience
              </a>
            </td>
          <td>
            Uros Sutulovic, Daniele Proverbio, Rami Katz, Giulia Giordano
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Physical learning is an emerging paradigm in science and engineering whereby (meta)materials acquire desired macroscopic behaviors by exposure to examples. So far, it has been applied to static properties such as elastic moduli and self-assembled structures encoded in minima of an energy landscape. Here, we extend this paradigm to dynamic functionalities, such as motion and shape change, that are instead encoded in limit cycles or pathways of a dynamical system. We identify the two ingredients needed to learn time-dependent behaviors irrespective of experimental platforms: (i) learning rules with time delays and (ii) exposure to examples that break time-reversal symmetry during training. After providing a hands-on demonstration of these requirements using programmable LEGO toys, we turn to realistic particle-based simulations where the training rules are not programmed on a computer. Instead, we elucidate how they emerge from physico-chemical processes involving the causal propagation of fields, like in recent experiments on moving oil droplets with chemotactic signalling. Our trainable particles can self-assemble into structures that move or change shape on demand, either by retrieving the dynamic behavior previously seen during training, or by learning on the fly. This rich phenomenology is captured by a modified Hopfield spin model amenable to analytical treatment. The principles illustrated here provide a step towards von Neumann's dream of engineering synthetic living systems that adapt to the environment.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65200d8b0fc58bbbf5835e07be38dc47b087a743" target='_blank'>
              Learning dynamical behaviors in physical systems
              </a>
            </td>
          <td>
            R. Mandal, Rosalind Huang, Michel Fruchart, P. Moerman, Suriyanarayanan Vaikuntanathan, Arvind Murugan, Vincenzo Vitelli
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Model predictive control (MPC) is a powerful control method for handling complex nonlinear systems that are subject to constraints. However, the real-time application of this approach can be severely limited by the need to solve constrained nonlinear optimization problems at each sampling time. To this end, this work introduces a novel learning-based iterative solver that provides highly accurate predictions, optimality certification, and fast evaluation of the MPC solution at each sampling time. To learn this iterative solver, we propose an unsupervised training algorithm that builds on the Karush-Kuhn-Tucker optimality conditions, modified by a Fischer-Burmeister formulation, and eliminates the need for prior sampling of exact optimizer solutions. By exploiting efficient vector-Jacobian and Jacobian-vector products via automatic differentiation, the proposed training algorithm can be efficiently executed. We demonstrate the potential of the proposed learning-based iterative solver on the example of nonlinear model predictive control of a nonlinear double integrator. We show its advantages when compared to exact optimizer solutions and with an imitation learning-based approach that directly obtains a data-based approximation of the MPC control law.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04655a12d07dee491135aaae2f055b737e275291" target='_blank'>
              Learning Iterative Solvers for Accurate and Fast Nonlinear Model Predictive Control via Unsupervised Training
              </a>
            </td>
          <td>
            Lukas Lüken, Sergio Lucia
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Motivated by important applications to the analysis of complex noise-induced phenomena, we consider a problem of the constructive description of randomly forced equilibria for nonlinear systems with multiplicative noise. Using the apparatus of the first approximation systems, we construct an approximation of mean square deviations that explicitly takes into account the presence of multiplicative noises, depending on the current system state. A spectral criterion of existence and exponential stability of the stationary second moments for the solution of the first approximation system is presented. For mean square deviation, we derive an expansion in powers of the small parameter of noise intensity. Based on this theory, we derive a new, more accurate approximation of mean square deviations in a general nonlinear system with multiplicative noises. This approximation is compared with the widely used approximation based on the stochastic sensitivity technique. The general mathematical results are illustrated with examples of the model of climate dynamics and the van der Pol oscillator with hard excitement.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f8b4c1e986ba88d48cf2674010a03c5713b44f90" target='_blank'>
              Approximations in Mean Square Analysis of Stochastically Forced Equilibria for Nonlinear Dynamical Systems
              </a>
            </td>
          <td>
            I. Bashkirtseva
          </td>
          <td>2024-07-13</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="To advance our understanding of the magnetohydrodynamic (MHD) processes in liquid metals, in this paper we propose an approach combining the classical methods in the dynamo theory based on numerical simulations of the partial differential equations governing the evolution of the magnetic field with the statistical methods. In this study, we intend to answer the following ``optimization'' question: Can we find a statistical explanation what makes a flow to favour magnetic field generation in the linear regime (i.e. the kinematic dynamo is considered), where the Lorenz force is neglected? The flow is assumed to be steady and incompressible, and the magnetic field generation is governed by the magnetic induction equation. The behaviour of its solution is determined by the dominant (i.e. with the largest real part) eigenvalue of the magnetic induction operator. Considering an ensemble of 2193 randomly generated flows, we solved the kinematic dynamo problem and performed an attempt to find a correlation between the dominant eigenvalue and the standard quantities used in hydrodynamics -- vorticity and kinetic helicity. We have found that there is no visible relation between the property of the flow to be a kinematic dynamo and these quantities. This enables us to conclude that the problem requires a more elaborated approach to ``recognize'' if the flow is a dynamo or not; we plan to solve it using contemporary data-driven approach based on deep neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9cb7930ec7e8ea8ad0c81e79c9fa58cacaf4204f" target='_blank'>
              What makes a steady flow to favour kinematic magnetic field generation: A statistical analysis
              </a>
            </td>
          <td>
            Francisco Stefano de Almeida, R. Chertovskih, Sílvio M. A. Gama, Rui Gonccalves
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="The purpose of this paper is to propose a novel perspective, based on Willems'"behavior theory", on the design of an unknown-input observer for a given linear time-invariant discrete-time state-space model, with unknown disturbances affecting both the state and the output equations. The problem is first addressed assuming that the original system model is known, and later assuming that the model is unknown but historical data satisfying a certain assumption are available. In both cases, fundamental concepts in behavior theory, as the projection of a behavior, the inclusion of a behavior in another one, and the use of kernel and image representations, provide quite powerful tools to determine necessary and sufficient conditions for the existence of an unknown-input observer (UIO), as well as algorithms to design one of them, if it exists.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d5ab8824253eedddbefb06d51b96d8e58632d5f" target='_blank'>
              Behaviors, trajectories and data: A novel perspective on the design of unknown-input observers
              </a>
            </td>
          <td>
            G. Disarò, M. E. Valcher
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65f70fe6d698072acfae68cb9be3a90281df6d72" target='_blank'>
              Application of physics encoded neural networks to improve predictability of properties of complex multi-scale systems
              </a>
            </td>
          <td>
            M. Meinders, Jack Yang, Erik van der Linden
          </td>
          <td>2024-07-01</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Stars orbiting a supermassive black hole in the center of galaxies undergo very efficient diffusion in their orbital orientations. This is"Vector Resonant Relaxation", a diffusion process formally occurring on the unit sphere. Such a dynamics is intrinsically non-linear, stochastic, and correlated, hence bearing deep similarities with turbulence in fluid mechanics or plasma physics. In that context, we show how generic methods stemming from statistical closure theory, namely the celebrated"Martin-Siggia-Rose formalism", can be used to characterize the correlations describing the redistribution of orbital orientations. In particular, limiting ourselves to the leading order truncation in this closure scheme, the so-called"Direct Interaction Approximation", and placing ourselves in the limit of an isotropic distribution of orientations, we explicitly compare the associated prediction for the two-point correlation function with measures from numerical simulations. We discuss the successes and limitations of this approach and present possible future venues.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e181cfb0c7c1e8ba73a1eb46297222d975f2e04" target='_blank'>
              Vector Resonant Relaxation and Statistical Closure Theory. I. Direct Interaction Approximation
              </a>
            </td>
          <td>
            Sofia Flores, Jean-Baptiste Fouvry
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Data driven system identification is the technique for learning models from input/output data. To increase the robustness of the model estimation, prior knowledge can be incorporated, the so-called gray-box identification. In finite impulse response (FIR) models, prior knowledge of the process under investigation can be introduced by regularization. In the regularization term basic impulse response characteristics such as smoothness and exponentially decaying behavior can be incorporated. For estimation of time-delay systems, the novel impulse response and time-delay preserving (IRDP) regularization matrix is proposed. In this contribution this method is extended to the estimation of multiple input single output (MISO) processes and is compared to other state-of-the-art approaches. A linear process with four inputs and different input dynamics and time-delays is investigated. The focus of the evaluation is placed on model quality, time-delay estimation, and computation time. The simulation results point out the superiority of the novel regularization approach in comparison to state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a8735d69bbb91950534f1fe2742f48352fd17202" target='_blank'>
              Handling of Time Delays in MISO Processes with Regularized Finite Impulse Response Models
              </a>
            </td>
          <td>
            Christopher Illg, Tarek Kösters, Oliver Nelles
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Networks in machine learning offer examples of complex high-dimensional dynamical systems inspired by and reminiscent of biological systems. Here, we study the learning dynamics of generalized Hopfield networks, which permit visualization of internal memories. These networks have been shown to proceed through a “feature-to-prototype” transition, as the strength of network nonlinearity is increased, wherein the learned, or terminal, states of internal memories transition from mixed to pure states. Focusing on the prototype learning dynamics of the internal memories, we observe stereotypical dynamics of memories wherein similar subgroups of memories sequentially split at well-defined saddles. The splitting order is interpretable and reproducible from one simulation to the other. The dynamics prior to splits are robust to variations in many features of the system. To develop a more rigorous understanding of these global dynamics, we study smaller subsystems that exhibit similar properties to the full system. Within these smaller systems, we combine analytical calculations with numerical simulations to study the dynamics of the feature-to-prototype transition, and the emergence of saddle points in the learning landscape. We exhibit regimes where saddles appear and disappear through saddle-node bifurcations, qualitatively changing the distribution of learned memories as the strength of the nonlinearity is varied—allowing us to systematically investigate the mechanisms that underlie the emergence of the learning dynamics. Several features of the learning dynamics are reminiscent of the Waddington's caricature of cellular differentiation, and we attempt to make this analogy more precise. Memories can thus differentiate in a predictive and controlled way, revealing bridges between experimental biology, dynamical systems theory, and machine learning.




 Published by the American Physical Society
 2024


">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/acfb0996b8a2dd52af629b0fc803ccf66a9f85a9" target='_blank'>
              Waddington landscape for prototype learning in generalized Hopfield networks
              </a>
            </td>
          <td>
            Nacer Eddine Boukacem, Allen Leary, Robin Thériault, Felix Gottlieb, Madhav Mani, Paul François
          </td>
          <td>2024-07-23</td>
          <td>Physical Review Research</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Many real-world applications demand accurate and fast predictions, as well as reliable uncertainty estimates. However, quantifying uncertainty on high-dimensional predictions is still a severely under-invested problem, especially when input-output relationships are non-linear. To handle this problem, the present work introduces an innovative approach that combines autoencoder deep neural networks with the probabilistic regression capabilities of Gaussian processes. The autoencoder provides a low-dimensional representation of the solution space, while the Gaussian process is a Bayesian method that provides a probabilistic mapping between the low-dimensional inputs and outputs. We validate the proposed framework for its application to surrogate modeling of non-linear finite element simulations. Our findings highlight that the proposed framework is computationally efficient as well as accurate in predicting non-linear deformations of solid bodies subjected to external forces, all the while providing insightful uncertainty assessments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c5ebe3e35ead48c1cec6afd16f55a4873eabdbd1" target='_blank'>
              Gaussian process regression + deep neural network autoencoder for probabilistic surrogate modeling in nonlinear mechanics of solids
              </a>
            </td>
          <td>
            Saurabh Deshpande, Hussein Rappel, Mark Hobbs, Stéphane P. A. Bordas, Jakub Lengiewicz
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The Fisher Information Matrix (FIM) provides a way for quantifying the information content of an observable random variable concerning unknown parameters within a model that characterizes the variable. When parameters in a model are directly linked to individual features, the diagonal elements of the FIM can signify the relative importance of each feature. However, in scenarios where feature interactions may exist, a comprehensive exploration of the full FIM is necessary rather than focusing solely on its diagonal elements. This paper presents an end-to-end black box system identification approach that integrates the FIM into the training process to gain insights into dynamic importance and overall model structure. A decision module is added to the first layer of the network to determine the relevance scores using the entire FIM as input. The forward propagation is then performed on element-wise multiplication of inputs and relevance scores. Simulation results demonstrate that the proposed methodology effectively captures various types of interactions between dynamics, outperforming existing methods limited to polynomial interactions. Moreover, the effectiveness of this novel approach is confirmed through its application in identifying a real-world industrial system, specifically the PH neutralization process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/223d9021fa6b1a08ec49a719354add6931b3f923" target='_blank'>
              Dynamic importance learning using fisher information gain for nonlinear system identification
              </a>
            </td>
          <td>
            V. M. Eivaghi, M. A. Shoorehdeli
          </td>
          <td>2024-06-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Probabilistic state estimation is essential for robots navigating uncertain environments. Accurately and efficiently managing uncertainty in estimated states is key to robust robotic operation. However, nonlinearities in robotic platforms pose significant challenges that require advanced estimation techniques. Gaussian variational inference (GVI) offers an optimization perspective on the estimation problem, providing analytically tractable solutions and efficiencies derived from the geometry of Gaussian space. We propose a Sequential Gaussian Variational Inference (S-GVI) method to address nonlinearity and provide efficient sequential inference processes. Our approach integrates sequential Bayesian principles into the GVI framework, which are addressed using statistical approximations and gradient updates on the information geometry. Validations through simulations and real-world experiments demonstrate significant improvements in state estimation over the Maximum A Posteriori (MAP) estimation method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/811d7774eca3cb0fd2b941a37229cdba2c834554" target='_blank'>
              Sequential Gaussian Variational Inference for Nonlinear State Estimation applied to Robotic Applications
              </a>
            </td>
          <td>
            Min-Won Seo, Solmaz S. Kia
          </td>
          <td>2024-07-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Physics-informed neural networks (PINNs) have recently emerged as a promising way to compute the solutions of partial differential equations (PDEs) using deep neural networks. However, despite their significant success in various fields, it remains unclear in many aspects how to effectively train PINNs if the solutions of PDEs exhibit stiff behaviors or high frequencies. In this paper, we propose a new method for training PINNs using variable-scaling techniques. This method is simple and it can be applied to a wide range of problems including PDEs with rapidly-varying solutions. Throughout various numerical experiments, we will demonstrate the effectiveness of the proposed method for these problems and confirm that it can significantly improve the training efficiency and performance of PINNs. Furthermore, based on the analysis of the neural tangent kernel (NTK), we will provide theoretical evidence for this phenomenon and show that our methods can indeed improve the performance of PINNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9124ec1fca5f99e93eb695497f13fd9c818c0fa1" target='_blank'>
              VS-PINN: A Fast and efficient training of physics-informed neural networks using variable-scaling methods for solving PDEs with stiff behavior
              </a>
            </td>
          <td>
            Seungchan Ko, Sang Hyeon Park
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present an efficient physics-informed neural networks (PINNs) framework, termed Adaptive Interface-PINNs (AdaI-PINNs), to improve the modeling of interface problems with discontinuous coefficients and/or interfacial jumps. This framework is an enhanced version of its predecessor, Interface PINNs or I-PINNs (Sarma et al.; https://dx.doi.org/10.2139/ssrn.4766623), which involves domain decomposition and assignment of different predefined activation functions to the neural networks in each subdomain across a sharp interface, while keeping all other parameters of the neural networks identical. In AdaI-PINNs, the activation functions vary solely in their slopes, which are trained along with the other parameters of the neural networks. This makes the AdaI-PINNs framework fully automated without requiring preset activation functions. Comparative studies on one-dimensional, two-dimensional, and three-dimensional benchmark elliptic interface problems reveal that AdaI-PINNs outperform I-PINNs, reducing computational costs by 2-6 times while producing similar or better accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a80ea310f8f136bc43ebd91e533eefb2294c2ee9" target='_blank'>
              Adaptive Interface-PINNs (AdaI-PINNs): An Efficient Physics-informed Neural Networks Framework for Interface Problems
              </a>
            </td>
          <td>
            Sumanta Roy, C. Annavarapu, P. Roy, A. K. Sarma
          </td>
          <td>2024-06-07</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="This work addresses stochastic optimal control problems where the unknown state evolves in continuous time while partial, noisy, and possibly controllable measurements are only available in discrete time. We develop a framework for controlling such systems, focusing on the measure-valued process of the system's state and the control actions that depend on noisy and incomplete data. Our approach uses a stochastic optimal control framework with a probability measure-valued state, which accommodates noisy measurements and integrates them into control decisions through a Bayesian update mechanism. We characterize the control optimality in terms of a sequence of interlaced Hamilton Jacobi Bellman (HJB) equations coupled with controlled impulse steps at the measurement times. For the case of Gaussian-controlled processes, we derive an equivalent HJB equation whose state variable is finite-dimensional, namely the state's mean and covariance. We demonstrate the effectiveness of our methods through numerical examples. These include control under perfect observations, control under no observations, and control under noisy observations. Our numerical results highlight significant differences in the control strategies and their performance, emphasizing the challenges and computational demands of dealing with uncertainty in state observation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6ba24031ebf3376cb6db64dd547b84c989cf92e" target='_blank'>
              Continuous time Stochastic optimal control under discrete time partial observations
              </a>
            </td>
          <td>
            Christian Bayer, Boualem Djehiche, Eliza Rezvanova, Raúl Tempone
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="We present the hidden-layer concatenated physics informed neural network (HLConcPINN) method, which combines hidden-layer concatenated feed-forward neural networks, a modified block time marching strategy, and a physics informed approach for approximating partial differential equations (PDEs). We analyze the convergence properties and establish the error bounds of this method for two types of PDEs: parabolic (exemplified by the heat and Burgers' equations) and hyperbolic (exemplified by the wave and nonlinear Klein-Gordon equations). We show that its approximation error of the solution can be effectively controlled by the training loss for dynamic simulations with long time horizons. The HLConcPINN method in principle allows an arbitrary number of hidden layers not smaller than two and any of the commonly-used smooth activation functions for the hidden layers beyond the first two, with theoretical guarantees. This generalizes several recent neural-network techniques, which have theoretical guarantees but are confined to two hidden layers in the network architecture and the $\tanh$ activation function. Our theoretical analyses subsequently inform the formulation of appropriate training loss functions for these PDEs, leading to physics informed neural network (PINN) type computational algorithms that differ from the standard PINN formulation. Ample numerical experiments are presented based on the proposed algorithm to validate the effectiveness of this method and confirm aspects of the theoretical analyses.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/756e42c72cc211bde08b9880438d676e69260474" target='_blank'>
              Error Analysis and Numerical Algorithm for PDE Approximation with Hidden-Layer Concatenated Physics Informed Neural Networks
              </a>
            </td>
          <td>
            Yianxia Qian, Yongchao Zhang, Suchuan Dong
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Real-world non-autonomous systems are open, out-of-equilibrium systems that evolve in and are driven by temporally varying environments. Such systems can show multiple timescale and transient dynamics together with transitions to very different and, at times, even disastrous dynamical regimes. Since such critical transitions disrupt the systems' intended or desired functionality, it is crucial to understand the underlying mechanisms, to identify precursors of such transitions, and to reliably detect them in time series of suitable system observables to enable forecasts. This review critically assesses the various steps of investigation involved in time-series-analysis-based detection of critical transitions in real-world non-autonomous systems: from the data recording to evaluating the reliability of offline and online detections. It will highlight pros and cons to stimulate further developments, which would be necessary to advance understanding and forecasting nonlinear behavior such as critical transitions in complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b774b7e6f09a7e0224e2806d7eeec8722233491" target='_blank'>
              Time-series-analysis-based detection of critical transitions in real-world non-autonomous systems.
              </a>
            </td>
          <td>
            Klaus Lehnertz
          </td>
          <td>2024-06-07</td>
          <td>Chaos</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Ever since the concepts of dynamic programming were introduced, one of the most difficult challenges has been to adequately address high-dimensional control problems. With growing dimensionality, the utilisation of Deep Neural Networks promises to circumvent the issue of an otherwise exponentially increasing complexity. The paper specifically investigates the sampling issues the Deep Galerkin Method is subjected to. It proposes a drift relaxation-based sampling approach to alleviate the symptoms of high-variance policy approximations. This is validated on mean-field control problems; namely, the variations of the opinion dynamics presented by the Sznajd and the Hegselmann-Krause model. The resulting policies induce a significant cost reduction over manually optimised control functions and show improvements on the Linear-Quadratic Regulator problem over the Deep FBSDE approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/436f33e4de177b45b5684897ce37764ab9e4d1a5" target='_blank'>
              Optimal Control of Agent-Based Dynamics under Deep Galerkin Feedback Laws
              </a>
            </td>
          <td>
            Frederik Kelbel
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model not only improves its distribution-matching accuracy but also makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are inflexible with different types of constraints. Recent work proposed to learn mirror diffusion models (MDMs) in an unconstrained space defined by a mirror map and to impose the constraint with an inverse mirror map, but analytical mirror maps are challenging to derive for complex constraints. We propose neural approximate mirror maps (NAMMs) for general constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that pushes data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41e376b129ad93d8ea21f2780e8a91014ca3759f" target='_blank'>
              Neural Approximate Mirror Maps for Constrained Diffusion Models
              </a>
            </td>
          <td>
            Berthy T. Feng, Ricardo Baptista, Katherine L. Bouman
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a new method to learn low-dimensional representations of nonlinear time series without supervision and can have provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method is based on the assumption that the observed sequences arise from a common domain, but each sequence obeys its own autoregressive models that are related to each other through low-rank regularization. We cast the problem as a computationally efficient convex matrix parameter recovery problem using monotone Variational Inequality and encode the common domain assumption via low-rank constraint across the learned representations, which can learn the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality. We show the competitive performance of our method on real-world time-series data with the baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/374d6dcbbd6b7bc58c4e0fdbbfe9c1648975a899" target='_blank'>
              Nonlinear time-series embedding by monotone variational inequality
              </a>
            </td>
          <td>
            Jonathan Y. Zhou, Yao Xie
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Flow estimation problems are ubiquitous in scientific imaging. Often, the underlying flows are subject to physical constraints that can be exploited in the flow estimation; for example, incompressible (divergence-free) flows are expected for many fluid experiments, while irrotational (curl-free) flows arise in the analysis of optical distortions and wavefront sensing. In this work, we propose a Physics- Inspired Neural Network (PINN) named HDNet, which performs a Helmholtz decomposition of an arbitrary flow field, i.e., it decomposes the input flow into a divergence-only and a curl-only component. HDNet can be trained exclusively on synthetic data generated by reverse Helmholtz decomposition, which we call Helmholtz synthesis. As a PINN, HDNet is fully differentiable and can easily be integrated into arbitrary flow estimation problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3cf74daf478f8997bca401680db82727a5ce0ee6" target='_blank'>
              HDNet: Physics-Inspired Neural Network for Flow Estimation based on Helmholtz Decomposition
              </a>
            </td>
          <td>
            Miao Qi, R. Idoughi, Wolfgang Heidrich
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Temporal data modelling techniques with neural networks are useful in many domain applications, including time-series forecasting and control engineering. This paper aims at developing a recurrent version of stochastic configuration networks (RSCNs) for problem solving, where we have no underlying assumption on the dynamic orders of the input variables. Given a collection of historical data, we first build an initial RSCN model in the light of a supervisory mechanism, followed by an online update of the output weights by using a projection algorithm. Some theoretical results are established, including the echo state property, the universal approximation property of RSCNs for both the offline and online learnings, and the convergence of the output weights. The proposed RSCN model is remarkably distinguished from the well-known echo state networks (ESNs) in terms of the way of assigning the input random weight matrix and a special structure of the random feedback matrix. A comprehensive comparison study among the long short-term memory (LSTM) network, the original ESN, and several state-of-the-art ESN methods such as the simple cycle reservoir (SCR), the polynomial ESN (PESN), the leaky-integrator ESN (LIESN) and RSCN is carried out. Numerical results clearly indicate that the proposed RSCN performs favourably over all of the datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/46e3c55f0b0e77192fb6cc422ecf599a47334d73" target='_blank'>
              Recurrent Stochastic Configuration Networks for Temporal Data Analytics
              </a>
            </td>
          <td>
            Dianhui Wang, Gang Dang
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="This letter presents a high-dimensional analysis of the training dynamics for a single-layer nonlinear contrastive learning model. The empirical distribution of the model weights converges to a deterministic measure governed by a McKean-Vlasov nonlinear partial differential equation (PDE). Under L2 regularization, this PDE reduces to a closed set of low-dimensional ordinary differential equations (ODEs), reflecting the evolution of the model performance during the training process. We analyze the fixed point locations and their stability of the ODEs unveiling several interesting findings. First, only the hidden variable's second moment affects feature learnability at the state with uninformative initialization. Second, higher moments influence the probability of feature selection by controlling the attraction region, rather than affecting local stability. Finally, independent noises added in the data argumentation degrade performance but negatively correlated noise can reduces the variance of gradient estimation yielding better performance. Despite of the simplicity of the analyzed model, it exhibits a rich phenomena of training dynamics, paving a way to understand more complex mechanism behind practical large models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a791eb3679c0d2b194a35ba38ca2fe814772263" target='_blank'>
              Training Dynamics of Nonlinear Contrastive Learning Model in the High Dimensional Limit
              </a>
            </td>
          <td>
            Lineghuan Meng, Chuang Wang
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modeling thermal states for complex space missions, such as the surface exploration of airless bodies, requires high computation, whether used in ground-based analysis for spacecraft design or during onboard reasoning for autonomous operations. For example, a finite-element thermal model with hundreds of elements can take significant time to simulate, which makes it unsuitable for onboard reasoning during time-sensitive scenarios such as descent and landing, proximity operations, or in-space assembly. Further, the lack of fast and accurate thermal modeling drives thermal designs to be more conservative and leads to spacecraft with larger mass and higher power budgets. The emerging paradigm of physics-informed machine learning (PIML) presents a class of hybrid modeling architectures that address this challenge by combining simplified physics models with machine learning (ML) models resulting in models which maintain both interpretability and robustness. Such techniques enable designs with reduced mass and power through onboard thermal-state estimation and control and may lead to improved onboard handling of off-nominal states, including unplanned down-time. The PIML model or hybrid model presented here consists of a neural network which predicts reduced nodalizations (distribution and size of coarse mesh) given on-orbit thermal load conditions, and subsequently a (relatively coarse) finite-difference model operates on this mesh to predict thermal states. We compare the computational performance and accuracy of the hybrid model to a data-driven neural net model, and a high-fidelity finite-difference model of a prototype Earth-orbiting small spacecraft. The PIML based active nodalization approach provides significantly better generalization than the neural net model and coarse mesh model, while reducing computing cost by up to 1.7x compared to the high-fidelity model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04894c31eb0dcaa0ede3881b219f3bfbb359c4b6" target='_blank'>
              Physics-Informed Machine Learning Towards A Real-Time Spacecraft Thermal Simulator
              </a>
            </td>
          <td>
            Manaswin Oddiraju, Zaki Hasnain, Saptarshi Bandyopadhyay, Eric Sunada, Souma Chowdhury
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d332b278e945ecc01ec826948d1f3554cba5312b" target='_blank'>
              On the estimation rate of Bayesian PINN for inverse problems
              </a>
            </td>
          <td>
            Yi Sun, Debarghya Mukherjee, Yves Atchadé
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Advances in experimental techniques allow the collection of high-space-and-time resolution data that track individual motile entities over time. This poses the question of how to use these data to efficiently and effectively calibrate motion models. However, typical mathematical models often overlook the inherent aspects of data collection, such as the discreteness and the experimental noise of the measured locations. In this paper, we focus on velocity-jump models suitable to describe single-agent motion in one spatial dimension, characterised by successive Markovian transitions between a finite network of $n$ states, each with a specified velocity and a fixed rate of switching to every other state. Since the problem of finding the exact distributions of discrete-time noisy data is generally intractable, we derive a series of approximations for the data distributions and compare them to in-silico data generated by the models using four example network structures. These comparisons suggest that the approximations are accurate given sufficiently infrequent state switching, or equivalently, a sufficiently high data collection frequency. Moreover, for infrequent switching, the PDFs comparisons highlight the importance of accounting for the correlation between subsequent measured locations, due to the likely permanence in the state visited in the previous measurement. The approximate distributions computed can be used for fast parameter inference and model selection between a range of velocity-jump models using single-agent tracking data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f4862ab7ce4151b9c0569c8c9b73d89ab08ca0e" target='_blank'>
              Approximate solutions of a general stochastic velocity-jump process subject to discrete-time noisy observations
              </a>
            </td>
          <td>
            Arianna Ceccarelli, Alexander P. Browning, Ruth E. Baker
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this paper, we propose and study several inverse problems of determining unknown parameters in nonlocal nonlinear coupled PDE systems, including the potentials, nonlinear interaction functions and time-fractional orders. In these coupled systems, we enforce non-negativity of the solutions, aligning with realistic scenarios in biology and ecology. There are several salient features of our inverse problem study: the drastic reduction in measurement/observation data due to averaging effects, the nonlinear coupling between multiple equations, and the nonlocality arising from fractional-type derivatives. These factors present significant challenges to our inverse problem, and such inverse problems have never been explored in previous literature. To address these challenges, we develop new and effective schemes. Our approach involves properly controlling the injection of different source terms to obtain multiple sets of mean flux data. This allows us to achieve unique identifiability results and accurately determine the unknown parameters. Finally, we establish a connection between our study and practical applications in biology, further highlighting the relevance of our work in real-world contexts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6630d9bfe19de762af36c38b8a48daf0ad1ea2eb" target='_blank'>
              Inverse problems for coupled nonlocal nonlinear systems arising in mathematical biology
              </a>
            </td>
          <td>
            Ming-Hui Ding, Hongyu Liu, Catharine W. K. Lo
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The integration of particle or Kalman filters with machine learning tools like support vector machines, Gaussian processes, or neural networks has seen extensive exploration in the context of prognostic and health management, particularly in model-based applications. This paper focuses on the Multi-Layer Perceptron Particle Filter (MLP-PF), a data-driven approach that harnesses the non-linearity of MLP to describe degradation trajectories without relying on a physical model. The Bayesian nature of the particle filter is utilized to update MLP parameters, providing flexibility to the method and accommodating unexpected changes in the degradation behavior. To showcase the versatility of MLP-PF, this work demonstrates its seamless integration into diverse use cases, such as lithium-ion battery analysis, virtual health monitoring for turbofans, and the assessment of fatigue crack growth. We illustrate how it effortlessly accommodates various contexts through slight parameter modifications. Adjustment includes variation in the number of neurons or layers in the MLP, threshold adjustments, initial training refinements and the adaptation of the process noise. Addressing different degradation processes across these applications, MLP-PF proves its adaptability and utility in various contexts. These findings highlight the method’s versatility in adapting to diverse use cases and its potential as a robust prognostic tool across various industries. MLP-PF offers a practical and efficient means of estimating remaining useful life and predicting degradation in complex systems, with implications for advancing prognostic tools in diverse applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb6fdd4461741278755a0c986fba0e2f5cd89de3" target='_blank'>
              Data-Driven Prognostics with Multi-Layer Perceptron Particle Filter: a Cross-Industry Exploration
              </a>
            </td>
          <td>
            Francesco Canceliere, Sylvain Girard, Jean-Marc Bourinet
          </td>
          <td>2024-06-27</td>
          <td>PHM Society European Conference</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In functional data analysis, replicate observations of a smooth functional process and its derivatives offer a unique opportunity to flexibly estimate continuous-time ordinary differential equation models. Ramsay (1996) first proposed to estimate a linear ordinary differential equation from functional data in a technique called Principal Differential Analysis, by formulating a functional regression in which the highest-order derivative of a function is modelled as a time-varying linear combination of its lower-order derivatives. Principal Differential Analysis was introduced as a technique for data reduction and representation, using solutions of the estimated differential equation as a basis to represent the functional data. In this work, we re-formulate PDA as a generative statistical model in which functional observations arise as solutions of a deterministic ODE that is forced by a smooth random error process. This viewpoint defines a flexible class of functional models based on differential equations and leads to an improved understanding and characterisation of the sources of variability in Principal Differential Analysis. It does, however, result in parameter estimates that can be heavily biased under the standard estimation approach of PDA. Therefore, we introduce an iterative bias-reduction algorithm that can be applied to improve parameter estimates. We also examine the utility of our approach when the form of the deterministic part of the differential equation is unknown and possibly non-linear, where Principal Differential Analysis is treated as an approximate model based on time-varying linearisation. We demonstrate our approach on simulated data from linear and non-linear differential equations and on real data from human movement biomechanics. Supplementary R code for this manuscript is available at \url{https://github.com/edwardgunning/UnderstandingOfPDAManuscript}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d0be3d0b8344a99cb8ce9f30ed01a1653af4baa" target='_blank'>
              An Understanding of Principal Differential Analysis
              </a>
            </td>
          <td>
            Edward Gunning, Giles Hooker
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this paper, a novel data-driven approach for the development of soft sensors (SSs) for multi-step-ahead prediction of industrial process variables is proposed. This method is based on the recent developments in Koopman operator theory and dynamic mode decomposition (DMD). It is derived from Hankel DMD with control (HDMDc) to deal with highly nonlinear dynamics using augmented linear models, exploiting input and output regressors. The proposed multi-step-ahead HDMDc (MSA-HDMDc) is designed to perform multi-step prediction and capture complex dynamics with a linear approximation for a highly nonlinear system. This enables the construction of SSs capable of estimating the output of a process over a long period of time and/or using the developed SSs for model predictive control purposes. Hyperparameter tuning and model order reduction are specifically designed to perform multi-step-ahead predictions. Two real-world case studies consisting of a sulfur recovery unit and a debutanizer column, which are widely used as benchmarks in the SS field, are used to validate the proposed methodology. Data covering multiple system operating points are used for identification. The proposed MSA-HDMDc outperforms currently adopted methods in the SSs domain, such as autoregressive models with exogenous inputs and finite impulse response models, and proves to be robust to the variability of systems operating points.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2e0e6ed6f54bb1c9a50518c9d0827eb6b3eeea90" target='_blank'>
              Soft Sensors for Industrial Processes Using Multi-Step-Ahead Hankel Dynamic Mode Decomposition with Control
              </a>
            </td>
          <td>
            Luca Patané, F. Sapuppo, M. Xibilia
          </td>
          <td>2024-08-01</td>
          <td>Electronics</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="The paper considers the observer synthesis for nonlinear, time-varying plants with uncertain parameters under multiharmonic disturbance. It is assumed that the relative degree of the plant is known, the regressor linearly depends on the state vector and may have a nonlinear relationship with the output signal. The proposed solution consists of three steps. Initially, an unknown input state observer is synthesized. This observer, however, necessitates the measurement of output derivatives equal to the plant's relative degree. To relax this limitation, an alternative representation of the observer is introduced. Further, based on this observer, the unknown parameters and disturbances are reconstructed using an autoregression model and the dynamic regressor extension and mixing (DREM) approach. This approach allows the estimates to be obtained in a finite time. Finally, based on these estimates, an observer has been constructed that does not require measurements of the output derivatives. The effectiveness and efficiency of this solution are demonstrated through a computer simulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f3f630f0776c6b0ed2b88908f39e12f1e0579daa" target='_blank'>
              State estimation for a class of nonlinear time-varying uncertain system under multiharmonic disturbance
              </a>
            </td>
          <td>
            A. A. Margun, V. Bui, A. A. Bobtsov, Denis V. Efimov
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="When a probe particle immersed in a fluid with nonlinear interactions is subject to strong driving, the cumulants of the stochastic force acting on the probe are nonlinear functionals of the driving protocol. We present a Volterra series for these nonlinear functionals, by applying nonlinear response theory in a path integral formalism, where the emerging kernels are shown to be expressed in terms of connected equilibrium correlation functions. The first cumulant is the mean force, the second cumulant characterizes the non-equilibrium force fluctuations (noise), and higher order cumulants quantify non-Gaussian fluctuations. We discuss the interpretation of this formalism in relation to Langevin dynamics. We highlight two example scenarios of this formalism: i) For a particle driven with prescribed trajectory, the formalism yields the non-equilibrium statistics of the interaction force with the fluid. ii) For a particle confined in a moving trapping potential, the formalism yields the non-equilibrium statistics of the trapping force. In simulations of a model of nonlinearly interacting Brownian particles, we find that nonlinear phenomena, such as shear-thinning and oscillating noise covariance, appear in third or second order response, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/452a0b698c99157a7fec6246f2787dc73de21682" target='_blank'>
              Nonlinear Langevin functionals for a driven probe
              </a>
            </td>
          <td>
            Juliana Caspers, Matthias Kruger
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Inverse problems describe the process of estimating the causal factors from a set of measurements or data. Mapping of often incomplete or degraded data to parameters is ill-posed, thus data-driven iterative solutions are required, for example when reconstructing clean images from poor signals. Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers. However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs). This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases. We provide an explanation for this gap through the lens of measure-preserving dynamics of Random Dynamical Systems (RDS) with which we analyse Temporal Distribution Discrepancy and thus introduce a theoretical framework based on RDS for SDE diffusion models. We uncover several strategies that inherently enhance the stability and generalizability of diffusion models for inverse problems and introduce a novel score-based diffusion framework, the \textbf{D}ynamics-aware S\textbf{D}E \textbf{D}iffusion \textbf{G}enerative \textbf{M}odel (D$^3$GM). The \textit{Measure-preserving property} can return the degraded measurement to the original state despite complex degradation with the RDS concept of \textit{stability}. Our extensive experimental results corroborate the effectiveness of D$^3$GM across multiple benchmarks including a prominent application for inverse problems, magnetic resonance imaging. Code and data will be publicly available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf17cd0dcbb94d9ebc02ca488860c685bf255511" target='_blank'>
              Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics
              </a>
            </td>
          <td>
            Weitong Zhang, Chengqi Zang, Liu Li, Sarah Cechnicka, Ouyang Cheng, Bernhard Kainz
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We are interested in the computational study of shock hydrodynamics, i.e. problems involving compressible solids, liquids, and gases that undergo large deformation. These problems are dynamic and nonlinear and can exhibit complex instabilities. Due to advances in high performance computing it is possible to parameterize a hydrodynamic problem and perform a computational study yielding $\mathcal{O}\left({\rm TB}\right)$ of simulation state data. We present an interactive machine learning tool that can be used to compress, browse, and interpolate these large simulation datasets. This tool allows computational scientists and researchers to quickly visualize"what-if"situations, perform sensitivity analyses, and optimize complex hydrodynamic experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a3d102318eac008b4f999d4fed788b9945f727e" target='_blank'>
              Machine Learning Visualization Tool for Exploring Parameterized Hydrodynamics
              </a>
            </td>
          <td>
            C. Jekel, D. Sterbentz, T. M. Stitt, P. Mocz, R. Rieben, D. A. White, J. Belof
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Beyond estimating parameters of interest from data, one of the key goals of statistical inference is to properly quantify uncertainty in these estimates. In Bayesian inference, this uncertainty is provided by the posterior distribution, the computation of which typically involves an intractable high-dimensional integral. Among available approximation methods, sampling-based approaches come with strong theoretical guarantees but scale poorly to large problems, while variational approaches scale well but offer few theoretical guarantees. In particular, variational methods are known to produce overconfident estimates of posterior uncertainty and are typically non-identifiable, with many latent variable configurations generating equivalent predictions. Here, we address these challenges by showing how diffusion-based models (DBMs), which have recently produced state-of-the-art performance in generative modeling tasks, can be repurposed for performing calibrated, identifiable Bayesian inference. By exploiting a previously established connection between the stochastic and probability flow ordinary differential equations (pfODEs) underlying DBMs, we derive a class of models, inflationary flows, that uniquely and deterministically map high-dimensional data to a lower-dimensional Gaussian distribution via ODE integration. This map is both invertible and neighborhood-preserving, with controllable numerical error, with the result that uncertainties in the data are correctly propagated to the latent space. We demonstrate how such maps can be learned via standard DBM training using a novel noise schedule and are effective at both preserving and reducing intrinsic data dimensionality. The result is a class of highly expressive generative models, uniquely defined on a low-dimensional latent space, that afford principled Bayesian inference.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/17907a21b0ea22f5a5548076e21700dea63c3931" target='_blank'>
              Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models
              </a>
            </td>
          <td>
            Daniela de Albuquerque, John Pearson
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Scenario-based optimization and control has proven to be an efficient approach to account for system uncertainty. In particular, the performance of scenario-based model predictive control (MPC) schemes depends on the accuracy of uncertainty quantification. However, current learning- and scenario-based MPC (sMPC) approaches employ a single timeinvariant probabilistic model (learned offline), which may not accurately describe time-varying uncertainties. Instead, this paper presents a model-agnostic meta-learning (MAML) of Bayesian neural networks (BNN) for adaptive uncertainty quantification that would be subsequently used for adaptive-scenario-tree model predictive control design of nonlinear systems with unknown dynamics to enhance control performance. In particular, the proposed approach learns both a global BNN model and an updating law to refine the BNN model. At each time step, the updating law transforms the global BNN model into more precise local BNN models in real time. The adapted local model is then used to generate scenarios for sMPC design at each time step. A probabilistic safety certificate is incorporated in the scenario generation to ensure that the trajectories of the generated scenarios contain the real trajectory of the system and that all the scenarios adhere to the constraints with a high probability. Experiments using closed-loop simulations of a numerical example demonstrate that the proposed approach can improve the performance of scenario-based MPC compared to using only one BNN model learned offline for all time steps.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/473eca6073be70a1728a3138bc7e859f877dc3e3" target='_blank'>
              Adaptive Uncertainty Quantification for Scenario-based Control Using Meta-learning of Bayesian Neural Networks
              </a>
            </td>
          <td>
            Yajie Bao, Javad Mohammadpour Velni
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Machine Learning (ML) is extensively used for predicting transfer times for general purpose Wide Area Networks (WANs) or public Internet applications, but for Research and Education Networks (RENs) two major gaps exist in literature. First, RENs i.e. networks carrying large data flows have received limited attention by the networking community. RENs behave differently compared to the general purpose Internet applications and other network types. Hence, ML models from other network types cannot be used interchangeably for large data transfers. Second, the ML models are used as blackboxes to train on measured network values and then used to predict transfer times or other runtime network parameters. In this paper, we present a dynamical systems model of the large data transfers typical of RENs in the form of a system of Ordinary Differential Equations (ODEs) inspired by the Lotka-Volterra competition model. We present a transfer time prediction component called Dynamic Transfer Time Predictor (DTTP) which solves the ODEs and predicts the future transfer times. Second we formulate a loss function based on Lyapunov function called Lyapunov Drift Correction (LDC) that self-corrects the transfer time prediction errors dynamically.To design and develop our model, we studied real-world datasets consisting of over 100 million transfer records collected from platforms such as Open Science Grid (OSG), Large Hadron Collider Optical Private Network (LHCOPN), Worldwide LHC Grid (WLCG), as well as the RENs of Internet2 and ESNet. We integrate our model into well-known neural network models and regressors and present evaluation results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4cf5d71336cf90025f7d641a600628cc1d15b0d7" target='_blank'>
              Improving Transfer Time Prediction of ML Models via Auto-correcting Dynamical Systems Modeling
              </a>
            </td>
          <td>
            Venkat Sai Suman Lamba Karanam, B. Ramamurthy
          </td>
          <td>2024-06-24</td>
          <td>2024 IEEE 10th International Conference on Network Softwarization (NetSoft)</td>
          <td>0</td>
          <td>38</td>
        </tr>

        <tr id="Energy based models (EBMs) are appealing for their generality and simplicity in data likelihood modeling, but have conventionally been difficult to train due to the unstable and time-consuming implicit MCMC sampling during contrastive divergence training. In this paper, we present a novel energy-based generative framework, Variational Potential Flow (VAPO), that entirely dispenses with implicit MCMC sampling and does not rely on complementary latent models or cooperative training. The VAPO framework aims to learn a potential energy function whose gradient (flow) guides the prior samples, so that their density evolution closely follows an approximate data likelihood homotopy. An energy loss function is then formulated to minimize the Kullback-Leibler divergence between density evolution of the flow-driven prior and the data likelihood homotopy. Images can be generated after training the potential energy, by initializing the samples from Gaussian prior and solving the ODE governing the potential flow on a fixed time interval using generic ODE solvers. Experiment results show that the proposed VAPO framework is capable of generating realistic images on various image datasets. In particular, our proposed framework achieves competitive FID scores for unconditional image generation on the CIFAR-10 and CelebA datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6bc60701775b0cc308c3beaf6e494e8920626217" target='_blank'>
              Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling
              </a>
            </td>
          <td>
            J. Loo, Michelle Adeline, Arghya Pal, Vishnu Monn Baskaran, Chee-Ming Ting, Raphaël C.-W. Phan
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="The efficient solution (fast and accurate) of parametric partial differential equations (pPDE) is of major interest in many domains of science and engineering, enabling evaluations of the quantities of interest, optimization, control, and uncertainty propagation—all them under stringent real-time constraints. Different methodologies have been proposed in the past within the model order reduction (MOR) community, based on the use of reduced bases (RB) or the separated representation at the heart of the so-called proper generalized decompositions (PGD). In PGD, an alternate-direction strategy is employed to circumvent the integration issues of operating in multi-dimensional domains. Recently, physics informed neural networks (PINNs), a particular collocation schema where the unknown field is approximated by a neural network (NN), have emerged in the domain of scientific machine learning. PNNs combine the versatility of NN-based approximation with the ease of collocating pPDE. The present paper proposes a combination of both procedures to find an efficient solution for pPDE, that can either be viewed as an efficient collocation procedure for PINN, or as a monolithic PGD that bypasses the use of the fixed-point alternated directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73ebcfa6347c0ff167a1d0bd01e8827132585776" target='_blank'>
              A Parsimonious Separated Representation Empowering PINN–PGD-Based Solutions for Parametrized Partial Differential Equations
              </a>
            </td>
          <td>
            C. Ghnatios, Francisco Chinesta
          </td>
          <td>2024-07-29</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Transformer-based models have demonstrated exceptional performance across diverse domains, becoming the state-of-the-art solution for addressing sequential machine learning problems. Even though we have a general understanding of the fundamental components in the transformer architecture, little is known about how they operate or what are their expected dynamics. Recently, there has been an increasing interest in exploring the relationship between attention mechanisms and Hopfield networks, promising to shed light on the statistical physics of transformer networks. However, to date, the dynamical regimes of transformer-like models have not been studied in depth. In this paper, we address this gap by using methods for the study of asymmetric Hopfield networks in nonequilibrium regimes --namely path integral methods over generating functionals, yielding dynamics governed by concurrent mean-field variables. Assuming 1-bit tokens and weights, we derive analytical approximations for the behavior of large self-attention neural networks coupled to a softmax output, which become exact in the large limit size. Our findings reveal nontrivial dynamical phenomena, including nonequilibrium phase transitions associated with chaotic bifurcations, even for very simple configurations with a few encoded features and a very short context window. Finally, we discuss the potential of our analytic approach to improve our understanding of the inner workings of transformer models, potentially reducing computational training costs and enhancing model interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2a1c230935b9a3eeb376741764f658296dcdd3b" target='_blank'>
              Dynamical Mean-Field Theory of Self-Attention Neural Networks
              </a>
            </td>
          <td>
            Ángel Poc-López, Miguel Aguilera
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In the realm of experimental fluid mechanics, accurately reconstructing high-resolution flow fields is notably challenging due to often sparse and incomplete data across time and space domains. This is exacerbated by the limitations of current experimental tools and methods, which leave critical areas without measurable data. This research suggests a feasible solution to this problem by employing an inverse physics-informed neural network (PINN) to merge available sparse data with physical laws. The method's efficacy is demonstrated using flow around a cylinder as a case study, with three distinct training sets. One was the sparse velocity data from a domain, and the other two datasets were limited velocity data obtained from the domain boundaries and sensors around the cylinder wall. The coefficient of determination (R2) coefficient and mean squared error (RMSE) metrics, indicative of model performance, have been determined for the velocity components of all models. For the 28 sensors model, the R2 value stands at 0.996 with an associated RMSE of 0.0251 for the u component, while for the v component, the R2 value registers at 0.969, accompanied by an RMSE of 0.0169. The outcomes indicate that the method can successfully recreate the actual velocity field with considerable precision with more than 28 sensors around the cylinder, highlighting PINN's potential as an effective data assimilation technique for experimental fluid mechanics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10c366d3620b0cb9b8ea13515c105a9559a32ae2" target='_blank'>
              Flow field reconstruction from sparse sensor measurements with physics-informed neural networks
              </a>
            </td>
          <td>
            Mohammad Yasin Hosseini, Yousef Shiri
          </td>
          <td>2024-07-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Many industrial processes can be described by distributed parameter systems (DPSs) governed by partial differential equations (PDEs). In this research, a spatiotemporal network is proposed for DPS modeling without any process knowledge. Since traditional linear modeling methods may not work well for nonlinear DPSs, the proposed method considers the nonlinear space-time separation, which is transformed into a Lagrange dual optimization problem under the orthogonal constraint. The optimization problem can be solved by the proposed neural network with good structural interpretability. The spatial construction method is employed to derive the continuous spatial basis functions (SBFs) based on the discrete spatial features. The nonlinear temporal model is derived by the Gaussian process regression (GPR). Benefiting from spatial construction and GPR, the proposed method enables spatially continuous modeling and provides a reliable output range under the given confidence level. Experiments on a catalytic reaction process and a battery thermal process demonstrate the effectiveness and superiority of the proposed method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6bcfa839d7defe2cde7d0acf69e5a592bdf81de3" target='_blank'>
              Spatiotemporal Transformation-Based Neural Network With Interpretable Structure for Modeling Distributed Parameter Systems.
              </a>
            </td>
          <td>
            Peng Wei, Han-Xiong Li
          </td>
          <td>2024-07-25</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="In this paper, we study efficient approximate sampling for probability distributions known up to normalization constants. We specifically focus on a problem class arising in Bayesian inference for large-scale inverse problems in science and engineering applications. The computational challenges we address with the proposed methodology are: (i) the need for repeated evaluations of expensive forward models; (ii) the potential existence of multiple modes; and (iii) the fact that gradient of, or adjoint solver for, the forward model might not be feasible. While existing Bayesian inference methods meet some of these challenges individually, we propose a framework that tackles all three systematically. Our approach builds upon the Fisher-Rao gradient flow in probability space, yielding a dynamical system for probability densities that converges towards the target distribution at a uniform exponential rate. This rapid convergence is advantageous for the computational burden outlined in (i). We apply Gaussian mixture approximations with operator splitting techniques to simulate the flow numerically; the resulting approximation can capture multiple modes thus addressing (ii). Furthermore, we employ the Kalman methodology to facilitate a derivative-free update of these Gaussian components and their respective weights, addressing the issue in (iii). The proposed methodology results in an efficient derivative-free sampler flexible enough to handle multi-modal distributions: Gaussian Mixture Kalman Inversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically and numerically in several experiments with multimodal target distributions, including proof-of-concept and two-dimensional examples, as well as a large-scale application: recovering the Navier-Stokes initial condition from solution data at positive times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d77575c529cb455ba8ac5289fe0e14615a7c4f1" target='_blank'>
              Efficient, Multimodal, and Derivative-Free Bayesian Inference With Fisher-Rao Gradient Flows
              </a>
            </td>
          <td>
            Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M. Stuart
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="We construct time-evolving gravitational potential models for a Milky Way-mass galaxy from the FIRE-2 suite of cosmological-baryonic simulations using basis function expansions. These models capture the angular variation with spherical harmonics for the halo and azimuthal harmonics for the disk, and the radial or meridional plane variation with splines. We fit low-order expansions (4 angular/harmonic terms) to the galaxy's potential for each snapshot, spaced roughly 25 Myr apart, over the last 4 Gyr of its evolution, then extract the forces at discrete times and interpolate them between adjacent snapshots for forward orbit integration. Our method reconstructs the forces felt by simulation particles with high fidelity, with 95% of both stars and dark matter, outside of self-gravitating subhalos, exhibiting errors $\leq4\%$ in both the disk and the halo. Imposing symmetry on the model systematically increases these errors, particularly for disk particles, which show greater sensitivity to imposed symmetries. The majority of orbits recovered using the models exhibit positional errors $\leq10\%$ for 2-3 orbital periods, with higher errors for orbits that spend more time near the galactic center. Approximate integrals of motion are retrieved with high accuracy even with a larger potential sampling interval of 200 Myr. After 4 Gyr of integration, 43% and 70% of orbits have total energy and angular momentum errors within 10%, respectively. Consequently, there is higher reliability in orbital shape parameters such as pericenters and apocenters, with errors $\sim10\%$ even after multiple orbital periods. These techniques have diverse applications, including studying satellite disruption in cosmological contexts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/de24194c7be3b3d4cbdf06cc58d5aad48f9bf27a" target='_blank'>
              Efficient and accurate force replay in cosmological-baryonic simulations
              </a>
            </td>
          <td>
            Arpit Arora, Robyn E. Sanderson, Christopher Regan, N. Garavito-Camargo, Emily Bregou, N. Panithanpaisal, Andrew Wetzel, E. Cunningham, S. Loebman, A. Dropulic, Nora Shipp
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Symmetry is one of the most central concepts in physics, and it is no surprise that it has also been widely adopted as an inductive bias for machine-learning models applied to the physical sciences. This is especially true for models targeting the properties of matter at the atomic scale. Both established and state-of-the-art approaches, with almost no exceptions, are built to be exactly equivariant to translations, permutations, and rotations of the atoms. Incorporating symmetries -- rotations in particular -- constrains the model design space and implies more complicated architectures that are often also computationally demanding. There are indications that non-symmetric models can easily learn symmetries from data, and that doing so can even be beneficial for the accuracy of the model. We put a model that obeys rotational invariance only approximately to the test, in realistic scenarios involving simulations of gas-phase, liquid, and solid water. We focus specifically on physical observables that are likely to be affected -- directly or indirectly -- by symmetry breaking, finding negligible consequences when the model is used in an interpolative, bulk, regime. Even for extrapolative gas-phase predictions, the model remains very stable, even though symmetry artifacts are noticeable. We also discuss strategies that can be used to systematically reduce the magnitude of symmetry breaking when it occurs, and assess their impact on the convergence of observables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5dc964683c6ce6be616e96982ba7cf5172752d68" target='_blank'>
              Probing the effects of broken symmetries in machine learning
              </a>
            </td>
          <td>
            Marcel F. Langer, S. Pozdnyakov, Michele Ceriotti
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this work, we propose a set of physics-informed geometric operators (GOs) to enrich the geometric data provided for training surrogate/discriminative models, dimension reduction, and generative models, typically employed for performance prediction, dimension reduction, and creating data-driven parameterisations, respectively. However, as both the input and output streams of these models consist of low-level shape representations, they often fail to capture shape characteristics essential for performance analyses. Therefore, the proposed GOs exploit the differential and integral properties of shapes--accessed through Fourier descriptors, curvature integrals, geometric moments, and their invariants--to infuse high-level intrinsic geometric information and physics into the feature vector used for training, even when employing simple model architectures or low-level parametric descriptions. We showed that for surrogate modelling, along with the inclusion of the notion of physics, GOs enact regularisation to reduce over-fitting and enhance generalisation to new, unseen designs. Furthermore, through extensive experimentation, we demonstrate that for dimension reduction and generative models, incorporating the proposed GOs enriches the training data with compact global and local geometric features. This significantly enhances the quality of the resulting latent space, thereby facilitating the generation of valid and diverse designs. Lastly, we also show that GOs can enable learning parametric sensitivities to a great extent. Consequently, these enhancements accelerate the convergence rate of shape optimisers towards optimal solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e89b2b8bdad2c98d764f8c828e40b1065ff7464d" target='_blank'>
              Physics-Informed Geometric Operators to Support Surrogate, Dimension Reduction and Generative Models for Engineering Design
              </a>
            </td>
          <td>
            Shahroz Khan, Zahid Masood, Muhammad Usama, Konstantinos V. Kostas, P. Kaklis, Wei Chen
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="We discuss an approach to mathematically modelling systems made of objects that are coupled together, using generative models of the dependence relationships between states (or trajectories) of the things comprising such systems. This broad class includes open or non-equilibrium systems and is especially relevant to self-organising systems. The ensuing variational free energy principle (FEP) has certain advantages over using random dynamical systems explicitly, notably, by being more tractable and offering a parsimonious explanation of why the joint system evolves in the way that it does, based on the properties of the coupling between system components. Using the FEP allows us to model the dynamics of an object as if it were a process of variational inference, because variational free energy (or surprisal) is a Lyapunov function for its dynamics. In short, we argue that using generative models to represent and track relations among subsystems leads us to a particular statistical theory of interacting systems. Conversely, this theory enables us to construct nested models that respect the known relations among subsystems. We point out that the fact that a physical object conforms to the FEP does not necessarily imply that this object performs inference in the literal sense; rather, it is a useful explanatory fiction which replaces the 'explicit' dynamics of the object with an 'implicit' flow on free energy gradients - a fiction that may or may not be entertained by the object itself.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7d9000095cab2d2187f9dc3eb5aeba6fe5752a46" target='_blank'>
              An approach to non-equilibrium statistical physics using variational Bayesian inference
              </a>
            </td>
          <td>
            M. Ramstead, D. A. R. Sakthivadivel, K. Friston
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Variational Physics-Informed Neural Networks often suffer from poor convergence when using stochastic gradient-descent-based optimizers. By introducing a Least Squares solver for the weights of the last layer of the neural network, we improve the convergence of the loss during training in most practical scenarios. This work analyzes the computational cost of the resulting hybrid Least-Squares/Gradient-Descent optimizer and explains how to implement it efficiently. In particular, we show that a traditional implementation based on backward-mode automatic differentiation leads to a prohibitively expensive algorithm. To remedy this, we propose using either forward-mode automatic differentiation or an ultraweak-type scheme that avoids the differentiation of trial functions in the discrete weak formulation. The proposed alternatives are up to 100 times faster than the traditional one, recovering a computational cost-per-iteration similar to that of a conventional gradient-descent-based optimizer alone. To support our analysis, we derive computational estimates and conduct numerical experiments in one- and two-dimensional problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/17eda77a3cdea99de9f795c5bb3e5312a9f5c667" target='_blank'>
              Optimizing Variational Physics-Informed Neural Networks Using Least Squares
              </a>
            </td>
          <td>
            C. Uriarte, Manuela Bastidas, David Pardo, Jamie M. Taylor, Sergio Rojas
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Time-delay fractional optimal control problems (OCPs) are an important research area for developing effective control and optimization strategies to address complex phenomena occurring in various natural sciences, such as physics, chemistry, biology, and engineering. By considering fractional OCPs with time delays, we can design control strategies that take into account the system's history and optimize its behavior over a given time horizon. However, applying the Pontryagin principle of maximization to solve these problems leads to a boundary value problem (BVP) that includes delay and advance terms, making analytical solutions difficult and demanding. To address this issue, this paper presents a precise finite difference formula to solve the aforementioned advance-delay BVP numerically. The suggested approximate method's error analysis and convergence properties are provided, and several illustrative examples demonstrate the applicability, validity, and accuracy of the proposed approach. Simulation results confirm the proposed technique's advantages for the optimal control of delay fractional dynamical equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a596cb9b8d036a751e9996eea24f4510663c8a7c" target='_blank'>
              An accurate finite difference formula for the numerical solution of delay-dependent fractional optimal control problems
              </a>
            </td>
          <td>
            D. Băleanu, M. Hajipour, A. Jajarmi
          </td>
          <td>2024-07-12</td>
          <td>An International Journal of Optimization and Control: Theories &amp; Applications (IJOCTA)</td>
          <td>0</td>
          <td>40</td>
        </tr>

        <tr id="Time-varying optimization problems are central to many engineering applications, where performance metrics and system constraints evolve dynamically with time. A number of algorithms have been proposed in recent years to solve such problems; a common feature of all these methods is that they implicitly require precise knowledge of the temporal variability of the solutions in order to exactly track the optimizers. In this paper, we seek to lift these stringent assumptions. Our main result is a fundamental characterization, showing that an algorithm can track an optimal trajectory if and only if it contains a model of the temporal variability of the problem. We refer to this concept to as the internal model principle of time-varying optimization. By recasting the optimization objective as a nonlinear regulation problem and using tools from center manifold theory, we provide necessary and sufficient conditions both for an optimization algorithm to achieve exact asymptotic tracking and for such an algorithm to exist. We illustrate the applicability of the approach numerically on both synthetic problems as well as practical problems in transportation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8aee1c4d1d9bfe7e68a03ee2eb770d81f18d112d" target='_blank'>
              The Internal Model Principle of Time-Varying Optimization
              </a>
            </td>
          <td>
            G. Bianchin, Bryan Van Scoy
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We introduce COBRA (Cosmology with Optimally factorized Bases of Radial Approximants), a novel framework for rapid computation of large-scale structure observables. COBRA separates scale dependence from cosmological parameters in the linear matter power spectrum while also minimising the number of necessary basis terms $N_b$, thus enabling direct and efficient computation of derived and nonlinear observables. Moreover, the dependence on cosmological parameters is efficiently approximated using radial basis function interpolation. We apply our framework to decompose the linear matter power spectrum in the standard $\Lambda$CDM scenario, as well as by adding curvature, dynamical dark energy and massive neutrinos, covering all redshifts relevant for Stage IV surveys. With only a dozen basis terms $N_b$, COBRA reproduces exact Boltzmann solver calculations to $\sim 0.1\%$ precision, which improves further to $0.02\%$ in the pure $\Lambda$CDM scenario. Using our decomposition, we recast the one-loop redshift space galaxy power spectrum in a separable minimal-basis form, enabling $\sim 4000$ model evaluations per second at $0.02\%$ precision on a single thread. This constitutes a considerable improvement over previously existing methods (e.g., FFTLog) opening a window for efficient computations of higher loop and higher order correlators involving multiple powers of the linear matter power spectra. The resulting factorisation can also be utilised in clustering, weak lensing and CMB analyses. Our implementation will be made public upon publication.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b3dbca5211f988f8e52a21fdea5bf0a183d346ba" target='_blank'>
              COBRA: Optimal Factorization of Cosmological Observables
              </a>
            </td>
          <td>
            Thomas Bakx, N. E. Chisari, Z. Vlah
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="It has recently been realized that many theories of physics beyond the Standard Model give rise to cosmological histories exhibiting extended epochs of cosmological stasis. During such epochs, the abundances of different energy components such as matter, radiation, and vacuum energy each remain fixed despite cosmological expansion. In previous analyses of the stasis phenomenon, these different energy components were modeled as fluids with fixed, unchanging equations of state. In this paper, by contrast, we consider more realistic systems involving dynamical scalars which pass through underdamping transitions as the universe expands. Indeed, such systems might be highly relevant for BSM scenarios involving higher-dimensional bulk moduli and inflatons. Remarkably, we find that stasis emerges even in such situations, despite the appearance of time-varying equations of state. Moreover, this stasis includes several new features which might have important phenomenological implications and applications. For example, in the presence of an additional"background"energy component, we find that the scalars evolve into a"tracking"stasis in which the stasis equation of state automatically tracks that of the background. This phenomenon exists even if the background has only a small initial abundance. We also discuss the intriguing possibility that our results might form the basis of a new"Stasis Inflation"scenario in which no ad-hoc inflaton potential is needed and in which there is no graceful-exit problem. Within such a scenario, the number of e-folds of cosmological expansion produced is directly related to the hierarchies between physical BSM mass scales. Moreover, non-zero matter and radiation abundances can be sustained throughout the inflationary epoch.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d2fb6599d1f8077e14afa3e54fb4855ccc7ef62c" target='_blank'>
              Cosmological Stasis from Dynamical Scalars: Tracking Solutions and the Possibility of a Stasis-Induced Inflation
              </a>
            </td>
          <td>
            K. Dienes, L. Heurtier, Fei Huang, T. Tait, B. Thomas
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="Areas of computational mechanics such as uncertainty quantification and optimization usually involve repeated evaluation of numerical models that represent the behavior of engineering systems. In the case of complex nonlinear systems however, these models tend to be expensive to evaluate, making surrogate models quite valuable. Artificial neural networks approximate systems very well by taking advantage of the inherent information of its given training data. In this context, this paper investigates the improvement of the training process by including sensitivity information, which are partial derivatives w.r.t. inputs, as outlined by Sobolev training. In computational mechanics, sensitivities can be applied to neural networks by expanding the training loss function with additional loss terms, thereby improving training convergence resulting in lower generalisation error. This improvement is shown in two examples of linear and non-linear material behavior. More specifically, the Sobolev designed loss function is expanded with residual weights adjusting the effect of each loss on the training step. Residual weighting is the given scaling to the different training data, which in this case are response and sensitivities. These residual weights are optimized by an adaptive scheme, whereby varying objective functions are explored, with some showing improvements in accuracy and precision of the general training convergence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bc84ce2dba59d576dd4d3f10a9b0f7b62826171a" target='_blank'>
              Sobolev neural network with residual weighting as a surrogate in linear and non-linear mechanics
              </a>
            </td>
          <td>
            A.O.M. Kilicsoy, J. Liedmann, M. Valdebenito, F. Barthold, M. Faes
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="The brain needs to perform time-critical computations to ensure survival. A potential solution lies in the non-local, distributed computation at the whole-brain level made possible by criticality and amplified by the rare long-range connections found in the brain’s unique anatomical structure. This non-locality can be captured by the mathematical structure of Schrödinger’s wave equation, which is at the heart of the novel CHARM (Complex Harmonics decomposition) framework that performs the necessary dimensional manifold reduction able to extract non-locality in critical spacetime brain dynamics. Using a large neuroimaging dataset of over 1,000 people, CHARM captured the critical, non-local/long-range nature of brain dynamics and the underlying mechanisms were established using a precise whole-brain model. Equally, CHARM revealed the significantly different critical dynamics of wakefulness and sleep. Overall, CHARM is a promising theoretical framework for capturing the low-dimensionality of the complex network dynamics observed in neuroscience and provides evidence that networks of brain regions rather than individual brain regions are the key computational engines of critical brain dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79c92bde562ae0356acda295ead7128e8c11928b" target='_blank'>
              Complex harmonics reveal low-dimensional manifolds of critical brain dynamics
              </a>
            </td>
          <td>
            G. Deco, Y. Perl, M. Kringelbach
          </td>
          <td>2024-06-16</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="A new particle-based sampling and approximate inference method, based on electrostatics and Newton mechanics principles, is introduced with theoretical ground, algorithm design and experimental validation. This method simulates an interacting particle system (IPS) where particles, i.e. the freely-moving negative charges and spatially-fixed positive charges with magnitudes proportional to the target distribution, interact with each other via attraction and repulsion induced by the resulting electric fields described by Poisson's equation. The IPS evolves towards a steady-state where the distribution of negative charges conforms to the target distribution. This physics-inspired method offers deterministic, gradient-free sampling and inference, achieving comparable performance as other particle-based and MCMC methods in benchmark tasks of inferring complex densities, Bayesian logistic regression and dynamical system identification. A discrete-time, discrete-space algorithmic design, readily extendable to continuous time and space, is provided for usage in more general inference problems occurring in probabilistic machine learning scenarios such as Bayesian inference, generative modelling, and beyond.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f084ff68562acff79ea7b538f61a3d7bacd36009" target='_blank'>
              Electrostatics-based particle sampling and approximate inference
              </a>
            </td>
          <td>
            Yongchao Huang
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>0</td>
        </tr>

        <tr id="In this work, we first show that the problem of parameter identification is often ill-conditioned and lacks the persistence of excitation required for the convergence of online learning schemes. To tackle these challenges, we introduce the notion of optimal and greedy excitation sets which contain data points with sufficient richness to aid in the identification task. We then present the greedy excitation set-based recursive least squares algorithm to alleviate the problem of the lack of persistent excitation, and prove that the iterates generated by the proposed algorithm minimize an auxiliary weighted least squares cost function. When data points are generated from time-varying parameters, online estimators tend to underfit the true parameter trajectory, and their predictability deteriorates. To tackle this problem, we propose a memory resetting scheme leveraging change point detection techniques. Finally, we illustrate the performance of the proposed algorithms via several numerical case studies to learn the (time-varying) parameters of networked epidemic dynamics, and compare it with results obtained using conventional approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f77ed1877cc1126478cecdbf4ca3316c240db17e" target='_blank'>
              Online Identification of Time-Varying Systems Using Excitation Sets and Change Point Detection
              </a>
            </td>
          <td>
            Chi Ho Leung, Ashish R. Hota, Philip E. Par'e
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online prediction of generic (possibly non-Markovian) stochastic processes with irregular (in time) and potentially incomplete (with respect to coordinates) observations. It is a model for which convergence to the $L^2$-optimal predictor, which is given by the conditional expectation, is established theoretically. Thereby, the training of the model is solely based on a dataset of realizations of the underlying stochastic process, without the need of knowledge of the law of the process. In the case where the underlying process is deterministic, the conditional expectation coincides with the process itself. Therefore, this framework can equivalently be used to learn the dynamics of ODE or PDE systems solely from realizations of the dynamical system with different initial conditions. We showcase the potential of our method by applying it to the chaotic system of a double pendulum. When training the standard PD-NJ-ODE method, we see that the prediction starts to diverge from the true path after about half of the evaluation time. In this work we enhance the model with two novel ideas, which independently of each other improve the performance of our modelling setup. The resulting dynamics match the true dynamics of the chaotic system very closely. The same enhancements can be used to provably enable the PD-NJ-ODE to learn long-term predictions for general stochastic datasets, where the standard model fails. This is verified in several experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba851d147a33cf0c6fcf639b5d4990df283a0b23" target='_blank'>
              Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs
              </a>
            </td>
          <td>
            F. Krach, Josef Teichmann
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We present a knowledge-guided machine learning (KGML) framework for modeling multi-scale processes, and study its performance in the context of streamflow forecasting in hydrology. Specifically, we propose a novel hierarchical recurrent neural architecture that factorizes the system dynamics at multiple temporal scales and captures their interactions. This framework consists of an inverse and a forward model. The inverse model is used to empirically resolve the system's temporal modes from data (physical model simulations, observed data, or a combination of them from the past), and these states are then used in the forward model to predict streamflow. In a hydrological system, these modes can represent different processes, evolving at different temporal scales (e.g., slow: groundwater recharge and baseflow vs. fast: surface runoff due to extreme rainfall). A key advantage of our framework is that once trained, it can incorporate new observations into the model's context (internal state) without expensive optimization approaches (e.g., EnKF) that are traditionally used in physical sciences for data assimilation. Experiments with several river catchments from the NWS NCRFC region show the efficacy of this ML-based data assimilation framework compared to standard baselines, especially for basins that have a long history of observations. Even for basins that have a shorter observation history, we present two orthogonal strategies of training our FHNN framework: (a) using simulation data from imperfect simulations and (b) using observation data from multiple basins to build a global model. We show that both of these strategies (that can be used individually or together) are highly effective in mitigating the lack of training data. The improvement in forecast accuracy is particularly noteworthy for basins where local models perform poorly because of data sparsity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/520332f72d1a6deaabdd7dd965c0cf76e89053b5" target='_blank'>
              Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems
              </a>
            </td>
          <td>
            Rahul Ghosh, Zac McEachran, Arvind Renganathan, Kelly Lindsay, Somya Sharma, M. Steinbach, John L. Nieber, Christopher J. Duffy, Vipin Kumar
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>44</td>
        </tr>

        <tr id="Skilful Machine Learned weather forecasts have challenged our approach to numerical weather prediction, demonstrating competitive performance compared to traditional physics-based approaches. Data-driven systems have been trained to forecast future weather by learning from long historical records of past weather such as the ECMWF ERA5. These datasets have been made freely available to the wider research community, including the commercial sector, which has been a major factor in the rapid rise of ML forecast systems and the levels of accuracy they have achieved. However, historical reanalyses used for training and real-time analyses used for initial conditions are produced by data assimilation, an optimal blending of observations with a physics-based forecast model. As such, many ML forecast systems have an implicit and unquantified dependence on the physics-based models they seek to challenge. Here we propose a new approach, training a neural network to predict future weather purely from historical observations with no dependence on reanalyses. We use raw observations to initialise a model of the atmosphere (in observation space) learned directly from the observations themselves. Forecasts of crucial weather parameters (such as surface temperature and wind) are obtained by predicting weather parameter observations (e.g. SYNOP surface data) at future times and arbitrary locations. We present preliminary results on forecasting observations 12-hours into the future. These already demonstrate successful learning of time evolutions of the physical processes captured in real observations. We argue that this new approach, by staying purely in observation space, avoids many of the challenges of traditional data assimilation, can exploit a wider range of observations and is readily expanded to simultaneous forecasting of the full Earth system (atmosphere, land, ocean and composition).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f12110407568e11614a4bec8a9d4a303c2c72458" target='_blank'>
              Data driven weather forecasts trained and initialised directly from observations
              </a>
            </td>
          <td>
            Anthony McNally, Christian Lessig, Peter Lean, Eulalie Boucher, Mihai Alexe, E. Pinnington, M. Chantry, Simon Lang, Chris Burrows, Marcin Chrust, F. Pinault, Ethel Villeneuve, Niels Bormann, Sean Healy
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Training neural networks involves optimizing parameters to minimize a loss function, where the nature of the loss function and the optimization strategy are crucial for effective training. Hyperparameter choices, such as the learning rate in gradient descent (GD), significantly affect the success and speed of convergence. Recent studies indicate that the boundary between bounded and divergent hyperparameters can be fractal, complicating reliable hyperparameter selection. However, the nature of this fractal boundary and methods to avoid it remain unclear. In this study, we focus on GD to investigate the loss landscape properties that might lead to fractal trainability boundaries. We discovered that fractal boundaries can emerge from simple non-convex perturbations, i.e., adding or multiplying cosine type perturbations to quadratic functions. The observed fractal dimensions are influenced by factors like parameter dimension, type of non-convexity, perturbation wavelength, and perturbation amplitude. Our analysis identifies"roughness of perturbation", which measures the gradient's sensitivity to parameter changes, as the factor controlling fractal dimensions of trainability boundaries. We observed a clear transition from non-fractal to fractal trainability boundaries as roughness increases, with the critical roughness causing the perturbed loss function non-convex. Thus, we conclude that fractal trainability boundaries can arise from very simple non-convexity. We anticipate that our findings will enhance the understanding of complex behaviors during neural network training, leading to more consistent and predictable training strategies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/da89cee139251ba0fe289fb5dd11c8d576adca50" target='_blank'>
              Complex fractal trainability boundary can arise from trivial non-convexity
              </a>
            </td>
          <td>
            Yizhou Liu
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Predictive models are a crucial component of many robotic systems. Yet, constructing accurate predictive models for a variety of deformable objects, especially those with unknown physical properties, remains a significant challenge. This paper introduces AdaptiGraph, a learning-based dynamics modeling approach that enables robots to predict, adapt to, and control a wide array of challenging deformable materials with unknown physical properties. AdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND) framework, which represents material bits as particles and employs a graph neural network (GNN) to predict particle motion. Its key innovation is a unified physical property-conditioned GBND model capable of predicting the motions of diverse materials with varying physical properties without retraining. Upon encountering new materials during online deployment, AdaptiGraph utilizes a physical property optimization process for a few-shot adaptation of the model, enhancing its fit to the observed interaction data. The adapted models can precisely simulate the dynamics and predict the motion of various deformable materials, such as ropes, granular media, rigid boxes, and cloth, while adapting to different physical properties, including stiffness, granular size, and center of pressure. On prediction and manipulation tasks involving a diverse set of real-world deformable objects, our method exhibits superior prediction accuracy and task proficiency over non-material-conditioned and non-adaptive models. The project page is available at https://robopil.github.io/adaptigraph/ .">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06d79953e85f7981f8a2cabad2165ca739af35af" target='_blank'>
              AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation
              </a>
            </td>
          <td>
            Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="A recent study in turbulent flow simulation demonstrated the potential of generative diffusion models for fast 3D surrogate modeling. This approach eliminates the need for specifying initial states or performing lengthy simulations, significantly accelerating the process. While adept at sampling individual frames from the learned manifold of turbulent flow states, the previous model lacks the capability to generate sequences, hindering analysis of dynamic phenomena. This work addresses this limitation by introducing a 4D generative diffusion model and a physics-informed guidance technique that enables the generation of realistic sequences of flow states. Our findings indicate that the proposed method can successfully sample entire subsequences from the turbulent manifold, even though generalizing from individual frames to sequences remains a challenging task. This advancement opens doors for the application of generative modeling in analyzing the temporal evolution of turbulent flows, providing valuable insights into their complex dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc78763ae5036e70c9aea5f2b3c74d7a5cb9b6e2" target='_blank'>
              Unfolding Time: Generative Modeling for Turbulent Flows in 4D
              </a>
            </td>
          <td>
            Abdullah Saydemir, Marten Lienen, Stephan Gunnemann
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We present a new step in our systematic effort to develop self-consistent dynamical models with a finite radial extent. The focus is on models with simple analytical density profiles allowing for analytical calculations of many dynamical properties. In this paper, we introduce a family of models, termed Wendland models, based on compactly supported radial basis functions. The family of models is characterised by a parameter $k$ that controls the smoothness of the transition at the truncation radius. In the limit $k\to\infty$, the Wendland model reduces to a non-truncated model with a Gaussian density profile. For each Wendland model, the density, mass and gravitational potential are simple truncated polynomial functions of radius. Via the SpheCow tool we demonstrate that all Wendland models can be supported by isotropic distribution functions. Surprisingly, the isotropic distribution function exhibits varied behaviour across different Wendland models. Additionally, each model can be supported by a continuum of Osipkov--Merritt orbital structures, ranging from radially anisotropic to completely tangential at the truncation radius. To the best of our knowledge, the Wendland models presented here are the first family of models accommodating both radial and tangential Osipkov--Merritt distribution functions. Using linear superposition, these models can easily be combined to generate Wendland models with even more diverse orbital structures. While the Wendland models are not fully representative of real dynamical systems due to their Gaussian-like density profile, this study lays important groundwork for constructing more realistic models with truncated density profiles that can be supported by a range of orbital structures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/602e48b057ea8900fb3ce6dcced4d37b2035392e" target='_blank'>
              Self-consistent dynamical models with a finite extent -- IV. Wendland models based on compactly supported radial basis functions
              </a>
            </td>
          <td>
            Maarten Baes
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Hybrid modeling aims to augment traditional theory-driven models with machine learning components that learn unknown parameters, sub-models or correction terms from data. In this work, we build on FluxRGNN, a recently developed hybrid model of continental-scale bird migration, which combines a movement model inspired by fluid dynamics with recurrent neural networks that capture the complex decision-making processes of birds. While FluxRGNN has been shown to successfully predict key migration patterns, its spatial resolution is constrained by the typically sparse observations obtained from weather radars. Additionally, its trainable components lack explicit incentives to adequately predict take-off and landing events. Both aspects limit our ability to interpret model results ecologically. To address this, we propose two major modifications that allow for more detailed predictions on any desired tessellation while providing control over the interpretability of model components. In experiments on the U.S. weather radar network, the enhanced model effectively leverages the underlying movement model, resulting in strong extrapolation capabilities to unobserved locations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db8ecd0f823bc56c0fe6b87a31bdeea470304f59" target='_blank'>
              Towards detailed and interpretable hybrid modeling of continental-scale bird migration
              </a>
            </td>
          <td>
            Fiona Lippert, Bart Kranstauber, Patrick Forr'e, E. E. V. Loon
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Scientific computing has been an indispensable tool in applied sciences and engineering, where traditional numerical methods are often employed due to their superior accuracy guarantees. However, these methods often encounter challenges when dealing with problems involving complex geometries. Machine learning-based methods, on the other hand, are mesh-free, thus providing a promising alternative. In particular, operator learning methods have been proposed to learn the mapping from the input space to the solution space, enabling rapid inference of solutions to partial differential equations (PDEs) once trained. In this work, we address the parametric elliptic interface problem. Building upon the deep operator network (DeepONet), we propose an extended interface deep operator network (XI-DeepONet). XI-DeepONet exhibits three unique features: (1) The interface geometry is incorporated into the neural network as an additional input, enabling the network to infer solutions for new interface geometries once trained; (2) The level set function associated with the interface geometry is treated as the input, on which the solution mapping is continuous and can be effectively approximated by the deep operator network; (3) The network can be trained without any input-output data pairs, thus completely avoiding the need for meshes of any kind, directly or indirectly. We conduct a comprehensive series of numerical experiments to demonstrate the accuracy and robustness of the proposed method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/649bca2a52c735c23a301cc270501ada4e4462c2" target='_blank'>
              XI-DeepONet: An operator learning method for elliptic interface problems
              </a>
            </td>
          <td>
            Ran Bi, Jingrun Chen, Weibing Deng
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The dynamics of supercooled liquids slow down and become increasingly heterogeneous as they are cooled. Recently, local structural variables identified using machine learning, such as"softness", have emerged as predictors of local dynamics. Here we construct a model using softness to describe the structural origins of dynamical heterogeneity in supercooled liquids. In our model, the probability of particles to rearrange is determined by their softness, and each rearrangement induces changes in the softness of nearby particles, describing facilitation. We show how to ensure that these changes respect the underlying time-reversal symmetry of the liquid's dynamics. The model reproduces the salient features of dynamical heterogeneity, and demonstrates how long-ranged dynamical correlations can emerge at long time scales from a relatively short softness correlation length.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d32819cfd5ee45a8ac43a269f5f4b86638fff470" target='_blank'>
              The dynamics of machine-learned"softness"in supercooled liquids describe dynamical heterogeneity
              </a>
            </td>
          <td>
            S. Ridout, Andrea J. Liu
          </td>
          <td>2024-06-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Verification of uncertain, complex dynamical systems is crucial in the modern day world. An increasingly common method to verify complex logic specifications for dynamical systems involves symbolic abstractions: simpler, finite-state models whose behaviour mimics the one of the systems of interest. By sampling trajectories of the concrete, unknown system and via robust analysis, we build a data-driven abstraction, related to the underlying model through a probabilistic behavioural inclusion relation. As the distribution from which the trajectories are drawn is unknown, we adopt two distinct distribution-free theories, namely scenario optimization and conformal prediction. We compare and discuss the differences between the two approaches in terms of the type of guarantees that they are able to provide. Furthermore, via experimental benchmarks we outline the efficiency of the two methods with respect to the number of samples available and the tightness of the guarantees.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4bc6fac276ef549b5a06e10e0b424eaf5ed1fd7" target='_blank'>
              Scenario Approach and Conformal Prediction for Verification of Unknown Systems via Data-Driven Abstractions
              </a>
            </td>
          <td>
            Rudi Coppola, Andrea Peruffo, Lars Lindemann, Manuel Mazo
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="This work introduces a novel, simple, and flexible method to quantify irreversibility in generic high-dimensional time series based on the well-known mapping to a binary classification problem. Our approach utilizes gradient boosting for estimation, providing a model-free, nonlinear analysis able to handle large-dimensional systems while requiring minimal or no calibration. Our procedure is divided into three phases: trajectory encoding, Markovian order identification, and hypothesis testing for variable interactions. The latter is the key innovation that allows us to selectively switch off variable interactions to discern their specific contribution to irreversibility. When applied to financial markets, our findings reveal a distinctive shift: during stable periods, irreversibility is mainly related to short-term patterns, whereas in unstable periods, these short-term patterns are disrupted, leaving only contributions from stable long-term ones. This observed transition underscores the crucial importance of high-order variable interactions in understanding the dynamics of financial markets, especially in times of turbulence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/13af2b3fdfb1da693055642a783d2277f03dfeab" target='_blank'>
              Functional Decomposition and Estimation of Irreversibility in Time Series via Machine Learning
              </a>
            </td>
          <td>
            Michele Vodret, Cristiano Pacini, Christian Bongiorno
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and identify near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method in 1D Burgers' equation and 2D jellyfish movement control in a fluid environment. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ed151b432bd4c9d4bafa7399dadc48d42ebe3d6" target='_blank'>
              A Generative Approach to Control Complex Physical Systems
              </a>
            </td>
          <td>
            Long Wei, Peiyan Hu, Ruiqi Feng, Haodong Feng, Yixuan Du, Tao Zhang, Rui Wang, Yue Wang, Zhi-Ming Ma, Tailin Wu
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Although optimal control (OC) has been studied in stochastic thermodynamics for systems with continuous state variables, less is known in systems with discrete state variables, such as Chemical Reaction Networks (CRNs). Here, we develop a general theoretical framework to study OC of CRNs for changing the system from an initial distribution of states to a final distribution with minimum dissipation. We derive a ``Kirchhoff's law"for the probability current in the adiabatic limit, from which the optimal kinetic rates are determined analytically for any given probability trajectory. By using the optimal rates, we show that the total dissipation is determined by a $L_2$-distance measure in the probability space and derive an analytical expression for the metric tensor that depends on the probability distribution, network topology, and capacity of each link. Minimizing the total dissipation leads to the geodesic trajectory in the probability space and the corresponding OC protocol is determined by the Kirchhoff's law. To demonstrate our general approach, we use it to find a lower bound for the minimum dissipation that is tighter than existing bounds obtained with only global constraints. We also apply it to simple networks, e.g., fully connected 3-state CRNs with different local constraints and show that indirect pathway and non-functional transient state can play a crucial role in switching between different probability distributions efficiently. Future directions in studying OC in CRNs by using our general framework are discussed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10b37de358ef271ea50059678271fca729dce156" target='_blank'>
              Geometry of optimal control in chemical reaction networks
              </a>
            </td>
          <td>
            Yikuan Zhang, Ouyang Qi, Yuhai Tu
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This paper presents a methodical dynamical analysis of the Big Bang model, taking into account dark matter and an arbitrary form of dark energy. Nevertheless, why dynamic analysis? The primary advantage of the dynamical analysis approach is that it obviates the necessity to precisely solve the system for every individual dependent variable. Instead, it enables the prediction of the overall evolutions. In this regard, there are some powerful tools of non-dynamical systems (theorems, perturbation, etc.) which can help us a lot. The model under consideration is a nonlinear system with two dimensions (2D). In order to investigate this, we employ a sophisticated form of nonlinear dynamical systems theory in our research. We have delved into the full potential of nonlinear dynamical system theory known until now in our work. This analysis has produced very interesting solutions, which are in quite a contrast with the linear analysis approach. The stability of the system is considered at length, and the corresponding evolutions of the universe are also discussed consequently. A handful number of figures are given to visualize the behavior of the evolutions. The plots that are presented here are vector field plots and a new plotting technique initiated by us and used in our earlier papers. Interestingly, our work indicates the universe might resemble “phantom” evolution. Also, among the fixed points of our model, one is able to escape the singular situation called “Big Rip”.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9649042fb8c1c032aa5bd8ad6a4249c0234e8a63" target='_blank'>
              Dark energy and dark matter interaction: A nonlinear dynamical system study
              </a>
            </td>
          <td>
            Jyotirmay Das Mandal, U. Debnath, Anirudh Pradhan
          </td>
          <td>2024-06-27</td>
          <td>International Journal of Geometric Methods in Modern Physics</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the neural network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we have to recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant, e.g., in many scientific inference tasks. At the same time, it stays competitive for black-box regression and classification tasks where neural networks typically excel.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c1a0555187db175d6e1fb7be3e3ea84e032a4d7c" target='_blank'>
              FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning
              </a>
            </td>
          <td>
            Tristan Cinquin, Marvin Pfortner, Vincent Fortuin, Philipp Hennig, Robert Bamler
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>14</td>
        </tr>

        <tr id="We consider quantum or classical many-body Hamiltonian systems, whose dynamics is given by an integrable, contact interactions, plus another, possibly long-range, generic two-body potential. We show how the dynamics of local observables is given in terms of a generalised version of Bogoliubov-Born-Green-Kirkwood-Yvon hierarchy, which we denote as gBBGKY, which is built for the densities, and their correlations, of the quasiparticles of the underlying integrable model. Unlike the usual cases of perturbation theory from free gases, the presence of local interactions in the integrable model"lifts"the so-called kinetic blocking, and the second layer of the hierarchy reproduces the dynamics at all time-scales. The latter consists of a fast pre-equilibration to a non-thermal steady state, and its subsequent thermalisation to a Gibbs ensemble. We show how the final relaxation is encoded into a Boltzmann scattering integral involving three or higher body-scatterings, and which, remarkably, is entirely determined by the diffusion constants of the underlying integrable model. We check our results with exact molecular dynamics simulations, finding perfect agreement. Our results show how gBBGKY can be successfully employed in quantum systems to compute scattering integrals and Fermi's golden rule transition rates.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e0ea32035f5732ebc13f1df5f3d70be5820746ea" target='_blank'>
              Generalised BBGKY hierarchy for near-integrable dynamics
              </a>
            </td>
          <td>
            Leonardo Biagetti, Maciej Lebek, M. Panfil, J. D. Nardis
          </td>
          <td>2024-08-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Symmetries play a pivotal role in our understanding of the properties of quantum many-body systems. While there are theorems and a well-established toolbox for systems in thermal equilibrium, much less is known about the role of symmetries and their connection to dynamics out of equilibrium. This arises due to the direct link between a system's thermal state and its Hamiltonian, which is generally not the case for nonequilibrium dynamics. Here we present a pathway to identify the effective symmetries and to extract them from data in nonequilibrium quantum many-body systems. Our approach is based on exact relations between correlation functions involving different numbers of spatial points, which can be viewed as nonequilibrium versions of (equal-time) Ward identities encoding the symmetries of the system. We derive symmetry witnesses, which are particularly suitable for the analysis of measured or simulated data at different snapshots in time. To demonstrate the potential of the approach, we apply our method to numerical and experimental data for a spinor Bose gas. We investigate the important question of a dynamical restoration of an explicitly broken symmetry of the Hamiltonian by the initial state. Remarkably, it is found that effective symmetry restoration can occur long before the system equilibrates. We also use the approach to define and identify spontaneous symmetry breaking far from equilibrium, which is of great relevance for applications to nonequilibrium phase transitions. Our work opens new avenues for the classification and analysis of quantum as well as classical many-body dynamics in a large variety of systems, ranging from ultracold quantum gases to cosmology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8a8950d1ce041653ba3716007a86405364e34167" target='_blank'>
              Extracting the symmetries of nonequilibrium quantum many-body systems
              </a>
            </td>
          <td>
            A. Mikheev, Viktoria Noel, Ido Siovitz, H. Strobel, M. Oberthaler, Jurgen Berges
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>48</td>
        </tr>

        <tr id="Large matrices arise in many machine learning and data analysis applications, including as representations of datasets, graphs, model weights, and first and second-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is an area which uses randomness to develop improved algorithms for ubiquitous matrix problems. The area has reached a certain level of maturity; but recent hardware trends, efforts to incorporate RandNLA algorithms into core numerical libraries, and advances in machine learning, statistics, and random matrix theory, have lead to new theoretical and practical challenges. This article provides a self-contained overview of RandNLA, in light of these developments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87ba4c0a0658c3c635c4a84120e5a3c8cd322fdd" target='_blank'>
              Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning
              </a>
            </td>
          <td>
            Michal Derezi'nski, Michael W. Mahoney
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Motivated by the recent successful application of physics-informed neural networks (PINNs) to solve Boltzmann-type equations [S. Jin, Z. Ma, and K. Wu, J. Sci. Comput., 94 (2023), pp. 57], we provide a rigorous error analysis for PINNs in approximating the solution of the Boltzmann equation near a global Maxwellian. The challenge arises from the nonlocal quadratic interaction term defined in the unbounded domain of velocity space. Analyzing this term on an unbounded domain requires the inclusion of a truncation function, which demands delicate analysis techniques. As a generalization of this analysis, we also provide proof of the asymptotic preserving property when using micro-macro decomposition-based neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/964863128ad956abcbc31dbcdc5eb8086a24f70d" target='_blank'>
              Error estimates of physics-informed neural networks for approximating Boltzmann equation
              </a>
            </td>
          <td>
            E. Abdo, Lihui Chai, Ruimeng Hu, Xu Yang
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/57132863d898425b060495280068009b30321f05" target='_blank'>
              A dynamical system analysis of bouncing cosmology with spatial curvature
              </a>
            </td>
          <td>
            Soumya Chakraborty, Sudip Mishra, S. Chakraborty
          </td>
          <td>2024-07-01</td>
          <td>General Relativity and Gravitation</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="The view that the laws of nature produce later states of the universe from earlier ones (prominently defended by Maudlin) faces difficult questions as to how the laws produce the future and whether that production is compatible with special relativity. This article grapples with those questions, arguing that the concerns can be overcome through a close analysis of the laws of classical mechanics and electromagnetism. The view that laws produce the future seems to require that the laws of nature take a certain form, fitting what Adlam has called"the time evolution paradigm."Making that paradigm precise, we might demand that there be temporally local dynamical laws that take properties of the present and the arbitrarily-short past as input, returning as output changes in such properties into the arbitrarily-short future. In classical mechanics, Newton's second law can be fit into this form if we follow a proposal from Easwaran and understand the acceleration that appears in the law to capture how velocity (taken to be a property of the present and the arbitrarily-short past) changes into the arbitrarily-short future. The dynamical laws of electromagnetism can be fit into this form as well, though because electromagnetism is a special relativistic theory we might require that the laws meet a higher standard: linking past light-cone to future light-cone. With some work, the laws governing the evolution of the vector and scalar potentials, as well as the evolution of charged matter, can be put in a form that meets this higher standard.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c45f0f4e7ef06ebe118ead2656895ec1a436690e" target='_blank'>
              How do Laws Produce the Future?
              </a>
            </td>
          <td>
            C. Sebens
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>9</td>
        </tr>

        <tr id="The class of Constitutive Artificial Neural Networks (CANNs) represents a new approach of neural networks in the field of constitutive modeling. So far, CANNs have proven to be a powerful tool in predicting elastic and inelastic material behavior. However, the specification of inelastic constitutive artificial neural networks (iCANNs) to capture plasticity remains to be discussed. We present the extension and application of an iCANN to the inelastic phenomena of plasticity. This includes the prediction of a formulation for the elastic and plastic Helmholtz free energies, the inelastic flow rule, and the yield condition that defines the onset of plasticity. Thus, we learn four feed-forward networks in combination with a recurrent neural network and use the second Piola-Kirchhoff stress measure for training. The presented formulation captures both, associative and non-associative plasticity. In addition, the formulation includes kinematic hardening effects by introducing the plastic Helmholtz free energy. This opens the range of application to a wider class of materials. The capabilities of the presented framework are demonstrated by training on artificially generated data of models for perfect plasticity of von-Mises type, tension-compression asymmetry, and kinematic hardening. We observe already satisfactory results for training on one load case only while extremely precise agreement is found for an increase in load cases. In addition, the performance of the specified iCANN was validated using experimental data of X10CrMoVNb9-1 steel. Training has been performed on both, uniaxial tension and cyclic loading, separately and the predicted results are then validated on the opposing set. The results underline that the autonomously discovered material model is capable to describe and predict the underlying experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c789f65f0c019bfe98255a7c00f1555dc262f7ca" target='_blank'>
              Accounting for plasticity: An extension of inelastic Constitutive Artificial Neural Networks
              </a>
            </td>
          <td>
            Birte Boes, Jaan‐Willem Simon, H. Holthusen
          </td>
          <td>2024-07-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Pretraining for partial differential equation (PDE) modeling has recently shown promise in scaling neural operators across datasets to improve generalizability and performance. Despite these advances, our understanding of how pretraining affects neural operators is still limited; studies generally propose tailored architectures and datasets that make it challenging to compare or examine different pretraining frameworks. To address this, we compare various pretraining methods without optimizing architecture choices to characterize pretraining dynamics on different models and datasets as well as to understand its scaling and generalization behavior. We find that pretraining is highly dependent on model and dataset choices, but in general transfer learning or physics-based pretraining strategies work best. In addition, pretraining performance can be further improved by using data augmentations. Lastly, pretraining is additionally beneficial when fine-tuning in scarce data regimes or when generalizing to downstream data similar to the pretraining distribution. Through providing insights into pretraining neural operators for physics prediction, we hope to motivate future work in developing and evaluating pretraining methods for PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b775198af561b89e562f258af605c3daa577f748" target='_blank'>
              Strategies for Pretraining Neural Operators
              </a>
            </td>
          <td>
            Anthony Y. Zhou, Cooper Lorsung, AmirPouya Hemmasian, A. Farimani
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>33</td>
        </tr>

        <tr id="We present an expansion of a many-body correlation function in a sum of pseudomodes - exponents with complex frequencies that encompass both decay and oscillations. We demonstrate that, typically, it is enough to take a few first terms of this infinite sum to obtain an excellent approximation to the correlation function at any time, with the large time behavior being determined solely by the first pseudomode. The pseudomode expansion emerges in the framework of the Heisenberg version of the recursion method. This method essentially solves Heisenberg equations in a Lanczos tridiagonal basis constructed in the Krylov space of a given observable. To obtain pseudomodes, we first add artificial dissipation satisfying the dissipative generalization of the universal operator growth hypothesis, and then take the limit of the vanishing dissipation strength. Fast convergence of the pseudomode expansion is facilitated by the localization in the Krylov space, which is generic in the presence of dissipation and can survive the limit of the vanishing dissipation strength. As an illustration, we apply the pseudomode expansion to calculate infinite-temperature autocorrelation functions in the quantum Ising and $XX$ spin-$1/2$ models on the square lattice.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c0e1c45eedb9a16b0066dded4a252c6974da710" target='_blank'>
              Pseudomode expansion of many-body correlation functions
              </a>
            </td>
          <td>
            Alexander Teretenkov, F. Uskov, Oleg Lychkovskiy
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The nonlinear filtering problem has enduringly been an active research topic in both academia and industry due to its ever-growing theoretical importance and practical significance. The main objective of nonlinear filtering is to infer the states of a nonlinear dynamical system of interest based on the available noisy measurements. In recent years, the advance of network communication technology has not only popularized the networked systems with apparent advantages in terms of installation, cost and maintenance, but also brought about a series of challenges to the design of nonlinear filtering algorithms, among which the communication constraint has been recognized as a dominating concern. In this context, a great number of investigations have been launched towards the networked nonlinear filtering problem with communication constraints, and many sample-based nonlinear filters have been developed to deal with the highly nonlinear and/or non-Gaussian scenarios. The aim of this paper is to provide a timely survey about the recent advances on the sample-based networked nonlinear filtering problem from the perspective of communication constraints. More specifically, we first review three important families of sample-based filtering methods known as the unscented Kalman filter, particle filter, and maximum correntropy filter. Then, the latest developments are surveyed with stress on the topics regarding incomplete/imperfect information, limited resources and cyber security. Finally, several challenges and open problems are highlighted to shed some lights on the possible trends of future research in this realm.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7b2ab547863c1e18aa730f1ca9040c34f915e81b" target='_blank'>
              Nonlinear Filtering With Sample-Based Approximation Under Constrained Communication: Progress, Insights and Trends
              </a>
            </td>
          <td>
            Weihao Song, Zidong Wang, Zhongkui Li, Jianan Wang, Qing‐Long Han
          </td>
          <td>2024-07-01</td>
          <td>IEEE/CAA Journal of Automatica Sinica</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Uncertainty-aware controllers that guarantee safety are critical for safety critical applications. Among such controllers, Control Barrier Functions (CBFs) based approaches are popular because they are fast, yet safe. However, most such works depend on Gaussian Processes (GPs) or MC-Dropout for learning and uncertainty estimation, and both approaches come with drawbacks: GPs are non-parametric methods that are slow, while MC-Dropout does not capture aleatoric uncertainty. On the other hand, modern Bayesian learning algorithms have shown promise in uncertainty quantification. The application of modern Bayesian learning methods to CBF-based controllers has not yet been studied. We aim to fill this gap by surveying uncertainty quantification algorithms and evaluating them on CBF-based safe controllers. We find that model variance-based algorithms (for example, Deep ensembles, MC-dropout, etc.) and direct estimation-based algorithms (such as DEUP) have complementary strengths. Algorithms in the former category can only estimate uncertainty accurately out-of-domain, while those in the latter category can only do so in-domain. We combine the two approaches to obtain more accurate uncertainty estimates both in- and out-of-domain. As measured by the failure rate of a simulated robot, this results in a safer CBF-based robot controller.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9b8b25ad6df5d3bd294327053185a6ebb1b8f074" target='_blank'>
              DADEE: Well-calibrated uncertainty quantification in neural networks for barriers-based robot safety
              </a>
            </td>
          <td>
            Masoud Ataei, Vikas Dhiman
          </td>
          <td>2024-06-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Validating safety-critical autonomous systems in high-dimensional domains such as robotics presents a significant challenge. Existing black-box approaches based on Markov chain Monte Carlo may require an enormous number of samples, while methods based on importance sampling often rely on simple parametric families that may struggle to represent the distribution over failures. We propose to sample the distribution over failures using a conditional denoising diffusion model, which has shown success in complex high-dimensional problems such as robotic task planning. We iteratively train a diffusion model to produce state trajectories closer to failure. We demonstrate the effectiveness of our approach on high-dimensional robotic validation tasks, improving sample efficiency and mode coverage compared to existing black-box techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/242ab76870b39bb8f1c8171690eb477f7170f167" target='_blank'>
              Diffusion-Based Failure Sampling for Cyber-Physical Systems
              </a>
            </td>
          <td>
            Harrison Delecki, Marc R. Schlichting, Mansur Arief, Anthony Corso, Marcell Vazquez-Chanlatte, Mykel J. Kochenderfer
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Quantum mechanics is able to produce challenging behaviors even in the simplest situations. This is possible because of the important dynamical role that phase plays in the evolution of quantum systems, very similar, nonetheless, to effects observable in analogous optical systems. The present work focuses on how Bohmian mechanics proves to be a rather convenient theoretical framework to analyze phase-based phenomena, since the phase constitutes the central element in this hydrodynamical formulation of quantum mechanics. More specifically, it allows us to understand how spatial phase variations give rise to velocity fields that eventually rule the dynamical behavior of quantum systems, and that, when integrated in time locally (i.e., taking into account specific positions), they provide us with a neat local (point by point) description of the system evolution in the configuration space. Indeed, it will also be seen that this idea transcends the quantum realm and can be profitably used to describe the behavior of optical analogs with rather singular behaviors. With this purpose, two interesting phenomena that take place in free space are considered, namely, the self-acceleration and shape-invariance of Airy beams, and spontaneous self-focusing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c0142e0cb3b8daeedf62b6b6c1e336cc21ef9694" target='_blank'>
              Symmetries and singular behaviors with Bohmian trajectories
              </a>
            </td>
          <td>
            ´Angel S. Sanz
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Perceptual rivalry, where conflicting sensory information leads to alternating perceptions crucial for associated cognitive function, has attracted researcher's attention for long. Despite progresses being made, recent studies have revealed limitations and inconsistencies in our understanding across various rivalry contexts. We develop a unified physical framework, where perception undergoes a consecutive phase transition process encompassing different multi-state competitions. We reveal the underlying mechanisms of perceptual rivalry by identifying dominant switching paths among perceptual states and quantifying mean perceptual durations, switching frequencies, and proportions of different perceptions. We uncover the underlying nonequilibrium dynamics and thermodynamics by analyzing average nonequilibrium flux and entropy production rate, while associated time series irreversibility reflects the underlying nonequilibrium mechanism of perceptual rivalry and link thermodynamical results with neuro-electrophysiological experiments. Our framework provides a global and physical understanding of brain perception, which may go beyond cognitive science or psychology but embodies the connection with wider fields as decision-making.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fc441c92c5ed6177e25f25948db30fc79b328e33" target='_blank'>
              Nonequilibrium dynamics and thermodynamics provide the underlying physical mechanism of the perceptual rivalry
              </a>
            </td>
          <td>
            Yuxuan Wu, Liufang Xu, Jin Wang
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In Clearing Up Mysteries -- The Original Goal (Maximum Entropy and Bayesian Methods: Cambridge, England, 1988 Springer, pp. 1-27) Jaynes derived Fick's Law for a dilute binary solution from Bayes' Theorem by considering, probabilistically, the motion of dilute solute molecules. Modifying Jaynes' prior, changing the frame of reference, and allowing for multicomponent systems, one can follow Jaynes' logic to arrive at several expressions for the diffusion coefficient that are widely used in application to solvent-polymer systems. These results, however, do not generally satisfy required conditions over the full concentration range. This limitation is resolved by considering the joint motion of all components in the solution with the inclusion of known physical constraints. Doing so, one arrives at a new set of constitutive equations for binary and multicomponent diffusion that include Darken's form as a limit and provide means to characterize inter-species correlations, removing the need for the ad hoc modifications of the thermodynamic term or substitution of bulk compositions with local compositions that have been proposed in the literature. In binary systems, the Bayesian expression for the mutual diffusion coefficient provides explicit limits on non-ideal mutual diffusion behavior, given component self-diffusion coefficients, that can be attributed to inter-species correlation versus stronger intra- and inter-species clustering. Applications of the model to published data for a set of binary systems are presented, along with pointers to further directions for research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/744d4dcaef77a8f1280b85645d110b5a7e2e1c43" target='_blank'>
              Fickian Insights Using Probability Theory as Logic
              </a>
            </td>
          <td>
            Peter E. Price
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this article we study if a Deep Learning technique can be used to obtain an approximated value of the Lyapunov exponents of a dynamical system. Moreover, we want to know if Machine Learning techniques are able, once trained, to provide the complete Lyapunov exponents spectrum with just single-variable time series. We train a Convolutional Neural Network and we use the resulting network to approximate the complete spectrum using the time series of just one variable from the studied systems (Lorenz system and coupled Lorenz system). The results are quite stunning as all the values are well approximated with only partial data. This strategy permits to speed up the complete analysis of the systems and also to study the hyperchaotic dynamics in the coupled Lorenz system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/09290aec6c4160820790c56611a3f3eca51095dc" target='_blank'>
              Full Lyapunov Exponents spectrum with Deep Learning from single-variable time series
              </a>
            </td>
          <td>
            C. Mayora-Cebollero, A. Mayora-Cebollero, 'Alvaro Lozano, Roberto Barrio
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure. Chizat et al. arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\"odinger bridges. Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models. In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking). We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting. We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals. Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00a1683e0adbd5e56aa4f83d49091c5542d29179" target='_blank'>
              Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior
              </a>
            </td>
          <td>
            Anming Gu, Edward Chien, K. Greenewald
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="In this study, we employ analytical and numerical techniques to examine a phase transition model with moving boundaries. The model displays two relevant spatial scales pointing out to a macroscopic phase and a microscopic phase, interacting on disjoint inclusions. The shrinkage or the growth of the inclusions is governed by a modified Gibbs-Thomson law depending on the macroscopic temperature, but without accessing curvature information. We use the Hanzawa transformation to transform the problem onto a fixed reference domain. Then a fixed-point argument is employed to demonstrate the well-posedness of the system for a finite time interval. Due to the model's nonlinearities and the macroscopic parameters, which are given by differential equations that depend on the size of the inclusions, the problem is computationally expensive to solve numerically. We introduce a precomputing approach that solves multiple cell problems in an offline phase and uses an interpolation scheme afterward to determine the needed parameters. Additionally, we propose a semi-implicit time-stepping method to resolve the nonlinearity of the problem. We investigate the errors of both the precomputing and time-stepping procedures and verify the theoretical results via numerical simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d908e703e8f09b2c3874e3f73ac0a92003566cae" target='_blank'>
              Precomputing approach for a two-scale phase transition model
              </a>
            </td>
          <td>
            Michael Eden, T. Freudenberg, Adrian Muntean
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="
 Reservoir computing is an efficient artificial neural network for model-free prediction and analysis of dynamical systems time series. As a data-based method, the capacity of reservoir computing is stongly affected by the time sampling interval of training data. In this paper, taking Lorenz system as an example, we explore the influence of this sampling interval on the performance of reservoir computing in predicting chaotic sequences. When the sampling interval increases, the prediction capacity of reservoir computing is first enhanced then weakened, presenting a bell-shaped curve. By slightly revising the calculation method of the output matrix, the prediction performance of reservoir computing with small sampling interval can be improved. Furthermore, reservoir computing can learn and reproduce the state of chaotic system with a large time interval, which is almost five times larger than that of the classic fourth-order Runge-Kutta method. Our results show the capacity of reservoir computing in the applications where the time sampling intervals are constrained and laid the foundation for building a fast algorithm with larger time iteration steps.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2299dd61e3d42dbb604ec45370f99d03bbd6e6f9" target='_blank'>
              Large sampling intervals for learning and predicting chaotic systems with reservoir computing
              </a>
            </td>
          <td>
            Qingyan Xie, Zixiang Yan, Hui Zhao, Jian Gao, Jinghua Xiao
          </td>
          <td>2024-06-28</td>
          <td>Journal of Physics A: Mathematical and Theoretical</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector $\theta \in \mathbb{R}^n$, aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with $n$ is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator--the generator gradient estimator--for which the computation time remains stable in $n$. In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as $n$ increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6fc1545e20fb78c9271560d34a3512c20ad26286" target='_blank'>
              An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations
              </a>
            </td>
          <td>
            Shengbo Wang, Jose Blanchet, Peter Glynn
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Stochastic systems are used to model a variety of phenomena in which noise plays an essential role. In these models, one potential goal is to determine if noise can induce transitions between states, and if so, to calculate the most probable escape path from an attractor. In the small noise limit, the Freidlin-Wentzell theory of large deviations provides a variational framework to calculate these paths. This work focuses on using large deviation theory to calculate such paths for stochastic gradient systems with non-gradient perturbations. While for gradient systems the most probable escape paths consist of time-reversed heteroclinic orbits, for general systems it can be a challenging calculation. By applying Melnikov theory to the resulting Euler-Lagrange equations recast in Hamiltonian form, we determine a condition for when the optimal escape path is the heteroclinic orbit for the perturbed system. We provide a numerical example to illustrate how the computed most probable escape path compares with the theoretical result.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/330a794bfa885f5551f29fba1960f92ebc6ccf55" target='_blank'>
              Most probable escape paths in perturbed gradient systems
              </a>
            </td>
          <td>
            Katherine Slyman, Mackenzie Simper, John A Gemmer, Bjorn Sandstede
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Many problems in Physics and Chemistry are formulated as the minimization of a functional. Therefore, methods for solving these problems typically require differentiating maps whose input and/or output are functions -- commonly referred to as variational differentiation. Such maps are not addressed at the mathematical level by the chain rule, which underlies modern symbolic and algorithmic differentiation (AD) systems. Although there are algorithmic solutions such as tracing and reverse accumulation, they do not provide human readability and introduce strict programming constraints that bottleneck performance, especially in high-performance computing (HPC) environments. In this manuscript, we propose a new computer theoretic model of differentiation by combining the pullback of the $\mathbf{B}$ and $\mathbf{C}$ combinators from the combinatory logic. Unlike frameworks based on the chain rule, this model differentiates a minimal complete basis for the space of computable functions. Consequently, the model is capable of analytic backpropagation and variational differentiation while supporting complex numbers. To demonstrate the generality of this approach we build a system named CombDiff, which can differentiate nontrivial variational problems such as Hartree-Fock (HF) theory and multilayer perceptrons.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c640b0ba0b06a2a81e8f349b2681297670ab04e" target='_blank'>
              Automating Variational Differentiation
              </a>
            </td>
          <td>
            Kangbo Li, Anil Damle
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Oscillatory behaviour underpins many essential biological functions and energy is required to sustain oscillation. In this paper, energy-based modelling of biochemical systems using the bond graph approach is combined with classical feedback control theory to give a novel approach to the analysis, and potentially synthesis, of biochemical oscillators. It is shown that oscillation is dependent on the interplay between active and passive feedback and this interplay is formalised using classical frequency-response analysis of feedback systems. In particular, the phase margin is suggested as a simple scalar indicator of the presence or absence of oscillations; it is shown how this indicator can be used to investigate the effect of both the structure and parameters of biochemical system on oscillation. It follows that the combination of classical feedback control theory and the bond graph approach to systems biology gives a novel analysis and design methodology for biochemical oscillators. The approach is illustrated using an introductory example similar to the Goodwin oscillator, the Sel’kov model of Glycolytic Oscillations and the Repressilator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3a36b9fb2552ca9bf78bb6bf02aa0e381a3d8141" target='_blank'>
              Energy-based Analysis of Biochemical Oscillators Using Bond Graphs and Linear Control Theory
              </a>
            </td>
          <td>
            P. Gawthrop, Michael Pan
          </td>
          <td>2024-06-09</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>43</td>
        </tr>

        <tr id="Parameter identifiability is an important aspect of parameter estimation of dynamic system modelling. Several methods exist to determine identifiability of parameter sets using the model definition and analysis of experimental data. There is also the possibility of some parameters being independently unidentifiable but forming identifiable parameter combinations. These identifiable parameter combinations are useful in model reparameterisation to estimate parameters experimentally. Multiple numerical and algebraic methods exist to detect identifiable parameter combinations of dynamic system models represented as ordinary differential equations (ODE). Local identifiability analysis of hybrid system models are available in the literature. However, methods for structural identifiability analysis and identifiable combination detection for hybrid systems are not explored. Here, we have developed a parameter identifiable combination detection method for non-linear hybrid systems along with ODE systems using an SMT based parameter space exploration method. For higher dimensional systems and larger parameter space, SMT based approaches may easily become computationally intractable. This problem has been mitigated to a large extent by heuristically limiting the parameter space to be explored, using Gaussian process regression and gradient based approaches. The developed method has been demonstrated for some simple hybrid models, biochemical models of ODE systems and non-linear hybrid systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b71e8c79446f13006506dfdbfeaeb54816a6399" target='_blank'>
              SMT based parameter identifiable combination detection for non-linear continuous and hybrid dynamics
              </a>
            </td>
          <td>
            Devleena Ghosh, C. Mandal
          </td>
          <td>2024-06-07</td>
          <td>Formal Aspects of Computing</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this paper, we extend our previous work on a simplified model of the nervous system by solving the general optimization problem for the evolutionary cost of the nervous system. This optimization takes into account constraints on the scales of membrane potential kinetics and sensory response function to ensure finite, biologically plausible solutions. Our analysis reduces the variational problem to a system of two integro-differential equations, which we solve asymptotically using series expansions. This study confirms the emergence of sharp finite neuronal spikes and robust sensory and motor response functions as evolutionarily optimal solutions. We note that, in principle, alternative evolutionary solutions with different biophysical interpretations might exist for this optimization problem. This work provides a rigorous mathematical framework bridging evolutionary optimization with the fundamental properties of nervous systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7efdfee978d2816c051990a2888f79b2d6828bae" target='_blank'>
              Simplest Model of Nervous System. IV. General Solution
              </a>
            </td>
          <td>
            Anton V. Sinitskiy
          </td>
          <td>2024-07-13</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this work, we explore the numerical solution of geometric shape optimization problems using neural network-based approaches. This involves minimizing a numerical criterion that includes solving a partial differential equation with respect to a domain, often under geometric constraints like constant volume. Our goal is to develop a proof of concept using a flexible and parallelizable methodology to tackle these problems. We focus on a prototypal problem: minimizing the so-called Dirichlet energy with respect to the domain under a volume constraint, involving a Poisson equation in $\mathbb R^2$. We use physics-informed neural networks (PINN) to approximate the Poisson equation's solution on a given domain and represent the shape through a neural network that approximates a volume-preserving transformation from an initial shape to an optimal one. These processes are combined in a single optimization algorithm that minimizes the Dirichlet energy. One of the significant advantages of this approach is its parallelizable nature, which makes it easy to handle the addition of parameters. Additionally, it does not rely on shape derivative or adjoint calculations. Our approach is tested on Dirichlet and Robin boundary conditions, parametric right-hand sides, and extended to Bernoulli-type free boundary problems. The source code for solving the shape optimization problem is open-source and freely available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb177a71238803ba9eff09921802eff318b55cec" target='_blank'>
              Volume-preserving physics-informed geometric shape optimization of the Dirichlet energy
              </a>
            </td>
          <td>
            Amaury B'elieres--Frendo, Emmanuel Franck, Victor Michel-Dansac, Yannick Privat
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The enduring legacy of Euclidean geometry underpins classical machine learning, which, for decades, has been primarily developed for data lying in Euclidean space. Yet, modern machine learning increasingly encounters richly structured data that is inherently nonEuclidean. This data can exhibit intricate geometric, topological and algebraic structure: from the geometry of the curvature of space-time, to topologically complex interactions between neurons in the brain, to the algebraic transformations describing symmetries of physical systems. Extracting knowledge from such non-Euclidean data necessitates a broader mathematical perspective. Echoing the 19th-century revolutions that gave rise to non-Euclidean geometry, an emerging line of research is redefining modern machine learning with non-Euclidean structures. Its goal: generalizing classical methods to unconventional data types with geometry, topology, and algebra. In this review, we provide an accessible gateway to this fast-growing field and propose a graphical taxonomy that integrates recent advances into an intuitive unified framework. We subsequently extract insights into current challenges and highlight exciting opportunities for future development in this field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/530b26ee44d4d565ee8661da86e03ef1f473385a" target='_blank'>
              Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures
              </a>
            </td>
          <td>
            S. Sanborn, Johan Mathe, Mathilde Papillon, Domas Buracas, Hansen Lillemark, Christian Shewmake, Abby Bertics, Xavier Pennec, Nina Miolane
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Among binary asteroids, (35107) 1991VH stands out as unique given the likely chaotic rotation within its secondary component. The source of this excited dynamical state is unknown. In this work we demonstrate that a past close encounter with Earth could have provided the necessary perturbation to allow the natural internal dynamics, characterized by spin-orbit coupling, to evolve the system into its current dynamical state. In this hypothesis, the secondary of 1991VH was previously in a classical 1:1 spin-orbit resonance with an orbit period likely between 28-35 hours before being perturbed by an Earth encounter within $\sim80,000$ km. We find if the energy dissipation within the secondary is relatively inefficient, this excited dynamical state could persist to today and produce the observed ground-based measurements. Coupled with the orbital history of 1991VH, we can then place a constraint on the tidal dissipation parameters of the secondary.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2f694a003e2f1cc8bc9b14d47f6caa55abdf0e9c" target='_blank'>
              An Earth Encounter As the Cause of Chaotic Dynamics in Binary Asteroid (35107) 1991VH
              </a>
            </td>
          <td>
            A. J. Meyer, Oscar Fuentes-Muñoz, I. Gkolias, K. Tsiganis, P. Pravec, S. Naidu, D. Scheeres
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="Configuration-controllable phononic crystals (CCPCs) have broad application prospects in engineering because of their adjustable vibration-reduction properties. Owing to the complicated constitutive relationship and nonlinear geometric deformation, it is difficult to accurately predict the dynamic characteristics of CCPCs using the finite element method (FEM) or theoretical methods. In this study, we employed a nonlinear autoregressive with exogenous input (NARX) artificial neural network (ANN) to identify the dynamic model of the CCPC under an impact load, using data from over 100 experiments and numerous accumulated samples. The corresponding experimental data for the CCPC were used to train the ANN and determine the rational ANN model. The identification results indicate that appropriate number of neurons and time-delay orders can effectively reduce the identification errors. Compared with the response predicted by the FEM, the identification model can describe the nonlinear characteristics emerging from phononic crystal (PC) experiments more accurately. This study provides an efficient and accurate online identification approach for PC-modeling.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/63f3f0badfa56355242e4715c8727c81ea87125c" target='_blank'>
              Dynamic modeling of configuration-controllable phononic crystal using NARX neural networks
              </a>
            </td>
          <td>
            Nan Li, Changqing Bai
          </td>
          <td>2024-06-14</td>
          <td>Journal of Vibration and Control</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The modelling of linear quadratic Gaussian optimal control problems on large complex networks is intractable computationally. Graphon theory provides an approach to overcome these issues by defining limit objects for infinite sequences of graphs permitting one to approximate arbitrarily large networks by infinite dimensional operators. This is extended to stochastic systems by the use of Q-noise, a generalization of Wiener processes in finite dimensional spaces to processes in function spaces. The optimal control of linear quadratic problems on graphon systems with Q-noise disturbances are defined and shown to be the limit of the corresponding finite graph optimal control problem. The theory is extended to low rank systems, and a fully worked special case is presented. In addition, the worst-case long-range average and infinite horizon discounted optimal control performance with respect to Q-noise distribution are computed for a small set of standard graphon limits.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79173733e4371062ced3a5a7d673a308b7ae7824" target='_blank'>
              Quadratic Optimal Control of Graphon Q-noise Linear Systems
              </a>
            </td>
          <td>
            Alex Dunyak, Peter E. Caines
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="


Ecosystems contain numerous species interacting with each other and the environment. However, data on all relevant state variables is rarely available, hampering inference and prediction. Empirical dynamic modelling (EDM) is a valuable tool for prediction, inference and control in such partially observed systems.

However, EDM typically assumes that the available time series are observed without error. Failing to account for observation noise strongly biases estimates of Lyapunov exponents and reduces forecast accuracy. To address this limitation, we propose incorporating EDM into a hidden Markov framework and using an iterative scheme based on the expectation maximization (EM) algorithm to obtain filtered state and parameter estimates.

We evaluate the performance of this approach on several simulated dynamical systems with a range of additive noise levels, as well as on insect population time series.

Accounting for observation noise improved accuracy of population forecasts and estimates of Lyapunov exponents (LE) over a wide range of noise levels relevant to ecological time series.

">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e9c4d6519c0328c66d02635359f84c4d6cf2f4d" target='_blank'>
              Accounting for observation noise in equation‐free forecasting: The hidden‐Markov S‐map
              </a>
            </td>
          <td>
            Dylan Esguerra, Stephan B. Munch
          </td>
          <td>2024-07-01</td>
          <td>Methods in Ecology and Evolution</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0faa7b48df91690f3e92cf2ef0638118805ed816" target='_blank'>
              Trajectory-based global sensitivity analysis in multiscale models
              </a>
            </td>
          <td>
            Valentina Bazyleva, Victoria M. Garibay, Debraj Roy
          </td>
          <td>2024-06-17</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This work addresses the synthesis of optimal feedback control laws via machine learning. In particular, the Averaged Feedback Learning Scheme (AFLS) and a data driven method are considered. Hypotheses for each method ensuring the convergence of the evaluation of the objective function of the underlying control problem at the obtained feedback-laws towards the optimal value function are provided. These hypotheses are connected to the regularity of the value function and the stability of the dynamics. In the case of AFLS these hypotheses only require H\"older continuity of the value function, whereas for the data driven method the value function must be at least $C^2$. It is demonstrated that these methods are connected via their optimality conditions. Additionally, numerical experiments are provided by applying both methods to a family control problems, parameterized by a positive real number which controls the regularity of the value function. For small parameters the value function is smooth and in contrast for large parameters it is non-differentiable, but semi-concave. The results of the experiments indicate that both methods have a similar performance for the case that the value function is smooth. On the other hand, if the value function is not differentiable, AFLS has a better performance which is consistent with the obtained convergence results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/307b109ee8daf5810e48f9fba58490e24a131f80" target='_blank'>
              Convergence of machine learning methods for feedback control laws: averaged feedback learning scheme and data driven method
              </a>
            </td>
          <td>
            Karl Kunisch, Donato V'asquez-Varas
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We investigate the accuracy of prediction in deterministic learning dynamics of zero-sum games with random initializations, specifically focusing on observer uncertainty and its relationship to the evolution of covariances. Zero-sum games are a prominent field of interest in machine learning due to their various applications. Concurrently, the accuracy of prediction in dynamical systems from mechanics has long been a classic subject of investigation since the discovery of the Heisenberg Uncertainty Principle. This principle employs covariance and standard deviation of particle states to measure prediction accuracy. In this study, we bring these two approaches together to analyze the Follow-the-Regularized-Leader (FTRL) algorithm in two-player zero-sum games. We provide growth rates of covariance information for continuous-time FTRL, as well as its two canonical discretization methods (Euler and Symplectic). A Heisenberg-type inequality is established for FTRL. Our analysis and experiments also show that employing Symplectic discretization enhances the accuracy of prediction in learning dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b71b055d5aeef1eef9b00c144b364421e6afd674" target='_blank'>
              Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg
              </a>
            </td>
          <td>
            Yi Feng, G. Piliouras, Xiao Wang
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024'],
    y: [25],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>