---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:06:00 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator.</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="The Koopman operator provides a powerful framework for data-driven analysis of dynamical systems. In the last few years, a wealth of numerical methods providing finite-dimensional approximations of the operator have been proposed [e.g., extended dynamic mode decomposition (EDMD) and its variants]. While convergence results for EDMD require an infinite number of dictionary elements, recent studies have shown that only a few dictionary elements can yield an efficient approximation of the Koopman operator, provided that they are well-chosen through a proper training process. However, this training process typically relies on nonlinear optimization techniques. In this paper, we propose two novel methods based on a reservoir computer to train the dictionary. These methods rely solely on linear convex optimization. We illustrate the efficiency of the method with several numerical examples in the context of data reconstruction, prediction, and computation of the Koopman operator spectrum. These results pave the way for the use of the reservoir computer in the Koopman operator framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d99279af5ea33ac16ec90a1d2480089d0af44faa" target='_blank'>Two methods to approximate the Koopman operator with a reservoir computer.</a></td>
          <td>
            Marvyn Gulina, A. Mauroy
          </td>
          <td>2020-08-24</td>
          <td>Chaos</td>
          <td>9</td>
          <td>15</td>
        </tr>
    
        <tr id="Nonlinear phenomena can be analyzed via linear techniques using operator-theoretic approaches. Data-driven method called the extended dynamic mode decomposition (EDMD) and its variants, which approximate the Koopman operator associated with the nonlinear phenomena, have been rapidly developing by incorporating machine learning methods. Neural ordinary differential equations (NODEs), which are a neural network equipped with a continuum of layers, and have high parameter and memory efficiencies, have been proposed. In this paper, we propose an algorithm to perform EDMD using NODEs. NODEs are used to find a parameter-efficient dictionary which provides a good finite-dimensional approximation of the Koopman operator. We show the superiority of the parameter efficiency of the proposed method through numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ca5b03a2a5f4b635ebfd442eab174fe73aa034e" target='_blank'>Extended dynamic mode decomposition with dictionary learning using neural ordinary differential equations</a></td>
          <td>
            H. Terao, Sho Shirasaka, Hideyuki Suzuki
          </td>
          <td>2021-10-01</td>
          <td>ArXiv</td>
          <td>5</td>
          <td>26</td>
        </tr>
    
        <tr id="Koopman operators are infinite-dimensional operators that linearize nonlinear dynamical systems, facilitating the study of their spectral properties and enabling the prediction of the time evolution of observable quantities. Recent methods have aimed to approximate Koopman operators while preserving key structures. However, approximating Koopman operators typically requires a dictionary of observables to capture the system's behavior in a finite-dimensional subspace. The selection of these functions is often heuristic, may result in the loss of spectral information, and can severely complicate structure preservation. This paper introduces Multiplicative Dynamic Mode Decomposition (MultDMD), which enforces the multiplicative structure inherent in the Koopman operator within its finite-dimensional approximation. Leveraging this multiplicative property, we guide the selection of observables and define a constrained optimization problem for the matrix approximation, which can be efficiently solved. MultDMD presents a structured approach to finite-dimensional approximations and can more accurately reflect the spectral properties of the Koopman operator. We elaborate on the theoretical framework of MultDMD, detailing its formulation, optimization strategy, and convergence properties. The efficacy of MultDMD is demonstrated through several examples, including the nonlinear pendulum, the Lorenz system, and fluid dynamics data, where we demonstrate its remarkable robustness to noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0cc59f7258cc0c3e219e7edb0f6cbaf13b67c680" target='_blank'>Multiplicative Dynamic Mode Decomposition</a></td>
          <td>
            Nicolas Boull'e, Matthew J. Colbrook
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>16</td>
        </tr>
    
        <tr id="Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here, we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a "pure" Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at each time step-this approach no longer satisfies the linearity of the true Koopman operator representation. For further comparisons, we also apply a state-space approach (neural ordinary differential equations). We consider systems encompassing two- and three-dimensional ordinary differential equation systems featuring steady, oscillatory, and chaotic attractors, as well as partial differential equations exhibiting increasingly complex and intricate behaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the state-space approach offers superior performance compared to the "pure" Koopman approach where the entire time evolution occurs in the space of observables. When the temporal evolution of the Koopman approach alternates between states and observables at each time step, however, its predictions become comparable to those of the state-space approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1598a1165ae2a7fa531ffc54c5a0cd3a8158b6e4" target='_blank'>Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches</a></td>
          <td>
            Ricardo Constante-Amores, Alec J. Linot, Michael D. Graham
          </td>
          <td>2023-10-10</td>
          <td>Chaos</td>
          <td>2</td>
          <td>7</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0142e817afd03e2c5d2180e2919239a73782a6a0" target='_blank'>Extended Dynamic Mode Decomposition with Invertible Dictionary Learning</a></td>
          <td>
            Yuhong Jin, Lei Hou, Shun Zhong
          </td>
          <td>2024-02-01</td>
          <td>Neural networks : the official journal of the International Neural Network Society</td>
          <td>1</td>
          <td>5</td>
        </tr>
    
        <tr id="PyKoopman is a Python package for the data-driven approximation of the Koopman operator associated with a dynamical system. The Koopman operator is a principled linear embedding of nonlinear dynamics and facilitates the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. In particular, PyKoopman provides tools for data-driven system identification for unforced and actuated systems that build on the equation-free dynamic mode decomposition (DMD) and its variants. In this work, we provide a brief description of the mathematical underpinnings of the Koopman operator, an overview and demonstration of the features implemented in PyKoopman (with code examples), practical advice for users, and a list of potential extensions to PyKoopman. Software is available at http://github.com/dynamicslab/pykoopman">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5a220582e1ff56332a3800470ffdc25f663f9030" target='_blank'>PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator</a></td>
          <td>
            Shaowu Pan, E. Kaiser, Brian M. de Silva, J. Kutz, S. Brunton
          </td>
          <td>2023-06-22</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>63</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4815aedae0a93d8e313e2f9c3e0f266d7132161e" target='_blank'>Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations</a></td>
          <td>
            Masih Haseli, Jorge Cort'es
          </td>
          <td>2021-08-08</td>
          <td>ArXiv</td>
          <td>7</td>
          <td>6</td>
        </tr>
    
        <tr id="Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/49ee811e5c7af5980d3dfe791f6b18e0080ad566" target='_blank'>Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition</a></td>
          <td>
            Naoya Takeishi, Y. Kawahara, T. Yairi
          </td>
          <td>2017-10-12</td>
          <td>ArXiv</td>
          <td>324</td>
          <td>24</td>
        </tr>
    
        <tr id="Koopman operators model nonlinear dynamics as a linear dynamic system acting on a nonlinear function as the state. This nonstandard state is often called a Koopman observable and is usually approximated numerically by a superposition of functions drawn from a dictionary . A widely used algorithm, is Extended Dynamic Mode Decomposition , where the dictionary functions are drawn from a ﬁxed, homogeneous class of functions. Recently, deep learning combined with EDMD has been used to learn novel dictionary functions in an algorithm called deep dynamic mode decomposition (deepDMD). The learned representation both (1) accurately models and (2) scales well with the dimension of the original nonlinear system. In this paper we analyze the learned dictionaries from deepDMD and explore the theoretical basis for their strong performance. We discover a novel class of dictionary functions to approximate Koopman observables. Error analysis of these dictionary functions show they satisfy a property of subspace approximation, which we deﬁne as uniform ﬁnite approximate closure. We discover that structured mixing of heterogeneous dictionary functions drawn from diﬀerent classes of nonlinear functions achieve the same accuracy and dimensional scaling as deepDMD. This mixed dictionary does so with an order of magnitude reduction in parameters, while maintaining geometric interpretability. Our results provide a hypothesis to explain the success of deep neural networks in learning numerical approximations to Koopman operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6eadb57b4978af7337a7ee5fdaf36a0e60505dbe" target='_blank'>Heterogeneous mixtures of dictionary functions to approximate subspace invariance in Koopman operators</a></td>
          <td>
            Charles A. Johnson, Shara Balakrishnan, Enoch Yeung
          </td>
          <td>2022-06-27</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>17</td>
        </tr>
    
        <tr id="Koopman operators model nonlinear dynamics as a linear dynamic system acting on a nonlinear function as the state. This nonstandard state is often called a Koopman observable and is usually approximated numerically by a superposition of functions drawn from a dictionary. In a widely used algorithm, Extended Dynamic Mode Decomposition, the dictionary functions are drawn from a fixed class of functions. Recently, deep learning combined with EDMD has been used to learn novel dictionary functions in an algorithm called deep dynamic mode decomposition (deepDMD). The learned representation both (1) accurately models and (2) scales well with the dimension of the original nonlinear system. In this paper we analyze the learned dictionaries from deepDMD and explore the theoretical basis for their strong performance. We explore State-Inclusive Logistic Lifting (SILL) dictionary functions to approximate Koopman observables. Error analysis of these dictionary functions show they satisfy a property of subspace approximation, which we define as uniform finite approximate closure. Our results provide a hypothesis to explain the success of deep neural networks in learning numerical approximations to Koopman operators. Part 2 of this paper will extend this explanation by demonstrating the subspace invariant of heterogeneous dictionaries and presenting a head-to-head numerical comparison of deepDMD and low-parameter heterogeneous dictionary learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3bc5cdd4c960594fe0690a326e49070938c893a" target='_blank'>Learning Invariant Subspaces of Koopman Operators-Part 1: A Methodology for Demonstrating a Dictionary's Approximate Subspace Invariance</a></td>
          <td>
            Charles A. Johnson, Shara Balakrishnan, Enoch Yeung
          </td>
          <td>2022-12-14</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>17</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>