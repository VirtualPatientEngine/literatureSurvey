---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:06:45 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e09301c9f56ba547e0a203efaeb9295594e5b840" target='_blank'>Deep Probabilistic Koopman: Long-term time-series forecasting under periodic uncertainties</a></td>
          <td>
            Alex Troy Mallen, Henning Lange, J. Kutz
          </td>
          <td>2021-06-10</td>
          <td>ArXiv</td>
          <td>6</td>
          <td>31</td>
        </tr>
    
        <tr id="Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE's superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb0e700722b318f57b605b0cb12d99e1c70318ef" target='_blank'>Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems</a></td>
          <td>
            I. Nayak, Debdipta Goswami, Mrinal Kumar, Fernando L. Teixeira
          </td>
          <td>2024-03-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>
    
        <tr id="Machine learning has countless applications in time series analysis: controlling smart grids, detecting mechanical failures, and analyzing stock prices. Fourier mode decomposition (FMD) is the most common method of analysis because it decomposes time series into finite waveform components, or modes, but its principal shortcoming is that FMD assumes every mode has a constant amplitude, an assumption that rarely holds in real-world data. In contrast, Koopman mode decomposition (KMD) can detect modes with exponentially-increasing or - decreasing amplitudes, although it has mostly been applied to diagnosing data errors, not to prediction. What has kept KMD from being applied to prediction is partly a shortcoming in a mathematical formulation. This paper seeks to remedy that shortcoming: it provides a mathematically-precise formulation of KMD as a practical tool. This formulation, in turn, allows us to develop a novel practical method for prediction of future data. We further demonstrate our method's effectiveness using both synthetic data and real plasma flow data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d38f5f062ba848e0b48918719b454626d1a8f0b7" target='_blank'>Predictive Nonlinear Modeling by Koopman Mode Decomposition</a></td>
          <td>
            Akira Kusaba, Kilho Shin, D. Shepard, T. Kuboyama
          </td>
          <td>2020-11-01</td>
          <td>2020 International Conference on Data Mining Workshops (ICDMW)</td>
          <td>0</td>
          <td>13</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7e49ad93693c3a21d3bedc135f409eb8055c17e" target='_blank'>Koopman Operator Framework for Time Series Modeling and Analysis</a></td>
          <td>
            A. Surana
          </td>
          <td>2018-01-09</td>
          <td>Journal of Nonlinear Science</td>
          <td>35</td>
          <td>24</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7e49ad93693c3a21d3bedc135f409eb8055c17e" target='_blank'>Koopman Operator Framework for Time Series Modeling and Analysis</a></td>
          <td>
            A. Surana
          </td>
          <td>2018-01-09</td>
          <td>Journal of Nonlinear Science</td>
          <td>35</td>
          <td>24</td>
        </tr>
    
        <tr id="We propose a neural network-based model for nonlinear dynamics in continuous time that can impose inductive biases on decay rates and/or frequencies. Inductive biases are helpful for training neural networks especially when training data are small. The proposed model is based on the Koopman operator theory, where the decay rate and frequency information is used by restricting the eigenvalues of the Koopman operator that describe linear evolution in a Koopman space. We use neural networks to find an appropriate Koopman space, which are trained by minimizing multi-step forecasting and backcasting errors using irregularly sampled time-series data. Experiments on various time-series datasets demonstrate that the proposed method achieves higher forecasting performance given a single short training sequence than the existing methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/90813609def379739274d4db8337b155ac43e48e" target='_blank'>Modeling Nonlinear Dynamics in Continuous Time with Inductive Biases on Decay Rates and/or Frequencies</a></td>
          <td>
            Tomoharu Iwata, Y. Kawahara
          </td>
          <td>2022-12-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1ca50905705b56d8e6fef689520be72e75951bd7" target='_blank'>High-dimensional time series prediction using kernel-based Koopman mode regression</a></td>
          <td>
            Jia-Chen Hua, Farzad Noorian, Duncan J. M. Moss, P. Leong, G. Gunaratne
          </td>
          <td>2017-09-15</td>
          <td>Nonlinear Dynamics</td>
          <td>25</td>
          <td>37</td>
        </tr>
    
        <tr id="Recurrent neural networks are widely used on time series data, yet such models often ignore the underlying physical structures in such sequences. A new class of physics-based methods related to Koopman theory has been introduced, offering an alternative for processing nonlinear dynamical systems. In this work, we propose a novel Consistent Koopman Autoencoder model which, unlike the majority of existing work, leverages the forward and backward dynamics. Key to our approach is a new analysis which explores the interplay between consistent dynamics and their associated Koopman operators. Our network is directly related to the derived analysis, and its computational requirements are comparable to other baselines. We evaluate our method on a wide range of high-dimensional and short-term dependent problems, and it achieves accurate estimates for significant prediction horizons, while also being robust to noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0872dfacadb282200f909e167e34dd9a7d9f27cd" target='_blank'>Forecasting Sequential Data using Consistent Koopman Autoencoders</a></td>
          <td>
            Omri Azencot, N. Benjamin Erichson, Vanessa Lin, Michael W. Mahoney
          </td>
          <td>2020-03-04</td>
          <td>ArXiv</td>
          <td>108</td>
          <td>64</td>
        </tr>
    
        <tr id="Electricity load forecasting is crucial for effectively managing and optimizing power grids. Over the past few decades, various statistical and deep learning approaches have been used to develop load forecasting models. This paper presents an interpretable machine learning approach that identifies load dynamics using data-driven methods within an operator-theoretic framework. We represent the load data using the Koopman operator, which is inherent to the underlying dynamics. By computing the corresponding eigenfunctions, we decompose the load dynamics into coherent spatiotemporal patterns that are the most robust features of the dynamics. Each pattern evolves independently according to its single frequency, making its predictability based on linear dynamics. We emphasize that the load dynamics are constructed based on coherent spatiotemporal patterns that are intrinsic to the dynamics and are capable of encoding rich dynamical features at multiple time scales. These features are related to complex interactions over interconnected power grids and different exogenous effects. To implement the Koopman operator approach more efficiently, we cluster the load data using a modern kernel-based clustering approach and identify power stations with similar load patterns, particularly those with synchronized dynamics. We evaluate our approach using a large-scale dataset from a renewable electric power system within the continental European electricity system and show that the Koopman-based approach outperforms a deep learning (LSTM) architecture in terms of accuracy and computational efficiency. The code for this paper has been deposited in a GitHub repository, which can be accessed at the following address github.com/Shakeri-Lab/Power-Grids.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/01f2acb6064e53144d500ff9b5a337330691a33b" target='_blank'>Characterizing the load profile in power grids by Koopman mode decomposition of interconnected dynamics</a></td>
          <td>
            A. Tavasoli, Behnaz Moradijamei, Heman Shakeri
          </td>
          <td>2023-04-16</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>11</td>
        </tr>
    
        <tr id="Koopman spectral analysis has attracted attention for understanding nonlinear dynamical systems by which we can analyze nonlinear dynamics with a linear regime by lifting observations using a nonlinear function. For analysis, we need to find an appropriate lift function. Although several methods have been proposed for estimating a lift function based on neural networks, the existing methods train neural networks without spectral analysis. In this paper, we propose neural dynamic mode decomposition, in which neural networks are trained such that the forecast error is minimized when the dynamics is modeled based on spectral decomposition in the lifted space. With our proposed method, the forecast error is backpropagated through the neural networks and the spectral decomposition, enabling end-to-end learning of Koopman spectral analysis. When information is available on the frequencies or the growth rates of the dynamics, the proposed method can exploit it as regularizers for training. We also propose an extension of our approach when observations are influenced by exogenous control time-series. Our experiments demonstrate the effectiveness of our proposed method in terms of eigenvalue estimation and forecast performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/729f577cdcfebf5f5bbc4e38eeec21e462147f99" target='_blank'>Neural Dynamic Mode Decomposition for End-to-End Modeling of Nonlinear Dynamics</a></td>
          <td>
            Tomoharu Iwata, Y. Kawahara
          </td>
          <td>2020-12-11</td>
          <td>ArXiv</td>
          <td>9</td>
          <td>30</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>