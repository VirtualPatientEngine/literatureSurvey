---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:06:26 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Solving real-world optimization tasks using physics-informed neural computing</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="The nuclear sector represents the primary source of carbon-free energy in the United States. Nevertheless, existing nuclear power plants face the threat of early shutdowns due to their inability to compete economically against alternatives such as gas power plants. Optimizing the fuel cycle cost through the optimization of core loading patterns is one approach to addressing this lack of competitiveness. However, this optimization task involves multiple objectives and constraints, resulting in a vast number of candidate solutions that cannot be explicitly solved. While stochastic optimization (SO) methodologies are utilized by various nuclear utilities and vendors for fuel cycle reload design, manual design remains the preferred approach. To advance the state-of-the-art in core reload patterns, we have developed methods based on Deep Reinforcement Learning. Previous research has laid the groundwork for this approach and demonstrated its ability to discover high-quality patterns within a reasonable timeframe. However, there is a need for comparison against legacy methods to demonstrate its utility in a single-objective setting. While RL methods have shown superiority in multi-objective settings, they have not yet been applied to address the competitiveness issue effectively. In this paper, we rigorously compare our RL-based approach against the most commonly used SO-based methods, namely Genetic Algorithm (GA), Simulated Annealing (SA), and Tabu Search (TS). Subsequently, we introduce a new hybrid paradigm to devise innovative designs, resulting in economic gains ranging from 2.8 to 3.3 million dollars per year per plant. This development leverages interpretable AI, enabling improved algorithmic efficiency by making black-box optimizations interpretable. Future work will focus on scaling this method to address a broader range of core designs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2f81024fe705a70ad1dcacecb371e2c31c8e49b9" target='_blank'>Surpassing legacy approaches and human intelligence with hybrid single- and multi-objective Reinforcement Learning-based optimization and interpretable AI to enable the economic operation of the US nuclear fleet</a></td>
          <td>
            Paul Seurin, K. Shirvan
          </td>
          <td>2024-02-16</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>22</td>
        </tr>
    
        <tr id="We present an open-source Python framework for NeuroEvolution Optimization with Reinforcement Learning (NEORL) developed at the Massachusetts Institute of Technology. NEORL offers a global optimization interface of state-of-the-art algorithms in the field of evolutionary computation, neural networks through reinforcement learning, and hybrid neuroevolution algorithms. NEORL features diverse set of algorithms, user-friendly interface, parallel computing support, automatic hyperparameter tuning, detailed documentation, and demonstration of applications in mathematical and real-world engineering optimization. NEORL encompasses various optimization problems from combinatorial, continuous, mixed discrete/continuous, to high-dimensional, expensive, and constrained engineering optimization. NEORL is tested in variety of engineering applications relevant to low carbon energy research in addressing solutions to climate change. The examples include nuclear reactor control and fuel cell power production. The results demonstrate NEORL competitiveness against other algorithms and optimization frameworks in the literature, and a potential tool to solve large-scale optimization problems. More examples and benchmarking of NEORL can be found here: https://neorl.readthedocs.io/en/latest/index.html">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33eb8f2acbaceeef39928028d4fc19116b327233" target='_blank'>NEORL: NeuroEvolution Optimization with Reinforcement Learning</a></td>
          <td>
            M. Radaideh, Katelin Du, Paul Seurin, Devin Seyler, Xubo Gu, Haijiang Wang, K. Shirvan
          </td>
          <td>2021-12-01</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>22</td>
        </tr>
    
        <tr id="Significance The scientific and engineering field has long sought an optimization method that is both efficient and accurate. While combining evolutionary algorithms with deep-learning methods offers a viable solution for complex problems, the simple combination does not achieve accurate and efficient optimization due to the nature of evolutionary principles, even with well-trained deep-learning models. We introduce a physics-supervised deep-learning optimization (PSDLO) algorithm that significantly improves convergence speed while maintaining optimization accuracy. PSDLO’s mechanism and demonstration are clearly explained and verified, presenting an ideal optimization method for various science and engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ed82ec82355b994906271b5a1dd22f51f3606ba2" target='_blank'>Physics-supervised deep learning–based optimization (PSDLO) with accuracy and efficiency</a></td>
          <td>
            Xiaowen Li, Lige Chang, Yajun Cao, Junqiang Lu, Xiaoli Lu, Hanqing Jiang
          </td>
          <td>2023-08-21</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>2</td>
          <td>4</td>
        </tr>
    
        <tr id="This paper proposes a descent reward (DR) system to enhance agent's navigation and landing capabilities in the LunarLander-v2 environment provided by OpenAI. The DR system is evaluated with Heuristics Control, Linear Function Approximation (LFA), and Deep Q-Networks (DQN) algorithms. Results demonstrate that the DQN algorithm with DR achieves a 97% average success rate, improving 17% over the Heuristic baseline and 37% over the default reward DQN in terms of fuel consumption. Furthermore, the resilience of DQN, Double Deep Q-Networks (Double-DQN), and Dueling Deep Q-Networks (Dueling-DQN) with the proposed DR system is evaluated. Despite increased unpredictability, results show the DR system effectively augments agent learning and decision-making. The Double-DQN approach exhibits superior stability and reward achievement compared to DQN and Dueling-DQN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b6e7434e7d08f02865c12654dd8da1440e8097c6" target='_blank'>Deep Reinforcement Learning for Unpredictability-Induced Rewards to Handle Spacecraft Landing</a></td>
          <td>
            Salman Shah, Nianmin Yao
          </td>
          <td>2023-12-08</td>
          <td>2023 13th International Conference on Information Science and Technology (ICIST)</td>
          <td>0</td>
          <td>0</td>
        </tr>
    
        <tr id="Aerobraking is used to insert a spacecraft into a low orbit around a planet through many orbital passages into its complex atmosphere. The aerobraking atmospheric passages are challenging because of the high variability of the atmospheric environment. This paper develops a parallel domain randomized deep reinforcement learning architecture for autonomous decision-making in a stochastic environment, such as aerobraking atmospheric passages. In this context, the architecture is used for planning aerobraking maneuvers to avoid the occurrence of thermal violations during the atmospheric aerobraking passages and target a final low-altitude orbit. The parallel domain randomized deep reinforcement learning architecture is designed to account for large variability of the physical model, as well as uncertain conditions. Also, the parallel approach speeds up the training process for simulation-based applications, and domain randomization improves resultant policy generalization. This framework is applied to the 2001 Mars Odyssey aerobraking campaign; with respect to the 2001 Mars Odyssey mission flight data and a Numerical Predictor Corrector (NPC)-based state-of-the-art heuristic for autonomous aerobraking, the proposed architecture outperforms the state-of-the-art heuristic algorithm with a decrease of 97.5% in the number of thermal violations. Furthermore, it yields a reduction of 98.7% in the number of thermal violations with respect to the Mars Odyssey mission flight data and requires 13.9% fewer orbits. Results also show that the proposed architecture can also learn a generalized policy in the presence of strong uncertainties, such as aggressive atmospheric density perturbations, different atmospheric density models, and a different simulator maximum step size and error accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5671f0558daa773dab04be13a10c7310c5a8e3ae" target='_blank'>Autonomous Decision-Making for Aerobraking via Parallel Randomized Deep Reinforcement Learning</a></td>
          <td>
            G. Falcone, Z. Putnam
          </td>
          <td>2023-06-01</td>
          <td>IEEE Transactions on Aerospace and Electronic Systems</td>
          <td>1</td>
          <td>14</td>
        </tr>
    
        <tr id="Most optimization problems require the user to select an algorithm and, to some extent, also tune it for better performance. Although intuition and knowledge about the problem can speed up these selection and fine-tuning processes, users often use trial-and-error methodologies, which can be time-consuming and inefficient. With all of that in mind and much more, the concept of "learned optimizers", "learning to learn", and "meta-learning" has been gathering attention in recent years. In this article, we propose MolOpt that uses multiagent reinforcement learning (MARL) for autonomous molecular geometry optimization (MGO). Typically MGO algorithms are hand-designed, but MolOpt uses MARL to learn a learned optimizer (policy) that can perform MGO without the need for other hand-designed optimizers. We cast MGO as a MARL problem, where each agent corresponds to a single atom in the molecule. MolOpt performs MGO by minimizing the forces on each atom of the molecule. Our experiments demonstrate the generalizing ability of MolOpt for the MGO of propane, pentane, heptane, hexane, and octane when trained on ethane, butane, and isobutane. In terms of performance, MolOpt outperforms the MDMin optimizer and demonstrates performance similar to that of the FIRE optimizer. However, it does not surpass the BFGS optimizer. The results demonstrate that MolOpt has the potential to introduce innovative advancements in MGO by providing a novel approach using reinforcement learning (RL), which may open up new research directions for MGO. Overall, this work serves as a proof-of-concept for the potential of MARL in MGO.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b56c230c9f7bb4d90c0cb6e31d8863ff2fadc6b8" target='_blank'>MolOpt: Autonomous Molecular Geometry Optimization Using Multiagent Reinforcement Learning.</a></td>
          <td>
            Rohit Modee, Sarvesh Mehta, Siddhartha Laghuvarapu, U. Priyakumar
          </td>
          <td>2023-11-27</td>
          <td>The journal of physical chemistry. B</td>
          <td>3</td>
          <td>29</td>
        </tr>
    
        <tr id="This article is a short introduction to AI4OPT, the NSF AI Institute for Advances in Optimization. AI4OPT fuses AI and optimization, inspired by societal challenges in supply chains, energy systems, chip design and manufacturing, and sustainable food systems. By combining machine learning and mathematical optimization, AI4OPT strives to develop AI‐assisted optimization systems that bring orders of magnitude improvements in efficiency, perform accurate uncertainty quantification, and address challenges in resiliency and sustainability. AI4OPT also applies its “teaching the teachers” philosophy to provide longitudinal educational pathways in AI for engineering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3198a0c86b6f4b943c4abc2fd6f4457ecb7c4581" target='_blank'>AI4OPT: AI Institute for Advances in Optimization</a></td>
          <td>
            P. V. Hentenryck, Kevin Dalmeijer
          </td>
          <td>2023-07-05</td>
          <td>AI Mag.</td>
          <td>1</td>
          <td>18</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b6a3b0b6956e496f726ffe945dc5cdae8718ba43" target='_blank'>A survey on artificial intelligence trends in spacecraft guidance dynamics and control</a></td>
          <td>
            D. Izzo, Marcus Märtens, Binfeng Pan
          </td>
          <td>2018-12-07</td>
          <td>Astrodynamics</td>
          <td>177</td>
          <td>36</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>