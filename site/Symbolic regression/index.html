<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Time-series%20forecasting/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Symbolic regression
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Time-series%20forecasting/" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Time-series%20forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Symbolic regression</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-08-19 06:05:48 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Symbolic regression</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Symbolic regression</a><br>
      <a href="#recommended_articles">3. Recommended articles on Symbolic regression</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Symbolic regression</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Symbolic regression</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5d150cec2775f9bc863760448f14104cc8f42368" target='_blank'>
                Discovering governing equations from data by sparse identification of nonlinear dynamical systems
                </a>
              </td>
          <td>
            S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2015-09-11</td>
          <td>Proceedings of the National Academy of Sciences, Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>3190</td>
          <td>65</td>

            <td><a href='../recommendations/5d150cec2775f9bc863760448f14104cc8f42368' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/60d0d998fa038182b3b69a57adb9b2f82d40589c" target='_blank'>
                Robust learning from noisy, incomplete, high-dimensional experimental data via physically constrained symbolic regression
                </a>
              </td>
          <td>
            Patrick A. K. Reinbold, Logan Kageorge, M. Schatz, R. Grigoriev
          </td>
          <td>2021-02-24</td>
          <td>Nature Communications</td>
          <td>85</td>
          <td>23</td>

            <td><a href='../recommendations/60d0d998fa038182b3b69a57adb9b2f82d40589c' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Significance Governing equations are essential to the study of physical systems, providing models that can generalize to predict previously unseen behaviors. There are many systems of interest across disciplines where large quantities of data have been collected, but the underlying governing equations remain unknown. This work introduces an approach to discover governing models from data. The proposed method addresses a key limitation of prior approaches by simultaneously discovering coordinates that admit a parsimonious dynamical model. Developing parsimonious and interpretable governing models has the potential to transform our understanding of complex systems, including in neuroscience, biology, and climate science. The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3c9961153493370500020c81527b3548c96f81e0" target='_blank'>
                Data-driven discovery of coordinates and governing equations
                </a>
              </td>
          <td>
            Kathleen P. Champion, Bethany Lusch, J. Kutz, S. Brunton
          </td>
          <td>2019-03-29</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>603</td>
          <td>65</td>

            <td><a href='../recommendations/3c9961153493370500020c81527b3548c96f81e0' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3df50e9b73cc2937dfd651f4c3344bc99b7ed3f2" target='_blank'>
                Chaos as an intermittently forced linear system
                </a>
              </td>
          <td>
            S. Brunton, Bingni W. Brunton, J. Proctor, E. Kaiser, J. Kutz
          </td>
          <td>2016-08-18</td>
          <td>Nature Communications</td>
          <td>448</td>
          <td>65</td>

            <td><a href='../recommendations/3df50e9b73cc2937dfd651f4c3344bc99b7ed3f2' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Data-driven discovery of dynamics via machine learning is pushing the frontiers of modelling and control efforts, providing a tremendous opportunity to extend the reach of model predictive control (MPC). However, many leading methods in machine learning, such as neural networks (NN), require large volumes of training data, may not be interpretable, do not easily include known constraints and symmetries, and may not generalize beyond the attractor where models are trained. These factors limit their use for the online identification of a model in the low-data limit, for example following an abrupt change to the system dynamics. In this work, we extend the recent sparse identification of nonlinear dynamics (SINDY) modelling procedure to include the effects of actuation and demonstrate the ability of these models to enhance the performance of MPC, based on limited, noisy data. SINDY models are parsimonious, identifying the fewest terms in the model needed to explain the data, making them interpretable and generalizable. We show that the resulting SINDY-MPC framework has higher performance, requires significantly less data, and is more computationally efficient and robust to noise than NN models, making it viable for online training and execution in response to rapid system changes. SINDY-MPC also shows improved performance over linear data-driven models, although linear models may provide a stopgap until enough data is available for SINDY. SINDY-MPC is demonstrated on a variety of dynamical systems with different challenges, including the chaotic Lorenz system, a simple model for flight control of an F8 aircraft, and an HIV model incorporating drug treatment.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b2eb064f432557c59ce99834d7dc7817e4687271" target='_blank'>
                Sparse identification of nonlinear dynamics for model predictive control in the low-data limit
                </a>
              </td>
          <td>
            E. Kaiser, J. Kutz, S. Brunton
          </td>
          <td>2017-11-15</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>428</td>
          <td>65</td>

            <td><a href='../recommendations/b2eb064f432557c59ce99834d7dc7817e4687271' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Inferring the structure and dynamics of network models is critical to understanding the functionality and control of complex systems, such as metabolic and regulatory biological networks. The increasing quality and quantity of experimental data enable statistical approaches based on information theory for model selection and goodness-of-fit metrics. We propose an alternative data-driven method to infer networked nonlinear dynamical systems by using sparsity-promoting optimization to select a subset of nonlinear interactions representing dynamics on a network. In contrast to standard model selection methods-based upon information content for a finite number of heuristic models (order 10 or less), our model selection procedure discovers a parsimonious model from a combinatorially large set of models, without an exhaustive search. Our particular innovation is appropriate for many biological networks, where the governing dynamical systems have rational function nonlinearities with cross terms, thus requiring an implicit formulation and the equations to be identified in the null-space of a library of mixed nonlinearities, including the state and derivative terms. This method, implicit-SINDy, succeeds in inferring three canonical biological models: 1) Michaelis-Menten enzyme kinetics; 2) the regulatory network for competence in bacteria; and 3) the metabolic network for yeast glycolysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06a0ba437d41a7c82c08a9636a4438c1b5031378" target='_blank'>
                Inferring Biological Networks by Sparse Identification of Nonlinear Dynamics
                </a>
              </td>
          <td>
            N. Mangan, S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2016-05-26</td>
          <td>IEEE Transactions on Molecular, Biological and Multi-Scale Communications, IEEE Transactions on Molecular Biological and Multi-Scale Communications</td>
          <td>317</td>
          <td>65</td>

            <td><a href='../recommendations/06a0ba437d41a7c82c08a9636a4438c1b5031378' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Accurately modelling the nonlinear dynamics of a system from measurement data is a challenging yet vital topic. The sparse identification of nonlinear dynamics (SINDy) algorithm is one approach to discover dynamical systems models from data. Although extensions have been developed to identify implicit dynamics, or dynamics described by rational functions, these extensions are extremely sensitive to noise. In this work, we develop SINDy-PI (parallel, implicit), a robust variant of the SINDy algorithm to identify implicit dynamics and rational nonlinearities. The SINDy-PI framework includes multiple optimization algorithms and a principled approach to model selection. We demonstrate the ability of this algorithm to learn implicit ordinary and partial differential equations and conservation laws from limited and noisy data. In particular, we show that the proposed approach is several orders of magnitude more noise robust than previous approaches, and may be used to identify a class of ODE and PDE dynamics that were previously unattainable with SINDy, including for the double pendulum dynamics and simplified model for the Belousov–Zhabotinsky (BZ) reaction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4971f9abd024e40fbbdff2e9492745b68a6bca01" target='_blank'>
                SINDy-PI: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics
                </a>
              </td>
          <td>
            Kadierdan Kaheman, J. Kutz, S. Brunton
          </td>
          <td>2020-04-05</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>194</td>
          <td>65</td>

            <td><a href='../recommendations/4971f9abd024e40fbbdff2e9492745b68a6bca01' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="A key task in the field of modeling and analyzing nonlinear dynamical systems is the recovery of unknown governing equations from measurement data only. There is a wide range of application areas for this important instance of system identification, ranging from industrial engineering and acoustic signal processing to stock market models. In order to find appropriate representations of underlying dynamical systems, various data-driven methods have been proposed by different communities. However, if the given data sets are high-dimensional, then these methods typically suffer from the curse of dimensionality. To significantly reduce the computational costs and storage consumption, we propose the method multidimensional approximation of nonlinear dynamical systems (MANDy) which combines data-driven methods with tensor network decompositions. The efficiency of the introduced approach will be illustrated with the aid of several high-dimensional nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2b2aa13d4959073f61ad70555bc8c7da7d116196" target='_blank'>
                Multidimensional Approximation of Nonlinear Dynamical Systems
                </a>
              </td>
          <td>
            Patrick Gelß, Stefan Klus, J. Eisert, Christof Schutte
          </td>
          <td>2018-09-07</td>
          <td>Journal of Computational and Nonlinear Dynamics</td>
          <td>61</td>
          <td>76</td>

            <td><a href='../recommendations/2b2aa13d4959073f61ad70555bc8c7da7d116196' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="First principles modeling of physical systems has led to significant technological advances across all branches of science. For nonlinear systems, however, small modeling errors can lead to significant deviations from the true, measured behavior. Even in mechanical systems, where the equations are assumed to be well-known, there are often model discrepancies corresponding to nonlinear friction, wind resistance, etc. Discovering models for these discrepancies remains an open challenge for many complex systems. In this work, we use the sparse identification of nonlinear dynamics (SINDy) algorithm to discover a model for the discrepancy between a simplified model and measurement data. In particular, we assume that the model mismatch can be sparsely represented in a library of candidate model terms. We demonstrate the efficacy of our approach on several examples including experimental data from a double pendulum on a cart. We further design and implement a feed-forward controller in simulations, showing improvement with a discrepancy model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73dd9c49f205280991826b2ea4b50344203916b4" target='_blank'>
                Learning Discrepancy Models From Experimental Data
                </a>
              </td>
          <td>
            Kadierdan Kaheman, E. Kaiser, B. Strom, J. Kutz, S. Brunton
          </td>
          <td>2019-09-18</td>
          <td>arXiv.org, ArXiv</td>
          <td>32</td>
          <td>65</td>

            <td><a href='../recommendations/73dd9c49f205280991826b2ea4b50344203916b4' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of nuanced issues that must be addressed by modern data-driven methods for automated physics discovery. Specifically, we show that measurement noise and complex secondary physical mechanisms, like unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. We use the sparse identification of non-linear dynamics (SINDy) method to identify governing equations for real-world measurement data and simulated trajectories. Incorporating into SINDy the assumption that each falling object is governed by a similar physical law is shown to improve the robustness of the learned models, but discrepancies between the predictions and observations persist due to subtleties in drag dynamics. This work highlights the fact that the naive application of ML/AI will generally be insufficient to infer universal physical laws without further modification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/35e2571c17246577e0bc1b9de57a314c3b60e220" target='_blank'>
                Discovery of Physics From Data: Universal Laws and Discrepancies
                </a>
              </td>
          <td>
            Brian M. de Silva, D. Higdon, S. Brunton, J. Kutz
          </td>
          <td>2019-06-19</td>
          <td>Frontiers in Artificial Intelligence</td>
          <td>67</td>
          <td>65</td>

            <td><a href='../recommendations/35e2571c17246577e0bc1b9de57a314c3b60e220' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Researchers propose sparse regression for identifying governing partial differential equations for spatiotemporal systems. We propose a sparse regression method capable of discovering the governing partial differential equation(s) of a given system by time series measurements in the spatial domain. The regression framework relies on sparsity-promoting techniques to select the nonlinear and partial derivative terms of the governing equations that most accurately represent the data, bypassing a combinatorially large search through all possible candidate models. The method balances model complexity and regression accuracy by selecting a parsimonious model via Pareto analysis. Time series measurements can be made in an Eulerian framework, where the sensors are fixed spatially, or in a Lagrangian framework, where the sensors move with the dynamics. The method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems spanning a number of scientific domains including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation. Moreover, the method is capable of disambiguating between potentially nonunique dynamical terms by using multiple time series taken with different initial data. Thus, for a traveling wave, the method can distinguish between a linear wave equation and the Korteweg–de Vries equation, for instance. The method provides a promising new technique for discovering governing equations and physical laws in parameterized spatiotemporal systems, where first-principles derivations are intractable.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0acd117521ef5aafb09fed02ab415523b330b058" target='_blank'>
                Data-driven discovery of partial differential equations
                </a>
              </td>
          <td>
            S. Rudy, S. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2016-09-21</td>
          <td>Science Advances</td>
          <td>1172</td>
          <td>65</td>

            <td><a href='../recommendations/0acd117521ef5aafb09fed02ab415523b330b058' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Sparse model identification enables the discovery of nonlinear dynamical systems purely from data; however, this approach is sensitive to noise, especially in the low-data limit. In this work, we leverage the statistical approach of bootstrap aggregating (bagging) to robustify the sparse identification of the nonlinear dynamics (SINDy) algorithm. First, an ensemble of SINDy models is identified from subsets of limited and noisy data. The aggregate model statistics are then used to produce inclusion probabilities of the candidate functions, which enables uncertainty quantification and probabilistic forecasts. We apply this ensemble-SINDy (E-SINDy) algorithm to several synthetic and real-world datasets and demonstrate substantial improvements to the accuracy and robustness of model discovery from extremely noisy and limited data. For example, E-SINDy uncovers partial differential equations models from data with more than twice as much measurement noise as has been previously reported. Similarly, E-SINDy learns the Lotka Volterra dynamics from remarkably limited data of yearly lynx and hare pelts collected from 1900 to 1920. E-SINDy is computationally efficient, with similar scaling as standard SINDy. Finally, we show that ensemble statistics from E-SINDy can be exploited for active learning and improved model predictive control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/883547fdbd88552328a6615ec620f96e39c57018" target='_blank'>
                Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control
                </a>
              </td>
          <td>
            Urban Fasel, J. Kutz, Bingni W. Brunton, S. Brunton
          </td>
          <td>2021-11-22</td>
          <td>Proceedings of the Royal Society A, Proceedings. Mathematical, Physical, and Engineering Sciences</td>
          <td>158</td>
          <td>65</td>

            <td><a href='../recommendations/883547fdbd88552328a6615ec620f96e39c57018' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6f0a85009481dcfd93aaa43ed3f980e5033b0d8" target='_blank'>
                Learning sparse nonlinear dynamics via mixed-integer optimization
                </a>
              </td>
          <td>
            D. Bertsimas, Wes Gurnee
          </td>
          <td>2022-06-01</td>
          <td>Nonlinear Dynamics</td>
          <td>28</td>
          <td>91</td>

            <td><a href='../recommendations/e6f0a85009481dcfd93aaa43ed3f980e5033b0d8' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Regularized regression problems are ubiquitous in statistical modeling, signal processing, and machine learning. Sparse regression, in particular, has been instrumental in scientific model discovery, including compressed sensing applications, variable selection, and high-dimensional analysis. We propose a broad framework for sparse relaxed regularized regression, called SR3. The key idea is to solve a relaxation of the regularized problem, which has three advantages over the state-of-the-art: 1) solutions of the relaxed problem are superior with respect to errors, false positives, and conditioning; 2) relaxation allows extremely fast algorithms for both convex and nonconvex formulations; and 3) the methods apply to composite regularizers, essential for total variation (TV) as well as sparsity-promoting formulations using tight frames. We demonstrate the advantages of SR3 (computational efficiency, higher accuracy, faster convergence rates, and greater flexibility) across a range of regularized regression problems with synthetic and real data, including applications in compressed sensing, LASSO, matrix completion, TV regularization, and group sparsity. Following standards of reproducible research, we also provide a companion MATLAB package that implements these examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c0fc3882a9976f6a9cdc3a724bce184b786503da" target='_blank'>
                A Unified Framework for Sparse Relaxed Regularized Regression: SR3
                </a>
              </td>
          <td>
            P. Zheng, T. Askham, S. Brunton, J. Kutz, A. Aravkin
          </td>
          <td>2018-07-14</td>
          <td>IEEE Access</td>
          <td>115</td>
          <td>65</td>

            <td><a href='../recommendations/c0fc3882a9976f6a9cdc3a724bce184b786503da' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Symbolic regression'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Symbolic regression</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="Over the past few years, equation discovery has gained popularity in different fields of science and engineering. However, existing equation discovery algorithms rely on the availability of noisy measurements of the state variables (i.e., displacement {and velocity}). This is a major bottleneck in structural dynamics, where we often only have access to acceleration measurements. To that end, this paper introduces a novel equation discovery algorithm for discovering governing equations of dynamical systems from acceleration-only measurements. The proposed algorithm employs a library-based approach for equation discovery. To enable equation discovery from acceleration-only measurements, we propose a novel Approximate Bayesian Computation (ABC) model that prioritizes parsimonious models. The efficacy of the proposed algorithm is illustrated using {four} structural dynamics examples that include both linear and nonlinear dynamical systems. The case studies presented illustrate the possible application of the proposed approach for equation discovery of dynamical systems from acceleration-only measurements.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f046e6be05e115bc55624ef726d92d6d06303346" target='_blank'>
              Discovering governing equation in structural dynamics from acceleration-only measurements
              </a>
            </td>
          <td>
            Calvin Alvares, Souvik Chakraborty
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper focuses on the application of experimental data-based system identification of unknown systems utilising sparse identification of nonlinear dynamics (SINDy). SINDy is used to detect the system dynamics in the three well-known nonlinear systems. Analyzed are SINDy’s abilities to accurately represent transient/steady-state behaviour, noise effect, and model structure. A sparse set of basis functions can effectively capture the dynamics of a system, according to the data-driven approach known as SINDy. The coefficients of these basis functions are determined via methods of sparse regression, and the final model is made up of a number of sparse ordinary differential equations. The findings demonstrate that SINDy, with sufficient time-series data, can capture both transient and steady-state phenomena. According to the analysis of the noise effect, SINDy’s performance declines as the system’s noise level rises. The feature library must contain the appropriate model structure in order for SINDy to function effectively. SINDy has the potential to extract unknown system dynamics from experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/315b7c145c5a86b92e656ad174cede9f67bf3f34" target='_blank'>
              Data-driven system identification of unknown systems utilising sparse identification of nonlinear dynamics (SINDy)
              </a>
            </td>
          <td>
            P. Pandey, H. Haddad Khodaparast, M. Friswell, T. Chatterjee, N. Jamia, T. Deighan
          </td>
          <td>2024-06-01</td>
          <td>Journal of Physics: Conference Series</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Modeling complex physical dynamics is a fundamental task in science and engineering. Traditional physics-based models are first-principled, explainable, and sample-efficient. However, they often rely on strong modeling assumptions and expensive numerical integration, requiring significant computational resources and domain expertise. While deep learning (DL) provides efficient alternatives for modeling complex dynamics, they require a large amount of labeled training data. Furthermore, its predictions may disobey the governing physical laws and are difficult to interpret. Physics-guided DL aims to integrate first-principled physical knowledge into data-driven methods. It has the best of both worlds and is well equipped to better solve scientific problems. Recently, this field has gained great progress and has drawn considerable interest across discipline Here, we introduce the framework of physics-guided DL with a special emphasis on learning dynamical systems. We describe the learning pipeline and categorize state-of-the-art methods under this framework. We also offer our perspectives on the open challenges and emerging opportunities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/60d721e89c2f9c549241a4982b77f9c752b34460" target='_blank'>
              Learning dynamical systems from data: An introduction to physics-guided deep learning
              </a>
            </td>
          <td>
            Rose Yu, Rui Wang
          </td>
          <td>2024-06-24</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="The Sparse Identification of Nonlinear Dynamics (SINDy) framework is a robust method for identifying governing equations, successfully applied to ordinary, partial, and stochastic differential equations. In this work we extend SINDy to identify delay differential equations by using an augmented library that includes delayed samples and Bayesian optimization. To identify a possibly unknown delay we minimize the reconstruction error over a set of candidates. The resulting methodology improves the overall performance by remarkably reducing the number of calls to SINDy with respect to a brute force approach. We also address a multivariate setting to identify multiple unknown delays and (non-multiplicative) parameters. Several numerical tests on delay differential equations with different long-term behavior, number of variables, delays, and parameters support the use of Bayesian optimization highlighting both the efficacy of the proposed methodology and its computational advantages. As a consequence, the class of discoverable models is significantly expanded.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/827638609aa7a84d10096156bc7ead43d8060c5a" target='_blank'>
              Data-driven Discovery of Delay Differential Equations with Discrete Delays
              </a>
            </td>
          <td>
            Alessandro Pecile, N. Demo, M. Tezzele, G. Rozza, Dimitri Breda
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>49</td>
        </tr>

        <tr id="We present a computational technique for modeling the evolution of dynamical systems in a reduced basis, with a focus on the challenging problem of modeling partially-observed partial differential equations (PDEs) on high-dimensional non-uniform grids. We address limitations of previous work on data-driven flow map learning in the sense that we focus on noisy and limited data to move toward data collection scenarios in real-world applications. Leveraging recent work on modeling PDEs in modal and nodal spaces, we present a neural network structure that is suitable for PDE modeling with noisy and limited data available only on a subset of the state variables or computational domain. In particular, spatial grid-point measurements are reduced using a learned linear transformation, after which the dynamics are learned in this reduced basis before being transformed back out to the nodal space. This approach yields a drastically reduced parameterization of the neural network compared with previous flow map models for nodal space learning. This primarily allows for smaller training data sets, but also enables reduced training times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33821b7c0acd5258ca0c60a09ffdc55439ac7ac2" target='_blank'>
              Principal Component Flow Map Learning of PDEs from Incomplete, Limited, and Noisy Data
              </a>
            </td>
          <td>
            Victor Churchill
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This work presents a data-driven method for learning low-dimensional time-dependent physics-based surrogate models whose predictions are endowed with uncertainty estimates. We use the operator inference approach to model reduction that poses the problem of learning low-dimensional model terms as a regression of state space data and corresponding time derivatives by minimizing the residual of reduced system equations. Standard operator inference models perform well with accurate training data that are dense in time, but producing stable and accurate models when the state data are noisy and/or sparse in time remains a challenge. Another challenge is the lack of uncertainty estimation for the predictions from the operator inference models. Our approach addresses these challenges by incorporating Gaussian process surrogates into the operator inference framework to (1) probabilistically describe uncertainties in the state predictions and (2) procure analytical time derivative estimates with quantified uncertainties. The formulation leads to a generalized least-squares regression and, ultimately, reduced-order models that are described probabilistically with a closed-form expression for the posterior distribution of the operators. The resulting probabilistic surrogate model propagates uncertainties from the observed state data to reduced-order predictions. We demonstrate the method is effective for constructing low-dimensional models of two nonlinear partial differential equations representing a compressible flow and a nonlinear diffusion-reaction process, as well as for estimating the parameters of a low-dimensional system of nonlinear ordinary differential equations representing compartmental models in epidemiology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6c14dacd4e17965fe6d0db5f5cefb7cb8547dbb3" target='_blank'>
              Bayesian learning with Gaussian processes for low-dimensional representations of time-dependent nonlinear systems
              </a>
            </td>
          <td>
            Shane A. McQuarrie, Anirban Chaudhuri, Karen Willcox, Mengwu Guo
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The quasipotential function allows for comprehension and prediction of the escape mechanisms from metastable states in nonlinear dynamical systems. This function acts as a natural extension of the potential function for non-gradient systems and it unveils important properties such as the maximum likelihood transition paths, transition rates and expected exit times of the system. Here, we leverage on machine learning via the combination of two data-driven techniques, namely a neural network and a sparse regression algorithm, to obtain symbolic expressions of quasipotential functions. The key idea is first to determine an orthogonal decomposition of the vector field that governs the underlying dynamics using neural networks, then to interpret symbolically the downhill and circulatory components of the decomposition. These functions are regressed simultaneously with the addition of mathematical constraints. We show that our approach discovers a parsimonious quasipotential equation for an archetypal model with a known exact quasipotential and for the dynamics of a nanomechanical resonator. The analytical forms deliver direct access to the stability of the metastable states and predict rare events with significant computational advantages. Our data-driven approach is of interest for a wide range of applications in which to assess the fluctuating dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/174f3a662f5a30364246d8cf0f34af33eac8ccfd" target='_blank'>
              Sparse identification of quasipotentials via a combined data-driven method
              </a>
            </td>
          <td>
            Bo Lin, P. Belardinelli
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Kolmogorov-Arnold networks (KANs) as an alternative to multi-layer perceptrons (MLPs) are a recent development demonstrating strong potential for data-driven modeling. This work applies KANs as the backbone of a neural ordinary differential equation (ODE) framework, generalizing their use to the time-dependent and temporal grid-sensitive cases often seen in dynamical systems and scientific machine learning applications. The proposed KAN-ODEs retain the flexible dynamical system modeling framework of Neural ODEs while leveraging the many benefits of KANs compared to MLPs, including higher accuracy and faster neural scaling, stronger interpretability and generalizability, and lower parameter counts. First, we quantitatively demonstrated these improvements in a comprehensive study of the classical Lotka-Volterra predator-prey model. We then showcased the KAN-ODE framework's ability to learn symbolic source terms and complete solution profiles in higher-complexity and data-lean scenarios including wave propagation and shock formation, the complex Schr\"odinger equation, and the Allen-Cahn phase separation equation. The successful training of KAN-ODEs, and their improved performance compared to traditional Neural ODEs, implies significant potential in leveraging this novel network architecture in myriad scientific machine learning applications for discovering hidden physics and predicting dynamic evolution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/efeeddb4162e3eefbe1d974d6ce7cd1d32498a6b" target='_blank'>
              KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for Learning Dynamical Systems and Hidden Physics
              </a>
            </td>
          <td>
            Benjamin C. Koenig, Suyong Kim, Sili Deng
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>22</td>
        </tr>

        <tr id="Model parsimony is an important \emph{cognitive bias} in data-driven modelling that aids interpretability and helps to prevent over-fitting. Sparse identification of nonlinear dynamics (SINDy) methods are able to learn sparse representations of complex dynamics directly from data, given a basis of library functions. In this work, a novel Bayesian treatment of dictionary learning system identification, as an alternative to SINDy, is envisaged. The proposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is distinct from previous approaches in that it targets the full joint posterior distribution over both the terms in the library and their parameterisation in the model. This formulation confers the advantage that an arbitrary prior may be placed over the model structure to produce models that are sparse in the model space rather than in parameter space. Because this posterior is defined over parameter vectors that can change in dimension, the inference cannot be performed by standard techniques. Instead, a Gibbs sampler based on reversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare favourably to ensemble SINDy in three benchmark case-studies. In particular, it is seen that the proposed method is better able to assign high probability to correct model terms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04666a9e47b5508a0c0f966265fe51da2b2c634a" target='_blank'>
              BINDy -- Bayesian identification of nonlinear dynamics with reversible-jump Markov-chain Monte-Carlo
              </a>
            </td>
          <td>
            M.D. Champneys, T. J. Rogers
          </td>
          <td>2024-08-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The parametric greedy latent space dynamics identification (gLaSDI) framework has demonstrated promising potential for accurate and efficient modeling of high-dimensional nonlinear physical systems. However, it remains challenging to handle noisy data. To enhance robustness against noise, we incorporate the weak-form estimation of nonlinear dynamics (WENDy) into gLaSDI. In the proposed weak-form gLaSDI (WgLaSDI) framework, an autoencoder and WENDy are trained simultaneously to discover intrinsic nonlinear latent-space dynamics of high-dimensional data. Compared to the standard sparse identification of nonlinear dynamics (SINDy) employed in gLaSDI, WENDy enables variance reduction and robust latent space discovery, therefore leading to more accurate and efficient reduced-order modeling. Furthermore, the greedy physics-informed active learning in WgLaSDI enables adaptive sampling of optimal training data on the fly for enhanced modeling accuracy. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including viscous and inviscid Burgers' equations, time-dependent radial advection, and the Vlasov equation for plasma physics. With data that contains 5-10% Gaussian white noise, WgLaSDI outperforms gLaSDI by orders of magnitude, achieving 1-7% relative errors. Compared with the high-fidelity models, WgLaSDI achieves 121 to 1,779x speed-up.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15fe2e82d4161103b69fe83f184552b9e9774633" target='_blank'>
              Physics-informed active learning with simultaneous weak-form latent space dynamics identification
              </a>
            </td>
          <td>
            Xiaolong He, April Tran, David M. Bortz, Youngsoo Choi
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This paper presents a sequence of two approaches for the data-driven control-oriented modeling of networked systems, i.e., the systems that involve many interacting dynamical components. First, a novel deep learning approach named the weak Latent Dynamics Model (wLDM) is developed for learning generic nonlinear dynamics with control. Leveraging the weak form, the wLDM enables more numerically stable and computationally efficient training as well as more accurate prediction, when compared to conventional methods such as neural ordinary differential equations. Building upon the wLDM framework, we propose the weak Graph Koopman Bilinear Form (wGKBF) model, which integrates geometric deep learning and Koopman theory to learn latent space dynamics for networked systems, especially for the challenging cases having multiple timescales. The effectiveness of the wLDM framework and wGKBF model are demonstrated on three example systems of increasing complexity - a controlled double pendulum, the stiff Brusselator dynamics, and an electrified aircraft energy system. These numerical examples show that the wLDM and wGKBF achieve superior predictive accuracy and training efficiency as compared to baseline models. Parametric studies provide insights into the effects of hyperparameters in the weak form. The proposed framework shows the capability to efficiently capture control-dependent dynamics in these systems, including stiff dynamics and multi-physics interactions, offering a promising direction for learning control-oriented models of complex networked systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2168dc84b931e82498692c0f9277bed180e91df" target='_blank'>
              Learning Networked Dynamical System Models with Weak Form and Graph Neural Networks
              </a>
            </td>
          <td>
            Yin Yu, Daning Huang, Seho Park, H. Pangborn
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="It has been found recently that more data can, counter-intuitively, hurt the performance of deep neural networks. Here, we show that a more extreme version of the phenomenon occurs in data-driven models of dynamical systems. To elucidate the underlying mechanism, we focus on next-generation reservoir computing (NGRC) -- a popular framework for learning dynamics from data. We find that, despite learning a better representation of the flow map with more training data, NGRC can adopt an ill-conditioned ``integrator'' and lose stability. We link this data-induced instability to the auxiliary dimensions created by the delayed states in NGRC. Based on these findings, we propose simple strategies to mitigate the instability, either by increasing regularization strength in tandem with data size, or by carefully introducing noise during training. Our results highlight the importance of proper regularization in data-driven modeling of dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b882f72ec6688e53edbf612e32d2b97c2a4c8236" target='_blank'>
              How more data can hurt: Instability and regularization in next-generation reservoir computing
              </a>
            </td>
          <td>
            Yuanzhao Zhang, Sean P. Cornelius
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Stochastic collocation (SC) is a well‐known non‐intrusive method of constructing surrogate models for uncertainty quantification. In dynamical systems, SC is especially suited for full‐field uncertainty propagation that characterizes the distributions of the high‐dimensional solution fields of a model with stochastic input parameters. However, due to the highly nonlinear nature of the parameter‐to‐solution map in even the simplest dynamical systems, the constructed SC surrogates are often inaccurate. This work presents an alternative approach, where we apply the SC approximation over the dynamics of the model, rather than the solution. By combining the data‐driven sparse identification of nonlinear dynamics framework with SC, we construct dynamics surrogates and integrate them through time to construct the surrogate solutions. We demonstrate that the SC‐over‐dynamics framework leads to smaller errors, both in terms of the approximated system trajectories as well as the model state distributions, when compared against full‐field SC applied to the solutions directly. We present numerical evidence of this improvement using three test problems: a chaotic ordinary differential equation, and two partial differential equations from solid mechanics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc0b58e254bb512300814c52b4a101046377d6e4" target='_blank'>
              Accurate data‐driven surrogates of dynamical systems for forward propagation of uncertainty
              </a>
            </td>
          <td>
            Saibal De, Reese E. Jones, H. Kolla
          </td>
          <td>2024-08-03</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="We propose a novel learning framework for Koopman operator of nonlinear dynamical systems that is informed by the governing equation and guarantees long-time stability and robustness to noise. In contrast to existing frameworks where either ad-hoc observables or blackbox neural networks are used to construct observables in the extended dynamic mode decomposition (EDMD), our observables are informed by governing equations via Polyflow. To improve the noise robustness and guarantee long-term stability, we designed a stable parameterization of the Koopman operator together with a progressive learning strategy for roll-out recurrent loss. To further improve model performance in the phase space, a simple iterative strategy of data augmentation was developed. Numerical experiments of prediction and control of classic nonlinear systems with ablation study showed the effectiveness of the proposed techniques over several state-of-the-art practices.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33d038a0e7775662768898c432284e433922c891" target='_blank'>
              Learning Noise-Robust Stable Koopman Operator for Control with Physics-Informed Observables
              </a>
            </td>
          <td>
            Shahriar Akbar Sakib, Shaowu Pan
          </td>
          <td>2024-08-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Dynamic Mode Decomposition (DMD) and its variants, such as extended DMD (EDMD), are broadly used to fit simple linear models to dynamical systems known from observable data. As DMD methods work well in several situations but perform poorly in others, a clarification of the assumptions under which DMD is applicable is desirable. Upon closer inspection, existing interpretations of DMD methods based on the Koopman operator are not quite satisfactory: they justify DMD under assumptions that hold only with probability zero for generic observables. Here, we give a justification for DMD as a local, leading-order reduced model for the dominant system dynamics under conditions that hold with probability one for generic observables and non-degenerate observational data. We achieve this for autonomous and for periodically forced systems of finite or infinite dimensions by constructing linearizing transformations for their dominant dynamics within attracting slow spectral submanifolds (SSMs). Our arguments also lead to a new algorithm, data-driven linearization (DDL), which is a higher-order, systematic linearization of the observable dynamics within slow SSMs. We show by examples how DDL outperforms DMD and EDMD on numerical and experimental data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ce624ece7e2f2933c1731bee6e5b03bf7a57c90b" target='_blank'>
              Data-Driven Linearization of Dynamical Systems
              </a>
            </td>
          <td>
            George Haller, B. Kasz'as
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cabb9ef9934cc450e8873d693c8dd8dfc73087ab" target='_blank'>
              MBD-NODE: Physics-informed data-driven modeling and simulation of constrained multibody systems
              </a>
            </td>
          <td>
            Jingquan Wang, Shu Wang, H. Unjhawala, Jinlong Wu, D. Negrut
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Data-driven discovery of partial differential equations (PDEs) has emerged as a promising approach for deriving governing physics when domain knowledge about observed data is limited. Despite recent progress, the identification of governing equations and their parametric dependencies using conventional information criteria remains challenging in noisy situations, as the criteria tend to select overly complex PDEs. In this paper, we introduce an extension of the uncertainty-penalized Bayesian information criterion (UBIC), which is adapted to solve parametric PDE discovery problems efficiently without requiring computationally expensive PDE simulations. This extended UBIC uses quantified PDE uncertainty over different temporal or spatial points to prevent overfitting in model selection. The UBIC is computed with data transformation based on power spectral densities to discover the governing parametric PDE that truly captures qualitative features in frequency space with a few significant terms and their parametric dependencies (i.e., the varying PDE coefficients), evaluated with confidence intervals. Numerical experiments on canonical PDEs demonstrate that our extended UBIC can identify the true number of terms and their varying coefficients accurately, even in the presence of noise. The code is available at \url{https://github.com/Pongpisit-Thanasutives/parametric-discovery}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d605721a21ed5e6e69cc14a1ab3eb9a559099ea9" target='_blank'>
              Adaptation of uncertainty-penalized Bayesian information criterion for parametric partial differential equation discovery
              </a>
            </td>
          <td>
            Pongpisit Thanasutives, Ken-ichi Fukui
          </td>
          <td>2024-08-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The identification of self-similarity is an indispensable tool for understanding and modelling physical phenomena. Unfortunately, this is not always possible to perform formally in highly complex problems. We propose a methodology to extract the similarity variables of a self-similar physical process directly from data, without prior knowledge of the governing equations or boundary conditions, based on an optimization problem and symbolic regression. We analyze the accuracy and robustness of our method in four problems which have been influential in fluid mechanics research: a laminar boundary layer, Burger's equation, a turbulent wake, and a collapsing cavity. Our analysis considers datasets acquired via both numerical and wind-tunnel experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b4e59a9e878c505fc222908a8279de60c32a29db" target='_blank'>
              Extracting self-similarity from data
              </a>
            </td>
          <td>
            Nikos Bempedelis, Luca Magri, Konstantinos Steiros
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="While linear systems are well-understood, no explicit solution exists for general nonlinear systems. Thus, it is desirable to make the understanding of linear system available in the nonlinear setting. This motivates the search for linearization techniques which are able to represent the nonlinear system by an equivalent, linear system. While much progress has been made extending various linearization techniques to larger domains and more delicate attractor geometries, the limitations of such techniques for truly nonlinear dynamics, such as dynamics with coexisting attractors, have been pointed out recently. In this work, we show that genuinely nonlinear dynamics can be globally linearized. To this end, we investigate systems with a continuous spectrum, a limit cycle, and coexisting solutions and explicitly construct linear systems mimicking these nonlinear behaviors as close as possible. We approximate the transformations between linear and nonlinear system with deep neural networks. The obtained linearizations are finite dimensional exceeding the phase space dimension of the underlying linear system by one at most.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2196ed95f5d349e07c7af94fd99240eec34dfdb2" target='_blank'>
              Learning Global Linear Representations of Truly Nonlinear Dynamics
              </a>
            </td>
          <td>
            Thomas Breunung, F. Kogelbauer
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The identification of a mathematical dynamics model is a crucial step in the designing process of a controller. However, it is often very difficult to identify the system's governing equations, especially in complex environments that combine physical laws of different disciplines. In this paper, we present a new approach that allows identifying an ordinary differential equation by means of a physics-informed machine learning algorithm. Our method introduces a special neural network that allows exploiting prior human knowledge to a certain degree and extends it autonomously, so that the resulting differential equations describe the system as accurately as possible. We validate the method on a Duffing oscillator with simulation data and, additionally, on a cascaded tank example with real-world data. Subsequently, we use the developed algorithm in a model-based reinforcement learning framework by alternately identifying and controlling a system to a target state. We test the performance by swinging-up an inverted pendulum on a cart.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fb84f2c4de6cf8d43f1207b6c58095273d711f1" target='_blank'>
              Identifying Ordinary Differential Equations for Data-efficient Model-based Reinforcement Learning
              </a>
            </td>
          <td>
            Tobias Nagel, Marco F. Huber
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Machine learning techniques have recently been of great interest for solving differential equations. Training these models is classically a data-fitting task, but knowledge of the expression of the differential equation can be used to supplement the training objective, leading to the development of physics-informed scientific machine learning. In this article, we focus on one class of models called nonlinear vector autoregression (NVAR) to solve ordinary differential equations (ODEs). Motivated by connections to numerical integration and physics-informed neural networks, we explicitly derive the physics-informed NVAR (piNVAR) which enforces the right-hand side of the underlying differential equation regardless of NVAR construction. Because NVAR and piNVAR completely share their learned parameters, we propose an augmented procedure to jointly train the two models. Then, using both data-driven and ODE-driven metrics, we evaluate the ability of the piNVAR model to predict solutions to various ODE systems, such as the undamped spring, a Lotka-Volterra predator-prey nonlinear model, and the chaotic Lorenz system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59096640a40ed6b70acbbfdaca5a505cd1330bb4" target='_blank'>
              Physics-informed nonlinear vector autoregressive models for the prediction of dynamical systems
              </a>
            </td>
          <td>
            James H. Adler, Samuel Hocking, Xiaozhe Hu, Shafiqul Islam
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d487b041502645ead57bcc0782fa0256ccec2fc" target='_blank'>
              Development of data-driven modeling method for nonlinear coupling components
              </a>
            </td>
          <td>
            Taesan Ryu, Seunghun Baek
          </td>
          <td>2024-06-27</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Structural dynamics models with nonlinear stiffness appear, for example, when analyzing systems with nonlinear material behavior or undergoing large deformations. For complex systems, these models become too large for real-time applications or multi-query workflows. Hence, model reduction is needed. However, the mathematical operators of these models are often not available since, as is common in industry practice, the models are constructed using commercial simulation software. In this work, we propose an operator inference-based approach aimed at inferring, from data generated by the simulation model, reduced-order models (ROMs) of structural dynamics systems with stiffness terms represented by polynomials of arbitrary degree. To ensure physically meaningful models, we impose constraints on the inference such that the model is guaranteed to exhibit stability properties. Convexity of the optimization problem associated with the inference is maintained by applying a sum-of-squares relaxation to the polynomial term. To further reduce the size of the ROM and improve numerical conditioning of the inference, we also propose a novel clustering-based sparsification of the polynomial term. We validate the proposed method on several numerical examples, including a representative 3D Finite Element Model (FEM) of a steel piston rod.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/12474e97123f0f45095cbb219da196a32731873d" target='_blank'>
              Stable Sparse Operator Inference for Nonlinear Structural Dynamics
              </a>
            </td>
          <td>
            P. D. Boef, Diana Manvelyan, Jos Maubach, W. Schilders, N. Wouw
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="Physics-Informed Neural Networks (PINNs) have revolutionized solving differential equations by integrating physical laws into neural network training. This paper explores PINNs for open-loop optimal control problems (OCPs) with incomplete information, such as unknown initial conditions and sparse boundary data. We derive optimality conditions from the Lagrangian multipliers and use a neural network to predict the state, adjoint, and control variables. In contrast with previous methods, our approach integrates these elements into a single neural network and addresses scenarios with consistently limited data. Specifically, we address the study of partially unknown equations identifying underlying parameters online by searching for the optimal solution. Numerical examples show the effectiveness of the proposed method even in scenarios characterized by a considerable lack of information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1de965a4fc10f47113e0b8b25e5ae6627f2c9cbb" target='_blank'>
              A PINN approach for the online identification and control of unknown PDEs
              </a>
            </td>
          <td>
            Alessandro Alla, Giulia Bertaglia, Elisa Calzola
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We present a pragmatic approach to the sparse identification of nonlinear dynamics for systems with discrete delays. It relies on approximating the underlying delay model with a system of ordinary differential equations via pseudospectral collocation. To minimize the reconstruction error, the new strategy avoids optimizing all possible multiple unknown delays, identifying only the maximum one. The computational burden is thus greatly reduced, improving the performance of recent implementations that work directly on the delay system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/af6a7301b4680a556da5635cdd6aedbb64f047b0" target='_blank'>
              Sparse identification of time delay systems via pseudospectral collocation
              </a>
            </td>
          <td>
            Enrico Bozzo, Dimitri Breda, Muhammad Tanveer
          </td>
          <td>2024-08-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In order to accurately model the dynamics of non-linear electro-mechanical systems, it is imperative to consider the contributions of coupling terms and dissipation. The Lagrangian formulation alone is insufficient to fully capture the holistic behavior of the system. Coupling and dissipation mechanisms play a pivotal role in shaping the system's response. Consequently, to effectively capture the dynamics of inter-coupled electro-mechanical systems with dissipation, we propose an extended Lagrangian-informed deep neural network framework in this paper. Our approach leverages the underlying physics-based knowledge of the system, incorporating it into the neural network architecture. By employing the Euler-Lagrange equations as constraints in the training process, we ensure that the learned dynamics conform to the true behavior of the system. To validate the theoretical framework, we conduct simulation experiments on a DC motor with a cart system, which serves as a representative model of dissipative nonlinear electro-mechanical systems. The experimental results demonstrate the efficacy of our approach in accurately capturing and integrating the dynamics to solve the reference tracking model predictive control design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fec77456fc41a426a57f697c4e29a12b83d9962" target='_blank'>
              Extended Lagrangian-Informed Deep Learning and Control for Electro-mechanical Systems
              </a>
            </td>
          <td>
            Nikhil Pagar, Pegah Ghaf-Ghanbari, Atul Kelkar, Javad Mohammadpour
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Conventional physics-based modeling techniques involve high effort, e.g., time and expert knowledge, while data-driven methods often lack interpretability, structure, and sometimes reliability. To mitigate this, we present a data-driven system identification framework that derives models in the port-Hamiltonian (pH) formulation. This formulation is suitable for multi-physical systems while guaranteeing the useful system theoretical properties of passivity and stability. Our framework combines linear and nonlinear reduction with structured, physics-motivated system identification. In this process, high-dimensional state data obtained from possibly nonlinear systems serves as input for an autoencoder, which then performs two tasks: (i) nonlinearly transforming and (ii) reducing this data onto a low-dimensional latent space. In this space, a linear pH system, that satisfies the pH properties per construction, is parameterized by the weights of a neural network. The mathematical requirements are met by defining the pH matrices through Cholesky factorizations. The neural networks that define the coordinate transformation and the pH system are identified in a joint optimization process to match the dynamics observed in the data while defining a linear pH system in the latent space. The learned, low-dimensional pH system can describe even nonlinear systems and is rapidly computable due to its small size. The method is exemplified by a parametric mass-spring-damper and a nonlinear pendulum example, as well as the high-dimensional model of a disc brake with linear thermoelastic behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c88f9ec4b5bd522ac8e312f90d2801489785951" target='_blank'>
              Data-driven identification of latent port-Hamiltonian systems
              </a>
            </td>
          <td>
            J. Rettberg, Jonas Kneifl, Julius Herb, Patrick Buchfink, J. Fehr, B. Haasdonk
          </td>
          <td>2024-08-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Closure models are widely used in simulating complex multiscale dynamical systems such as turbulence and the earth system, for which direct numerical simulation that resolves all scales is often too expensive. For those systems without a clear scale separation, deterministic and local closure models often lack enough generalization capability, which limits their performance in many real-world applications. In this work, we propose a data-driven modeling framework for constructing stochastic and non-local closure models via conditional diffusion model and neural operator. Specifically, the Fourier neural operator is incorporated into a score-based diffusion model, which serves as a data-driven stochastic closure model for complex dynamical systems governed by partial differential equations (PDEs). We also demonstrate how accelerated sampling methods can improve the efficiency of the data-driven stochastic closure model. The results show that the proposed methodology provides a systematic approach via generative machine learning techniques to construct data-driven stochastic closure models for multiscale dynamical systems with continuous spatiotemporal fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5b117384f237e71d202a61374a42d92e97b0c697" target='_blank'>
              Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator
              </a>
            </td>
          <td>
            Xinghao Dong, Chuanqi Chen, Jin-Long Wu
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Forecasting dynamical systems is of importance to numerous real-world applications. When possible, dynamical systems forecasts are constructed based on first-principles-based models such as through the use of differential equations. When these equations are unknown, non-intrusive techniques must be utilized to build predictive models from data alone. Machine learning (ML) methods have recently been used for such tasks. Moreover, ML methods provide the added advantage of significant reductions in time-to-solution for predictions in contrast with first-principle based models. However, many state-of-the-art ML-based methods for forecasting rely on neural networks, which may be expensive to train and necessitate requirements for large amounts of memory. In this work, we propose a quantum mechanics inspired ML modeling strategy for learning nonlinear dynamical systems that provides data-driven forecasts for complex dynamical systems with reduced training time and memory costs. This approach, denoted the quantum reservoir computing technique (QRC), is a hybrid quantum-classical framework employing an ensemble of interconnected small quantum systems via classical linear feedback connections. By mapping the dynamical state to a suitable quantum representation amenable to unitary operations, QRC is able to predict complex nonlinear dynamical systems in a stable and accurate manner. We demonstrate the efficacy of this framework through benchmark forecasts of the NOAA Optimal Interpolation Sea Surface Temperature dataset and compare the performance of QRC to other ML methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff3c3b75e08bf25e53956e10e92ec03e5cda30f6" target='_blank'>
              Higher order quantum reservoir computing for non-intrusive reduced-order models
              </a>
            </td>
          <td>
            Vinamr Jain, R. Maulik
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d69703a20ce4710f71900130dd935761bbf01fa" target='_blank'>
              Promising directions of machine learning for partial differential equations
              </a>
            </td>
          <td>
            Steve Brunton, J. Kutz
          </td>
          <td>2024-06-28</td>
          <td>Nature computational science</td>
          <td>3</td>
          <td>2</td>
        </tr>

        <tr id="Neural simulators for modeling complex dynamical systems have been extensively studied for various real-world applications, such as weather forecasting, ocean current prediction, and computational fluid dynamics simulation. Although they have demonstrated powerful fitting and predicting, most existing models are only built to learn single-system dynamics. Several advanced researches have considered learning dynamics across environments, which can exploit the potential commonalities among the dynamics across environments and adapt to new environments. However, these methods still are prone to scarcity problems where per-environment data is sparse or limited. Therefore, we propose a novel CoNDP (Context-Informed Neural ODE Processes) to achieve learning system dynamics from sparse observations across environments. It can fully use contextual information of each environment to better capture the intrinsic commonalities across environments and distinguishable differences among environments while modeling uncertainty of system evolution, producing more accurate predictions. Intensive experiments are conducted on five complex dynamical systems in various fields. Results show that the proposed CoNDP can achieve optimal results compared with common neural simulators and state-of-the-art cross-environmental models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/40337006a651c2110229b8ae928a5d5fe9a01da1" target='_blank'>
              Stochastic Neural Simulator for Generalizing Dynamical Systems across Environments
              </a>
            </td>
          <td>
            Liu Jiaqi, Jiaxu Cui, Jiayi Yang, Bo Yang
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Simulating spatiotemporal turbulence with high fidelity remains a cornerstone challenge in computational fluid dynamics (CFD) due to its intricate multiscale nature and prohibitive computational demands. Traditional approaches typically employ closure models, which attempt to represent small-scale features in an unresolved manner. However, these methods often sacrifice accuracy and lose high-frequency/wavenumber information, especially in scenarios involving complex flow physics. In this paper, we introduce an innovative neural differentiable modeling framework designed to enhance the predictability and efficiency of spatiotemporal turbulence simulations. Our approach features differentiable hybrid modeling techniques that seamlessly integrate deep neural networks with numerical PDE solvers within a differentiable programming framework, synergizing deep learning with physics-based CFD modeling. Specifically, a hybrid differentiable neural solver is constructed on a coarser grid to capture large-scale turbulent phenomena, followed by the application of a Bayesian conditional diffusion model that generates small-scale turbulence conditioned on large-scale flow predictions. Two innovative hybrid architecture designs are studied, and their performance is evaluated through comparative analysis against conventional large eddy simulation techniques with physics-based subgrid-scale closures and purely data-driven neural solvers. The findings underscore the potential of the neural differentiable modeling framework to significantly enhance the accuracy and computational efficiency of turbulence simulations. This study not only demonstrates the efficacy of merging deep learning with physics-based numerical solvers but also sets a new precedent for advanced CFD modeling techniques, highlighting the transformative impact of differentiable programming in scientific computing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/28636be802bd2c5ddbd07e3772c8b3a7822e64fa" target='_blank'>
              Neural Differentiable Modeling with Diffusion-Based Super-resolution for Two-Dimensional Spatiotemporal Turbulence
              </a>
            </td>
          <td>
            Xiantao Fan, Deepak Akhare, Jian-Xun Wang
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience. To this end, statistical methods which describe high-dimensional neural time series in terms of low-dimensional latent dynamics have played a fundamental role in characterizing neural systems. Yet, what constitutes a successful method involves two opposing criteria: (1) methods should be expressive enough to capture complex nonlinear dynamics, and (2) they should maintain a notion of interpretability often only warranted by simpler linear models. In this paper, we develop an approach that balances these two objectives: the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method builds on previous work modeling the latent state evolution via a stochastic differential equation whose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We propose a novel kernel function which enforces smoothly interpolated locally linear dynamics, and therefore expresses flexible -- yet interpretable -- dynamics akin to those of recurrent switching linear dynamical systems (rSLDS). Our approach resolves key limitations of the rSLDS such as artifactual oscillations in dynamics near discrete state boundaries, while also providing posterior uncertainty estimates of the dynamics. To fit our models, we leverage a modified learning objective which improves the estimation accuracy of kernel hyperparameters compared to previous GP-SDE fitting approaches. We apply our method to synthetic data and data recorded in two neuroscience experiments and demonstrate favorable performance in comparison to the rSLDS.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b38dffa6030cac42cce7564c8fe38875646c5f4f" target='_blank'>
              Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems
              </a>
            </td>
          <td>
            Amber Hu, D. Zoltowski, Aditya Nair, David Anderson, Lea Duncker, Scott W. Linderman
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Accurately predicting the long-term behavior of chaotic systems is crucial for various applications such as climate modeling. However, achieving such predictions typically requires iterative computations over a dense spatiotemporal grid to account for the unstable nature of chaotic systems, which is expensive and impractical in many real-world situations. An alternative approach to such a full-resolved simulation is using a coarse grid and then correcting its errors through a \textit{closure model}, which approximates the overall information from fine scales not captured in the coarse-grid simulation. Recently, ML approaches have been used for closure modeling, but they typically require a large number of training samples from expensive fully-resolved simulations (FRS). In this work, we prove an even more fundamental limitation, i.e., the standard approach to learning closure models suffers from a large approximation error for generic problems, no matter how large the model is, and it stems from the non-uniqueness of the mapping. We propose an alternative end-to-end learning approach using a physics-informed neural operator (PINO) that overcomes this limitation by not using a closure model or a coarse-grid solver. We first train the PINO model on data from a coarse-grid solver and then fine-tune it with (a small amount of) FRS and physics-based losses on a fine grid. The discretization-free nature of neural operators means that they do not suffer from the restriction of a coarse grid that closure models face, and they can provably approximate the long-term statistics of chaotic systems. In our experiments, our PINO model achieves a 120x speedup compared to FRS with a relative error $\sim 5\%$. In contrast, the closure model coupled with a coarse-grid solver is $58$x slower than PINO while having a much higher error $\sim205\%$ when the closure model is trained on the same FRS dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba56bfb99a735364b70e9f185e5b2a87b7ff7d3f" target='_blank'>
              Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators
              </a>
            </td>
          <td>
            Chuwei Wang, Julius Berner, Zong-Yi Li, Di Zhou, Jiayun Wang, Jane Bae, A. Anandkumar
          </td>
          <td>2024-08-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="In this paper, we propose a physics-informed learning-based Koopman modeling approach and present a Koopman-based self-tuning moving horizon estimation design for a class of nonlinear systems. Specifically, we train Koopman operators and two neural networks - the state lifting network and the noise characterization network - using both data and available physical information. The two neural networks account for the nonlinear lifting functions for Koopman modeling and describing system noise distributions, respectively. Accordingly, a stochastic linear Koopman model is established in the lifted space to forecast the dynamic behavior of the nonlinear system. Based on the Koopman model, a self-tuning linear moving horizon estimation (MHE) scheme is developed. The weighting matrices of the MHE design are updated using the pre-trained noise characterization network at each sampling instant. The proposed estimation scheme is computationally efficient because only convex optimization is involved during online implementation, and updating the weighting matrices of the MHE scheme does not require re-training the neural networks. We verify the effectiveness and evaluate the performance of the proposed method via the application to a simulated chemical process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7f4e3675450b08e1344396a30181355d5565efe8" target='_blank'>
              Self-tuning moving horizon estimation of nonlinear systems via physics-informed machine learning Koopman modeling
              </a>
            </td>
          <td>
            Mingxue Yan, Minghao Han, A. Law, Xunyuan Yin
          </td>
          <td>2024-08-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="Reconstructing a nonlinear dynamical system from empirical time series is a fundamental task in data-driven analysis. One of the main challenges is the existence of hidden variables; we only have records for some variables, and those for hidden variables are unavailable. In this work, the techniques for Carleman linearization, phase-space embedding, and dynamic mode decomposition are integrated to rebuild an optimal dynamical system from time series for one specific variable. Using the Takens theorem, the embedding dimension is determined, which is adopted as the dynamical system's dimension. The Carleman linearization is then used to transform this finite nonlinear system into an infinite linear system, which is further truncated into a finite linear system using the dynamic mode decomposition technique. We illustrate the performance of this integrated technique using data generated by the well-known Lorenz model, the Duffing oscillator, and empirical records of electrocardiogram, electroencephalogram, and measles outbreaks. The results show that this solution accurately estimates the operators of the nonlinear dynamical systems. This work provides a new data-driven method to estimate the Carleman operator of nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dfecb35e8dbf1de60a32397c57f1dd761dcbe4ee" target='_blank'>
              Estimation of Carleman operator from a univariate time series.
              </a>
            </td>
          <td>
            Sherehe Semba, Huijie Yang, Xiaolu Chen, Huiyun Wan, C. Gu
          </td>
          <td>2024-08-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Neural manifolds are an attractive theoretical framework for characterizing the complex behaviors of neural populations. However, many of the tools for identifying these low-dimensional subspaces are correlational and provide limited insight into the underlying dynamics. The ability to precisely control this latent activity would allow researchers to investigate the structure and function of neural manifolds. Employing techniques from the field of optimal control, we simulate controlling the latent dynamics of a neural population using closed-loop, dynamically generated sensory inputs. Using a spiking neural network (SNN) as a model of a neural circuit, we find low-dimensional representations of both the network activity (the neural manifold) and a set of salient visual stimuli. With a data-driven latent dynamics model, we apply model predictive control (MPC) to provide anticipatory, optimal control over the trajectory of the circuit in a latent space. We are able to control the latent dynamics of the SNN to follow several reference trajectories despite observing only a subset of neurons and with a substantial amount of unknown noise injected into the network. These results provide a framework to experimentally test for causal relationships between manifold dynamics and other variables of interest such as organismal behavior and BCI performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2f3319a9926e88582124c34d6157bfd48e1d637" target='_blank'>
              Model Predictive Control of the Neural Manifold
              </a>
            </td>
          <td>
            Christof Fehrman, C. D. Meliza
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>6</td>
        </tr>

        <tr id="A physics-informed neural network (PINN) is used to produce a variety of self-trapped necklace solutions of the (2+1)-dimensional nonlinear Schr\"{o}dinger/Gross-Pitaevskii equation. We elaborate the analysis for the existence and evolution of necklace patterns with integer, half-integer, and fractional reduced orbital angular momenta by means of PINN. The patterns exhibit phenomena similar to rotation of rigid bodies and centrifugal force. Even though the necklaces slowly expand (or shrink), they preserve their structure in the course of the quasi-stable propagation over several diffraction lengths, which is completely different from the ordinary fast diffraction-dominated dynamics. By comparing different ingredients, including the training time, loss value and $\mathbb{L}_{2}$ error, PINN accurately predicts specific nonlinear dynamical properties of the evolving necklace patterns. Furthermore, we perform the data-driven discovery of parameters for both clean and perturbed training data, adding $1\%$ random noise in the latter case. The results reveal that PINN not only effectively emulates the solution of partial differential equations, but also offers applications for predicting the nonlinear dynamics of physically relevant types of patterns.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/27df54bec46e632334278c89b601027aa8a3cf9e" target='_blank'>
              Physics-informed neural network for nonlinear dynamics of self-trapped necklace beams
              </a>
            </td>
          <td>
            Dongshuai Liu, Wen Zhang, Yanxia Gao, Dianyuan Fan, B. Malomed, Lifu Zhang
          </td>
          <td>2024-08-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>79</td>
        </tr>

        <tr id="Lifted linear predictor (LLP) is an artificial linear dynamical system designed to predict trajectories of a generally nonlinear dynamical system based on the current state (or measurements) and the input. The main benefit of the LLP is its potential ability to capture the nonlinear system's dynamics with precision superior to other linearization techniques, such as local linearization about the operation point. The idea of lifting is supported by the theory of Koopman Operators. For LLP identification, we focus on the data-driven method based on the extended dynamic mode decomposition (EDMD) algorithm. However, while the EDMD algorithm presents an extremely simple and efficient way to obtain the LLP, it can also yield poor results. In this paper, we present some less intuitive practical guidelines for data-driven identification of the LLPs, aiming at improving usability of LLPs for designing control. We support the guidelines with two motivating examples. The implementation of the examples are shared on a public repository.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf1ce9a8c8346f1264e7cecb5aea44d98aa65af7" target='_blank'>
              Practical Guidelines for Data-driven Identification of Lifted Linear Predictors for Control
              </a>
            </td>
          <td>
            Loi Do, Adam Uchytil, Zdenvek Hur'ak
          </td>
          <td>2024-08-02</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Modeling the non-linear dynamics of a system from measurement data accurately is an open challenge. Over the past few years, various tools such as SINDy and DySMHO have emerged as approaches to distill dynamics from data. However, challenges persist in accurately capturing dynamics of a system especially when the physical knowledge about the system is unknown. A promising solution is to use a hybrid paradigm, that combines mechanistic and black-box models to leverage their respective strengths. In this study, we combine a hybrid modeling paradigm with sparse regression, to develop and identify models simultaneously. Two methods are explored, considering varying complexities, data quality, and availability and by comparing different case studies. In the first approach, we integrate SINDy-discovered models with neural ODE structures, to model unknown physics. In the second approach, we employ Multifidelity Surrogate Models (MFSMs) to construct composite models comprised of SINDy-discovered models and error-correction models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4848ba60b88cc3dffed5030c77e9174131c7f918" target='_blank'>
              Integrating Hybrid Modeling and Multifidelity Approaches for Data-Driven Process Model Discovery
              </a>
            </td>
          <td>
            Suryateja Ravutla, Fani Boukouvala
          </td>
          <td>2024-07-09</td>
          <td>Systems and Control Transactions</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Neural Ordinary Differential Equations (ODEs) represent a significant advancement at the intersection of machine learning and dynamical systems, offering a continuous-time analog to discrete neural networks. Despite their promise, deploying neural ODEs in practical applications often encounters the challenge of stiffness, a condition where rapid variations in some components of the solution demand prohibitively small time steps for explicit solvers. This work addresses the stiffness issue when employing neural ODEs for model order reduction by introducing a suitable reparametrization in time. The considered map is data-driven and it is induced by the adaptive time-stepping of an implicit solver on a reference solution. We show the map produces a nonstiff system that can be cheaply solved with an explicit time integration scheme. The original, stiff, time dynamic is recovered by means of a map learnt by a neural network that connects the state space to the time reparametrization. We validate our method through extensive experiments, demonstrating improvements in efficiency for the neural ODE inference while maintaining robustness and accuracy. The neural network model also showcases good generalization properties for times beyond the training data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fc642cd2dc03d50f310399a91145bd5a14a7f646" target='_blank'>
              Neural Ordinary Differential Equations for Model Order Reduction of Stiff Systems
              </a>
            </td>
          <td>
            Matteo Caldana, J. Hesthaven
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>63</td>
        </tr>

        <tr id="We present a numerical method for learning unknown nonautonomous stochastic dynamical system, i.e., stochastic system subject to time dependent excitation or control signals. Our basic assumption is that the governing equations for the stochastic system are unavailable. However, short bursts of input/output (I/O) data consisting of certain known excitation signals and their corresponding system responses are available. When a sufficient amount of such I/O data are available, our method is capable of learning the unknown dynamics and producing an accurate predictive model for the stochastic responses of the system subject to arbitrary excitation signals not in the training data. Our method has two key components: (1) a local approximation of the training I/O data to transfer the learning into a parameterized form; and (2) a generative model to approximate the underlying unknown stochastic flow map in distribution. After presenting the method in detail, we present a comprehensive set of numerical examples to demonstrate the performance of the proposed method, especially for long-term system predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4edbecee7f2d1381939691be911b0026cab616bb" target='_blank'>
              Modeling Unknown Stochastic Dynamical System Subject to External Excitation
              </a>
            </td>
          <td>
            Yuan Chen, Dongbin Xiu
          </td>
          <td>2024-06-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Initial value problems -- a system of ordinary differential equations and corresponding initial conditions -- can be used to describe many physical phenomena including those arise in classical mechanics. We have developed a novel approach to solve physics-based initial value problems using unsupervised machine learning. We propose a deep learning framework that models the dynamics of a variety of mechanical systems through neural networks. Our framework is flexible, allowing us to solve non-linear, coupled, and chaotic dynamical systems. We demonstrate the effectiveness of our approach on systems including a free particle, a particle in a gravitational field, a classical pendulum, and the H\'enon--Heiles system (a pair of coupled harmonic oscillators with a non-linear perturbation, used in celestial mechanics). Our results show that deep neural networks can successfully approximate solutions to these problems, producing trajectories which conserve physical properties such as energy and those with stationary action. We note that probabilistic activation functions, as defined in this paper, are required to learn any solutions of initial value problems in their strictest sense, and we introduce coupled neural networks to learn solutions of coupled systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71b2f475e6dd93def3f335fbbfac10259c5dba45" target='_blank'>
              Solving physics-based initial value problems with unsupervised machine learning
              </a>
            </td>
          <td>
            Jack Griffiths, S. A. Wrathmall, Simon A. Gardiner
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4648ca55b2c3ba80a310409f8b48533ab6d4a66a" target='_blank'>
              Learning interpretable dynamics of stochastic complex systems from experimental data
              </a>
            </td>
          <td>
            Tingting Gao, B. Barzel, Gang Yan
          </td>
          <td>2024-07-17</td>
          <td>Nature Communications</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="
 We introduce a novel neural network structure called Strongly Constrained Theory-Guided Neural Network (SCTgNN), to investigate the behaviours of the localized solutions of the generalized nonlinear Schrödinger (NLS) equation. This equation comprises four physically significant nonlinear evolution equations, namely, (i) NLS equation, Hirota equation Lakshmanan-Porsezian-Daniel (LPD) equation and fifth-order NLS equation. The generalized NLS equation demonstrates nonlinear effects up to quintic order, indicating rich and complex dynamics in various fields of physics. By combining concepts from the Physics-Informed Neural Network (PINN) and Theory-Guided Neural Network (TgNN) models, SCTgNN aims to enhance our understanding of complex phenomena, particularly within nonlinear systems that defy conventional patterns. To begin, we employ the TgNN method to predict the behaviours of localized waves, including solitons, rogue waves, and breathers, within the generalized NLS equation. We then use SCTgNN to predict the aforementioned localized solutions and calculate the mean square errors in both SCTgNN and TgNN in predicting these three localized solutions. Our findings reveal that both models excel in understanding complex behaviours and provide predictions across a wide variety of situations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e405cc588c01c3af8b8d5bde5f1526dd7d964ae3" target='_blank'>
              On examining the predictive capabilities of two variants of PINN in validating localised wave solutions in the generalized nonlinear Schrödinger equation
              </a>
            </td>
          <td>
            Thulasidharan K, Sinthuja N, Vishnu Priya N, Senthilvelan M
          </td>
          <td>2024-07-10</td>
          <td>Communications in Theoretical Physics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We introduce a method based on Gaussian process regression to identify discrete variational principles from observed solutions of a field theory. The method is based on the data-based identification of a discrete Lagrangian density. It is a geometric machine learning technique in the sense that the variational structure of the true field theory is reflected in the data-driven model by design. We provide a rigorous convergence statement of the method. The proof circumvents challenges posed by the ambiguity of discrete Lagrangian densities in the inverse problem of variational calculus. Moreover, our method can be used to quantify model uncertainty in the equations of motions and any linear observable of the discrete field theory. This is illustrated on the example of the discrete wave equation and Schr\"odinger equation. The article constitutes an extension of our previous article arXiv:2404.19626 for the data-driven identification of (discrete) Lagrangians for variational dynamics from an ode setting to the setting of discrete pdes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e38b7de6c7022b28628627bbf045123b665619b4" target='_blank'>
              Machine learning of discrete field theories with guaranteed convergence and uncertainty quantification
              </a>
            </td>
          <td>
            Christian Offen
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Machine learning presents opportunities to improve the scale-specific accuracy of mechanistic models in a data-driven manner. Here we demonstrate the use of a machine learning technique called Sparse Identification of Nonlinear Dynamics (SINDy) to improve a simple mechanistic model of algal growth. Time-series measurements of the microalga Chlorella Vulgaris were generated under controlled photobioreactor conditions at the University of Technology Sydney. A simple mechanistic growth model based on intensity of light and temperature was integrated over time and compared to the time-series data. While the mechanistic model broadly captured the overall growth trend, discrepancies remained between the model and data due to the model's simplicity and non-ideal behavior of real-world measurement. SINDy was applied to model the residual error by identifying an error derivative correction term. Addition of this SINDy-informed error dynamics term shows improvement to model accuracy while maintaining interpretability of the underlying mechanistic framework. This work demonstrates the potential for machine learning techniques like SINDy to aid simple mechanistic models in scale-specific predictive accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0efd19b71be3ac13e47f098d9a6e63082477a540" target='_blank'>
              Improving Mechanistic Model Accuracy with Machine Learning Informed Physics
              </a>
            </td>
          <td>
            Will Farlessyost, Shweta Singh
          </td>
          <td>2024-07-09</td>
          <td>Systems and Control Transactions</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data assimilation is a central problem in many geophysical applications, such as weather forecasting. It aims to estimate the state of a potentially large system, such as the atmosphere, from sparse observations, supplemented by prior physical knowledge. The size of the systems involved and the complexity of the underlying physical equations make it a challenging task from a computational point of view. Neural networks represent a promising method of emulating the physics at low cost, and therefore have the potential to considerably improve and accelerate data assimilation. In this work, we introduce a deep learning approach where the physical system is modeled as a sequence of coarse-to-fine Gaussian prior distributions parametrized by a neural network. This allows us to define an assimilation operator, which is trained in an end-to-end fashion to minimize the reconstruction error on a dataset with different observation processes. We illustrate our approach on chaotic dynamical physical systems with sparse observations, and compare it to traditional variational data assimilation methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2950eebe3fe16f882003d5d9945c97a8517a5f89" target='_blank'>
              Neural Incremental Data Assimilation
              </a>
            </td>
          <td>
            Matthieu Blanke, R. Fablet, Marc Lelarge
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Non-stationary systems are found throughout the world, from climate patterns under the influence of variation in carbon dioxide concentration, to brain dynamics driven by ascending neuromodulation. Accordingly, there is a need for methods to analyze non-stationary processes, and yet most time-series analysis methods that are used in practice, on important problems across science and industry, make the simplifying assumption of stationarity. One important problem in the analysis of non-stationary systems is the problem class that we refer to as Parameter Inference from a Non-stationary Unknown Process (PINUP). Given an observed time series, this involves inferring the parameters that drive non-stationarity of the time series, without requiring knowledge or inference of a mathematical model of the underlying system. Here we review and unify a diverse literature of algorithms for PINUP. We formulate the problem, and categorize the various algorithmic contributions. This synthesis will allow researchers to identify gaps in the literature and will enable systematic comparisons of different methods. We also demonstrate that the most common systems that existing methods are tested on - notably the non-stationary Lorenz process and logistic map - are surprisingly easy to perform well on using simple statistical features like windowed mean and variance, undermining the practice of using good performance on these systems as evidence of algorithmic performance. We then identify more challenging problems that many existing methods perform poorly on and which can be used to drive methodological advances in the field. Our results unify disjoint scientific contributions to analyzing non-stationary systems and suggest new directions for progress on the PINUP problem and the broader study of non-stationary phenomena.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/585c3c4f3b30a814bc05e6f9e38a5a28b702ee79" target='_blank'>
              Parameter inference from a non-stationary unknown process
              </a>
            </td>
          <td>
            Kieran S. Owens, Ben D. Fulcher
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this study, we investigate the effect of reservoir computing training data on the reconstruction of chaotic dynamics. Our findings indicate that a training time series comprising a few periodic orbits of low periods can successfully reconstruct the Lorenz attractor. We also demonstrate that biased training data does not negatively impact reconstruction success. Our method's ability to reconstruct a physical measure is much better than the so-called cycle expansion approach, which relies on weighted averaging. Additionally, we demonstrate that fixed point attractors and chaotic transients can be accurately reconstructed by a model trained from a few periodic orbits, even when using different parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f833e966a95961819f625f1095e380b4cf40eb4f" target='_blank'>
              Data-driven modeling from biased small training data using periodic orbits
              </a>
            </td>
          <td>
            Kengo Nakai, Yoshitaka Saiki
          </td>
          <td>2024-07-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Stability is a basic requirement when studying the behavior of dynamical systems. However, stabilizing dynamical systems via reinforcement learning is challenging because only little data can be collected over short time horizons before instabilities are triggered and data become meaningless. This work introduces a reinforcement learning approach that is formulated over latent manifolds of unstable dynamics so that stabilizing policies can be trained from few data samples. The unstable manifolds are minimal in the sense that they contain the lowest dimensional dynamics that are necessary for learning policies that guarantee stabilization. This is in stark contrast to generic latent manifolds that aim to approximate all -- stable and unstable -- system dynamics and thus are higher dimensional and often require higher amounts of data. Experiments demonstrate that the proposed approach stabilizes even complex physical systems from few data samples for which other methods that operate either directly in the system state space or on generic latent manifolds fail.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89660d65895827cad089c32e3f1a9f0b892601ff" target='_blank'>
              System stabilization with policy optimization on unstable latent manifolds
              </a>
            </td>
          <td>
            Steffen W. R. Werner, B. Peherstorfer
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Learning linear and nonlinear dynamical systems from available data is a timely topic in scientific machine learning. Learning must be performed while enforcing the numerical stability of the learned model, the existing knowledge within an informed or augmented setting, or by taking into account the multiscale dynamics—for both linear and nonlinear dynamics. However, when the final objective of such a learned dynamical system is to be used for control purposes, learning transformed dynamics can be advantageous. Therefore, many alternatives exists, and the present paper focuses on two of them: the first based on the discovery and use of the so-called flat control and the second one based on the use of the Koopman theory. The main contributions when addressing the first is the discovery of the flat output transformation by using an original neural framework. Moreover, when using the Koopman theory, this paper proposes an original procedure for learning parametric dynamics in the latent space, which is of particular interest in control-based engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bde1af4107f9ee8f6740e26e9c5d03b288c163cb" target='_blank'>
              Learning Transformed Dynamics for Efficient Control Purposes
              </a>
            </td>
          <td>
            C. Ghnatios, Joel Mouterde, Jerome Tomezyk, Joaquim Da Silva, F. Chinesta
          </td>
          <td>2024-07-19</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Dynamical systems provide a comprehensive way to study complex and changing behaviors across various sciences. Many modern systems are too complicated to analyze directly or we do not have access to models, driving significant interest in learning methods. Koopman operators have emerged as a dominant approach because they allow the study of nonlinear dynamics using linear techniques by solving an infinite-dimensional spectral problem. However, current algorithms face challenges such as lack of convergence, hindering practical progress. This paper addresses a fundamental open question: \textit{When can we robustly learn the spectral properties of Koopman operators from trajectory data of dynamical systems, and when can we not?} Understanding these boundaries is crucial for analysis, applications, and designing algorithms. We establish a foundational approach that combines computational analysis and ergodic theory, revealing the first fundamental barriers -- universal for any algorithm -- associated with system geometry and complexity, regardless of data quality and quantity. For instance, we demonstrate well-behaved smooth dynamical systems on tori where non-trivial eigenfunctions of the Koopman operator cannot be determined by any sequence of (even randomized) algorithms, even with unlimited training data. Additionally, we identify when learning is possible and introduce optimal algorithms with verification that overcome issues in standard methods. These results pave the way for a sharp classification theory of data-driven dynamical systems based on how many limits are needed to solve a problem. These limits characterize all previous methods, presenting a unified view. Our framework systematically determines when and how Koopman spectral properties can be learned.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb7265ab4b5b6dce43f8b2a37e99d9fd03f2bef7" target='_blank'>
              Limits and Powers of Koopman Learning
              </a>
            </td>
          <td>
            Matthew J. Colbrook, Igor Mezi'c, Alexei Stepanenko
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>16</td>
        </tr>

        <tr id="The manipulation of deformable linear objects (DLOs) via model-based control requires an accurate and computationally efficient dynamics model. Yet, data-driven DLO dynamics models require large training data sets while their predictions often do not generalize, whereas physics-based models rely on good approximations of physical phenomena and often lack accuracy. To address these challenges, we propose a physics-informed neural ODE capable of predicting agile movements with significantly less data and hyper-parameter tuning. In particular, we model DLOs as serial chains of rigid bodies interconnected by passive elastic joints in which interaction forces are predicted by neural networks. The proposed model accurately predicts the motion of an robotically-actuated aluminium rod and an elastic foam cylinder after being trained on only thirty seconds of data. The project code and data are available at: \url{https://tinyurl.com/neuralprba}">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/815ff430d40660092c67ed599422c4c980a0db8d" target='_blank'>
              Learning deformable linear object dynamics from a single trajectory
              </a>
            </td>
          <td>
            Shamil Mamedov, A. R. Geist, Ruan Viljoen, Sebastian Trimpe, Jan Swevers
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Physics-informed neural networks have proven to be a powerful tool for solving differential equations, leveraging the principles of physics to inform the learning process. However, traditional deep neural networks often face challenges in achieving high accuracy without incurring significant computational costs. In this work, we implement the Physics-Informed Kolmogorov-Arnold Neural Networks (PIKAN) through efficient-KAN and WAV-KAN, which utilize the Kolmogorov-Arnold representation theorem. PIKAN demonstrates superior performance compared to conventional deep neural networks, achieving the same level of accuracy with fewer layers and reduced computational overhead. We explore both B-spline and wavelet-based implementations of PIKAN and benchmark their performance across various ordinary and partial differential equations using unsupervised (data-free) and supervised (data-driven) techniques. For certain differential equations, the data-free approach suffices to find accurate solutions, while in more complex scenarios, the data-driven method enhances the PIKAN's ability to converge to the correct solution. We validate our results against numerical solutions and achieve $99 \%$ accuracy in most scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/785706911b444c085e5f732a9fef3bb0159349db" target='_blank'>
              Physics Informed Kolmogorov-Arnold Neural Networks for Dynamical Analysis via Efficent-KAN and WAV-KAN
              </a>
            </td>
          <td>
            Subhajit Patra, Sonali Panda, B. K. Parida, Mahima Arya, Kurt Jacobs, Denys I. Bondar, Abhijit Sen
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Trained neural networks (NN) have attractive features for closing governing equations, but in the absence of additional constraints, they can stray from physical reality. A NN formulation is introduced to preclude spurious oscillations that violate solution boundedness or positivity. It is embedded in the discretized equations as a machine learning closure and strictly constrained, inspired by total variation diminishing (TVD) methods for hyperbolic conservation laws. The constraint is exactly enforced during gradient-descent training by rescaling the NN parameters, which maps them onto an explicit feasible set. Demonstrations show that the constrained NN closure model usefully recovers linear and nonlinear hyperbolic phenomena and anti-diffusion while enforcing the non-oscillatory property. Finally, the model is applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for which it suppresses spurious oscillations in scalar fields that otherwise violate the solution boundedness. It outperforms a simple penalization of oscillations in the loss function.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79145122aa7a9d0690a2cdf42f2290f96d562ae8" target='_blank'>
              A TVD neural network closure and application to turbulent combustion
              </a>
            </td>
          <td>
            Seung Won Suh, J. MacArt, Luke N Olson, Jonathan B Freund
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Reduced Order Models (ROMs) form essential tools across engineering domains by virtue of their function as surrogates for computationally intensive digital twinning simulators. Although purely data-driven methods are available for ROM construction, schemes that allow to retain a portion of the physics tend to enhance the interpretability and generalization of ROMs. However, physics-based techniques can adversely scale when dealing with nonlinear systems that feature parametric dependencies. This study introduces a generative physics-based ROM that is suited for nonlinear systems with parametric dependencies and is additionally able to quantify the confidence associated with the respective estimates. A main contribution of this work is the conditioning of these parametric ROMs to features that can be derived from monitoring measurements, feasibly in an online fashion. This is contrary to most existing ROM schemes, which remain restricted to the prescription of the physics-based, and usually a priori unknown, system parameters. Our work utilizes conditional Variational Autoencoders to continuously map the required reduction bases to a feature vector extracted from limited output measurements, while additionally allowing for a probabilistic assessment of the ROM-estimated Quantities of Interest. An auxiliary task using a neural network-based parametrization of suitable probability distributions is introduced to re-establish the link with physical model parameters. We verify the proposed scheme on a series of simulated case studies incorporating effects of geometric and material nonlinearity under parametric dependencies related to system properties and input load characteristics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb8cb80fb7c1708ef89c5e655f1cf2a1a98b8f0a" target='_blank'>
              A Reduced Order Model conditioned on monitoring features for estimation and uncertainty quantification in engineered systems
              </a>
            </td>
          <td>
            Konstantinos Vlachas, Thomas Simpson, Anthony Garland, D. Quinn, Charbel Farhat, Eleni Chatzi
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The goal of this paper is to make a strong point for the usage of dynamical models when using reinforcement learning (RL) for feedback control of dynamical systems governed by partial differential equations (PDEs). To breach the gap between the immense promises we see in RL and the applicability in complex engineering systems, the main challenges are the massive requirements in terms of the training data, as well as the lack of performance guarantees. We present a solution for the first issue using a data-driven surrogate model in the form of a convolutional Long-Short Term Memory network with actuation. We demonstrate that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system. Furthermore, we show that iteratively updating the model is of major importance to avoid biases in the RL training. Detailed ablation studies reveal the most important ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky equation do demonstrate our findings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73d91c3ba7ba5d34e084bcd3bd2c0c397052d50a" target='_blank'>
              Numerical Evidence for Sample Efficiency of Model-Based Over Model-Free Reinforcement Learning Control of Partial Differential Equations
              </a>
            </td>
          <td>
            Stefan Werner, Sebastian Peitz
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
 The time evolution of physical systems is described by differential equations, which depend on abstract quantities like energy and force. Traditionally, these quantities are derived as functionals based on observables such as positions and velocities. Discovering these governing symbolic laws is the key to comprehending the interactions in nature. Here, we present a Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the dynamics of systems directly from their trajectory. We demonstrate the performance of HGNN on n-springs, n-pendulums, gravitational systems, and binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement with the ground truth from small amounts of data. We also evaluate the ability of HGNN to generalize to larger system sizes, and to a hybrid spring-pendulum system that is a combination of two original systems (spring and pendulum) on which the models are trained independently. Finally, employing symbolic regression on the learned HGNN, we infer the underlying equations relating to the energy functionals, even for complex systems such as the binary Lennard-Jones liquid. Our framework facilitates the interpretable discovery of interaction laws directly from physical system trajectories. Furthermore, this approach can be extended to other systems with topology-dependent dynamics, such as cells, polydisperse gels, or deformable bodies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c632fd8fa34b6e12e92828eda9a171b587d0e51e" target='_blank'>
              DISCOVERING SYMBOLIC LAWS DIRECTLY FROM TRAJECTORIES WITH HAMILTONIAN GRAPH NEURAL NETWORKS
              </a>
            </td>
          <td>
            S. Bishnoi, Ravinder Bhattoo, Sayan Ranu, N. Krishnan
          </td>
          <td>2024-08-06</td>
          <td>Machine Learning: Science and Technology</td>
          <td>1</td>
          <td>10</td>
        </tr>

        <tr id="Complex dynamic systems are typically either modeled using expert knowledge in the form of differential equations, or via data-driven universal approximation models such as artificial neural networks (ANN). While the first approach has advantages with respect to interpretability, transparency, data-efficiency, and extrapolation, the second approach is able to learn completely unknown functional relations from data and may result in models that can be evaluated more efficiently. To combine the complementary advantages, universal differential equations (UDE) have been suggested, which replace unknown terms in the differential equations with ANN. These hybrid models allow to both encode prior domain knowledge such as first principles and to learn unknown mechanisms from data. Often, data for the training of UDE can only be obtained via costly experiments. We consider optimal experimental design (OED) for the planning of experiments and generation of data needed to train UDE. The number of weights in the embedded ANN usually leads to an overfitting of the regression problem. To make the OED problem tractable for optimization, we propose and compare dimension reduction methods that are based on lumping of weights and singular value decomposition of the Fisher information matrix (FIM), respectively. They result in lower-dimensional variational differential equations which are easier to solve and which yield regular FIM. Our numerical results showcase the advantages of OED for UDE, such as an increased data-efficiency and better extrapolation properties.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2230fd0495ec13469526bd77cb385acbdb4bddfa" target='_blank'>
              Optimal Experimental Design for Universal Differential Equations
              </a>
            </td>
          <td>
            Christoph Plate, Carl Julius Martensen, Sebastian Sager
          </td>
          <td>2024-08-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Estimating and controlling dynamical systems from observable time-series data are essential for understanding and manipulating nonlinear dynamics. This paper proposes a probabilistic framework for simultaneously estimating and controlling nonlinear dynamics under noisy observation conditions. Our proposed method utilizes the particle filter not only as a state estimator and a prior estimator for the dynamics but also as a controller. This approach allows us to handle the nonlinearity of the dynamics and uncertainty of the latent state. We apply two distinct dynamics to verify the effectiveness of our proposed framework: a chaotic system defined by the Lorenz equation and a nonlinear neuronal system defined by the Morris–Lecar neuron model. The results indicate that our proposed framework can simultaneously estimate and control complex nonlinear dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/682bd11b89e6862f9721d25af1d528fadb6746f4" target='_blank'>
              Probabilistic Estimation and Control of Dynamical Systems Using Particle Filter with Adaptive Backward Sampling
              </a>
            </td>
          <td>
            Taketo Omi, Toshiaki Omori
          </td>
          <td>2024-07-30</td>
          <td>Entropy</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We introduce new machine-learning techniques for analyzing chaotic dynamical systems. The primary objectives of the study include the development of a new and simple method for calculating the Lyapunov exponent using only two trajectory data points unlike traditional methods that require an averaging procedure, the exploration of phase transition graphs from regular periodic to chaotic dynamics to identify"almost integrable"trajectories where conserved quantities deviate from whole numbers, and the identification of"integrable regions"within chaotic trajectories. These methods are applied and tested on two dynamical systems:"Two objects moving on a rod"and the"Henon-Heiles"systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8beba1025021fc988f71772e30840ce6845fdf65" target='_blank'>
              Deciphering Complexity: Machine Learning Insights into Chaotic Dynamical Systems
              </a>
            </td>
          <td>
            Lazare Osmanov
          </td>
          <td>2024-08-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many physical systems exhibit translational invariance, meaning that the underlying physical laws are independent of the position in space. Data driven approximations of the infinite dimensional but linear Koopman operator of non-linear dynamical systems need to be physically informed in order to respect such physical symmetries. In the current work, we introduce a translation invariant extended dynamic mode decomposition (tieDMD) for coupled non-linear systems on periodic domains. This is achieved by exploiting a block-diagonal structure of the Koopman operator in Fourier space. Variants of tieDMD are applied to data obtained on one-dimensional periodic domains from the non-linear phase-diffusion equation, the Burgers equation, the Korteweg-de Vries equation, and a coupled FitzHugh-Nagumo system of partial differential equations. The reconstruction capability of tieDMD is compared to existing linear and non-linear variants of the dynamic mode decomposition applied to the same data. For the regarded data, tieDMD consistently shows superior capabilities in data reconstruction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3bb71d64de3bbe99545b75cc623bacfad3e1e76" target='_blank'>
              Approximation of translation invariant Koopman operators for coupled non-linear systems.
              </a>
            </td>
          <td>
            Thomas Hochrainer, Gurudas Kar
          </td>
          <td>2024-08-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Koopman operators and transfer operators represent nonlinear dynamics in state space through its induced action on linear spaces of observables and measures, respectively. This framework enables the use of linear operator theory for supervised and unsupervised learning of nonlinear dynamical systems, and has received considerable interest in recent years. Here, we propose a data-driven technique for spectral approximation of Koopman operators of continuous-time, measure-preserving ergodic systems that is asymptotically consistent and makes direct use of known equations of motion (physics). Our approach is based on a bounded transformation of the Koopman generator (an operator implementing directional derivatives of observables along the dynamical flow), followed by smoothing by a Markov semigroup of kernel integral operators. This results in a skew-adjoint, compact operator whose eigendecomposition is expressible as a variational generalized eigenvalue problem. We develop Galerkin methods to solve this eigenvalue problem and study their asymptotic consistency in the large-data limit. A key aspect of these methods is that they are physics-informed, in the sense of making direct use of dynamical vector field information through automatic differentiation of kernel functions. Solutions of the eigenvalue problem reconstruct evolution operators that preserve unitarity of the underlying Koopman group while spectrally converging to it in a suitable limit. In addition, the computed eigenfunctions have representatives in a reproducing kernel Hilbert space, enabling out-of-sample evaluation of learned dynamical features. Numerical experiments performed with this method on integrable and chaotic low-dimensional systems demonstrate its efficacy in extracting dynamically coherent observables under complex dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7bfdb15c2217fa4993fd812bd48a56f8b7c2df5e" target='_blank'>
              Physics-informed spectral approximation of Koopman operators
              </a>
            </td>
          <td>
            C. Valva, Dimitrios Giannakis
          </td>
          <td>2024-08-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This proposed work introduces a data-assimilation-assisted approach to train neural networks, aimed at effectively reducing epistemic uncertainty in state estimates of separated flows. This method, referred to as model-consistent training, ensures that input features are derived directly from physics-based models, such as Reynolds Averaged Navier Stokes (RANS) turbulence models, to accurately represent the current state of the flow. Autoencoders have been selected for this task due to their capability to capture essential information from large datasets, making them particularly suitable for handling high-dimensional data with numerous discretization points in both spatial and temporal dimensions. This innovative approach integrates the ensemble Kalman method to enhance the training process, providing a robust framework for improving model accuracy and performance in turbulent flow predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c44941f2e715c2b0202ef4e5d7ba0fe96659fe0" target='_blank'>
              Developing a Model-Consistent Reduced-Dimensionality training approach to quantify and reduce epistemic uncertainty in separated flows
              </a>
            </td>
          <td>
            Minghan Chu
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents, for the first time, a framework for Kolmogorov-Arnold Networks (KANs) in power system applications. Inspired by the recently proposed KAN architecture, this paper proposes physics-informed Kolmogorov-Arnold Networks (PIKANs), a novel KAN-based physics-informed neural network (PINN) tailored to efficiently and accurately learn dynamics within power systems. The PIKANs present a promising alternative to conventional Multi-Layer Perceptrons (MLPs) based PINNs, achieving superior accuracy in predicting power system dynamics while employing a smaller network size. Simulation results on a single-machine infinite bus system and a 4-bus 2- generator system underscore the accuracy of the PIKANs in predicting rotor angle and frequency with fewer learnable parameters than conventional PINNs. Furthermore, the simulation results demonstrate PIKANs capability to accurately identify uncertain inertia and damping coefficients. This work opens up a range of opportunities for the application of KANs in power systems, enabling efficient determination of grid dynamics and precise parameter identification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba0e7f4ba7daa48bc5af967eba3f7af6be45476e" target='_blank'>
              Physics-Informed Kolmogorov-Arnold Networks for Power System Dynamics
              </a>
            </td>
          <td>
            H. Shuai, Fangxing Li
          </td>
          <td>2024-08-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Reservoir computing (RC) is known as a powerful machine learning approach for learning complex dynamics from limited data. Here, we use RC to predict highly stochastic dynamics of cell shapes. We find that RC is able to predict the steady state climate from very limited data. Furthermore, the RC learns the timescale of transients from only four observations. We find that these capabilities of the RC to act as a dynamic twin allows us to also infer important statistics of cell shape dynamics of unobserved conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/20f5cfa59cf3363d794a2715ae08d7e092121b0f" target='_blank'>
              Machine learning from limited data: Predicting biological dynamics under a time-varying external input
              </a>
            </td>
          <td>
            Hoony Kang, Keshav Srinivasan, Wolfgang Losert
          </td>
          <td>2024-08-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We propose CoNSAL (Combining Neural networks and Symbolic regression for Analytical Lyapunov function) to construct analytical Lyapunov functions for nonlinear dynamic systems. This framework contains a neural Lyapunov function and a symbolic regression component, where symbolic regression is applied to distill the neural network to precise analytical forms. Our approach utilizes symbolic regression not only as a tool for translation but also as a means to uncover counterexamples. This procedure terminates when no counterexamples are found in the analytical formulation. Compared with previous results, CoNSAL directly produces an analytical form of the Lyapunov function with improved interpretability in both the learning process and the final results. We apply CoNSAL to 2-D inverted pendulum, path following, Van Der Pol Oscillator, 3-D trig dynamics, 4-D rotating wheel pendulum, 6-D 3-bus power system, and demonstrate that our algorithm successfully finds their valid Lyapunov functions. Code examples are available at https://github.com/HaohanZou/CoNSAL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6829a0516285c595b5e8d5fcbc9c4858827f5784" target='_blank'>
              Combining Neural Networks and Symbolic Regression for Analytical Lyapunov Function Discovery
              </a>
            </td>
          <td>
            Jie Feng, Haohan Zou, Yuanyuan Shi
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Port-Hamiltonian systems (pHS) allow for a structure-preserving modeling of dynamical systems. Coupling pHS via linear relations between input and output defines an overall pHS, which is structure preserving. However, in multiphysics applications, some subsystems do not allow for a physical pHS description, as (a) this is not available or (b) too expensive. Here, data-driven approaches can be used to deliver a pHS for such subsystems, which can then be coupled to the other subsystems in a structure-preserving way. In this work, we derive a data-driven identification approach for port-Hamiltonian differential algebraic equation (DAE) systems. The approach uses input and state space data to estimate nonlinear effort functions of pH-DAEs. As underlying technique, we us (multi-task) Gaussian processes. This work thereby extends over the current state of the art, in which only port-Hamiltonian ordinary differential equation systems could be identified via Gaussian processes. We apply this approach successfully to two applications from network design and constrained multibody system dynamics, based on pH-DAE system of index one and three, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f006ed4bbd09834cab7ba0f2c3258b36e07050c2" target='_blank'>
              Data-driven identification of port-Hamiltonian DAE systems by Gaussian processes
              </a>
            </td>
          <td>
            Peter Zaspel, Michael Günther
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Many systems in biology, physics, and engineering are modeled by nonlinear dynamical systems where the states are usually unknown and only a subset of the state variables can be physically measured. Can we understand the full system from what we measure? In the mathematics literature, this question is framed as the observability problem. It has to do with recovering information about the state variables from the observed states (the measurements). In this paper, we relate the observability problem to another structural feature of many models relevant in the physical and biological sciences: the conserved quantity. For models based on systems of differential equations, conserved quantities offer desirable properties such as dimension reduction which simplifies model analysis. Here, we use differential embeddings to show that conserved quantities involving a set of special variables provide more flexibility in what can be measured to address the observability problem for systems of interest in biology. Specifically, we provide conditions under which a collection of conserved quantities make the system observable. We apply our methods to provide alternate measurable variables in models where conserved quantities have been used for model analysis historically in biological contexts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ac75a2c31069a04d822b3532f120e93eba3378c7" target='_blank'>
              Observability of complex systems via conserved quantities
              </a>
            </td>
          <td>
            B. Karamched, Jack Schmidt, David Murrugarra
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed trial-to-trial variability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/579f56813a03517163b86fa89044dc78505bf2cb" target='_blank'>
              Inferring stochastic low-rank recurrent neural networks from neural data
              </a>
            </td>
          <td>
            Matthijs Pals, A. E. Saugtekin, Felix Pei, Manuel Gloeckler, J. H. Macke
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The effective inclusion of a priori knowledge when embedding known data in physics‐based models of dynamical systems can ensure that the reconstructed model respects physical principles, while simultaneously improving the accuracy of the solution in the previously unseen regions of state space. This paper presents a physics‐constrained data‐driven discrepancy modeling method that variationally embeds known data in the modeling framework. The hierarchical structure of the method yields fine scale variational equations that facilitate the derivation of residuals which are comprised of the first‐principles theory and sensor‐based data from the dynamical system. The embedding of the sensor data via residual terms leads to discrepancy‐informed closure models that yield a method which is driven not only by boundary and initial conditions, but also by measurements that are taken at only a few observation points in the target system. Specifically, the data‐embedding term serves as residual‐based least‐squares loss function, thus retaining variational consistency. Another important relation arises from the interpretation of the stabilization tensor as a kernel function, thereby incorporating a priori knowledge of the problem and adding computational intelligence to the modeling framework. Numerical test cases show that when known data is taken into account, the data driven variational (DDV) method can correctly predict the system response in the presence of several types of discrepancies. Specifically, the damped solution and correct energy time histories are recovered by including known data in the undamped situation. Morlet wavelet analyses reveal that the surrogate problem with embedded data recovers the fundamental frequency band of the target system. The enhanced stability and accuracy of the DDV method is manifested via reconstructed displacement and velocity fields that yield time histories of strain and kinetic energies which match the target systems. The proposed DDV method also serves as a procedure for restoring eigenvalues and eigenvectors of a deficient dynamical system when known data is taken into account, as shown in the numerical test cases presented here.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dda505dc42399a19a75eff263166e84931a95cf6" target='_blank'>
              Data‐driven variational method for discrepancy modeling: Dynamics with small‐strain nonlinear elasticity and viscoelasticity
              </a>
            </td>
          <td>
            Arif Masud, Shoaib A. Goraya
          </td>
          <td>2024-07-04</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Kolmogorov-Arnold networks (KANs) have attracted attention recently as an alternative to multilayer perceptrons (MLPs) for scientific machine learning. However, KANs can be expensive to train, even for relatively small networks. Inspired by finite basis physics-informed neural networks (FBPINNs), in this work, we develop a domain decomposition method for KANs that allows for several small KANs to be trained in parallel to give accurate solutions for multiscale problems. We show that finite basis KANs (FBKANs) can provide accurate results with noisy data and for physics-informed training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6be75a69bb14192a26be00ff51c9a2f086f26b41" target='_blank'>
              Finite basis Kolmogorov-Arnold networks: domain decomposition for data-driven and physics-informed problems
              </a>
            </td>
          <td>
            Amanda Howard, Bruno Jacob, Sarah H. Murphy, Alexander Heinlein, P. Stinis
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>12</td>
        </tr>

        <tr id="The continuously increasing amount of noisy data demands the development of accurate and efficient models for analysis, modeling, and control. In this article, we propose a novel data-driven moment matching method which employs Tikhonov regularization in the Reproducing Kernel Hilbert Spaces (RKHSs). Specifically, considering a realistic scenario in which the system's plant is unknown and only noisy measured data are available, we provide an estimation of the moment of the unknown plant by solving a regularized optimization problem on RKHS. For, we first demonstrate that the estimation of the moment can be improved via tuning the regularization term, and further, we show under which condition the effect of the transient improves the performance of the estimation. Then, we construct a parameterized model characterized by a kernel-based output mapping. Finally, the proposed data-driven approach is validated and discussed by means of a DC-to-DC Cuk converter driven by a Van der Pol oscillator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/970212021b4134e841e09f425053018067ffcdbb" target='_blank'>
              Nonlinear Data-Driven Moment Matching in Reproducing Kernel Hilbert Spaces
              </a>
            </td>
          <td>
            Alessio Moreschini, Matteo Scandella, Thomas Parisini
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This paper presents an algorithm for developing sparse surrogate models that satisfy mass/energy conservation even when the training data are noisy and violate the conservation laws. In the first step, we employ the Bayesian Identification of Dynamic Sparse Algebraic Model (BIDSAM) algorithm proposed in our previous work to obtain a set of hierarchically ranked sparse models which approximate system behaviors with linear combinations of a set of well-defined basis functions. Although the model building algorithm was shown to be robust to noisy data, conservation laws may not be satisfied by the surrogate models. In this work we propose an algorithm that augments a data reconciliation step with the BIDSAM model for satisfaction of conservation laws. This method relies only on known boundary conditions and hence is generic for any chemical system. Two case studies are considered-one focused on mass conservation and another on energy conservation. Results show that models with minimum bias are built by using the developed algorithm while exactly satisfying the conservation laws for all data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3b228eccbfc381c1bf8429fb1e6ed55fe3fb34c" target='_blank'>
              Development of Mass/Energy Constrained Sparse Bayesian Surrogate Models from Noisy Data
              </a>
            </td>
          <td>
            Samuel Adeyemo, D. Bhattacharyya
          </td>
          <td>2024-07-09</td>
          <td>Systems and Control Transactions</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="
 The growing time-series data make it possible to glimpse the hidden dynamics in various fields. However, developing a computational toolbox with high interpretability to unveil the interaction dynamics from data remains a crucial challenge. Here, we propose a new computational approach called Automated Dynamical Model Inference based on Expression Trees (ADMIET), in which the machine learning algorithm, the numerical integration of ordinary differential equations and the interpretability from prior knowledge are embedded into the symbolic learning scheme to establish a general framework for revealing the hidden dynamics in time-series data. ADMIET takes full advantage of both machine learning algorithm and expression tree. Firstly, we translate the prior knowledge into constraints on the structure of expression tree, reducing the search space and enhancing the interpretability. Secondly, we utilize the proposed adaptive penalty function to ensure the convergence of gradient descent algorithm and the selection of the symbols. Compared to gene expression programming, ADMIET exhibits its remarkable capability in function fitting with higher accuracy and broader applicability. Moreover, ADMIET can better fit parameters in nonlinear forms compared to regression methods. Furthermore, we apply ADMIET to two typical biological systems and one real data with different prior knowledge to infer the dynamical equations. The results indicate that ADMIET can not only discover the interaction relationships but also provide accurate estimates of the parameters in the equations. These results demonstrate ADMIET's superiority in revealing interpretable dynamics from time-series biological data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9687aa0373c8e0262b0df16232ea9a08145ff46c" target='_blank'>
              Inferring dynamical models from time-series biological data using an interpretable machine learning method based on weighted expression trees
              </a>
            </td>
          <td>
            Yu Zhou, Xiufen Zou
          </td>
          <td>2024-07-09</td>
          <td>Inverse Problems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fd49cad114aa0d9be0df7a779ad44da18fbea25" target='_blank'>
              Stochastic modeling of stationary scalar Gaussian processes in continuous time from autocorrelation data
              </a>
            </td>
          <td>
            Martin Hanke
          </td>
          <td>2024-06-24</td>
          <td>Advances in Computational Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The development of data-driven approaches for solving differential equations has been followed by a plethora of applications in science and engineering across a multitude of disciplines and remains a central focus of active scientific inquiry. However, a large body of natural phenomena incorporates memory effects that are best described via fractional integro-differential equations (FIDEs), in which the integral or differential operators accept non-integer orders. Addressing the challenges posed by nonlinear FIDEs is a recognized difficulty, necessitating the application of generic methods with immediate practical relevance. This work introduces the Universal Fractional Integro-Differential Equation Solvers (UniFIDES), a comprehensive machine learning platform designed to expeditiously solve a variety of FIDEs in both forward and inverse directions, without the need for ad hoc manipulation of the equations. The effectiveness of UniFIDES is demonstrated through a collection of integer-order and fractional problems in science and engineering. Our results highlight UniFIDES' ability to accurately solve a wide spectrum of integro-differential equations and offer the prospect of using machine learning platforms universally for discovering and describing dynamical and complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41fa0a29da947de99741b68960e6591a9bfd2771" target='_blank'>
              UniFIDES: Universal Fractional Integro-Differential Equation Solvers
              </a>
            </td>
          <td>
            Milad Saadat, Deepak Mangal, Safa Jamali
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Constraining a numerical weather prediction (NWP) model with observations via 4D variational (4D-Var) data assimilation is often difficult to implement in practice due to the need to develop and maintain a software-based tangent linear model and adjoint model. One of the most common 4D-Var algorithms uses an incremental update procedure, which has been shown to be an approximation of the Gauss-Newton method. Here we demonstrate that when using a forecast model that supports automatic differentiation, an efficient and in some cases more accurate alternative approximation of the Gauss-Newton method can be applied by combining backpropagation of errors with Hessian approximation. This approach can be used with either a conventional numerical model implemented within a software framework that supports automatic differentiation, or a machine learning (ML) based surrogate model. We test the new approach on a variety of Lorenz-96 and quasi-geostrophic models. The results indicate potential for a deeper integration of modeling, data assimilation, and new technologies in a next-generation of operational forecast systems that leverage weather models designed to support automatic differentiation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4b43d025345ba51dc1b8aa4bb1314e91f7a7174b" target='_blank'>
              4D-Var using Hessian approximation and backpropagation applied to automatically-differentiable numerical and machine learning models
              </a>
            </td>
          <td>
            Kylen Solvik, Stephen G. Penny, Stephan Hoyer
          </td>
          <td>2024-08-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="For mathematical and experimental ease, models with time varying parameters are often simplified to assume constant parameters. However, this simplification can potentially lead to identifiability issues (lack of uniqueness of parameter estimates). Methods have been developed to algebraically and numerically determine the identifiability of a model, as well as resolve identifiability issues. This specific type of simplification presents an alternate opportunity to instead use this information to resolve the unidentifiability. Given that re-parameterizing, collecting more data, and adding inputs can be potentially costly or impractical, this could present new alternatives. We present a method for resolving unidentifiability in a system by introducing a new data stream correlated with a parameter of interest. First, we demonstrate how and when non-constant input data can be introduced into any rational function ODE system without worsening the model identifiability. Then, we prove when these input functions improve structural and potentially also practical identifiability for a given model and relevant data. By utilizing pre-existing data streams, these methods can potentially reduce experimental costs, while still answering key questions. By connecting mathematical proofs to application, our framework removes guesswork from when, where, and how researchers can best introduce new data to improve model outcomes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fbc38cc84b188d260525dcc553f64371eca970c3" target='_blank'>
              Examining the impact of forcing function inputs on structural identifiability
              </a>
            </td>
          <td>
            Jessica R Conrad, Marisa C Eisenberg
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper examines the application of the Kernel Sum of Squares (KSOS) method for enhancing kernel learning from data, particularly in the context of dynamical systems. Traditional kernel-based methods, despite their theoretical soundness and numerical efficiency, frequently struggle with selecting optimal base kernels and parameter tuning, especially with gradient-based methods prone to local optima. KSOS mitigates these issues by leveraging a global optimization framework with kernel-based surrogate functions, thereby achieving more reliable and precise learning of dynamical systems. Through comprehensive numerical experiments on the Logistic Map, Henon Map, and Lorentz System, KSOS is shown to consistently outperform gradient descent in minimizing the relative-$\rho$ metric and improving kernel accuracy. These results highlight KSOS's effectiveness in predicting the behavior of chaotic dynamical systems, demonstrating its capability to adapt kernels to underlying dynamics and enhance the robustness and predictive power of kernel-based approaches, making it a valuable asset for time series analysis in various scientific fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be7befd5338268cfdfa3b26e66bece3caef3b91d" target='_blank'>
              Kernel Sum of Squares for Data Adapted Kernel Learning of Dynamical Systems from Data: A global optimization approach
              </a>
            </td>
          <td>
            Daniel Lengyel, P. Parpas, B. Hamzi, H. Owhadi
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Traditional partial differential equations with constant coefficients often struggle to capture abrupt changes in real-world phenomena, leading to the development of variable coefficient PDEs and Markovian switching models. Recently, research has introduced the concept of PDEs with Markov switching models, established their well-posedness and presented numerical methods. However, there has been limited discussion on parameter estimation for the jump coefficients in these models. This paper addresses this gap by focusing on parameter inference for the wave equation with Markovian switching. We propose a Bayesian statistical framework using discrete sparse Bayesian learning to establish its convergence and a uniform error bound. Our method requires fewer assumptions and enables independent parameter inference for each segment by allowing different underlying structures for the parameter estimation problem within each segmented time interval. The effectiveness of our approach is demonstrated through three numerical cases, which involve noisy spatiotemporal data from different wave equations with Markovian switching. The results show strong performance in parameter estimation for variable coefficient PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5eda7824c0ecf7f45c2be3abef5f95f30fc1aafe" target='_blank'>
              Parameters Inference for Nonlinear Wave Equations with Markovian Switching
              </a>
            </td>
          <td>
            Yi Zhang, Zhikun Zhang, Xiangjun Wang
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Predicting the dynamics of turbulent fluid flows has long been a central goal of science and engineering. Yet, even with modern computing technology, accurate simulation of all but the simplest turbulent flow-fields remains impossible: the fields are too chaotic and multi-scaled to directly store them in memory and perform time-evolution. An alternative is to treat turbulence $\textit{probabilistically}$, viewing flow properties as random variables distributed according to joint probability density functions (PDFs). Turbulence PDFs are neither chaotic nor multi-scale, but are still challenging to simulate due to their high dimensionality. Here we show how to overcome the dimensionality problem by parameterising turbulence PDFs into an extremely compressed format known as a"tensor network"(TN). The TN paradigm enables simulations on single CPU cores that would otherwise be impractical even with supercomputers: for a $5+1$ dimensional PDF of a chemically reactive turbulent flow, we achieve reductions in memory and computational costs by factors of $\mathcal{O}(10^6)$ and $\mathcal{O}(10^3)$, respectively, compared to standard finite difference algorithms. A future path is opened towards something heretofore regarded as infeasible: directly simulating high-dimensional PDFs of both turbulent flows and other chaotic systems that are useful to describe probabilistically.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/241b90969989bdb43a42587945b6b357630bfd72" target='_blank'>
              Tensor networks enable the calculation of turbulence probability distributions
              </a>
            </td>
          <td>
            Nikita Gourianov, P. Givi, Dieter Jaksch, Stephen B. Pope
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="This paper presents the open-source stochastic model predictive control framework GRAMPC-S for nonlinear uncertain systems with chance constraints. It provides several uncertainty propagation methods to predict stochastic moments of the system state and can consider unknown parts of the system dynamics using Gaussian process regression. These methods are used to reformulate a stochastic MPC formulation as a deterministic one that is solved with GRAMPC. The performance of the presented framework is evaluated using examples from a wide range of technical areas. The experimental evaluation shows that GRAMPC-S can be used in practice for the control of nonlinear uncertain systems with sampling times in the millisecond range.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c62a7baf784d003dda62b0480d24a61f0d5beb4" target='_blank'>
              A software framework for stochastic model predictive control of nonlinear continuous-time systems (GRAMPC-S)
              </a>
            </td>
          <td>
            D. Landgraf, Andreas Völz, Knut Graichen
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this study, we present a novel non-intrusive reduced-order model (ROM) for solving time-dependent stochastic partial differential equations (SPDEs). Utilizing proper orthogonal decomposition (POD), we extract spatial modes from high-fidelity solutions. A dynamic mode decomposition (DMD) method is then applied to vertically stacked matrices of projection coefficients for future prediction of coefficient fields. Polynomial chaos expansion (PCE) is employed to construct a mapping from random parameter inputs to the DMD-predicted coefficient field. These lead to the POD-DMD-PCE method. The innovation lies in vertically stacking projection coefficients, ensuring time-dimensional consistency in the coefficient matrix for DMD and facilitating parameter integration for PCE analysis. This method combines the model reduction of POD with the time extrapolation strengths of DMD, effectively recovering field solutions both within and beyond the training time interval. The efficiency and time extrapolation capabilities of the proposed method are validated through various nonlinear SPDEs. These include a reaction-diffusion equation with 19 parameters, a two-dimensional heat equation with two parameters, and a one-dimensional Burgers equation with three parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f03b1ec02154fef08c7c119096855e273290c56c" target='_blank'>
              Non-intrusive reduced-order model for time-dependent stochastic partial differential equations utilizing dynamic mode decomposition and polynomial chaos expansion.
              </a>
            </td>
          <td>
            Shuman Wang, Afshan Batool, Xiang Sun, Xiaomin Pan
          </td>
          <td>2024-07-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="We investigate the ability to discover data assimilation (DA) schemes meant for chaotic dynamics with deep learning (DL). The focus is on learning the analysis step of sequential DA, from state trajectories and their observations, using a simple residual convolutional neural network, while assuming the dynamics to be known. Experiments are performed with the Lorenz 96 dynamics, which display spatiotemporal chaos and for which solid benchmarks for DA performance exist. The accuracy of the states obtained from the learned analysis approaches that of the best possibly tuned ensemble Kalman filter (EnKF), and is far better than that of variational DA alternatives. Critically, this can be achieved while propagating even just a single state in the forecast step. We investigate the reason for achieving ensemble filtering accuracy without an ensemble. We diagnose that the analysis scheme actually identifies key dynamical perturbations, mildly aligned with the unstable subspace, from the forecast state alone, without any ensemble-based covariances representation. This reveals that the analysis scheme has learned some multiplicative ergodic theorem associated to the DA process seen as a non-autonomous random dynamical system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/049cb29c29c44d7bcd0b2872f2cc68742751159c" target='_blank'>
              Deep learning-based sequential data assimilation for chaotic dynamics identifies local instabilities from single state forecasts
              </a>
            </td>
          <td>
            Marc Bocquet, A. Farchi, Tobias S. Finn, Charlotte Durand, Sibo Cheng, Yumeng Chen, I. Pasmans, A. Carrassi
          </td>
          <td>2024-08-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="We provide an algorithm for the simultaneous system identification and model predictive control of nonlinear systems. The algorithm has finite-time near-optimality guarantees and asymptotically converges to the optimal (non-causal) controller. Particularly, the algorithm enjoys sublinear dynamic regret, defined herein as the suboptimality against an optimal clairvoyant controller that knows how the unknown disturbances and system dynamics will adapt to its actions. The algorithm is self-supervised and applies to control-affine systems with unknown dynamics and disturbances that can be expressed in reproducing kernel Hilbert spaces. Such spaces can model external disturbances and modeling errors that can even be adaptive to the system's state and control input. For example, they can model wind and wave disturbances to aerial and marine vehicles, or inaccurate model parameters such as inertia of mechanical systems. The algorithm first generates random Fourier features that are used to approximate the unknown dynamics or disturbances. Then, it employs model predictive control based on the current learned model of the unknown dynamics (or disturbances). The model of the unknown dynamics is updated online using least squares based on the data collected while controlling the system. We validate our algorithm in both hardware experiments and physics-based simulations. The simulations include (i) a cart-pole aiming to maintain the pole upright despite inaccurate model parameters, and (ii) a quadrotor aiming to track reference trajectories despite unmodeled aerodynamic drag effects. The hardware experiments include a quadrotor aiming to track a circular trajectory despite unmodeled aerodynamic drag effects, ground effects, and wind disturbances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f6fdbd86334d255c0b4f1f232d3ee77ab3286d0f" target='_blank'>
              Simultaneous System Identification and Model Predictive Control with No Dynamic Regret
              </a>
            </td>
          <td>
            Hongyu Zhou, Vasileios Tzoumas
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="We consider the problem of data-driven stochastic optimal control of an unknown LTI dynamical system. Assuming the process noise is normally distributed, we pose the problem of steering the state's mean and covariance to a target normal distribution, under noisy data collected from the underlying system, a problem commonly referred to as covariance steering (CS). A novel framework for Data-driven Uncertainty quantification and density STeering (DUST) is presented that simultaneously characterizes the noise affecting the measured data and designs an optimal affine-feedback controller to steer the density of the state to a prescribed terminal value. We use both indirect and direct data-driven design approaches based on the notions of persistency of excitation and subspace predictors to exactly represent the mean and covariance dynamics of the state in terms of the data and noise realizations. Since both the mean and the covariance steering sub-problems are plagued with distributional uncertainty arising from noisy data collection, we first estimate the noise realization from this dataset and subsequently compute tractable upper bounds on the estimation errors. The moment steering problems are then solved to optimality using techniques from robust control and robust optimization. Lastly, we present an alternative control design approach based on the certainty equivalence principle and interpret the problem as one of CS under multiplicative uncertainties. We analyze the performance and efficacy of each of these data-driven approaches using a case study and compare them with their model-based counterparts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/63a1f296701b24baf2d77272d401b9179d8d8312" target='_blank'>
              DUST: A Framework for Data-Driven Density Steering
              </a>
            </td>
          <td>
            Joshua Pilipovsky, Panagiotis Tsiotras
          </td>
          <td>2024-08-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Pedestrian crowds encompass a complex interplay of intentional movements aimed at reaching specific destinations, fluctuations due to personal and interpersonal variability, and interactions with each other and the environment. Previous work showed the effectiveness of Langevin-like equations in capturing the statistical properties of pedestrian dynamics in simple settings, such as almost straight trajectories. However, modeling more complex dynamics, e.g. when multiple routes and origin-destinations are involved, remains a significant challenge. In this work, we introduce a novel and generic framework to describe the dynamics of pedestrians in any geometric setting, significantly extending previous works. Our model is based on Langevin dynamics with two timescales. The fast timescale corresponds to the stochastic fluctuations present when a pedestrian is walking. The slow timescale is associated with the dynamics that a pedestrian plans to follow, thus a smoother path. Employing a data-driven approach inspired by statistical field theories, we learn the complex potentials directly from the data, namely a high-statistics database of real-life pedestrian trajectories. This approach makes the model generic as the potentials can be read from any trajectory data set and the underlying Langevin structure enables physics-based insights. We validate our model through a comprehensive statistical analysis, comparing simulated trajectories with actual pedestrian measurements across five complementary settings, including a real-life train platform scenario, underscoring its practical societal relevance. We show that our model effectively captures fluctuation statistics in pedestrian motion. Beyond providing fundamental insights and predictive capabilities in pedestrian dynamics, our model could be used to investigate generic active dynamics such as vehicular traffic and collective animal behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c76fb4ec34bb7b21a1b522e6fe51864181436102" target='_blank'>
              Data-driven physics-based modeling of pedestrian dynamics
              </a>
            </td>
          <td>
            C. Pouw, Geert G.M. van der Vleuten, Alessandro Corbetta, Federico Toschi
          </td>
          <td>2024-07-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Understanding the dynamics of nonequilibrium quantum many-body systems is an important research topic in a wide range of fields across condensed matter physics, quantum optics, and high-energy physics. However, numerical studies of large-scale nonequilibrium phenomena in realistic materials face serious challenges due to intrinsic high-dimensionality of quantum many-body problems. The nonequilibrium properties of many-body systems can be described by the dynamics of the Green's function of the system, whose time evolution is given by a high-dimensional system of integro-differential equations, known as the Kadanoff-Baym equations (KBEs). The time-convolution term in KBEs, which needs to be recalculated at each time step, makes it difficult to perform long-time simulations. In this paper, we develop an operator-learning framework based on Recurrent Neural Networks (RNNs) to address this challenge. The proposed framework utilizes RNNs to learn the nonlinear mapping between Green's functions and convolution integrals in KBEs. By using the learned operators as a surrogate model in the KBE solver, we obtain a general machine-learning scheme for predicting the dynamics of nonequilibrium Green's functions. This new methodology reduces the temporal computational complexity from $O(N_t^3)$ to $O(N_t)$, where $N_t$ is the total time steps taken in a simulation, thereby making it possible to study large many-body problems which are currently infeasible with conventional KBE solvers. Through different numerical examples, we demonstrate the effectiveness of the operator-learning based approach in providing accurate predictions of physical observables such as the reduced density matrix and time-resolved photoemission spectra. Moreover, our framework exhibits clear numerical convergence and can be easily parallelized, thereby facilitating many possible further developments and applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/526a9cb7b29055f6d28d2a654e064ac8a68bb587" target='_blank'>
              Predicting nonequilibrium Green's function dynamics and photoemission spectra via nonlinear integral operator learning
              </a>
            </td>
          <td>
            Yuanran Zhu, Jia Yin, Cian C. Reeves, Chao Yang, Vojtěch Vlček
          </td>
          <td>2024-07-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Predicting chaotic systems is crucial for understanding complex behaviors, yet challenging due to their sensitivity to initial conditions and inherent unpredictability. Probabilistic Reservoir Computing (RC) is well-suited for long-term chaotic predictions by handling complex dynamic systems. Spin-Orbit Torque (SOT) devices in spintronics, with their nonlinear and probabilistic operations, can enhance performance in these tasks. This study proposes an RC system utilizing SOT devices for predicting chaotic dynamics. By simulating the reservoir in an RC network with SOT devices that achieve nonlinear resistance changes with random distribution, we enhance the robustness for the predictive capability of the model. The RC network predicted the behaviors of the Mackey-Glass and Lorenz chaotic systems, demonstrating that stochastic SOT devices significantly improve long-term prediction accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c63d0c8c5a8e26832c065f07e28872c0828b13c7" target='_blank'>
              Improved Long-Term Prediction of Chaos Using Reservoir Computing Based on Stochastic Spin-Orbit Torque Devices
              </a>
            </td>
          <td>
            Cen Wang, Xinyao Lei, Kaiming Cai, Xiaofei Yang, Yue Zhang
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="With the rapid development of articial intelligence, especially deep learning technology, scientic researchers have begun to explore its application in the eld of traditional scientic computing. Traditional scientic computing relies on mathematical equations to describe and predict the scientic laws of nature, while deep learning provides a new perspective to solve complex mathematical problems by learning patterns in data. The introduction of the Physical Information Neural Network (PINN) and the Ordinary Dierential Equation (ODENet) network layer enables deep learning technology to more accurately simulate and predict scientic phenomena. This study shows that by embedding an ODE-Net network layer in a physical information neural network (PINN), the tting accuracy and generalization performance of the model can be signicantly improved. Experimental results show that compared with traditional numerical methods and fully connected neural networks, this model combined with deep learning technology not only shows higher accuracy when solving partial dierential equations, but also exhibits faster convergence speed and stronger adaptability. These ndings not only promote the integration of scientic computing and deep learning, but also provide new research directions and practical strategies for using deep learning technology to solve complex scientic problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dedb6a263e8acff267028f948e0fbc5cc501e50c" target='_blank'>
              A numerical method to solve PDE through PINN based on ODENet
              </a>
            </td>
          <td>
            Ziyi Wang
          </td>
          <td>2024-06-24</td>
          <td>Applied and Computational Engineering</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Chaotic systems, such as turbulent flows, are ubiquitous in science and engineering. However, their study remains a challenge due to the large range scales, and the strong interaction with other, often not fully understood, physics. As a consequence, the spatiotemporal resolution required for accurate simulation of these systems is typically computationally infeasible, particularly for applications of long-term risk assessment, such as the quantification of extreme weather risk due to climate change. While data-driven modeling offers some promise of alleviating these obstacles, the scarcity of high-quality simulations results in limited available data to train such models, which is often compounded by the lack of stability for long-horizon simulations. As such, the computational, algorithmic, and data restrictions generally imply that the probability of rare extreme events is not accurately captured. In this work we present a general strategy for training neural network models to non-intrusively correct under-resolved long-time simulations of chaotic systems. The approach is based on training a post-processing correction operator on under-resolved simulations nudged towards a high-fidelity reference. This enables us to learn the dynamics of the underlying system directly, which allows us to use very little training data, even when the statistics thereof are far from converged. Additionally, through the use of probabilistic network architectures we are able to leverage the uncertainty due to the limited training data to further improve extrapolation capabilities. We apply our framework to severely under-resolved simulations of quasi-geostrophic flow and demonstrate its ability to accurately predict the anisotropic statistics over time horizons more than 30 times longer than the data seen in training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04ef0e9cecd284e7e1805b26a448f7108f474e9b" target='_blank'>
              A probabilistic framework for learning non-intrusive corrections to long-time climate simulations from short-time training data
              </a>
            </td>
          <td>
            Benedikt Barthel Sorensen, Leonardo Zepeda-N'unez, Ignacio Lopez-Gomez, Zhong Yi Wan, Rob Carver, Fei Sha, T. Sapsis
          </td>
          <td>2024-08-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="There is overwhelming evidence that cognition, perception, and action rely on feedback control. However, if and how neural population dynamics are amenable to different control strategies is poorly understood, in large part because machine learning methods to directly assess controllability in neural population dynamics are lacking. To address this gap, we developed a novel dimensionality reduction method, Feedback Controllability Components Analysis (FCCA), that identifies subspaces of linear dynamical systems that are most feedback controllable based on a new measure of feedback controllability. We further show that PCA identifies subspaces of linear dynamical systems that maximize a measure of feedforward controllability. As such, FCCA and PCA are data-driven methods to identify subspaces of neural population data (approximated as linear dynamical systems) that are most feedback and feedforward controllable respectively, and are thus natural contrasts for hypothesis testing. We developed new theory that proves that non-normality of underlying dynamics determines the divergence between FCCA and PCA solutions, and confirmed this in numerical simulations. Applying FCCA to diverse neural population recordings, we find that feedback controllable dynamics are geometrically distinct from PCA subspaces and are better predictors of animal behavior. Our methods provide a novel approach towards analyzing neural population dynamics from a control theoretic perspective, and indicate that feedback controllable subspaces are important for behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b12cb2ef2eca3c8ee46436c8d7fddcc5787df892" target='_blank'>
              Identifying Feedforward and Feedback Controllable Subspaces of Neural Population Dynamics
              </a>
            </td>
          <td>
            Ankit Kumar, Loren M. Frank, Kristofer E. Bouchard
          </td>
          <td>2024-08-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Studies show that artificial intelligence (AI) with embedded physics solvers has improved the accuracy of predictions on various physics problems, especially those associated with fluid dynamics. The crucial element in optimizing weight training for estimating flow fields within the AI network lies in the choice of the loss function. In addressing regression-type problems, particularly those involving the temporal evolution of flow fields, the mean square error (MSE) loss function is commonly employed at the current and single time step. However, an issue arises in existing methodologies that utilize MSE-based loss functions with single-time step information for predicting unsteady flow. Most of these approaches overlook the significance of incorporating the temporal history of the flow, a factor that cannot be disregarded in the context of numerical solvers. Hence, in this work, a physics-based AI (PbAI) method with higher-order loss functions is applied to unsteady scenarios, in particular to two distinct turbulent flows where a multitude of fine structures is present, namely, forced and decaying turbulence. Direct numerical simulations on uniform Cartesian grids are conducted to simulate these scenarios, generating two distinct datasets for training and inference. Each dataset comprises 32 randomly initialized conditions spanning 4, 848 time steps for each turbulent flow type. Five distinct models are devised, incorporating features such as rollouts from coarse numerical solvers and temporal considerations in the loss function calculation. The constructed PbAI models demonstrate consistent improvements in predictive performance over the entire temporal domain. These findings are further corroborated through vorticity correlation analyses. The empirical result demonstrates that the accuracy of the baseline case improves by up to 48% and 30% for forced and decaying turbulence, respectively. These results significantly underscore the importance of the temporal histories of flow in the loss function in enhancing predictive capabilities for complex and unsteady turbulent flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6a508269bfb08f5c8c66413efa4d444da72838f" target='_blank'>
              Multi-Order Loss Functions For Accelerating Unsteady Flow Simulations with Physics-Based AI
              </a>
            </td>
          <td>
            Wei Xian Lim, Naheed Anjum Arafat, Wai Lee Chan, Adams Kong
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="There is a growing interest in methods for detecting and interpreting changes in experimental time evolution data. Based on measured time series, the quantitative characterization of dynamical phase transitions at bifurcation points of the underlying chaotic systems is a notoriously difficult task. Building on prior theoretical studies that focus on the discontinuities at $q=1$ in the order-$q$ R\'enyi-entropy of the trajectory space, we measure the derivative of the spectrum. We derive within the general context of Markov processes a computationally efficient closed-form expression for this measure. We investigate its properties through well-known dynamical systems exploring its scope and limitations. The proposed mathematical instrument can serve as a predictor of dynamical phase transitions in time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68dbc71449fe5142fd6c84de4af2b46f1f3412b9" target='_blank'>
              Measuring dynamical phase transitions in time series
              </a>
            </td>
          <td>
            Bulcs'u S'andor, Andr'as Rusu, K'aroly D'enes, M'aria Ercsey-Ravasz, Zs.I. L'az'ar
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id=""AI for Science"aims to solve fundamental scientific problems using AI techniques. As most physical phenomena can be described as Partial Differential Equations (PDEs) , approximating their solutions using neural networks has evolved as a central component of scientific-ML. Physics-Informed Neural Networks (PINNs) is the general method that has evolved for this task but its training is well-known to be very unstable. In this work we explore the possibility of changing the model being trained from being just a neural network to being a non-linear transformation of it - one that algebraically includes the boundary/initial conditions. This reduces the number of terms in the loss function than the standard PINN losses. We demonstrate that our modification leads to significant performance gains across a range of benchmark tasks, in various dimensions and without having to tweak the training algorithm. Our conclusions are based on conducting hundreds of experiments, in the fully unsupervised setting, over multiple linear and non-linear PDEs set to exactly solvable scenarios, which lends to a concrete measurement of our performance gains in terms of order(s) of magnitude lower fractional errors being achieved, than by standard PINNs. The code accompanying this manuscript is publicly available at, https://github.com/MorganREN/Improving-PINNs-By-Algebraic-Inclusion-of-Boundary-and-Initial-Conditions">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/50ec057bbac038765a71bbcf35743418ac73a1d3" target='_blank'>
              Improving PINNs By Algebraic Inclusion of Boundary and Initial Conditions
              </a>
            </td>
          <td>
            Mohan Ren, Zhihao Fang, Keren Li, Anirbit Mukherjee
          </td>
          <td>2024-07-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Abstract The advent of machine learning has led to innovative approaches in dealing with clinical data. Among these, Neural Ordinary Differential Equations (Neural ODEs), hybrid models merging mechanistic with deep learning models have shown promise in accurately modeling continuous dynamical systems. Although initial applications of Neural ODEs in the field of model‐informed drug development and clinical pharmacology are becoming evident, applying these models to actual clinical trial datasets—characterized by sparse and irregularly timed measurements—poses several challenges. Traditional models often have limitations with sparse data, highlighting the urgent need to address this issue, potentially through the use of assumptions. This review examines the fundamentals of Neural ODEs, their ability to handle sparse and irregular data, and their applications in model‐informed drug development.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d55a1a4acc93d905399f2126e7beb524fd64ec38" target='_blank'>
              Bridging pharmacology and neural networks: A deep dive into neural ordinary differential equations
              </a>
            </td>
          <td>
            Idris Bachali Losada, N. Terranova
          </td>
          <td>2024-07-11</td>
          <td>CPT: Pharmacometrics & Systems Pharmacology</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="The treatment of Hall-effect thrusters as nonlinear, dynamical systems has emerged as a new perspective to understand and analyze data acquired from the thrusters. The acquisition of high-speed data that can resolve the characteristic high-frequency oscillations of these thruster enables additional levels of classification in these thrusters. Notably, these signals may serve as unique indicators for the full state of the system that can aid digital representations of thrusters and predictions of thruster dynamics. In this work, a Reservoir Computing framework is explored to build surrogate models from experimental time-series measurements of a Hall-effect thruster. Such a framework has shown immense promise for predicting the behavior of low-dimensional yet chaotic dynamical systems. In particular, the surrogates created by the Reservoir Computing framework are capable of both predicting the observed behavior of the thruster and estimating the values of certain measurements from others, known as inference.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/408d323572dab3c6a6dc96e645703d0f78eee449" target='_blank'>
              Time-Resolved Data-Driven Surrogates of Hall-effect Thrusters
              </a>
            </td>
          <td>
            Adrian S. Wong, Christine M. Greve, Daniel Q. Eckhardt
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We introduce neural information field filter, a Bayesian state and parameter estimation method for high-dimensional nonlinear dynamical systems given large measurement datasets. Solving such a problem using traditional methods, such as Kalman and particle filters, is computationally expensive. Information field theory is a Bayesian approach that can efficiently reconstruct dynamical model state paths and calibrate model parameters from noisy measurement data. To apply the method, we parameterize the time evolution state path using the span of a finite linear basis. The existing method has to reparameterize the state path by initial states to satisfy the initial condition. Designing an expressive yet simple linear basis before knowing the true state path is crucial for inference accuracy but challenging. Moreover, reparameterizing the state path using the initial state is easy to perform for a linear basis, but is nontrivial for more complex and expressive function parameterizations, such as neural networks. The objective of this paper is to simplify and enrich the class of state path parameterizations using neural networks for the information field theory approach. To this end, we propose a generalized physics-informed conditional prior using an auxiliary initial state. We show the existing reparameterization is a special case. We parameterize the state path using a residual neural network that consists of a linear basis function and a Fourier encoding fully connected neural network residual function. The residual function aims to correct the error of the linear basis function. To sample from the intractable posterior distribution, we develop an optimization algorithm, nested stochastic variational inference, and a sampling algorithm, nested preconditioned stochastic gradient Langevin dynamics. A series of numerical and experimental examples verify and validate the proposed method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f1f25c26be1528226a494d8e61aceb48b888ac8" target='_blank'>
              Neural information field filter
              </a>
            </td>
          <td>
            Kairui Hao, Ilias Bilionis
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We previously presented a novel loss formulation for efficient learning of complex dynamics from governing physics, typically described by partial differential equations (PDEs), using physics-informed neural networks (PINNs). In these experiments, the incorporation of a Boundary Connectivity (BCXN) loss function was shown to greatly improve physics-informed learning across many problems, especially those with complex geometries. However, this imposition may not always be ideal, as the previously presented BCXN-loss strongly enforces a linear local structure at the boundary. While this assumption helps facilitate faster learning with an order of magnitude sparser training samples, this can adversely impact convergence in other situations. Hence, we propose a modification of this BCXN-loss that reduces the imposed structure to a soft constraint, allowing for more flexible learning and convergence. We further demonstrate the potential for this method to improve the convergence and performance of LSA-PINN across additional numerical experiments, with much smaller errors than existing methods in terms of the standard L2-norm metric. In particular, we have applied this method to the modelling of flow past complex geometries such as airfoils, which serve as the basic building block for various applications in fluid dynamics and renewable energy (e.g., in wind and tidal turbine design).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/535c64c537da066482154d77b3dfa4334e26bb4e" target='_blank'>
              Soft Constraint in Local Structure Approximation-PINN
              </a>
            </td>
          <td>
            Jian Cheng Wong, P. Chiu, C. Ooi, M. Dao
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Multi-fidelity machine learning methods address the accuracy-efficiency trade-off by integrating scarce, resource-intensive high-fidelity data with abundant but less accurate low-fidelity data. We propose a practical multi-fidelity strategy for problems spanning low- and high-dimensional domains, integrating a non-probabilistic regression model for the low-fidelity with a Bayesian model for the high-fidelity. The models are trained in a staggered scheme, where the low-fidelity model is transfer-learned to the high-fidelity data and a Bayesian model is trained for the residual. This three-model strategy -- deterministic low-fidelity, transfer learning, and Bayesian residual -- leads to a prediction that includes uncertainty quantification both for noisy and noiseless multi-fidelity data. The strategy is general and unifies the topic, highlighting the expressivity trade-off between the transfer-learning and Bayesian models (a complex transfer-learning model leads to a simpler Bayesian model, and vice versa). We propose modeling choices for two scenarios, and argue in favor of using a linear transfer-learning model that fuses 1) kernel ridge regression for low-fidelity with Gaussian processes for high-fidelity; or 2) deep neural network for low-fidelity with a Bayesian neural network for high-fidelity. We demonstrate the effectiveness and efficiency of the proposed strategies and contrast them with the state-of-the-art based on various numerical examples. The simplicity of these formulations makes them practical for a broad scope of future engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f0aac32171f95be4080bfb1caf4a77b613f4b14d" target='_blank'>
              Practical multi-fidelity machine learning: fusion of deterministic and Bayesian models
              </a>
            </td>
          <td>
            Jiaxiang Yi, Ji Cheng, Miguel A. Bessa
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper proposes a generalized algorithmic approach for learning linear model representations for nonlinear systems within the Koopman framework. We focus on schemes that rely on learning the nonlinear transformation functions using deep neural networks. Beyond achieving dynamical accuracy, our primary objective is to develop models capable of simulating nonlinear systems across multiple time steps in the linear space. An algorithm that is based on recursive least squares is proposed to address the optimization complexities inherent in learning such models. In addition, we leverage the learned linear representation to design a linear quadratic regulator to control the original nonlinear system. The effectiveness of the proposed algorithm is demonstrated in two numerical examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1dc9bf823159462d30a9a2199b18f6959ef1ccd3" target='_blank'>
              Recursive Least Squares-Based Identification for Multi-Step Koopman Operators
              </a>
            </td>
          <td>
            Omar Sayed, Sergio Lucia
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Bayesian analysis enables combining prior knowledge with measurement data to learn model parameters. Commonly, one resorts to computing the maximum a posteriori (MAP) estimate, when only a point estimate of the parameters is of interest. We apply MAP estimation in the context of structural dynamic models, where the system response can be described by the frequency response function. To alleviate high computational demands from repeated expensive model calls, we utilize a rational polynomial chaos expansion (RPCE) surrogate model that expresses the system frequency response as a rational of two polynomials with complex coefficients. We propose an extension to an existing sparse Bayesian learning approach for RPCE based on Laplace's approximation for the posterior distribution of the denominator coefficients. Furthermore, we introduce a Bayesian optimization approach, which allows to adaptively enrich the experimental design throughout the optimization process of MAP estimation. Thereby, we utilize the expected improvement acquisition function as a means to identify sample points in the input space that are possibly associated with large objective function values. The acquisition function is estimated through Monte Carlo sampling based on the posterior distribution of the expansion coefficients identified in the sparse Bayesian learning process. By combining the sparsity-inducing learning procedure with the sequential experimental design, we effectively reduce the number of model evaluations in the MAP estimation problem. We demonstrate the applicability of the presented methods on the parameter updating problem of an algebraic two-degree-of-freedom system and the finite element model of a cross-laminated timber plate.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a03332f3ec82d4697eb9f8880eb0ae668a91b1a" target='_blank'>
              Maximum a Posteriori Estimation for Linear Structural Dynamics Models Using Bayesian Optimization with Rational Polynomial Chaos Expansions
              </a>
            </td>
          <td>
            Felix Schneider, I. Papaioannou, Bruno Sudret, Gerhard Muller
          </td>
          <td>2024-08-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="One of the most promising applications of machine learning (ML) in computational physics is to accelerate the solution of partial differential equations (PDEs). The key objective of ML-based PDE solvers is to output a sufficiently accurate solution faster than standard numerical methods, which are used as a baseline comparison. We first perform a systematic review of the ML-for-PDE solving literature. Of articles that use ML to solve a fluid-related PDE and claim to outperform a standard numerical method, we determine that 79% (60/76) compare to a weak baseline. Second, we find evidence that reporting biases, especially outcome reporting bias and publication bias, are widespread. We conclude that ML-for-PDE solving research is overoptimistic: weak baselines lead to overly positive results, while reporting biases lead to underreporting of negative results. To a large extent, these issues appear to be caused by factors similar to those of past reproducibility crises: researcher degrees of freedom and a bias towards positive results. We call for bottom-up cultural changes to minimize biased reporting as well as top-down structural reforms intended to reduce perverse incentives for doing so.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fda0812099547cd3b91031851f644e1929b4b77c" target='_blank'>
              Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations
              </a>
            </td>
          <td>
            N. McGreivy, Ammar Hakim
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Tipping points occur in many real-world systems, at which the system shifts suddenly from one state to another. The ability to predict the occurrence of tipping points from time series data remains an outstanding challenge and a major interest in a broad range of research fields. Particularly, the widely used methods based on bifurcation theory are neither reliable in prediction accuracy nor applicable for irregularly-sampled time series which are commonly observed from real-world systems. Here we address this challenge by developing a deep learning algorithm for predicting the occurrence of tipping points in untrained systems, by exploiting information about normal forms. Our algorithm not only outperforms traditional methods for regularly-sampled model time series but also achieves accurate predictions for irregularly-sampled model time series and empirical time series. Our ability to predict tipping points for complex systems paves the way for mitigation risks, prevention of catastrophic failures, and restoration of degraded systems, with broad applications in social science, engineering, and biology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6885f6f695d68eb9294efe8cb408c1d39a6fbe58" target='_blank'>
              Deep learning for predicting the occurrence of tipping points
              </a>
            </td>
          <td>
            Chengzuo Zhuge, Jiawei Li, Wei Chen
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Most scientific machine learning (SciML) applications of neural networks involve hundreds to thousands of parameters, and hence, uncertainty quantification for such models is plagued by the curse of dimensionality. Using physical applications, we show that $L_0$ sparsification prior to Stein variational gradient descent ($L_0$+SVGD) is a more robust and efficient means of uncertainty quantification, in terms of computational cost and performance than the direct application of SGVD or projected SGVD methods. Specifically, $L_0$+SVGD demonstrates superior resilience to noise, the ability to perform well in extrapolated regions, and a faster convergence rate to an optimal solution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ef84e51598bc76cbb9e56bd3fdb600206e852c2" target='_blank'>
              Improving the performance of Stein variational inference through extreme sparsification of physically-constrained neural network models
              </a>
            </td>
          <td>
            G. A. Padmanabha, J. Fuhg, C. Safta, Reese E. Jones, N. Bouklas
          </td>
          <td>2024-06-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Uncertainty quantification (UQ) is a crucial but challenging task in many high-dimensional regression or learning problems to increase the confidence of a given predictor. We develop a new data-driven approach for UQ in regression that applies both to classical regression approaches such as the LASSO as well as to neural networks. One of the most notable UQ techniques is the debiased LASSO, which modifies the LASSO to allow for the construction of asymptotic confidence intervals by decomposing the estimation error into a Gaussian and an asymptotically vanishing bias component. However, in real-world problems with finite-dimensional data, the bias term is often too significant to be neglected, resulting in overly narrow confidence intervals. Our work rigorously addresses this issue and derives a data-driven adjustment that corrects the confidence intervals for a large class of predictors by estimating the means and variances of the bias terms from training data, exploiting high-dimensional concentration phenomena. This gives rise to non-asymptotic confidence intervals, which can help avoid overestimating uncertainty in critical applications such as MRI diagnosis. Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning. Our findings bridge the gap between established theory and the practical applicability of such debiased methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b74e7097e6bf5e51f1391656df98c0c04de0d93" target='_blank'>
              Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning
              </a>
            </td>
          <td>
            Frederik Hoppe, C. M. Verdun, Hannah Laus, Felix Krahmer, Holger Rauhut
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dcaa44f8b49302272e5cab62125e644f57aefae1" target='_blank'>
              Characterizing the dynamics of multi-scale global high impact weather events
              </a>
            </td>
          <td>
            L. R. Frank, V. Galinsky, Zhenhai Zhang, F. M. Ralph
          </td>
          <td>2024-08-15</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In the field of machine learning, comprehending the intricate training dynamics of neural networks poses a significant challenge. This paper explores the training dynamics of neural networks, particularly whether these dynamics can be expressed in a general closed-form solution. We demonstrate that the dynamics of the gradient flow in two-layer narrow networks is not an integrable system. Integrable systems are characterized by trajectories confined to submanifolds defined by level sets of first integrals (invariants), facilitating predictable and reducible dynamics. In contrast, non-integrable systems exhibit complex behaviors that are difficult to predict. To establish the non-integrability, we employ differential Galois theory, which focuses on the solvability of linear differential equations. We demonstrate that under mild conditions, the identity component of the differential Galois group of the variational equations of the gradient flow is non-solvable. This result confirms the system's non-integrability and implies that the training dynamics cannot be represented by Liouvillian functions, precluding a closed-form solution for describing these dynamics. Our findings highlight the necessity of employing numerical methods to tackle optimization problems within neural networks. The results contribute to a deeper understanding of neural network training dynamics and their implications for machine learning optimization strategies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ddef324ecc792db4cfe809ea02647e7e5e7d877" target='_blank'>
              Absence of Closed-Form Descriptions for Gradient Flow in Two-Layer Narrow Networks
              </a>
            </td>
          <td>
            Yeachan Park
          </td>
          <td>2024-08-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many dynamical systems are subjected to stochastic influences, such as random excitations, noise, and unmodeled behavior. Tracking the system's state and parameters based on a physical model is a common task for which filtering algorithms, such as Kalman filters and their non-linear extensions, are typically used. However, many of these filters use assumptions on the transition probabilities or the covariance model, which can lead to inaccuracies in non-linear systems. We will show the application of a stochastic coupling filter that can approximate arbitrary transition densities under non-Gaussian noise. The filter is based on transport maps, which couple the approximation densities to a user-chosen reference density, allowing for straightforward sampling and evaluation of probabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/674a1134430bb9d26de4b9e60e39258373cc6b87" target='_blank'>
              Transport Map Coupling Filter for State-Parameter Estimation
              </a>
            </td>
          <td>
            J. Grashorn, M. Broggi, Ludovic Chamoin, Michael Beer
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Simulating Darcy flows in porous media is fundamental to understand the future flow behavior of fluids in hydrocarbon and carbon storage reservoirs. Geological models of reservoirs are often associated with high uncertainly leading to many numerical simulations for history matching and production optimization. Machine learning models trained with simulation data can provide a faster alternative to traditional simulators. In this paper we present a single Fourier Neural Operator (FNO) surrogate that outperforms traditional reservoir simulators by the ability to predict pressures and saturations on varying permeability fields, well locations, well controls, and number of wells. The maximum-mean relative error of 95\% of pressure and saturation predictions is less than 5\%. This is achieved by employing a simple yet very effective data augmentation technique that reduces the dataset size by 75\% and reduces overfitting. Also, constructing the input tensor in a binary fashion enables predictions on unseen well locations, well controls, and number of wells. Such model can accelerate history matching and reservoir characterization procedures by several orders of magnitude. The ability to predict on new well locations, well controls, and number of wells enables highly efficient reservoir management and optimization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7d7a6b2e3b8fbcb3a223ec6dc61bbed982599799" target='_blank'>
              Neural Operator-Based Proxy for Reservoir Simulations Considering Varying Well Settings, Locations, and Permeability Fields
              </a>
            </td>
          <td>
            Daniel Badawi, Eduardo Gildin
          </td>
          <td>2024-07-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We provide an approach enabling one to employ physics-informed neural networks (PINNs) for uncertainty quantification. Our approach is applicable to systems where observations are scarce (or even lacking), these being typical situations associated with subsurface water bodies. Our novel physics-informed neural network under uncertainty (PINN-UU) integrates the space-time domain across which processes take place and uncertain parameter spaces within a unique computational domain. PINN-UU is then trained to satisfy the relevant physical principles (e.g., mass conservation) in the defined input domain. We employ a stage training approach via transfer learning to accommodate high-dimensional solution spaces. We demonstrate the effectiveness of PINN-UU in a scenario associated with reactive transport in porous media, showcasing its reliability, efficiency, and applicability to sensitivity analysis. PINN-UU emerges as a promising tool for robust uncertainty quantification, with broad applicability to groundwater systems. As such, it can be considered as a valuable alternative to traditional methods such as multi-realization Monte Carlo simulations based on direct solvers or black-box surrogate models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9eb1778f55d55df5b6fa3d3b473be94da31915f8" target='_blank'>
              Modelling parametric uncertainty in PDEs models via Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Milad Panahi, G. Porta, M. Riva, A. Guadagnini
          </td>
          <td>2024-08-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>43</td>
        </tr>

        <tr id="A dynamical system may be defined by a simple transition law - such as a map or a vector field. Most learning techniques primarily try to recreate the dynamic evolution law. This is a major shortcoming, as most dynamic properties of interest are asymptotic properties such as an attractor or invariant measure. One of the major theoretical challenges for numerical methods is that approximating the dynamical law may not be sufficient to approximate these asymptotic properties. This article presents a method of representing a discrete-time deterministic dynamical system as a Markov process. The procedure is completely data-driven. The technique is proved to be convergent -- the stationary density of the Markov process has a support that converges to the targeted invariant set. Thus invariant sets of arbitrary dynamical systems, even with complicated non-smooth topology, can be approximated by this technique. Under further assumptions of stochastic stability of the targeted system, the technique is also shown to provide a convergent statistical approximation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7a9c8ed7750a7d4d14152ec7fe7c869a1655699" target='_blank'>
              Reconstructing dynamical systems as zero-noise limits
              </a>
            </td>
          <td>
            Suddhasattwa Das
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Echo state network (ESN) implements an alternative paradigm called reservoir computing to train recurrent neural networks (RNNs), where internal weights are randomly generated and kept fixed, and only readout weights need to be trained, which greatly reduces the training complexity of RNNs. ESN not only facilitates the practical implementation of RNNs but also shows superior performance over fully trained RNNs across a range of applications. However, the conventional ESN suffers from the drawbacks of stringent conditions for weight convergence and slow convergence speed. This paper proposes a memory regressor extended learning method to update the readout weights of ESNs. By constructing and incorporating a generalized prediction error based on regressor extension and filtering, the capacity of ESN to utilize historical data can be greatly improved. In the discrete-time domain, it is proven that exponential convergence of readout weights is achieved under a condition termed interval excitation that is strictly weaker than the classical condition of persistent excitation. Simulation results on modeling a 10th-order nonlinear autoregressive moving-average (NARMA) system have revealed that the proposed approach accelerates weight convergence speed almost ten times higher compared to the conventional ESN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e0fa7b7dfe357970b3e72acb8d86266f028d299" target='_blank'>
              Memory Regressor Extended Echo State Networks for Nonlinear Dynamics Modeling
              </a>
            </td>
          <td>
            Kai Hu, Qian Wang, Tian Shi, Kohei Nakajima, Yongping Pan
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This work proposes a data‐driven state observation algorithm for nonlinear dynamical systems, when the true state trajectory is not measurable and hence the states information needs to be reconstructed from input and output measurements. Such a reduction is formed by kernel canonical correlation analysis (KCCA), which (i) implicitly maps the available input–output data into a higher‐dimensional feature space, namely the reproducing kernel Hilbert space (RKHS); (ii) finds a projection of the past input–output data and a projection of the future input–output data with maximal correlation; and (iii) identifies the projected inputs and outputs, namely the canonical variates, as the observed states. We adopt a least squares support vector machine (LS‐SVM) formulation for KCCA, which imposes regularization on the vectors that specify the projections and is amenable to convex optimization. We prove theoretically that, based on the statistical consistency of KCCA, the observed states determined by the proposed state observer has a guaranteed correlativity with the actual states (when properly transformed). Furthermore, such observed states, when supplemented with the information of succeeding inputs, can be used to predict the succeeding outputs with guaranteed upper bound on the prediction error. Case studies are performed on two numerical examples and an exothermic continuously stirred tank reactor (CSTR).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afc53bd0e86a21cb48431210cc83dc9943ac1742" target='_blank'>
              Data‐driven nonlinear state observation for controlled systems: A kernel method and its analysis
              </a>
            </td>
          <td>
            Moritz Woelk, Wentao Tang
          </td>
          <td>2024-07-08</td>
          <td>The Canadian Journal of Chemical Engineering</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Accurate estimate of long-term risk is critical for safe decision-making, but sampling from rare risk events and long-term trajectories can be prohibitively costly. Risk gradient can be used in many first-order techniques for learning and control methods, but gradient estimate is difficult to obtain using Monte Carlo (MC) methods because the infinitesimal divisor may significantly amplify sampling noise. Motivated by this gap, we propose an efficient method to evaluate long-term risk probabilities and their gradients using short-term samples without sufficient risk events. We first derive that four types of long-term risk probability are solutions of certain partial differential equations (PDEs). Then, we propose a physics-informed learning technique that integrates data and physics information (aforementioned PDEs). The physics information helps propagate information beyond available data and obtain provable generalization beyond available data, which in turn enables long-term risk to be estimated using short-term samples of safe events. Finally, we demonstrate in simulation that the proposed technique has improved sample efficiency, generalizes well to unseen regions, and adapts to changing system parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b4b5d11c4861e28deddce08df1d92f4a1c23b924" target='_blank'>
              Generalizable Physics-Informed Learning for Stochastic Safety-Critical Systems
              </a>
            </td>
          <td>
            Zhuoyuan Wang, Albert Chern, Yorie Nakahira
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Cyclostationary linear inverse models (CS-LIMs), generalized versions of the classical (stationary) LIM, are advanced data-driven techniques for extracting the first-order time-dependent dynamics and random forcing relevant information from complex non-linear stochastic processes. Though CS-LIMs lead to a breakthrough in climate sciences, their mathematical background and properties are worth further exploration. This study focuses on the mathematical perspective of CS-LIMs and introduces two variants: e-CS-LIM and l-CS-LIM. The former refines the original CS-LIM using the interval-wise linear Markov approximation, while the latter serves as an analytic inverse model for the linear periodic stochastic systems. Although relying on approximation, e-CS-LIM converges to l-CS-LIM under specific conditions and shows noise-robust performance. Numerical experiments demonstrate that each CS-LIM reveals the temporal structure of the system. The e-CS-LIM optimizes the original model for better dynamics performance, while l-CS-LIM excels in diffusion estimation due to reduced approximation reliance. Moreover, CS-LIMs are applied to real-world ENSO data, yielding a consistent result aligning with observations and current ENSO understanding.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7f20f17be02b5ed8df3e9de809737832dc15d7e7" target='_blank'>
              On the Cyclostationary Linear Inverse Models: A Mathematical Insight and Implication
              </a>
            </td>
          <td>
            Justin Lien, Yan-Ning Kuo, Hiroyasu Ando
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Koopman operator framework is a promising direction of analysis and synthesis of systems with nonlinear dynamics based on (linear) Koopman operators. In this paper, we address the resolvent of a Koopman operator for a nonlinear autonomous discrete-time system, which we call the Koopman resolvent, and its identification problem. First, we show that for the nonlinear system with a scalar-valued output, the $z$- transform of the output is represented by the action of Koop-man resolvent. Second, we describe an identification method of the Koopman resolvent directly from time-series data of the output, in which we estimate parameters of the resolvent as well as poles and residues of the z-transform of the output. By combining the so-called frequency-domain Prony method with the Vandermonde-Cauchy form in the Dynamic Mode Decomposition (DMD), we propose the method which we call the frequency-domain DMD, in which all the unknowns can be estimated in the frequency domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/df691892a89d4f0c1e7b43d373b04818f75bcc0b" target='_blank'>
              Koopman Resolvents of Nonlinear Discrete-Time Systems: Formulation and Identification
              </a>
            </td>
          <td>
            Yoshihiko Susuki, A. Mauroy, Z. Drmač
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="We propose an $\alpha$-separable graph Hamiltonian network ($\alpha$-SGHN) that reveals complex interaction patterns between particles in lattice systems. Utilizing trajectory data, $\alpha$-SGHN infers potential interactions without prior knowledge about particle coupling, overcoming the limitations of traditional graph neural networks that require predefined links. Furthermore, $\alpha$-SGHN preserves all conservation laws during trajectory prediction. Experimental results demonstrate that our model, incorporating structural information, outperforms baseline models based on conventional neural networks in predicting lattice systems. We anticipate that the results presented will be applicable beyond the specific onsite and inter-site interaction lattices studied, including the Frenkel-Kontorova model, the rotator lattice, and the Toda lattice.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fa79f8d7dee50b48f0bb37e6b9aef968f5166f29" target='_blank'>
              $\alpha$-SGHN: A Robust Model for Learning Particle Interactions in Lattice Systems
              </a>
            </td>
          <td>
            Yixian Gao, R. Geng, Panayotis Kevrekidis, Hong-Kun Zhang, Jian Zu
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We propose a meta-learning method for modeling Hamiltonian dynamics from a limited number of data. Although Hamiltonian neural networks have been successfully used for modeling dynamics that obey the energy conservation law, they require many data to achieve high performance. The proposed method meta-learns our neural network-based model using datasets in various dynamical systems, such that our model can predict vector fields of unseen systems. In our model, a system representation is inferred from given small data using an encoder network. Then, the system-specific vector field is predicted by modeling the Hamiltonian using a Gaussian process (GP) with neural network-based mean and kernel functions that depend on the inferred system representation. This GP-based Hamiltonian allows us to analytically obtain predictions that are adapted to small data while imposing the constraint of the conservation law. The neural networks are shared across systems, which enables us to learn knowledge from multiple systems, and use it for unseen systems. In our experiments, we demonstrate that the proposed method outperforms existing methods for predicting dynamics from a small number of observations in target systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37ac3a5b751bd136ecb4369b5f0c68f06bf4a91f" target='_blank'>
              Symplectic Neural Gaussian Processes for Meta-learning Hamiltonian Dynamics
              </a>
            </td>
          <td>
            Tomoharu Iwata, Yusuke Tanaka
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="This paper is aimed at using the newly developing field of physics informed machine learning (PIML) to develop models for predicting the remaining useful lifetime (RUL) aircraft engines. We consider the well-known benchmark NASA Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) data as the main data for this paper, which consists of sensor outputs in a variety of different operating modes. C-MAPSS is a well-studied dataset with much existing work in the literature that address RUL prediction with classical and deep learning methods. In the absence of published empirical physical laws governing the C-MAPSS data, our approach first uses stochastic methods to estimate the governing physics models from the noisy time series data. In our approach, we model the various sensor readings as being governed by stochastic differential equations, and we estimate the corresponding transition density mean and variance functions of the underlying processes. We then augment LSTM (long-short term memory) models with the learned mean and variance functions during training and inferencing. Our PIML based approach is different from previous methods, and we use the data to first learn the physics. Our results indicate that PIML discovery and solutions methods are well suited for this problem and outperform previous data-only deep learning methods for this data set and task. Moreover, the framework developed herein is flexible, and can be adapted to other situations (other sensor modalities or combined multi-physics environments), including cases where the underlying physics is only partially observed or known.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/47c68f22e1c160e3cdac30be6f54af8a48a8eccd" target='_blank'>
              Physics Informed Machine Learning (PIML) methods for estimating the remaining useful lifetime (RUL) of aircraft engines
              </a>
            </td>
          <td>
            Sriram Nagaraj, Truman Hickok
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Physics informed neural networks (PINNs) have effectively demonstrated the ability to approximate the solutions of a system of partial differential equations (PDEs) by embedding the governing equations and auxiliary conditions directly into the loss function using automatic differentiation. Despite demonstrating potential across diverse applications, PINNs have encountered challenges in accurately predicting solutions for time-dependent problems. In response, this study presents a novel methodology aimed at enhancing the predictive capability of PINNs for time-dependent scenarios. Our approach involves dividing the temporal domain into multiple subdomains and employing an adaptive weighting strategy at the initial condition and at the interfaces between these subdomains. By employing such interfacial conditioning in physics informed neural networks (IcPINN), we have solved several unsteady PDEs (e.g., Allen–Cahn equation, advection equation, Korteweg–De Vries equation, Cahn–Hilliard equation, and Navier–Stokes equations) and conducted a comparative analysis with numerical results. The results have demonstrated that IcPINN was successful in obtaining highly accurate results in each case without the need for using any labeled data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c70375c9f4682ea136b267ae844b868bd46ae3c" target='_blank'>
              Interfacial conditioning in physics informed neural networks
              </a>
            </td>
          <td>
            Saykat Kumar Biswas, N. K. Anand
          </td>
          <td>2024-07-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The objective of system identification is to derive models from input/output data. To extend advancements in regularization techniques for linear finite impulse response (FIR) models to the nonlinear domain, we employ local model networks (LMNs) with locally regularized FIR models to identify nonlinear processes. The training of the LMN is performed using the local linear model tree (LOLIMOT) algorithm, resulting in both the partitioning of the model space and the estimation of the corresponding linear local models for each region. One key advantage of this algorithm lies in its ability to create separate input spaces for the linear models (x-space) and the validity functions (z-space) comprising the partitioning. While the most straightforward choice (x-space = z-space) results in an extremely high-dimensional z-space for local FIR models, we ad-dress this drawback by proposing different z-spaces spanned by the input spaces of autoregressive with exogenous inputs (ARX) models or Laguerre filter models, respectively. The theoretical capabilities for the proposed z-spaces are characterized. The superiority in terms of computation time, as well as comparable performance for Laguerre z-spaces and FIR z-spaces, is demon-strated through numerical examples. Additionally, the limitations for the utilization of ARX z-spaces are highlighted. Finally, all z-spaces have been evaluated on real-world data of a Wiener-Hammerstein benchmark. The FIR and Laguerre z- space showed comparable performance, while the ARX z-space performed worse.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d7f2bb8d2c64f0e73154c706dc8921696b583cb" target='_blank'>
              On the Choice of the Scheduling Variables for Dynamic Local Model Networks with Local Regularized FIR Models
              </a>
            </td>
          <td>
            Christopher Illg, T. Kösters, Oliver Nelles
          </td>
          <td>2024-06-30</td>
          <td>2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The task of sampling from a probability density can be approached as transporting a tractable density function to the target, known as dynamical measure transport. In this work, we tackle it through a principled unified framework using deterministic or stochastic evolutions described by partial differential equations (PDEs). This framework incorporates prior trajectory-based sampling methods, such as diffusion models or Schr\"odinger bridges, without relying on the concept of time-reversals. Moreover, it allows us to propose novel numerical methods for solving the transport task and thus sampling from complicated targets without the need for the normalization constant or data samples. We employ physics-informed neural networks (PINNs) to approximate the respective PDE solutions, implying both conceptional and computational advantages. In particular, PINNs allow for simulation- and discretization-free optimization and can be trained very efficiently, leading to significantly better mode coverage in the sampling task compared to alternative methods. Moreover, they can readily be fine-tuned with Gauss-Newton methods to achieve high accuracy in sampling.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be6b71f84d3c641029effa69aa23e0c32eb9de03" target='_blank'>
              Dynamical Measure Transport and Neural PDE Solvers for Sampling
              </a>
            </td>
          <td>
            Jingtong Sun, Julius Berner, Lorenz Richter, Marius Zeinhofer, Johannes Müller, K. Azizzadenesheli, A. Anandkumar
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Latent dynamical systems have been widely used to characterize the dynamics of neural population activity in the brain. However, these models typically ignore the fact that the brain contains multiple cell types. This limits their ability to capture the functional roles of distinct cell classes, or to accurately predict the effects of cell-specific optogenetic perturbations on neural activity or behavior. To overcome these limitations, we introduce the “cell-type dynamical systems” (CTDS) model. This model extends latent linear dynamical systems to contain distinct latent variables for each cell class, with biologically inspired constraints on both dynamics and emissions. To illustrate our approach, we consider neural recordings with distinct excitatory (E) and inhibitory (I) populations. The CTDS model defines separate latents for E and I cells, and constrains the dynamics so that E (I) latents have a strictly positive (negative) effects on other latents. We applied CTDS to recordings from rat frontal orienting fields (FOF) and anterior dorsal striatum (ADS) during an auditory decision-making task. The model achieved higher accuracy than a standard linear dynamical system (LDS), and revealed that both E and I latents could be used to decode the animal’s choice, showing that choice-related information is not restricted to a single cell class. We also performed in-silico optogenetic perturbation experiments in the FOF and ADS, and found that CTDS was able to replicate the causal effects of different perturbations on behavior, whereas a standard LDS model—which lacks the ability to capture cell-specific perturbations—did not. Crucially, our model allowed us to understand the effects of these perturbations by revealing the dynamics of different cell-specific latents. Finally, CTDS can also be used to identify cell types for neurons whose class labels are unknown in electrophysiological recordings. These results illustrate the power of the CTDS model to provide more accurate and more biologically interpretable descriptions of neural population dynamics and their relationship to behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/31c1cd3d2f0ffab7d6fe212ee7ca7fdd655d8dfa" target='_blank'>
              Disentangling the Roles of Distinct Cell Classes with Cell-Type Dynamical Systems
              </a>
            </td>
          <td>
            Aditi Jha, Diksha Gupta, C. Brody, Jonathan W. Pillow
          </td>
          <td>2024-07-11</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>45</td>
        </tr>

        <tr id="Due to their flexibility and ease of use, Dynamical Movement Primitives (DMPs) are widely used in robotics applications and research. DMPs combine linear dynamical systems to achieve robustness to perturbations and adaptation to moving targets with non-linear function approximators to fit a wide range of demonstrated trajectories.We propose a novel DMP formulation with a generalized logistic function as a delayed goal system. This formulation inherently has low initial jerk, and generates the bell-shaped velocity profiles that are typical of human movement. As the novel formulation is more expressive, it is able to fit a wide range of human demonstrations well, also without a non-linear forcing term. We exploit this increased expressiveness by automating the fitting of the dynamical system parameters through opti-mization. Our experimental evaluation demonstrates that this optimization regularizes the forcing term, and improves the interpolation accuracy of parametric DMPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/904213d0ea3643d20cba8b38f58690176de07076" target='_blank'>
              Fitting Parameters of Linear Dynamical Systems to Regularize Forcing Terms in Dynamical Movement Primitives
              </a>
            </td>
          <td>
            F. Stulp, Adrià Colomé, Carme Torras
          </td>
          <td>2024-05-13</td>
          <td>2024 IEEE International Conference on Robotics and Automation (ICRA)</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) is a fundamental problem in engineering and science. While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active Learning (AL) could help surrogate models reach the same accuracy with smaller training sets by querying classical solvers with more informative initial conditions and PDE parameters. While AL is more common in other domains, it has yet to be studied extensively for neural PDE solvers. To bridge this gap, we introduce AL4PDE, a modular and extensible active learning benchmark. It provides multiple parametric PDEs and state-of-the-art surrogate models for the solver-in-the-loop setting, enabling the evaluation of existing and the development of new AL methods for PDE solving. We use the benchmark to evaluate batch active learning algorithms such as uncertainty- and feature-based methods. We show that AL reduces the average error by up to 71% compared to random sampling and significantly reduces worst-case errors. Moreover, AL generates similar datasets across repeated runs, with consistent distributions over the PDE parameters and initial conditions. The acquired datasets are reusable, providing benefits for surrogate models not involved in the data generation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1dd92c53455982680c81f73c1708da2a22eaa764" target='_blank'>
              Active Learning for Neural PDE Solvers
              </a>
            </td>
          <td>
            Daniel Musekamp, Marimuthu Kalimuthu, David Holzmüller, Makoto Takamoto, Mathias Niepert
          </td>
          <td>2024-08-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="We study nonequilibrium quantum dynamics of spin chains by employing principal component analysis (PCA) on data sets of wave function snapshots and examine how information propagates within these data sets. The quantities we employ are derived from the spectrum of the sample second moment matrix, built directly from data sets. Our investigations on several interacting spin chains featuring distinct spin or energy transport reveal that the growth of data information spreading follows the same dynamical exponents as that of the underlying quantum transport of spin or energy. Specifically, our approach enables an easy, data-driven, and importantly interpretable diagnostic to track energy transport with a limited number of samples, which is usually challenging without any assumption on the Hamiltonian form. These observations are obtained at a modest finite size and evolution time, which aligns with experimental and numerical constraints. Our framework directly applies to experimental quantum simulator data sets of dynamics in higher-dimensional systems, where classical simulation methods usually face significant limitations and apply equally to both near- and far-from-equilibrium quenches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/94b4a5b7b344b91f9543dbd6c577fd548f273348" target='_blank'>
              Diagnosing quantum transport from wave function snapshots
              </a>
            </td>
          <td>
            D. S. Bhakuni, Roberto Verdel, Cristiano Muzzi, Riccardo Andreoni, Monika Aidelsburger, Marcello Dalmonte
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="To understand and control the dynamics of coupled oscillators, it is important to reveal the structure of the interaction network from observed data. While various techniques have been developed for inferring the network of asynchronous systems, it remains challenging to infer the network of synchronized oscillators without external stimulations. In this study, we develop a method for non-invasively inferring the network of synchronized and/or de-synchronized oscillators. An approach to network inference would be to fit the data to a set of differential equations describing the dynamics of phase oscillators. However, we show that this method fails to infer the true network due to the problems that arise when we use short-time phase differences. Therefore, we propose a method based on the circle map, which describes the phase change in one oscillatory cycle. We demonstrate the efficacy of the proposed method through the successful inference of the network structure from simulated data of limit cycle oscillator models. Our method provides a unified and concise framework for network estimation for a wide class of oscillator systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a85617c5e51b81ee7f45457c35d92af54f040aed" target='_blank'>
              Network inference from oscillatory signals based on circle map
              </a>
            </td>
          <td>
            Akari Matsuki, Hiroshi Kori, Ryota Kobayashi
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Physics-Informed Neural Networks (PINNs) have emerged as a robust framework for solving Partial Differential Equations (PDEs) by approximating their solutions via neural networks and imposing physics-based constraints on the loss function. Traditionally, Multilayer Perceptrons (MLPs) are the neural network of choice, and significant progress has been made in optimizing their training. Recently, Kolmogorov-Arnold Networks (KANs) were introduced as a viable alternative, with the potential of offering better interpretability and efficiency while requiring fewer parameters. In this paper, we present a fast JAX-based implementation of grid-dependent Physics-Informed Kolmogorov-Arnold Networks (PIKANs) for solving PDEs. We propose an adaptive training scheme for PIKANs, incorporating known MLP-based PINN techniques, introducing an adaptive state transition scheme to avoid loss function peaks between grid updates, and proposing a methodology for designing PIKANs with alternative basis functions. Through comparative experiments we demonstrate that these adaptive features significantly enhance training efficiency and solution accuracy. Our results illustrate the effectiveness of PIKANs in improving performance for PDE solutions, highlighting their potential as a superior alternative in scientific and engineering applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c1aaa25c69658448f0abd4e2430cf6925f39fdd5" target='_blank'>
              Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Spyros Rigas, M. Papachristou, Theofilos Papadopoulos, Fotios Anagnostopoulos, Georgios Alexandridis
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="The research topic is: data-driven Bayesian state estimation with compressed measurement (BSCM) of model-free process, say for a (causal) tracking application. The dimension of the temporal measurement vector is lower than the dimension of the temporal state vector to be estimated. Hence the state estimation problem is an underdetermined inverse problem. The state-space-model (SSM) of the underlying dynamical process is assumed to be unknown and hence, we use the terminology 'model-free process'. In absence of the SSM, we can not employ traditional model-driven methods like Kalman Filter (KF) and Particle Filter (PF) and instead require data-driven methods. We first experimentally show that two existing unsupervised learning-based data-driven methods fail to address the BSCM problem for model-free process; they are data-driven nonlinear state estimation (DANSE) method and deep Markov model (DMM) method. The unsupervised learning uses unlabelled data comprised of only noisy measurements. While DANSE provides a good predictive performance to model the temporal measurement data as time-series, its unsupervised learning lacks a regularization for state estimation. We then investigate use of a semi-supervised learning approach, and develop a semi-supervised learning-based DANSE method, referred to as SemiDANSE. In the semi-supervised learning, we use a limited amount of labelled data along-with a large amount of unlabelled data, and that helps to bring the desired regularization for BSCM problem in the absence of SSM. The labelled data means pairwise measurement-and-state data. Using three chaotic dynamical systems (or processes) with nonlinear SSMs as benchmark, we show that the data-driven SemiDANSE provides competitive performance for BSCM against three SSM-informed methods - a hybrid method called KalmanNet, and two traditional model-driven methods called extended KF and unscented KF.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2671168e70fb8ea79bbef366bd1d59fc9ae9415" target='_blank'>
              Data-driven Bayesian State Estimation with Compressed Measurement of Model-free Process using Semi-supervised Learning
              </a>
            </td>
          <td>
            Anubhab Ghosh, Y. Eldar, Saikat Chatterjee
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4063dac20e1d0f5e11b6e07992c712df9558080d" target='_blank'>
              Latent Diffusion for Neural Spiking Data
              </a>
            </td>
          <td>
            J. Kapoor, Auguste Schulz, Julius Vetter, Felix Pei, Richard Gao, J. H. Macke
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This paper proposes a fully data-driven approach for optimal control of nonlinear control-affine systems represented by a stochastic diffusion. The focus is on the scenario where both the nonlinear dynamics and stage cost functions are unknown, while only control penalty function and constraints are provided. Leveraging the theory of reproducing kernel Hilbert spaces, we introduce novel kernel mean embeddings (KMEs) to identify the Markov transition operators associated with controlled diffusion processes. The KME learning approach seamlessly integrates with modern convex operator-theoretic Hamilton-Jacobi-Bellman recursions. Thus, unlike traditional dynamic programming methods, our approach exploits the ``kernel trick'' to break the curse of dimensionality. We demonstrate the effectiveness of our method through numerical examples, highlighting its ability to solve a large class of nonlinear optimal control problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69ffcc674bf67738c50bad11d4a1a6996a25767b" target='_blank'>
              Data-Driven Optimal Feedback Laws via Kernel Mean Embeddings
              </a>
            </td>
          <td>
            Petar Bevanda, Nicolas Hoischen, Stefan Sosnowski, Sandra Hirche, Boris Houska
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="We introduce data to predictive control, D2PC, a framework to facilitate the design of robust and predictive controllers from data. The proposed framework is designed for discrete-time stochastic linear systems with output measurements and provides a principled design of a predictive controller based on data. The framework starts with a parameter identification method based on the Expectation-Maximization algorithm, which incorporates pre-defined structural constraints. Additionally, we provide an asymptotically correct method to quantify uncertainty in parameter estimates. Next, we develop a strategy to synthesize robust dynamic output-feedback controllers tailored to the derived uncertainty characterization. Finally, we introduce a predictive control scheme that guarantees recursive feasibility and satisfaction of chance constraints. This framework marks a significant advancement in integrating data into robust and predictive control schemes. We demonstrate the efficacy of D2PC through a numerical example involving a $10$-dimensional spring-mass-damper system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ea52a361e1eddc6fae9770357307a43f124b006" target='_blank'>
              From Data to Predictive Control: A Framework for Stochastic Linear Systems with Output Measurements
              </a>
            </td>
          <td>
            Haldun Balim, Andrea Carron, M. Zeilinger, Johannes Kohler
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="We study the model reduction by moment matching problem for linear systems in a data-driven framework. We show that reduced-order models can be directly computed from data without knowledge of the structure of the signal generator or of its internal state. The reduced-order models thus obtained match the moments of the unknown underlying system asymptotically. Our construction provides a simple way to enforce additional constraints in the reduced-order model. We demonstrate the applicability of the results using data from a high-dimensional model of a building.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83399652779edc533b785a20de6431d8d0060e66" target='_blank'>
              Data-Driven Model Reduction by Moment Matching for Linear Systems Driven by an Unknown Implicit Signal Generator
              </a>
            </td>
          <td>
            Debraj Bhattacharjee, Alessandro Astolfi
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Computer simulations based on partial differential equations (PDEs) describing physical phenomena, are widely used for analyzing the performance of high voltage insulation. Such simulation models require access to physical model parameters, which are often hard to obtain. The purpose of the present paper is to explore how Physics-Informed Neural Network (PINN) tailored for specific PDEs, can be used to extract relevant physical parameters from synthetic data. Electric potential or charge distributions in 1-dimensional coaxial and Cartesian domains were used as training data for PINN models. The following physical parameters were successfully extracted: (i) space charge profiles, (ii) mobilities of ions, and (iii) the radius of the inner electrode in the coaxial geometry. The impact of noisy training data on the accuracy was also studied for the different cases. The main finding is that PINN models can successfully be used to extract model parameters for electrical charge transport problems using synthetic data as input. This approach has large potential to strengthen the research on charge dynamics in gas, liquid, and solid insulation as well as other topics related to high voltage insulation systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f9d44af663a8e4f7e16a2dffe2c289b99726e746" target='_blank'>
              Usage of Physics-Informed Neural Network to Extract Physical Parameters From High Voltage Experiments
              </a>
            </td>
          <td>
            O. Hjortstam, C. Björnson, F. Ågren, T. Hammarström, Y. V. Serdyuk, C. Häger
          </td>
          <td>2024-06-30</td>
          <td>2024 IEEE 5th International Conference on Dielectrics (ICD)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We apply two data assimilation (DA) methods, a smoother and a filter, and a model-free machine learning (ML) shallow network to forecast two weakly turbulent systems. We analyse the effect of the spatial sparsity of observations on accuracy of the predictions obtained from these data-driven methods. Based on the results, we divide the spatial sparsity levels in three zones. First is the good-predictions zone in which both DA and ML methods work. We find that in the good-predictions zone the observations remain dense enough to accurately capture the fractal manifold of the system's dynamics, which is measured using the correlation dimension. The accuracy of the DA methods in this zone remains almost as good as for full-resolution observations. Second is the reasonable-predictions zone in which the DA methods still work but at reduced prediction accuracy. Third is the bad-predictions zone in which even the DA methods fail. We find that the sparsity level up to which the DA methods work is almost the same up to which chaos synchronisation of these systems can be achieved. The main implications of these results are that they (i) firmly establish the spatial resolution up to which the data-driven methods can be utilised, (ii) provide measures to determine if adding more sensors will improve the predictions, and (iii) quantify the advantage (in terms of the required measurement resolution) of using the governing equations within data-driven methods. We also discuss the applicability of these results to fully developed turbulence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f37955fcf738b892c9cedb2952e279d466fad9b8" target='_blank'>
              Predictability of weakly turbulent systems from spatially sparse observations using data assimilation and machine learning
              </a>
            </td>
          <td>
            Vikrant Gupta, Yuanqing Chen, Minping Wan
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Macroscopic features of dynamical systems such as almost-invariant sets and coherent sets provide crucial high-level information on how the dynamics organises phase space. We introduce a method to identify time-parameterised families of almost-invariant sets in time-dependent dynamical systems, as well as the families' emergence and disappearance. In contrast to coherent sets, which may freely move about in phase space over time, our technique focuses on families of metastable sets that are quasi-stationary in space. Our straightforward approach extends successful transfer operator methods for almost-invariant sets to time-dependent dynamics and utilises the Ulam scheme for the generator of the transfer operator on a time-expanded domain. The new methodology is illustrated with an idealised fluid flow and with atmospheric velocity data. We identify atmospheric blocking events in the 2003 European heatwave and compare our technique to existing geophysical methods of blocking diagnosis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85cdef684e76e594fd24e0e2863fe622d0a8314d" target='_blank'>
              Identifying the onset and decay of quasi-stationary families of almost-invariant sets with an application to atmospheric blocking events
              </a>
            </td>
          <td>
            Aleksandar Badza, Gary Froyland School of Mathematics, Statistics Unsw Sydney Nsw Australia
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="High-precision control for nonlinear systems is impeded by the low-fidelity dynamical model and external disturbance. Especially, the intricate coupling between internal uncertainty and external disturbance is usually difficult to be modeled explicitly. Here we show an effective and convergent algorithm enabling accurate estimation of the coupled disturbance via combining control and learning philosophies. Specifically, by resorting to Chebyshev series expansion, the coupled disturbance is firstly decomposed into an unknown parameter matrix and two known structures depending on system state and external disturbance respectively. A Regularized Least Squares (RLS) algorithm is subsequently formalized to learn the parameter matrix by using historical time-series data. Finally, a higher-order disturbance observer (HODO) is developed to achieve a high-precision estimation of the coupled disturbance by utilizing the learned portion. The efficiency of the proposed algorithm is evaluated through extensive simulations. We believe this work can offer a new option to merge learning schemes into the control framework for addressing existing intractable control problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c2b582f6ad335402a40cb9dffeaea1baf4781dd" target='_blank'>
              Disturbance Observer for Estimating Coupled Disturbances
              </a>
            </td>
          <td>
            Jindou Jia, Yuhang Liu, Kexin Guo, Xiang Yu, Lihua Xie, Lei Guo
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Filtering - the task of estimating the conditional distribution of states of a dynamical system given partial, noisy, observations - is important in many areas of science and engineering, including weather and climate prediction. However, the filtering distribution is generally intractable to obtain for high-dimensional, nonlinear systems. Filters used in practice, such as the ensemble Kalman filter (EnKF), are biased for nonlinear systems and have numerous tuning parameters. Here, we present a framework for learning a parameterized analysis map - the map that takes a forecast distribution and observations to the filtering distribution - using variational inference. We show that this methodology can be used to learn gain matrices for filtering linear and nonlinear dynamical systems, as well as inflation and localization parameters for an EnKF. Future work will apply this framework to learn new filtering algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0caedb45ee3429ec36e56edd056c7ae412f7b6b8" target='_blank'>
              Learning Optimal Filters Using Variational Inference
              </a>
            </td>
          <td>
            Enoch Luk, Eviatar Bach, Ricardo Baptista, Andrew Stuart
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>10</td>
        </tr>

        <tr id="A data-driven approach to calculating tight-binding models for discrete coupled-mode systems is presented. Specifically, spectral and topological data is used to build an appropriate discrete model that accurately replicates these properties. This work is motivated by topological insulator systems that are often described by tight-binding models. The problem is formulated as the minimization of an appropriate residual (objective) function. Given bulk spectral data and a topological index (e.g. winding number), an appropriate discrete model is obtained to arbitrary precision. A nonlinear least squares method is used to determine the coefficients. The effectiveness of the scheme is highlighted against a Schr\"odinger equation with a periodic potential that can be described by the Su-Schrieffer-Heeger model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b260d38c46caaea8a67dc40a10c56986fd078985" target='_blank'>
              Data-driven approximations of topological insulator systems
              </a>
            </td>
          <td>
            Justin T. Cole, Michael J. Nameika
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
Fusing known physics into data-driven learners allows modelling practitioners to combine the expressive power of traditional machine learning with known mechanistic laws, where the objective is to enhance predictive performance, interpretability, and model generalisation. A core consideration that must be made when implementing a physics-informed learning architecture is how relevant knowledge will be embedded into the model structure, which, generally, is informed by the type of physics that is available. Frequently this knowledge may not be complete, with only a partial understanding of the governing physics available. 

In this work, possible paths for deriving Gaussian process kernels that are representative of partial knowledge will be considered. How the type of knowledge that is possessed influences the derivation will be explored, particularly when there is the potential for some aspect of misspecified physics. An example of deriving partially structured kernels will be investigated for modelling the decoupled response of a GARTEUR laboratory aircraft structure, where the derived kernels are used to decompose the dynamics of the aircraft into modal contributions. 

">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/da9bbf07470b8cc6d6606c4b51a040b8e45ce3d6" target='_blank'>
              Gaussian process kernels for partial physical insight
              </a>
            </td>
          <td>
            Matthew R. Jones, D. J. Pitchforth, E. J. Cross
          </td>
          <td>2024-07-01</td>
          <td>e-Journal of Nondestructive Testing</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In recent decades, the utilization of machine learning (ML) and artificial intelligence (AI) approaches have been explored for process modelling applications. However, different types of ML models may have contrasting advantages and disadvantages, which become critical during the optimal selection of a specific data‐driven model for a particular application as well as estimation of parameters during model training. This paper compares and contrasts two different types of data‐driven modelling approaches, namely the series/parallel all‐nonlinear static‐dynamic neural network models and models from a Bayesian ML approach. Both types of AI modelling approaches considered in this work have shown to significantly outperform several state‐of‐the‐art steady‐state and dynamic data‐driven modelling techniques for various performance measures, specifically, model sparsity, predictive capabilities, and computational expense. The performances of the proposed model structures and algorithms have been evaluated for two nonlinear dynamic chemical engineering systems—a plug‐flow reactor for vapour phase cracking of acetone for production of acetic anhydride and a pilot‐plant for post‐combustion CO2 capture using monoethanolamine as the solvent. For the validation data from the CO2 capture pilot plant, root mean squared error (RMSE) for flue gas outlet temperature, flowrate and CO2 concentration is 0.05%, 1.07%, and 5.0%, respectively, for the all‐nonlinear static‐dynamic neural networks and 0.1%, 1.75%, and 14.14%, respectively, for the Bayesian ML models. For the plug flow reactor data, the Bayesian ML models yield superior RMSE compared to the all‐nonlinear static‐dynamic neural networks when the measurement data are corrupted with Gaussian, auto‐correlated, or cross‐correlated noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/056ceb214531414b85db491a54767e185fb5c38a" target='_blank'>
              All‐nonlinear static‐dynamic neural networks versus Bayesian machine learning for data‐driven modelling of chemical processes
              </a>
            </td>
          <td>
            Angan Mukherjee, Samuel Adeyemo, D. Bhattacharyya
          </td>
          <td>2024-06-24</td>
          <td>The Canadian Journal of Chemical Engineering</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Equations of State model relations between thermodynamic variables and are ubiquitous in scientific modelling, appearing in modern day applications ranging from Astrophysics to Climate Science. The three desired properties of a general Equation of State model are adherence to the Laws of Thermodynamics, incorporation of phase transitions, and multiscale accuracy. Analytic models that adhere to all three are hard to develop and cumbersome to work with, often resulting in sacrificing one of these elements for the sake of efficiency. In this work, two deep-learning methods are proposed that provably satisfy the first and second conditions on a large-enough region of thermodynamic variable space. The first is based on learning the generating function (thermodynamic potential) while the second is based on structure-preserving, symplectic neural networks, respectively allowing modifications near or on phase transition regions. They can be used either"from scratch"to learn a full Equation of State, or in conjunction with a pre-existing consistent model, functioning as a modification that better adheres to experimental data. We formulate the theory and provide several computational examples to justify both approaches, and highlight their advantages and shortcomings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6515ce69075cc8fe93c06073e12dcf389d87c37b" target='_blank'>
              Neural Network Representations of Multiphase Equations of State
              </a>
            </td>
          <td>
            George A. Kevrekidis, Daniel A. Serino, Alexander Kaltenborn, J. Gammel, J. Burby, Marc L. Klasky
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="The paper presents a framework for online learning of the Koopman operator using streaming data. Many complex systems for which data-driven modeling and control are sought provide streaming sensor data, the abundance of which can present computational challenges but cannot be ignored. Streaming data can intermittently sample dynamically different regimes or rare events which could be critical to model and control. Using ideas from subspace identification, we present a method where the Grassmannian distance between the subspace of an extended observability matrix and the streaming segment of data is used to assess the `novelty' of the data. If this distance is above a threshold, it is added to an archive and the Koopman operator is updated if not it is discarded. Therefore, our method identifies data from segments of trajectories of a dynamical system that are from different dynamical regimes, prioritizes minimizing the amount of data needed in updating the Koopman model and furthermore reduces the number of basis functions by learning them adaptively. Therefore, by dynamically adjusting the amount of data used and learning basis functions, our method optimizes the model's accuracy and the system order.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7ee04d648defa5ecd02e1e1b2368c710c5df6c2" target='_blank'>
              Online learning of Koopman operator using streaming data from different dynamical regimes
              </a>
            </td>
          <td>
            Kartik Loya, Phanindra Tallapragada
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Dynamic systems described by differential equations often involve feedback among system components. When there are time delays for components to sense and respond to feedback, delay differential equation (DDE) models are commonly used. This paper considers the problem of inferring unknown system parameters, including the time delays, from noisy and sparse experimental data observed from the system. We propose an extension of manifold-constrained Gaussian processes to conduct parameter inference for DDEs, whereas the time delay parameters have posed a challenge for existing methods that bypass numerical solvers. Our method uses a Bayesian framework to impose a Gaussian process model over the system trajectory, conditioned on the manifold constraint that satisfies the DDEs. For efficient computation, a linear interpolation scheme is developed to approximate the values of the time-delayed system outputs, along with corresponding theoretical error bounds on the approximated derivatives. Two simulation examples, based on Hutchinson's equation and the lac operon system, together with a real-world application using Ontario COVID-19 data, are used to illustrate the efficacy of our method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59505531dc6e56a864c5a883b0860d09309620d3" target='_blank'>
              Inference for Delay Differential Equations Using Manifold-Constrained Gaussian Processes
              </a>
            </td>
          <td>
            Yuxuan Zhao, Samuel W. K. Wong
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we develop the Asymptotic-Preserving Neural Networks (APNNs) approach to study the forward and inverse problem for the semiconductor Boltzmann equation. The goal of the neural network is to resolve the computational challenges of conventional numerical methods and multiple scales of the model. To guarantee the network can operate uniformly in different regimes, it is desirable to carry the Asymptotic-Preservation (AP) property in the learning process. In a micro-macro decomposition framework, we design such an AP formulation of loss function. The convergence analysis of both the loss function and its neural network is shown, based on the Universal Approximation Theorem and hypocoercivity theory of the model equation. We show a series of numerical tests for forward and inverse problems of both the semiconductor Boltzmann and the Boltzmann-Poisson system to validate the effectiveness of our proposed method, which addresses the significance of the AP property when dealing with inverse problems of multiscale Boltzmann equations especially when only sparse or partially observed data are available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb3329e7d90a830059df34e4bc6454db16ae0c60" target='_blank'>
              Asymptotic-preserving neural networks for the semiconductor Boltzmann equation and its application on inverse problems
              </a>
            </td>
          <td>
            Liu Liu, Yating Wang, Xueyu Zhu, Zhenyi Zhu
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="We introduce mochi_class, an extension of the Einstein-Boltzmann solver hi_class, designed to unlock the full phenomenological potential of Horndeski gravity. This extension allows for general input functions of time without the need for hard-coded parametrisations or covariant Lagrangians. By replacing the traditional $\alpha$-parametrisation with a set of stable basis functions, mochi_class ensures that the resulting effective theories are inherently free from gradient and ghost instabilities. Additionally, mochi_class features a quasi-static approximation implemented at the level of modified metric potentials, enhancing prediction accuracy, especially for models transitioning between a super- and sub-Compton regime. mochi_class can robustly handle a wide range of models without fine-tuning, and introduces a new approximation scheme that activates modifications to the standard cosmology deep in the matter-dominated era. Furthermore, it incorporates viability conditions on the equation of motion for the scalar field fluctuations, aiding in the identification of numerical instabilities. Through comprehensive validation against other Einstein-Boltzmann solvers, mochi_class demonstrates excellent performance and accuracy, broadening the scope of hi_class by facilitating the study of specific modified gravity models and enabling exploration of previously inaccessible regions of the Horndeski landscape. The code is publicly available at https://github.com/mcataneo/mochi_class_public">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/45c44ba92da169b2eb987805f3b6ad66528aeee4" target='_blank'>
              mochi_class: Modelling Optimisation to Compute Horndeski In class
              </a>
            </td>
          <td>
            Matteo Cataneo, Emilio Bellini
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In recent years, surrogate models based on deep neural networks have been widely used to solve partial differential equations for fluid flow physics. This kind of model focuses on global interpolation of the training data and thus requires a large network structure. The process is both time consuming and computationally costly. In the present study, we develop a neural network with local converging input (NNLCI) for high-fidelity prediction using unstructured data. The framework uses the local domain of dependence with converging coarse solutions as input, thereby greatly reducing computational resource and training time. As a validation case, the NNLCI method is applied to study two-dimensional inviscid supersonic flows in channels with bumps. Different bump geometries and locations are examined to benchmark the effectiveness and versatility of this new approach. The NNLCI method can accurately and efficiently capture the structure and dynamics of the entire flowfield, including regions with shock discontinuities. For a new bump configuration, the method can perform prediction with only one neural network, eliminating the need for repeated training of multiple networks for different geometries. A saving of computing wall time is achieved by several orders of magnitude against the high-fidelity simulation with the same level of accuracy. The demand on training data is modest, and the training data can be allocated sparsely. These features are especially advantageous compared with conventional global-to-global deep learning methods and physics-informed methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/88415c3817a7058d9678f49a393b306a231a9953" target='_blank'>
              Neural Network with Local Converging Input for Unstructured-Grid Computational Fluid Dynamics
              </a>
            </td>
          <td>
            Weiming Ding, Haoxiang Huang, T. Lee, Yingjie Liu, Vigor Yang
          </td>
          <td>2024-07-01</td>
          <td>AIAA Journal</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Accurate quantum dynamics simulations of nonadiabatic processes are important for studies of electron transfer, energy transfer, and photochemical reactions in complex systems. In this comparative study, we benchmark various approximate nonadiabatic dynamics methods with mapping variables against numerically exact calculations based on the tensor-train (TT) representation of high-dimensional arrays, including TT-KSL for zero-temperature dynamics and TT-thermofield dynamics for finite-temperature dynamics. The approximate nonadiabatic dynamics methods investigated include mixed quantum-classical Ehrenfest mean-field and fewest-switches surface hopping, linearized semiclassical mapping dynamics, symmetrized quasiclassical dynamics, the spin-mapping method, and extended classical mapping models. Different model systems were evaluated, including the spin-boson model for nonadiabatic dynamics in the condensed phase, the linear vibronic coupling model for electronic transition through conical intersections, the photoisomerization model of retinal, and Tully's one-dimensional scattering models. Our calculations show that the optimal choice of approximate dynamical method is system-specific, and the accuracy is sensitively dependent on the zero-point-energy parameter and the initial sampling strategy for the mapping variables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/328d4dede1c70e1ea2e979a03782edbe7fbd9ab1" target='_blank'>
              Benchmarking various nonadiabatic semiclassical mapping dynamics methods with tensor-train thermo-field dynamics.
              </a>
            </td>
          <td>
            Zengkui Liu, Ningyi Lyu, Zhubin Hu, Hao Zeng, Victor S. Batista, Xiang Sun
          </td>
          <td>2024-07-09</td>
          <td>The Journal of chemical physics</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Reservoir computing has been shown to be a useful framework for predicting critical transitions of a dynamical system if the bifurcation parameter is also provided as an input. Its utility is significant because in real-world scenarios, the exact model equations are unknown. This Letter shows how the theory of dynamical system provides the underlying mechanism behind the prediction. Using numerical methods, by considering dynamical systems which show Hopf bifurcation, we demonstrate that the map produced by the reservoir after a successful training undergoes a Neimark-Sacker bifurcation such that the critical point of the map is in immediate proximity to that of the original dynamical system. In addition, we have compared and analyzed different structures in the phase space. Our findings provide insight into the functioning of machine learning algorithms for predicting critical transitions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e38c7638dd9a0b907d25cf396095bd4a2d25e12" target='_blank'>
              Dynamical analysis of a parameter-aware reservoir computer
              </a>
            </td>
          <td>
            Dishant Sisodia, S. Jalan
          </td>
          <td>2024-07-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="In this work, we present a novel methodology for performing the supervised classification of time-ordered noisy data; we call this methodology Entropic Sparse Probabilistic Approximation with Markov regularization (eSPA-Markov). It is an extension of entropic learning methodologies, allowing the simultaneous learning of segmentation patterns, entropy-optimal feature space discretizations, and Bayesian classification rules. We prove the conditions for the existence and uniqueness of the learning problem solution and propose a one-shot numerical learning algorithm that—in the leading order—scales linearly in dimension. We show how this technique can be used for the computationally scalable identification of persistent (metastable) regime affiliations and regime switches from high-dimensional non-stationary and noisy time series, i.e., when the size of the data statistics is small compared to their dimensionality and when the noise variance is larger than the variance in the signal. We demonstrate its performance on a set of toy learning problems, comparing eSPA-Markov to state-of-the-art techniques, including deep learning and random forests. We show how this technique can be used for the analysis of noisy time series from DNA and RNA Nanopore sequencing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eadfee466307a1a3b8e63299cb3e600477c1f902" target='_blank'>
              On Entropic Learning from Noisy Time Series in the Small Data Regime
              </a>
            </td>
          <td>
            Davide Bassetti, Lukáš Pospíšil, I. Horenko
          </td>
          <td>2024-06-28</td>
          <td>Entropy</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="
 Turbulence closure modeling using machine learning is at an early crossroads. The extraordinary success of machine learning (ML) in a variety of challenging fields had given rise to an expectation of similar transformative advances in the area of turbulence closure modeling. However, by most accounts, the current rate of progress toward accurate and predictive ML-RANS (Reynolds Averaged Navier-Stokes) closure models has been very slow. Upon retrospection, the absence of rapid transformative progress can be attributed to two factors: the underestimation of the intricacies of turbulence modeling and the overestimation of ML’s ability to capture all features without employing targeted strategies. To pave the way for more meaningful ML closures tailored to address the nuances of turbulence, this article seeks to review the foundational flow physics to assess the challenges in the context of data-driven approaches. Revisiting analogies with statistical mechanics and stochastic systems, the key physical complexities and mathematical limitations are explicated. It is noted that the current ML approaches do not systematically address the inherent limitations of a statistical approach or the inadequacies of the mathematical forms of closure expressions. The study underscores the drawbacks of supervised learning-based closures and stresses the importance of a more discerning ML modeling framework. As ML methods evolve (which is happening at a rapid pace) and our understanding of the turbulence phenomenon improves, the inferences expressed here should be suitably modified.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d5cf3709be3b3ab9de17733558b062f9c6b30ac8" target='_blank'>
              Turbulence Closure Modeling with Machine Learning: A Foundational Physics Perspective
              </a>
            </td>
          <td>
            S. Girimaji
          </td>
          <td>2024-07-23</td>
          <td>New Journal of Physics</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="Gaussian Processes (GPs) and Linear Dynamical Systems (LDSs) are essential time series and dynamic system modeling tools. GPs can handle complex, nonlinear dynamics but are computationally demanding, while LDSs offer efficient computation but lack the expressive power of GPs. To combine their benefits, we introduce a universal method that allows an LDS to mirror stationary temporal GPs. This state-space representation, known as the Markovian Gaussian Process (Markovian GP), leverages the flexibility of kernel functions while maintaining efficient linear computation. Unlike existing GP-LDS conversion methods, which require separability for most multi-output kernels, our approach works universally for single- and multi-output stationary temporal kernels. We evaluate our method by computing covariance, performing regression tasks, and applying it to a neuroscience application, demonstrating that our method provides an accurate state-space representation for stationary temporal GPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f45b6ab8336d143d48c626e23d36a99a22707758" target='_blank'>
              Markovian Gaussian Process: A Universal State-Space Representation for Stationary Temporal Gaussian Process
              </a>
            </td>
          <td>
            Weihan Li, Yule Wang, Chengrui Li, Anqi Wu
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e47886cf1fed1baf5e2839789863c0355296cdc" target='_blank'>
              The Distance Between: An Algorithmic Approach to Comparing Stochastic Models to Time-Series Data
              </a>
            </td>
          <td>
            Brock D. Sherlock, Marko A. A. Boon, Maria Vlasiou, A. Coster
          </td>
          <td>2024-07-26</td>
          <td>Bulletin of Mathematical Biology</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="For many industrial processes, a digital twin is available, which is essentially a highly complex model whose parameters may not be properly tuned for the specific process. By relying on the availability of such a digital twin, this paper introduces a novel approach to data-driven control, where the digital twin is used to generate samples and suitable controllers for various perturbed versions of its parameters. A supervised learning algorithm is then employed to estimate a direct mapping from the data to the best controller to use. This map consists of a model reduction step, followed by a neural network architecture whose output provides the parameters of the controller. The data-to-controller map is pre-computed based on artificially generated data, but its execution once deployed is computationally very efficient, thus providing a simple and inexpensive way to tune and re-calibrate controllers directly from data. The benefits of this novel approach are illustrated via numerical simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6916b9be2764f075e341e143b9e58edc122152ad" target='_blank'>
              From Data to Control: A Two-Stage Simulation-Based Approach
              </a>
            </td>
          <td>
            Federico Dettù, Braghadeesh Lakshminarayanan, Simone Formentin, Cristian R. Rojas
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Model predictive control (MPC) is a powerful control method for handling complex nonlinear systems that are subject to constraints. However, the real-time application of this approach can be severely limited by the need to solve constrained nonlinear optimization problems at each sampling time. To this end, this work introduces a novel learning-based iterative solver that provides highly accurate predictions, optimality certification, and fast evaluation of the MPC solution at each sampling time. To learn this iterative solver, we propose an unsupervised training algorithm that builds on the Karush-Kuhn-Tucker optimality conditions, modified by a Fischer-Burmeister formulation, and eliminates the need for prior sampling of exact optimizer solutions. By exploiting efficient vector-Jacobian and Jacobian-vector products via automatic differentiation, the proposed training algorithm can be efficiently executed. We demonstrate the potential of the proposed learning-based iterative solver on the example of nonlinear model predictive control of a nonlinear double integrator. We show its advantages when compared to exact optimizer solutions and with an imitation learning-based approach that directly obtains a data-based approximation of the MPC control law.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04655a12d07dee491135aaae2f055b737e275291" target='_blank'>
              Learning Iterative Solvers for Accurate and Fast Nonlinear Model Predictive Control via Unsupervised Training
              </a>
            </td>
          <td>
            Lukas Lüken, Sergio Lucia
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a94495dae3c918bd50c5f83ca6c9f29d9b4dba12" target='_blank'>
              Active learning for adaptive surrogate model improvement in high-dimensional problems
              </a>
            </td>
          <td>
            Yulin Guo, Paromita Nath, Sankaran Mahadevan, Paul Witherell
          </td>
          <td>2024-07-01</td>
          <td>Structural and Multidisciplinary Optimization</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Inferring parameters of models of biochemical kinetics from single-cell data remains challenging because of the uncertainty arising from the intractability of the likelihood function of stochastic reaction networks. Such uncertainty falls beyond current error quantification measures, which focus on the effects of finite sample size and identifiability but lack theoretical guarantees when likelihood approximations are needed. Here, we propose an inference method for stochastic reaction networks with nonlinear and rational propensities at steady state that provides bounds on the parameters via convex optimisation over sets constrained by moment equations and moment matrices. Our approach takes observations from the stochastic reaction network and forms moment intervals, which are then used to constrain parameters through convex sets. The bounds on the parameters contain the true parameters under the condition that the moment intervals contain the true stationary moments, thus providing uncertainty quantification and error guarantees. Our approach does not need to predict moments and distributions for given parameters (i.e., it avoids solving or simulating the forward problem), and hence circumvents intractable likelihood computations or computationally expensive simulations. We demonstrate its use for uncertainty quantification, data integration and prediction of latent species statistics through synthetic data from common nonlinear biochemical models including the Schl\"ogl model, the toggle switch and post-transcriptional regulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/de4b88b799eae872be620beb74ef0b6049b65d90" target='_blank'>
              Moment-based parameter inference with error guarantees for stochastic reaction networks
              </a>
            </td>
          <td>
            Zekai Li, Mauricio Barahona, Philipp Thomas
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this study, we delve into the Structured State Space Model (S4), Change Point Detection methodologies, and the Switching Non-linear Dynamics System (SNLDS). Our central proposition is an enhanced inference technique and long-range dependency method for SNLDS. The cornerstone of our approach is the fusion of S4 and SNLDS, leveraging the strengths of both models to effectively address the intricacies of long-range dependencies in switching time series. Through rigorous testing, we demonstrate that our proposed methodology adeptly segments and reproduces long-range dependencies in both the 1-D Lorenz dataset and the 2-D bouncing ball dataset. Notably, our integrated approach outperforms the standalone SNLDS in these tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/737fccae30a52b719925aa3bdefbccf7758f66f5" target='_blank'>
              Long Range Switching Time Series Prediction via State Space Model
              </a>
            </td>
          <td>
            Jiaming Zhang, Yang Ding, Yunfeng Gao
          </td>
          <td>2024-07-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This study presents a novel approach that leverages Neural Ordinary Differential Equations (Neural ODEs) to unravel the intricate relationships between inputs and outputs in Large Language Models (LLMs), and employs robust control to fine-tune outputs to meet predefined standards. Central to our methodology is the transformation of LLM inputs and outputs into a lower-dimensional latent space, facilitating a detailed examination of the information processing pathways within LLMs. Neural ODEs play a pivotal role in this investigation by providing a dynamic model that captures the continuous evolution of data within the LLMs. Additionally, robust control mechanisms are applied to strategically adjust the model's outputs, ensuring they not only maintain high quality and reliability but also adhere to specific performance criteria. This fusion of Neural ODEs and robust control represents a significant advancement in LLM interpretability, offering a comprehensive framework that elucidates the previously opaque mechanisms of these complex models. Our empirical results validate the effectiveness of this integrated approach, making a substantial contribution to the field of explainable AI by merging advanced machine learning techniques with the critical need for transparency and control in AI outputs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e11388eb181a61495a9fb75dd0509dda44b5e3b3" target='_blank'>
              Unveiling LLM Mechanisms Through Neural ODEs and Control Theory
              </a>
            </td>
          <td>
            Yukun Zhang
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Natural and technological networks exhibit dynamics that can lead to complex cooperative behaviors, such as synchronization in coupled oscillators and rhythmic activity in neuronal networks. Understanding these collective dynamics is crucial for deciphering a range of phenomena from brain activity to power grid stability. Recent interest in co-evolutionary networks has highlighted the intricate interplay between dynamics on and of the network with mixed time scales. Here, we explore the collective behavior of excitable oscillators in a simple networks of two Theta neurons with adaptive coupling without self-interaction. Through a combination of bifurcation analysis and numerical simulations, we seek to understand how the level of adaptivity in the coupling strength, $a$, influences the dynamics. We first investigate the dynamics possible in the non-adaptive limit; our bifurcation analysis reveals stability regions of quiescence and spiking behaviors, where the spiking frequencies mode-lock in a variety of configurations. Second, as we increase the adaptivity $a$, we observe a widening of the associated Arnol'd tongues, which may overlap and give room for multi-stable configurations. For larger adaptivity, the mode-locked regions may further undergo a period-doubling cascade into chaos. Our findings contribute to the mathematical theory of adaptive networks and offer insights into the potential mechanisms underlying neuronal communication and synchronization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7ddfebfd3c68f11d067d5fd26480c3939e159e0a" target='_blank'>
              Co-evolutionary dynamics for two adaptively coupled Theta neurons
              </a>
            </td>
          <td>
            Felix Augustsson, E. A. Martens
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Solving complex Partial Differential Equations (PDEs) accurately and efficiently is an essential and challenging problem in all scientific and engineering disciplines. Mesh movement methods provide the capability to improve the accuracy of the numerical solution without increasing the overall mesh degree of freedom count. Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries. However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements. In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries. UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh. We evaluate our method on advection and Navier-Stokes based examples, as well as a real-world tsunami simulation case. Our method outperforms existing learning-based mesh movement methods in terms of the benchmarks described above. In comparison to the conventional sophisticated Monge-Amp\`ere PDE-solver based method, our approach not only significantly accelerates mesh movement, but also proves effective in scenarios where the conventional method fails. Our project page is at https://erizmr.github.io/UM2N/.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/569f07b5381b1df88a804a15614ac2ddef7715f2" target='_blank'>
              Towards Universal Mesh Movement Networks
              </a>
            </td>
          <td>
            Mingrui Zhang, Chunyang Wang, Stephan Kramer, Joseph G. Wallwork, Siyi Li, Jiancheng Liu, Xiang Chen, M. Piggott
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>38</td>
        </tr>

        <tr id="In this article, a distributed neural network modeling framework including a novel neural hybrid system model is proposed for enhancing the scalability of neural network models in modeling dynamical systems. First, high-dimensional training data samples will be mapped to a low-dimensional feature space through the principal component analysis (PCA) featuring process. Following that, the feature space is bisected into multiple partitions based on the variation of the Shannon entropy under the maximum entropy (ME) bisecting process. The behavior of subsystems in the prespecified state space partitions will then be approximated using a group of shallow neural networks (SNNs) known as extreme learning machines (ELMs), and then it can further simplify the model by merging the redundant lattices based on their training error performance. The proposed modeling framework can handle high-dimensional dynamical system modeling problems with the advantages of reducing model complexity and improving model performance in training and verification. To demonstrate the effectiveness of the proposed modeling framework, examples of modeling the LASA dataset and an industrial robot are presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a2a2b97f2bbf0735400cc24119a8d6645252543" target='_blank'>
              A Distributed Neural Hybrid System Learning Framework in Modeling Complex Dynamical Systems.
              </a>
            </td>
          <td>
            Yejiang Yang, Tao Wang, Weiming Xiang
          </td>
          <td>2024-06-28</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Motivated by important applications to the analysis of complex noise-induced phenomena, we consider a problem of the constructive description of randomly forced equilibria for nonlinear systems with multiplicative noise. Using the apparatus of the first approximation systems, we construct an approximation of mean square deviations that explicitly takes into account the presence of multiplicative noises, depending on the current system state. A spectral criterion of existence and exponential stability of the stationary second moments for the solution of the first approximation system is presented. For mean square deviation, we derive an expansion in powers of the small parameter of noise intensity. Based on this theory, we derive a new, more accurate approximation of mean square deviations in a general nonlinear system with multiplicative noises. This approximation is compared with the widely used approximation based on the stochastic sensitivity technique. The general mathematical results are illustrated with examples of the model of climate dynamics and the van der Pol oscillator with hard excitement.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f8b4c1e986ba88d48cf2674010a03c5713b44f90" target='_blank'>
              Approximations in Mean Square Analysis of Stochastically Forced Equilibria for Nonlinear Dynamical Systems
              </a>
            </td>
          <td>
            I. Bashkirtseva
          </td>
          <td>2024-07-13</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="A surrogate model approximates the outputs of a solver of Partial Differential Equations (PDEs) with a low computational cost. In this article, we propose a method to build learning-based surrogates in the context of parameterized PDEs, which are PDEs that depend on a set of parameters but are also temporal and spatial processes. Our contribution is a method hybridizing the Proper Orthogonal Decomposition and several Support Vector Regression machines. This method is conceived to work in real-time, thus aimed for being used in the context of digital twins, where a user can perform an interactive analysis of results based on the proposed surrogate. We present promising results on two use cases concerning electrical machines. These use cases are not toy examples but are produced an industrial computational code, they use meshes representing non-trivial geometries and contain non-linearities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e63bc34c8490fd304b6cbb27c3dc5e2405d617e2" target='_blank'>
              A Fast Learning-Based Surrogate of Electrical Machines using a Reduced Basis
              </a>
            </td>
          <td>
            Alejandro Rib'es, Nawfal Benchekroun, Théo Delagnes
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The intersection of artificial intelligence (AI) and partial differential equations (PDEs), emphasizing how AI techniques can revolutionize the analysis and solution of PDEs in various scientific and engineering applications. Traditional methods for solving PDEs often face challenges related to computational complexity, high-dimensionality, and nonlinearity. By leveraging advanced AI algorithms, particularly deep learning and neural networks, we propose novel approaches to approximate solutions, reduce computational costs, and handle complex boundary conditions more effectively. The study highlights the advantages of AI-driven methods in terms of accuracy, efficiency, and scalability, presenting case studies from fluid dynamics, quantum mechanics, and financial mathematics. Our findings suggest that AI has the potential to significantly enhance the analytical capabilities and practical applications of PDEs, paving the way for new advancements in both theoretical research and real-world problem solving">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c86e95ee6e33bf3061f0363c446c469fe71a1b5c" target='_blank'>
              AI Based Analysis and Partial Differential Equations
              </a>
            </td>
          <td>
            M. Krishna Reddy, N. Vijayabhaskar Reddy
          </td>
          <td>2024-07-21</td>
          <td>International Journal of Advanced Research in Science, Communication and Technology</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65f70fe6d698072acfae68cb9be3a90281df6d72" target='_blank'>
              Application of physics encoded neural networks to improve predictability of properties of complex multi-scale systems
              </a>
            </td>
          <td>
            M. Meinders, Jack Yang, Erik van der Linden
          </td>
          <td>2024-07-01</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="The purpose of this paper is to propose a novel perspective, based on Willems'"behavior theory", on the design of an unknown-input observer for a given linear time-invariant discrete-time state-space model, with unknown disturbances affecting both the state and the output equations. The problem is first addressed assuming that the original system model is known, and later assuming that the model is unknown but historical data satisfying a certain assumption are available. In both cases, fundamental concepts in behavior theory, as the projection of a behavior, the inclusion of a behavior in another one, and the use of kernel and image representations, provide quite powerful tools to determine necessary and sufficient conditions for the existence of an unknown-input observer (UIO), as well as algorithms to design one of them, if it exists.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d5ab8824253eedddbefb06d51b96d8e58632d5f" target='_blank'>
              Behaviors, trajectories and data: A novel perspective on the design of unknown-input observers
              </a>
            </td>
          <td>
            G. Disarò, M. E. Valcher
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="Many real-world applications demand accurate and fast predictions, as well as reliable uncertainty estimates. However, quantifying uncertainty on high-dimensional predictions is still a severely under-invested problem, especially when input-output relationships are non-linear. To handle this problem, the present work introduces an innovative approach that combines autoencoder deep neural networks with the probabilistic regression capabilities of Gaussian processes. The autoencoder provides a low-dimensional representation of the solution space, while the Gaussian process is a Bayesian method that provides a probabilistic mapping between the low-dimensional inputs and outputs. We validate the proposed framework for its application to surrogate modeling of non-linear finite element simulations. Our findings highlight that the proposed framework is computationally efficient as well as accurate in predicting non-linear deformations of solid bodies subjected to external forces, all the while providing insightful uncertainty assessments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c5ebe3e35ead48c1cec6afd16f55a4873eabdbd1" target='_blank'>
              Gaussian process regression + deep neural network autoencoder for probabilistic surrogate modeling in nonlinear mechanics of solids
              </a>
            </td>
          <td>
            Saurabh Deshpande, Hussein Rappel, Mark Hobbs, Stéphane P. A. Bordas, Jakub Lengiewicz
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4eb1f5b104b75cee5eba224ac4724ead85e4c238" target='_blank'>
              Physics-informed neural network simulation of thermal cavity flow
              </a>
            </td>
          <td>
            Eric Fowler, Christopher J McDevitt, Subrata Roy
          </td>
          <td>2024-07-02</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Partial differential equations (PDEs) are extensively utilized for modeling various physical phenomena. These equations often depend on certain parameters, necessitating either the identification of optimal parameters or solving the equation across multiple parameters to understand how a structure might react under different conditions. Performing an exhaustive search over the parameter space requires solving the PDE multiple times, which is generally impractical. To address this challenge, Reduced Order Models (ROMs) are constructed from a snapshot dataset comprising parameter-solution pairs. ROMs facilitate the rapid solving of PDEs for new parameters. Recently, Deep Learning ROMs (DL-ROMs) have been introduced as a new method to obtain ROM. Additionally, the PDE of interest may depend on the domain, which can be characterized by parameters or measurements and may evolve with the system, requiring parametrization for ROM construction. In this paper, we develop a Deep-ROM capable of extracting and efficiently utilizing domain parametrization. Unlike traditional domain parametrization methods, our approach does not require user-defined control points and can effectively handle domains with varying numbers of components. Moreover, our model can derive meaningful parametrization even when a domain mesh is unavailable, a common scenario in biomedical applications. Our work leverages Deep Neural Networks to effectively reduce the dimensionality of the PDE and the domain characteristic function.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/90e23c188396aefabed7701142b8c0a910c3d228" target='_blank'>
              Deep Learning Reduced Order Modelling on Parametric and Data driven domains
              </a>
            </td>
          <td>
            Martina Bukavc, Iva Manojlovi'c, B. Muha, Domagoj Vlah University of Notre Dame, Notre Dame, In, University of Utah Department of Biomedical Engineering, Computing, Departmento Mathematics, Faculty of Computer Science, U. Zagreb
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="We consider a typical learning problem of point estimations for modeling of nonlinear functions or dynamical systems in which generalization, i.e., verifying a given learned model, can be embedded as an integral part of the learning process or dynamics. In particular, we consider an empirical risk minimization based learning problem that exploits gradient methods from continuous-time perspective with small random perturbations, which is guided by the training dataset loss. Here, we provide an asymptotic probability estimate in the small noise limit based-on the Freidlin-Wentzell theory of large deviations, when the sample path of the random process corresponding to the randomly perturbed gradient dynamical system hits a certain target set, i.e., a rare event, when the latter is specified by the testing dataset loss landscape. Interestingly, the proposed framework can be viewed as one way of improving generalization and robustness in learning problems that provides new insights leading to optimal point estimates which is guided by training data loss, while, at the same time, the learning dynamics has an access to the testing dataset loss landscape in some form of future achievable or anticipated target goal. Moreover, as a by-product, we establish a connection with optimal control problem, where the target set, i.e., the rare event, is considered as the desired outcome or achievable target goal for a certain optimal control problem, for which we also provide a verification result reinforcing the rationale behind the proposed framework. Finally, we present a computational algorithm that solves the corresponding variational problem leading to an optimal point estimates and, as part of this work, we also present some numerical results for a typical case of nonlinear regression problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3eb8a6a20e76ca76075c140c11561ca482a74778" target='_blank'>
              Embedding generalization within the learning dynamics: An approach based-on sample path large deviation theory
              </a>
            </td>
          <td>
            G. Befekadu
          </td>
          <td>2024-08-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Networks in machine learning offer examples of complex high-dimensional dynamical systems inspired by and reminiscent of biological systems. Here, we study the learning dynamics of generalized Hopfield networks, which permit visualization of internal memories. These networks have been shown to proceed through a “feature-to-prototype” transition, as the strength of network nonlinearity is increased, wherein the learned, or terminal, states of internal memories transition from mixed to pure states. Focusing on the prototype learning dynamics of the internal memories, we observe stereotypical dynamics of memories wherein similar subgroups of memories sequentially split at well-defined saddles. The splitting order is interpretable and reproducible from one simulation to the other. The dynamics prior to splits are robust to variations in many features of the system. To develop a more rigorous understanding of these global dynamics, we study smaller subsystems that exhibit similar properties to the full system. Within these smaller systems, we combine analytical calculations with numerical simulations to study the dynamics of the feature-to-prototype transition, and the emergence of saddle points in the learning landscape. We exhibit regimes where saddles appear and disappear through saddle-node bifurcations, qualitatively changing the distribution of learned memories as the strength of the nonlinearity is varied—allowing us to systematically investigate the mechanisms that underlie the emergence of the learning dynamics. Several features of the learning dynamics are reminiscent of the Waddington's caricature of cellular differentiation, and we attempt to make this analogy more precise. Memories can thus differentiate in a predictive and controlled way, revealing bridges between experimental biology, dynamical systems theory, and machine learning.




 Published by the American Physical Society
 2024


">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/acfb0996b8a2dd52af629b0fc803ccf66a9f85a9" target='_blank'>
              Waddington landscape for prototype learning in generalized Hopfield networks
              </a>
            </td>
          <td>
            Nacer Eddine Boukacem, Allen Leary, Robin Thériault, Felix Gottlieb, Madhav Mani, Paul François
          </td>
          <td>2024-07-23</td>
          <td>Physical Review Research</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The formation of dynamical patterns is one of the most striking features of non-equilibrium physical systems. Recent work has shown that such patterns arise generically from forces that violate Newton's third law, known as non-reciprocal interactions. These non-equilibrium phenomena are challenging for modern theories. Here, we introduce a model mixture of active (self-propelled) and passive (diffusive) particles with non-reciprocal effective interactions, which is amenable to exact mathematical analysis. We exploit state-of-the-art methods to derive exact hydrodynamic equations for the particle densities. We study the resulting collective behavior, including the linear stability of homogeneous states and phase coexistence in large systems. This reveals a novel phase diagram with the spinodal associated with active phase separation protruding through the associated binodal, heralding the emergence of dynamical steady states. We analyze these states in the thermodynamic limit of large system size, showing, for example, that sharp interfaces may travel at finite velocities, but traveling phase-separated states are forbidden. The model's mathematical tractability enables precise new conclusions beyond those available by numerical simulation of particle models or field theories.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2db205ad1b1ae5cb38f6a1257d35fe0eec005cfc" target='_blank'>
              Dynamical patterns in active-passive particle mixtures with non-reciprocal interactions: Exact hydrodynamic analysis
              </a>
            </td>
          <td>
            James Mason, Robert L. Jack, Maria Bruna
          </td>
          <td>2024-08-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Data driven system identification is the technique for learning models from input/output data. To increase the robustness of the model estimation, prior knowledge can be incorporated, the so-called gray-box identification. In finite impulse response (FIR) models, prior knowledge of the process under investigation can be introduced by regularization. In the regularization term basic impulse response characteristics such as smoothness and exponentially decaying behavior can be incorporated. For estimation of time-delay systems, the novel impulse response and time-delay preserving (IRDP) regularization matrix is proposed. In this contribution this method is extended to the estimation of multiple input single output (MISO) processes and is compared to other state-of-the-art approaches. A linear process with four inputs and different input dynamics and time-delays is investigated. The focus of the evaluation is placed on model quality, time-delay estimation, and computation time. The simulation results point out the superiority of the novel regularization approach in comparison to state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a8735d69bbb91950534f1fe2742f48352fd17202" target='_blank'>
              Handling of Time Delays in MISO Processes with Regularized Finite Impulse Response Models
              </a>
            </td>
          <td>
            Christopher Illg, T. Kösters, Oliver Nelles
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Neural operators such as the Fourier Neural Operator (FNO) have been shown to provide resolution-independent deep learning models that can learn mappings between function spaces. For example, an initial condition can be mapped to the solution of a partial differential equation (PDE) at a future time-step using a neural operator. Despite the popularity of neural operators, their use to predict solution functions over a domain given only data over the boundary (such as a spatially varying Dirichlet boundary condition) remains unexplored. In this paper, we refer to such problems as boundary-to-domain problems; they have a wide range of applications in areas such as fluid mechanics, solid mechanics, heat transfer etc. We present a novel FNO-based architecture, named Lifting Product FNO (or LP-FNO) which can map arbitrary boundary functions defined on the lower-dimensional boundary to a solution in the entire domain. Specifically, two FNOs defined on the lower-dimensional boundary are lifted into the higher dimensional domain using our proposed lifting product layer. We demonstrate the efficacy and resolution independence of the proposed LP-FNO for the 2D Poisson equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f352a2d81715807d302b4b1be51ece5c849052d2" target='_blank'>
              Learning the boundary-to-domain mapping using Lifting Product Fourier Neural Operators for partial differential equations
              </a>
            </td>
          <td>
            Aditya Kashi, Arka Daw, Muralikrishnan Gopalakrishnan Meena, Hao Lu
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="PurposeSimulation-based digital twins represent an effort to provide high-accuracy real-time insights into operational physical processes. However, the computation time of many multi-physical simulation models is far from real-time. It might even exceed sensible time frames to produce sufficient data for training data-driven reduced-order models. This study presents TwinLab, a framework for data-efficient, yet accurate training of neural-ODE type reduced-order models with only two data sets.Design/methodology/approachCorrelations between test errors of reduced-order models and distinct features of corresponding training data are investigated. Having found the single best data sets for training, a second data set is sought with the help of similarity and error measures to enrich the training process effectively.FindingsAdding a suitable second training data set in the training process reduces the test error by up to 49% compared to the best base reduced-order model trained only with one data set. Such a second training data set should at least yield a good reduced-order model on its own and exhibit higher levels of dissimilarity to the base training data set regarding the respective excitation signal. Moreover, the base reduced-order model should have elevated test errors on the second data set. The relative error of the time series ranges from 0.18% to 0.49%. Prediction speed-ups of up to a factor of 36,000 are observed.Originality/valueThe proposed computational framework facilitates the automated, data-efficient extraction of non-intrusive reduced-order models for digital twins from existing simulation models, independent of the simulation software.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4aeb7a00d93947f8e222188bb9f980ac834448d1" target='_blank'>
              TwinLab: a framework for data-efficient training of non-intrusive reduced-order models for digital twins
              </a>
            </td>
          <td>
            Maximilian Kannapinn, M. Schäfer, Oliver Weeger
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This work addresses stochastic optimal control problems where the unknown state evolves in continuous time while partial, noisy, and possibly controllable measurements are only available in discrete time. We develop a framework for controlling such systems, focusing on the measure-valued process of the system's state and the control actions that depend on noisy and incomplete data. Our approach uses a stochastic optimal control framework with a probability measure-valued state, which accommodates noisy measurements and integrates them into control decisions through a Bayesian update mechanism. We characterize the control optimality in terms of a sequence of interlaced Hamilton Jacobi Bellman (HJB) equations coupled with controlled impulse steps at the measurement times. For the case of Gaussian-controlled processes, we derive an equivalent HJB equation whose state variable is finite-dimensional, namely the state's mean and covariance. We demonstrate the effectiveness of our methods through numerical examples. These include control under perfect observations, control under no observations, and control under noisy observations. Our numerical results highlight significant differences in the control strategies and their performance, emphasizing the challenges and computational demands of dealing with uncertainty in state observation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6ba24031ebf3376cb6db64dd547b84c989cf92e" target='_blank'>
              Continuous time Stochastic optimal control under discrete time partial observations
              </a>
            </td>
          <td>
            Christian Bayer, Boualem Djehiche, Eliza Rezvanova, Raúl Tempone
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Biological neural networks can perform complex computations to predict their environment, far above the limited predictive capabilities of individual neurons. While conventional approaches to understanding these computations often focus on isolating the contributions of single neurons, here we argue that a deeper understanding requires considering emergent dynamics - dynamics that make the whole system"more than the sum of its parts". Specifically, we examine the relationship between prediction performance and emergence by leveraging recent quantitative metrics of emergence, derived from Partial Information Decomposition, and by modelling the prediction of environmental dynamics in a bio-inspired computational framework known as reservoir computing. Notably, we reveal a bidirectional coupling between prediction performance and emergence, which generalises across task environments and reservoir network topologies, and is recapitulated by three key results: 1) Optimising hyperparameters for performance enhances emergent dynamics, and vice versa; 2) Emergent dynamics represent a near sufficient criterion for prediction success in all task environments, and an almost necessary criterion in most environments; 3) Training reservoir computers on larger datasets results in stronger emergent dynamics, which contain task-relevant information crucial for performance. Overall, our study points to a pivotal role of emergence in facilitating environmental predictions in a bio-inspired computational architecture.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2572feb06e7ea060ecb305043a5d4b589af5886d" target='_blank'>
              Evolving reservoir computers reveals bidirectional coupling between predictive power and emergent dynamics
              </a>
            </td>
          <td>
            Hanna M. Tolle, A. Luppi, Anil K. Seth, P. Mediano
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="This paper introduces a novel nonlinear stochastic model predictive control path integral (MPPI) method, which considers chance constraints on system states. The proposed belief-space stochastic MPPI (BSS-MPPI) applies Monte-Carlo sampling to evaluate state distributions resulting from underlying systematic disturbances, and utilizes a Control Barrier Function (CBF) inspired heuristic in belief space to fulfill the specified chance constraints. Compared to several previous stochastic predictive control methods, our approach applies to general nonlinear dynamics without requiring the computationally expensive system linearization step. Moreover, the BSS-MPPI controller can solve optimization problems without limiting the form of the objective function and chance constraints. By multi-threading the sampling process using a GPU, we can achieve fast real-time planning for time- and safety-critical tasks such as autonomous racing. Our results on a realistic race-car simulation study show significant reductions in constraint violation compared to some of the prior MPPI approaches, while being comparable in computation times.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ec0a95b01ae133b323176ef995b8490fff8991be" target='_blank'>
              Chance-Constrained Information-Theoretic Stochastic Model Predictive Control with Safety Shielding
              </a>
            </td>
          <td>
            Ji Yin, P. Tsiotras, K. Berntorp
          </td>
          <td>2024-08-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="The development of novel materials in recent years has been accelerated greatly by the use of computational modelling techniques aimed at elucidating the complex physics controlling microstructure formation in materials, the properties of which control material function. One such technique is the phase field method, a field theoretic approach that couples various thermophysical fields to microscopic order parameter fields that track the phases of microstructure. Phase field models are framed as multiple, non-linear, partial differential equations, which are extremely challenging to compute efficiently. Recent years have seen an explosion of computational algorithms aimed at enhancing the efficiency of phase field simulations. One such technique, adaptive mesh refinement (AMR), dynamically adapts numerical meshes to be highly refined around steep spatial gradients of the PDE fields and coarser where the fields are smooth. This reduces the number of computations per time step significantly, thus reducing the total time of computation. What AMR doesn't do is allow for adaptive time stepping. This work combines AMR with a neural network algorithm that uses a U-Net with a Convolutional Long-Short Term Memory (CLSTM) base to accelerate phase field simulations. Our neural network algorithm is described in detail and tested in on simulations of directional solidification of a dilute binary alloy, a paradigm that is highly practical for its relevance to the solidification of alloys.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7c3b439cab9bc1574b9ee846128f87f45347f16" target='_blank'>
              LeapFrog: Getting the Jump on Multi-Scale Materials Simulations Using Machine Learning
              </a>
            </td>
          <td>
            Damien Pinto, M. Greenwood, N. Provatas
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Temporal data modelling techniques with neural networks are useful in many domain applications, including time-series forecasting and control engineering. This paper aims at developing a recurrent version of stochastic configuration networks (RSCNs) for problem solving, where we have no underlying assumption on the dynamic orders of the input variables. Given a collection of historical data, we first build an initial RSCN model in the light of a supervisory mechanism, followed by an online update of the output weights by using a projection algorithm. Some theoretical results are established, including the echo state property, the universal approximation property of RSCNs for both the offline and online learnings, and the convergence of the output weights. The proposed RSCN model is remarkably distinguished from the well-known echo state networks (ESNs) in terms of the way of assigning the input random weight matrix and a special structure of the random feedback matrix. A comprehensive comparison study among the long short-term memory (LSTM) network, the original ESN, and several state-of-the-art ESN methods such as the simple cycle reservoir (SCR), the polynomial ESN (PESN), the leaky-integrator ESN (LIESN) and RSCN is carried out. Numerical results clearly indicate that the proposed RSCN performs favourably over all of the datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/46e3c55f0b0e77192fb6cc422ecf599a47334d73" target='_blank'>
              Recurrent Stochastic Configuration Networks for Temporal Data Analytics
              </a>
            </td>
          <td>
            Dianhui Wang, Gang Dang
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Reliable uncertainty measures are required when using data based machine learning interatomic potentials (MLIPs) for atomistic simulations. In this work, we propose for sparse Gaussian Process Regression type MLIP a stochastic uncertainty measure akin to the query-by-committee approach often used in conjunction with neural network based MLIPs. The uncertainty measure is coined \textit{"label noise"} ensemble uncertainty as it emerges from adding noise to the energy labels in the training data. We find that this method of calculating an ensemble uncertainty is as well calibrated as the one obtained from the closed-form expression for the posterior variance when the sparse GPR is treated as a projected process. Comparing the two methods, our proposed ensemble uncertainty is, however, faster to evaluate than the closed-form expression. Finally, we demonstrate that the proposed uncertainty measure acts better to support a Bayesian search for optimal structure of Au$_{20}$ clusters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cbe74de7bdeef59e212d49ccb746e4deaac70d5f" target='_blank'>
              Efficient ensemble uncertainty estimation in Gaussian Processes Regression
              </a>
            </td>
          <td>
            Mads-Peter V. Christiansen, Nikolaj Rønne, Bjork Hammer
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Predictive estimation, which comprises model calibration, model prediction, and validation, is a common objective when performing inverse uncertainty quantification (UQ) in diverse scientific applications. These techniques typically require thousands to millions of realisations of the forward model, leading to high computational costs. Surrogate models are often used to approximate these simulations. However, many surrogate models suffer from the fundamental limitation of being unable to estimate plausible high-dimensional outputs, inevitably compromising their use in the UQ framework. To address this challenge, this study introduces an efficient surrogate modelling workflow tailored for high-dimensional outputs. Specifically, a two-step approach is developed: (1) a dimensionality reduction technique is used for extracting data features and mapping the original output space into a reduced space; and (2) a multivariate surrogate model is constructed directly on the reduced space. The combined approach is shown to improve the accuracy of the surrogate model while retaining the computational efficiency required for UQ inversion. The proposed surrogate method, combined with Bayesian inference, is evaluated for a civil engineering application by performing inverse analyses on a laterally loaded pile problem. The results demonstrate the superiority of the proposed framework over traditional surrogate methods in dealing with high-dimensional outputs for sequential inversion analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dace1d7c26c300e5c21c5561b92782d077d84947" target='_blank'>
              A two-step surrogate method for sequential uncertainty quantification in high-dimensional inverse problems
              </a>
            </td>
          <td>
            Ningxin Yang, Truong Le, Lidija Zdravkovi'c, David M. Potts
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The probability that a configuration of a physical system reacts, or transitions from one metastable state to another, is quantified by the committor function. This function contains richly detailed mechanistic information about transition pathways, but a full parameterization of the committor requires building representing a high-dimensional function, a generically challenging task. Recent efforts to leverage neural networks as a means to solve high-dimensional partial differential equations, often called"physics-informed"machine learning, have brought the committor into computational reach. Here, we build on the semigroup approach to learning the committor and assess its utility for predicting dynamical quantities such as transition rates. We show that a careful reframing of the objective function and improved adaptive sampling strategies provide highly accurate representations of the committor. Furthermore, by directly applying the Hill relation, we show that these committors provide accurate transition rates for molecular system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/61781a644a9abeb378998d8505d6add714bb238b" target='_blank'>
              Committor guided estimates of molecular transition rates
              </a>
            </td>
          <td>
            Andrew R. Mitchell, Grant M. Rotskoff
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="In functional data analysis, replicate observations of a smooth functional process and its derivatives offer a unique opportunity to flexibly estimate continuous-time ordinary differential equation models. Ramsay (1996) first proposed to estimate a linear ordinary differential equation from functional data in a technique called Principal Differential Analysis, by formulating a functional regression in which the highest-order derivative of a function is modelled as a time-varying linear combination of its lower-order derivatives. Principal Differential Analysis was introduced as a technique for data reduction and representation, using solutions of the estimated differential equation as a basis to represent the functional data. In this work, we re-formulate PDA as a generative statistical model in which functional observations arise as solutions of a deterministic ODE that is forced by a smooth random error process. This viewpoint defines a flexible class of functional models based on differential equations and leads to an improved understanding and characterisation of the sources of variability in Principal Differential Analysis. It does, however, result in parameter estimates that can be heavily biased under the standard estimation approach of PDA. Therefore, we introduce an iterative bias-reduction algorithm that can be applied to improve parameter estimates. We also examine the utility of our approach when the form of the deterministic part of the differential equation is unknown and possibly non-linear, where Principal Differential Analysis is treated as an approximate model based on time-varying linearisation. We demonstrate our approach on simulated data from linear and non-linear differential equations and on real data from human movement biomechanics. Supplementary R code for this manuscript is available at \url{https://github.com/edwardgunning/UnderstandingOfPDAManuscript}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d0be3d0b8344a99cb8ce9f30ed01a1653af4baa" target='_blank'>
              An Understanding of Principal Differential Analysis
              </a>
            </td>
          <td>
            Edward Gunning, Giles Hooker
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="
Of great relevance to climate engineering is the systematic relationship between the radiative forcing to the climate system and the response of the system, a relationship often represented by the linear response function (LRF) of the system. However, estimating the LRF often becomes an ill-posed inverse problem due to high-dimensionality and non-unique relationships between the forcing and response. Recent advances in machine learning make it possible to address the ill-posed inverse problem through regularization and sparse system fitting. Here we develop a convolutional neural network (CNN) for regularized inversion. The CNN is trained using the surface temperature responses from a set of Green’s function perturbation experiments as imagery input data together with data sample densification. The resulting CNN model can infer the forcing pattern responsible for the temperature response from out-of-sample forcing scenarios. This promising proof-of-concept suggests a possible strategy for estimating the optimal forcing to negate certain undesirable effects of climate change. The limited success of this effort underscores the challenges of solving an inverse problem for a climate system with inherent nonlinearity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f4dd739f36233f85b623bc7abb71b0aaaed422e9" target='_blank'>
              Neural networks to find the optimal forcing for offsetting the anthropogenic climate change effects
              </a>
            </td>
          <td>
            Huiying Ren, Jian Lu, Z. J. Hou, Tse-Chun Chen, L. R. Leung, Fukai Liu
          </td>
          <td>2024-07-03</td>
          <td>Artificial Intelligence for the Earth Systems</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Modeling thermal states for complex space missions, such as the surface exploration of airless bodies, requires high computation, whether used in ground-based analysis for spacecraft design or during onboard reasoning for autonomous operations. For example, a finite-element thermal model with hundreds of elements can take significant time to simulate, which makes it unsuitable for onboard reasoning during time-sensitive scenarios such as descent and landing, proximity operations, or in-space assembly. Further, the lack of fast and accurate thermal modeling drives thermal designs to be more conservative and leads to spacecraft with larger mass and higher power budgets. The emerging paradigm of physics-informed machine learning (PIML) presents a class of hybrid modeling architectures that address this challenge by combining simplified physics models with machine learning (ML) models resulting in models which maintain both interpretability and robustness. Such techniques enable designs with reduced mass and power through onboard thermal-state estimation and control and may lead to improved onboard handling of off-nominal states, including unplanned down-time. The PIML model or hybrid model presented here consists of a neural network which predicts reduced nodalizations (distribution and size of coarse mesh) given on-orbit thermal load conditions, and subsequently a (relatively coarse) finite-difference model operates on this mesh to predict thermal states. We compare the computational performance and accuracy of the hybrid model to a data-driven neural net model, and a high-fidelity finite-difference model of a prototype Earth-orbiting small spacecraft. The PIML based active nodalization approach provides significantly better generalization than the neural net model and coarse mesh model, while reducing computing cost by up to 1.7x compared to the high-fidelity model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04894c31eb0dcaa0ede3881b219f3bfbb359c4b6" target='_blank'>
              Physics-Informed Machine Learning Towards A Real-Time Spacecraft Thermal Simulator
              </a>
            </td>
          <td>
            Manaswin Oddiraju, Zaki Hasnain, Saptarshi Bandyopadhyay, Eric Sunada, Souma Chowdhury
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this paper, we propose and study several inverse problems of determining unknown parameters in nonlocal nonlinear coupled PDE systems, including the potentials, nonlinear interaction functions and time-fractional orders. In these coupled systems, we enforce non-negativity of the solutions, aligning with realistic scenarios in biology and ecology. There are several salient features of our inverse problem study: the drastic reduction in measurement/observation data due to averaging effects, the nonlinear coupling between multiple equations, and the nonlocality arising from fractional-type derivatives. These factors present significant challenges to our inverse problem, and such inverse problems have never been explored in previous literature. To address these challenges, we develop new and effective schemes. Our approach involves properly controlling the injection of different source terms to obtain multiple sets of mean flux data. This allows us to achieve unique identifiability results and accurately determine the unknown parameters. Finally, we establish a connection between our study and practical applications in biology, further highlighting the relevance of our work in real-world contexts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6630d9bfe19de762af36c38b8a48daf0ad1ea2eb" target='_blank'>
              Inverse problems for coupled nonlocal nonlinear systems arising in mathematical biology
              </a>
            </td>
          <td>
            Ming-Hui Ding, Hongyu Liu, Catharine W. K. Lo
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The integration of particle or Kalman filters with machine learning tools like support vector machines, Gaussian processes, or neural networks has seen extensive exploration in the context of prognostic and health management, particularly in model-based applications. This paper focuses on the Multi-Layer Perceptron Particle Filter (MLP-PF), a data-driven approach that harnesses the non-linearity of MLP to describe degradation trajectories without relying on a physical model. The Bayesian nature of the particle filter is utilized to update MLP parameters, providing flexibility to the method and accommodating unexpected changes in the degradation behavior. To showcase the versatility of MLP-PF, this work demonstrates its seamless integration into diverse use cases, such as lithium-ion battery analysis, virtual health monitoring for turbofans, and the assessment of fatigue crack growth. We illustrate how it effortlessly accommodates various contexts through slight parameter modifications. Adjustment includes variation in the number of neurons or layers in the MLP, threshold adjustments, initial training refinements and the adaptation of the process noise. Addressing different degradation processes across these applications, MLP-PF proves its adaptability and utility in various contexts. These findings highlight the method’s versatility in adapting to diverse use cases and its potential as a robust prognostic tool across various industries. MLP-PF offers a practical and efficient means of estimating remaining useful life and predicting degradation in complex systems, with implications for advancing prognostic tools in diverse applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb6fdd4461741278755a0c986fba0e2f5cd89de3" target='_blank'>
              Data-Driven Prognostics with Multi-Layer Perceptron Particle Filter: a Cross-Industry Exploration
              </a>
            </td>
          <td>
            Francesco Canceliere, Sylvain Girard, Jean-Marc Bourinet
          </td>
          <td>2024-06-27</td>
          <td>PHM Society European Conference</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d332b278e945ecc01ec826948d1f3554cba5312b" target='_blank'>
              On the estimation rate of Bayesian PINN for inverse problems
              </a>
            </td>
          <td>
            Yi Sun, Debarghya Mukherjee, Yves Atchadé
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Neural networks are popular data-driven modeling tools that come with high data collection costs. This paper proposes a residual-based multipeaks adaptive sampling (RMAS) algorithm, which can reduce the demand for a large number of samples in the identification of stochastic dynamical systems. Compared to classical residual-based sampling algorithms, the RMAS algorithm achieves higher system identification accuracy without relying on any hyperparameters. Subsequently, combining the RMAS algorithm and neural network, a few-shot identification (FSI) method for stochastic dynamical systems is proposed, which is applied to the identification of a vegetation biomass change model and the Rayleigh-Van der Pol impact vibration model. We show that the RMAS algorithm modifies residual-based sampling algorithms and, in particular, reduces the system identification error by 76% with the same sample sizes. Moreover, the surrogate model accurately predicts the first escape probability density function and the P bifurcation behavior in the systems, with the error of less than 1.59×10-2. Finally, the robustness of the FSI method is validated.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/295396f31e547b6c81f17b2bb53b2116ac400a8e" target='_blank'>
              A few-shot identification method for stochastic dynamical systems based on residual multipeaks adaptive sampling.
              </a>
            </td>
          <td>
            Xiao-Kai An, Lin Du, Feng Jiang, Yu-jia Zhang, Zi-Chen Deng, Jürgen Kurths
          </td>
          <td>2024-07-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="The paper considers the observer synthesis for nonlinear, time-varying plants with uncertain parameters under multiharmonic disturbance. It is assumed that the relative degree of the plant is known, the regressor linearly depends on the state vector and may have a nonlinear relationship with the output signal. The proposed solution consists of three steps. Initially, an unknown input state observer is synthesized. This observer, however, necessitates the measurement of output derivatives equal to the plant's relative degree. To relax this limitation, an alternative representation of the observer is introduced. Further, based on this observer, the unknown parameters and disturbances are reconstructed using an autoregression model and the dynamic regressor extension and mixing (DREM) approach. This approach allows the estimates to be obtained in a finite time. Finally, based on these estimates, an observer has been constructed that does not require measurements of the output derivatives. The effectiveness and efficiency of this solution are demonstrated through a computer simulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f3f630f0776c6b0ed2b88908f39e12f1e0579daa" target='_blank'>
              State estimation for a class of nonlinear time-varying uncertain system under multiharmonic disturbance
              </a>
            </td>
          <td>
            A. A. Margun, V. Bui, A. A. Bobtsov, Denis V. Efimov
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Advances in experimental techniques allow the collection of high-space-and-time resolution data that track individual motile entities over time. This poses the question of how to use these data to efficiently and effectively calibrate motion models. However, typical mathematical models often overlook the inherent aspects of data collection, such as the discreteness and the experimental noise of the measured locations. In this paper, we focus on velocity-jump models suitable to describe single-agent motion in one spatial dimension, characterised by successive Markovian transitions between a finite network of $n$ states, each with a specified velocity and a fixed rate of switching to every other state. Since the problem of finding the exact distributions of discrete-time noisy data is generally intractable, we derive a series of approximations for the data distributions and compare them to in-silico data generated by the models using four example network structures. These comparisons suggest that the approximations are accurate given sufficiently infrequent state switching, or equivalently, a sufficiently high data collection frequency. Moreover, for infrequent switching, the PDFs comparisons highlight the importance of accounting for the correlation between subsequent measured locations, due to the likely permanence in the state visited in the previous measurement. The approximate distributions computed can be used for fast parameter inference and model selection between a range of velocity-jump models using single-agent tracking data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f4862ab7ce4151b9c0569c8c9b73d89ab08ca0e" target='_blank'>
              Approximate solutions of a general stochastic velocity-jump process subject to discrete-time noisy observations
              </a>
            </td>
          <td>
            Arianna Ceccarelli, Alexander P. Browning, Ruth E. Baker
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The chaotic nature of fluid flow and the uncertainties in initial conditions limit predictability. Small errors that occur in the initial condition can grow exponentially until they saturate at $\mathcal{O}$(1). Ensemble forecasting averages multiple runs with slightly different initial conditions and other data to produce more accurate results and extend the predictability horizon. However, they can be computationally expensive. We develop a penalty-based ensemble method with a shared coefficient matrix to reduce required memory and computational cost and thereby allow larger ensemble sizes. Penalty methods relax the incompressibility condition to decouple the pressure and velocity, reducing memory requirements. This report gives stability proof and an error estimate of the penalty-based ensemble method, extends it to the Navier-Stokes equations with random variables using Monte Carlo sampling, and validates the method's accuracy and efficiency with three numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/52cdf874930613b2823b442be8bdfe4e5d17d2e9" target='_blank'>
              Numerical Analysis of Penalty-based Ensemble Methods
              </a>
            </td>
          <td>
            Rui Fang
          </td>
          <td>2024-07-13</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>2</td>
        </tr>

        <tr id="Beyond estimating parameters of interest from data, one of the key goals of statistical inference is to properly quantify uncertainty in these estimates. In Bayesian inference, this uncertainty is provided by the posterior distribution, the computation of which typically involves an intractable high-dimensional integral. Among available approximation methods, sampling-based approaches come with strong theoretical guarantees but scale poorly to large problems, while variational approaches scale well but offer few theoretical guarantees. In particular, variational methods are known to produce overconfident estimates of posterior uncertainty and are typically non-identifiable, with many latent variable configurations generating equivalent predictions. Here, we address these challenges by showing how diffusion-based models (DBMs), which have recently produced state-of-the-art performance in generative modeling tasks, can be repurposed for performing calibrated, identifiable Bayesian inference. By exploiting a previously established connection between the stochastic and probability flow ordinary differential equations (pfODEs) underlying DBMs, we derive a class of models, inflationary flows, that uniquely and deterministically map high-dimensional data to a lower-dimensional Gaussian distribution via ODE integration. This map is both invertible and neighborhood-preserving, with controllable numerical error, with the result that uncertainties in the data are correctly propagated to the latent space. We demonstrate how such maps can be learned via standard DBM training using a novel noise schedule and are effective at both preserving and reducing intrinsic data dimensionality. The result is a class of highly expressive generative models, uniquely defined on a low-dimensional latent space, that afford principled Bayesian inference.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/17907a21b0ea22f5a5548076e21700dea63c3931" target='_blank'>
              Inflationary Flows: Calibrated Bayesian Inference with Diffusion-Based Models
              </a>
            </td>
          <td>
            Daniela de Albuquerque, John Pearson
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many application areas rely on models that can be readily simulated but lack a closed-form likelihood, or an accurate approximation under arbitrary parameter values. Existing parameter estimation approaches in this setting are generally approximate. Recent work on using neural network models to reconstruct the mapping from the data space to the parameters from a set of synthetic parameter-data pairs suffers from the curse of dimensionality, resulting in inaccurate estimation as the data size grows. We propose a dimension-reduced approach to likelihood-free estimation which combines the ideas of reconstruction map estimation with dimension-reduction approaches based on subject-specific knowledge. We examine the properties of reconstruction map estimation with and without dimension reduction and explore the trade-off between approximation error due to information loss from reducing the data dimension and approximation error. Numerical examples show that the proposed approach compares favorably with reconstruction map estimation, approximate Bayesian computation, and synthetic likelihood estimation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e928447f6c322a7beac72466a09947bc31240ec4" target='_blank'>
              Dimension-reduced Reconstruction Map Learning for Parameter Estimation in Likelihood-Free Inference Problems
              </a>
            </td>
          <td>
            Rui Zhang, O. Chkrebtii, Dongbin Xiu
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Several forms for constructing novel physics-informed neural-networks (PINN) for the solution of partial-differential-algebraic equations based on derivative operator splitting are proposed, using the nonlinear Kirchhoff rod as a prototype for demonstration. The open-source DeepXDE is likely the most well documented framework with many examples. Yet, we encountered some pathological problems and proposed novel methods to resolve them. Among these novel methods are the PDE forms, which evolve from the lower-level form with fewer unknown dependent variables to higher-level form with more dependent variables, in addition to those from lower-level forms. Traditionally, the highest-level form, the balance-of-momenta form, is the starting point for (hand) deriving the lowest-level form through a tedious (and error prone) process of successive substitutions. The next step in a finite element method is to discretize the lowest-level form upon forming a weak form and linearization with appropriate interpolation functions, followed by their implementation in a code and testing. The time-consuming tedium in all of these steps could be bypassed by applying the proposed novel PINN directly to the highest-level form. We developed a script based on JAX. While our JAX script did not show the pathological problems of DDE-T (DDE with TensorFlow backend), it is slower than DDE-T. That DDE-T itself being more efficient in higher-level form than in lower-level form makes working directly with higher-level form even more attractive in addition to the advantages mentioned further above. Since coming up with an appropriate learning-rate schedule for a good solution is more art than science, we systematically codified in detail our experience running optimization through a normalization/standardization of the network-training process so readers can reproduce our results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ea66520b54fdb2078af780a8675f7d6bbe935817" target='_blank'>
              Partial-differential-algebraic equations of nonlinear dynamics by Physics-Informed Neural-Network: (I) Operator splitting and framework assessment
              </a>
            </td>
          <td>
            L. Vu-Quoc, A. Humer
          </td>
          <td>2024-07-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Machine Learning (ML) is extensively used for predicting transfer times for general purpose Wide Area Networks (WANs) or public Internet applications, but for Research and Education Networks (RENs) two major gaps exist in literature. First, RENs i.e. networks carrying large data flows have received limited attention by the networking community. RENs behave differently compared to the general purpose Internet applications and other network types. Hence, ML models from other network types cannot be used interchangeably for large data transfers. Second, the ML models are used as blackboxes to train on measured network values and then used to predict transfer times or other runtime network parameters. In this paper, we present a dynamical systems model of the large data transfers typical of RENs in the form of a system of Ordinary Differential Equations (ODEs) inspired by the Lotka-Volterra competition model. We present a transfer time prediction component called Dynamic Transfer Time Predictor (DTTP) which solves the ODEs and predicts the future transfer times. Second we formulate a loss function based on Lyapunov function called Lyapunov Drift Correction (LDC) that self-corrects the transfer time prediction errors dynamically.To design and develop our model, we studied real-world datasets consisting of over 100 million transfer records collected from platforms such as Open Science Grid (OSG), Large Hadron Collider Optical Private Network (LHCOPN), Worldwide LHC Grid (WLCG), as well as the RENs of Internet2 and ESNet. We integrate our model into well-known neural network models and regressors and present evaluation results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4cf5d71336cf90025f7d641a600628cc1d15b0d7" target='_blank'>
              Improving Transfer Time Prediction of ML Models via Auto-correcting Dynamical Systems Modeling
              </a>
            </td>
          <td>
            Venkat Sai Suman Lamba Karanam, B. Ramamurthy
          </td>
          <td>2024-06-24</td>
          <td>2024 IEEE 10th International Conference on Network Softwarization (NetSoft)</td>
          <td>0</td>
          <td>38</td>
        </tr>

        <tr id="Scenario-based optimization and control has proven to be an efficient approach to account for system uncertainty. In particular, the performance of scenario-based model predictive control (MPC) schemes depends on the accuracy of uncertainty quantification. However, current learning- and scenario-based MPC (sMPC) approaches employ a single timeinvariant probabilistic model (learned offline), which may not accurately describe time-varying uncertainties. Instead, this paper presents a model-agnostic meta-learning (MAML) of Bayesian neural networks (BNN) for adaptive uncertainty quantification that would be subsequently used for adaptive-scenario-tree model predictive control design of nonlinear systems with unknown dynamics to enhance control performance. In particular, the proposed approach learns both a global BNN model and an updating law to refine the BNN model. At each time step, the updating law transforms the global BNN model into more precise local BNN models in real time. The adapted local model is then used to generate scenarios for sMPC design at each time step. A probabilistic safety certificate is incorporated in the scenario generation to ensure that the trajectories of the generated scenarios contain the real trajectory of the system and that all the scenarios adhere to the constraints with a high probability. Experiments using closed-loop simulations of a numerical example demonstrate that the proposed approach can improve the performance of scenario-based MPC compared to using only one BNN model learned offline for all time steps.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/473eca6073be70a1728a3138bc7e859f877dc3e3" target='_blank'>
              Adaptive Uncertainty Quantification for Scenario-based Control Using Meta-learning of Bayesian Neural Networks
              </a>
            </td>
          <td>
            Yajie Bao, Javad Mohammadpour Velni
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="The efficient solution (fast and accurate) of parametric partial differential equations (pPDE) is of major interest in many domains of science and engineering, enabling evaluations of the quantities of interest, optimization, control, and uncertainty propagation—all them under stringent real-time constraints. Different methodologies have been proposed in the past within the model order reduction (MOR) community, based on the use of reduced bases (RB) or the separated representation at the heart of the so-called proper generalized decompositions (PGD). In PGD, an alternate-direction strategy is employed to circumvent the integration issues of operating in multi-dimensional domains. Recently, physics informed neural networks (PINNs), a particular collocation schema where the unknown field is approximated by a neural network (NN), have emerged in the domain of scientific machine learning. PNNs combine the versatility of NN-based approximation with the ease of collocating pPDE. The present paper proposes a combination of both procedures to find an efficient solution for pPDE, that can either be viewed as an efficient collocation procedure for PINN, or as a monolithic PGD that bypasses the use of the fixed-point alternated directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73ebcfa6347c0ff167a1d0bd01e8827132585776" target='_blank'>
              A Parsimonious Separated Representation Empowering PINN–PGD-Based Solutions for Parametrized Partial Differential Equations
              </a>
            </td>
          <td>
            C. Ghnatios, F. Chinesta
          </td>
          <td>2024-07-29</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="The interdependence and high dimensionality of multivariate signals present significant challenges for denoising, as conventional univariate methods often struggle to capture the complex interactions between variables. A successful approach must consider not only the multivariate dependencies of the desired signal but also the multivariate dependencies of the interfering noise. In our previous research, we introduced a method using machine learning to extract the maximum portion of ``predictable information"from univariate signal. We extend this approach to multivariate signals, with the key idea being to properly incorporate the interdependencies of the noise back into the interdependent reconstruction of the signal. The method works successfully for various multivariate signals, including chaotic signals and highly oscillating sinusoidal signals which are corrupted by spatially correlated intensive noise. It consistently outperforms other existing multivariate denoising methods across a wide range of scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/53d763f53bd7322558de4eef640fa8ea0e0002cc" target='_blank'>
              Unsupervised Reservoir Computing for Multivariate Denoising of Severely Contaminated Signals
              </a>
            </td>
          <td>
            Jaesung Choi, Pilwon Kim
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper establishes a method for solving partial differential equations using a multi-step physics-informed deep operator neural network. The network is trained by embedding physics-informed constraints. Different from traditional neural networks for solving partial differential equations, the proposed method uses a deep neural operator network to indirectly construct the mapping relationship between the variable functions and solution functions. This approach makes full use of the hidden information between the variable functions and independent variables. The process whereby the model captures incredibly complex and highly nonlinear relationships is simplified, thereby making network learning easier and enhancing the extraction of information about the independent variables in partial differential systems. In terms of solving partial differential equations, we verify that the multi-step physics-informed deep operator neural network markedly improves the solution accuracy compared with a traditional physics-informed deep neural operator network, especially when the problem involves complex physical phenomena with large gradient changes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ac5d801ec2ae339a109eb310769fe243cbff7025" target='_blank'>
              Multi-Step Physics-Informed Deep Operator Neural Network for Directly Solving Partial Differential Equations
              </a>
            </td>
          <td>
            Jing Wang, Yubo Li, Anping Wu, Zheng Chen, Jun Huang, Qingfeng Wang, Feng Liu
          </td>
          <td>2024-06-25</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Symmetry is one of the most central concepts in physics, and it is no surprise that it has also been widely adopted as an inductive bias for machine-learning models applied to the physical sciences. This is especially true for models targeting the properties of matter at the atomic scale. Both established and state-of-the-art approaches, with almost no exceptions, are built to be exactly equivariant to translations, permutations, and rotations of the atoms. Incorporating symmetries -- rotations in particular -- constrains the model design space and implies more complicated architectures that are often also computationally demanding. There are indications that non-symmetric models can easily learn symmetries from data, and that doing so can even be beneficial for the accuracy of the model. We put a model that obeys rotational invariance only approximately to the test, in realistic scenarios involving simulations of gas-phase, liquid, and solid water. We focus specifically on physical observables that are likely to be affected -- directly or indirectly -- by symmetry breaking, finding negligible consequences when the model is used in an interpolative, bulk, regime. Even for extrapolative gas-phase predictions, the model remains very stable, even though symmetry artifacts are noticeable. We also discuss strategies that can be used to systematically reduce the magnitude of symmetry breaking when it occurs, and assess their impact on the convergence of observables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5dc964683c6ce6be616e96982ba7cf5172752d68" target='_blank'>
              Probing the effects of broken symmetries in machine learning
              </a>
            </td>
          <td>
            Marcel F. Langer, S. Pozdnyakov, Michele Ceriotti
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Physics‐Informed Neural Networks (PINNs) have solved numerous mechanics problems by training to minimize the loss functions of governing partial differential equations (PDEs). Despite successful development of PINNs in various systems, computational efficiency and fidelity prediction have remained profound challenges. To fill such gaps, this study proposed a Physics‐Informed Neural Operator Solver (PINOS) to achieve accurate and fast simulations without any required data set. The training of PINOS adopts a weak form based on the principle of least work for static simulations and a storng form for dynamic systems in solid mechanics. Results from numerical examples indicated that PINOS is capable of approximating solutions notably faster than the benchmarks of PINNs in both static an dynamic systems. The comparisons also showed that PINOS reached a convergence speed of over 20 times faster than finite element software in two‐dimensional and three‐dimensional static problems. Furthermore, this study examined the zero‐shot super‐resolution capability by developing Super‐Resolution PINOS (SR‐PINOS) that was trained on a coarse mesh and validated on fine mesh. The numerical results demonstrate the great performance of the model to obtain accurate solutions with a speed up, suggesting effectiveness in increasing sampling points and scaling a simulation. This study also discusses the differentiation methods of PINOS and SR‐PINOS and suggests potential implementations related to forward applications for promising machine learning methods for structural designs and optimization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bebd3feaac10c5ad88eafaf917f41d32f2aa051e" target='_blank'>
              Physics‐informed neural operator solver and super‐resolution for solid mechanics
              </a>
            </td>
          <td>
            Chawit Kaewnuratchadasorn, Jiaji Wang, Chul‐Woo Kim
          </td>
          <td>2024-07-11</td>
          <td>Computer-Aided Civil and Infrastructure Engineering</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Variational Physics-Informed Neural Networks often suffer from poor convergence when using stochastic gradient-descent-based optimizers. By introducing a Least Squares solver for the weights of the last layer of the neural network, we improve the convergence of the loss during training in most practical scenarios. This work analyzes the computational cost of the resulting hybrid Least-Squares/Gradient-Descent optimizer and explains how to implement it efficiently. In particular, we show that a traditional implementation based on backward-mode automatic differentiation leads to a prohibitively expensive algorithm. To remedy this, we propose using either forward-mode automatic differentiation or an ultraweak-type scheme that avoids the differentiation of trial functions in the discrete weak formulation. The proposed alternatives are up to 100 times faster than the traditional one, recovering a computational cost-per-iteration similar to that of a conventional gradient-descent-based optimizer alone. To support our analysis, we derive computational estimates and conduct numerical experiments in one- and two-dimensional problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/17eda77a3cdea99de9f795c5bb3e5312a9f5c667" target='_blank'>
              Optimizing Variational Physics-Informed Neural Networks Using Least Squares
              </a>
            </td>
          <td>
            C. Uriarte, Manuela Bastidas, David Pardo, Jamie M. Taylor, Sergio Rojas
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Time-varying optimization problems are central to many engineering applications, where performance metrics and system constraints evolve dynamically with time. A number of algorithms have been proposed in recent years to solve such problems; a common feature of all these methods is that they implicitly require precise knowledge of the temporal variability of the solutions in order to exactly track the optimizers. In this paper, we seek to lift these stringent assumptions. Our main result is a fundamental characterization, showing that an algorithm can track an optimal trajectory if and only if it contains a model of the temporal variability of the problem. We refer to this concept to as the internal model principle of time-varying optimization. By recasting the optimization objective as a nonlinear regulation problem and using tools from center manifold theory, we provide necessary and sufficient conditions both for an optimization algorithm to achieve exact asymptotic tracking and for such an algorithm to exist. We illustrate the applicability of the approach numerically on both synthetic problems as well as practical problems in transportation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8aee1c4d1d9bfe7e68a03ee2eb770d81f18d112d" target='_blank'>
              The Internal Model Principle of Time-Varying Optimization
              </a>
            </td>
          <td>
            G. Bianchin, Bryan Van Scoy
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Modeling atmospheric chemistry is complex and computationally intense. Given the recent success of Deep neural networks in digital signal processing, we propose a Neural Network Emulator for fast chemical concentration modeling. We consider atmospheric chemistry as a time-dependent Ordinary Differential Equation. To extract the hidden correlations between initial states and future time evolution, we propose ChemNNE, an Attention based Neural Network Emulator (NNE) that can model the atmospheric chemistry as a neural ODE process. To efficiently simulate the chemical changes, we propose the sinusoidal time embedding to estimate the oscillating tendency over time. More importantly, we use the Fourier neural operator to model the ODE process for efficient computation. We also propose three physical-informed losses to supervise the training optimization. To evaluate our model, we propose a large-scale chemical dataset that can be used for neural network training and evaluation. The extensive experiments show that our approach achieves state-of-the-art performance in modeling accuracy and computational speed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/803f6e09980225170873259886d0fd771d2085f8" target='_blank'>
              Neural Network Emulator for Atmospheric Chemical ODE
              </a>
            </td>
          <td>
            Zhi-Song Liu, Petri S. Clusius, Michael Boy
          </td>
          <td>2024-08-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this work, we discuss the use of a recently introduced machine learning (ML) technique known as Fourier neural operators (FNO) as an efficient alternative to the traditional solution of the time-dependent Schrödinger equation (TDSE). FNOs are ML models which are employed in the approximated solution of partial differential equations. For a wavepacket propagating in an anharmonic potential and for a tunneling system, we show that the FNO approach can accurately and faithfully model wavepacket propagation via the density. Additionally, we demonstrate that FNOs can be a suitable replacement for traditional TDSE solvers in cases where the results of the quantum dynamical simulation are required repeatedly such as in the case of parameter optimization problems (e.g., control). The speed-up from the FNO method allows for its combination with the Markov-chain Monte Carlo approach in applications that involve solving inverse problems such as optimal and coherent laser control of the outcome of dynamical processes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e9adc38ac192d36f40f2d38abb968ebc1c04c08b" target='_blank'>
              Accelerating wavepacket propagation with machine learning.
              </a>
            </td>
          <td>
            Kanishka Singh, Ka Hei Lee, Daniel Peláez, A. Bande
          </td>
          <td>2024-06-21</td>
          <td>Journal of computational chemistry</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Inverse stochastic resonance (ISR) is a phenomenon where noise reduces rather than increases the firing rate of a neuron, sometimes leading to complete quiescence. ISR was first experimentally verified with cerebellar Purkinje neurons. These experiments showed that ISR enables optimal information transfer between the input and output spike train of neurons. Subsequent studies demonstrated the efficiency of information processing and transfer in neural networks with small-world topology. We conducted a numerical investigation into the impact of adaptivity on ISR in a small-world network of noisy FitzHugh-Nagumo (FHN) neurons, operating in a bistable regime with a stable fixed point and a limit cycle -- a prerequisite for ISR. Our results show that the degree of ISR is highly dependent on the FHN model's timescale separation parameter $\epsilon$. The network structure undergoes dynamic adaptation via mechanisms of either spike-time-dependent plasticity (STDP) with potentiation-/depression-domination parameter $P$, or homeostatic structural plasticity (HSP) with rewiring frequency $F$. We demonstrate that both STDP and HSP amplify ISR when $\epsilon$ lies within the bistability region of FHN neurons. Specifically, at larger values of $\epsilon$ within the bistability regime, higher rewiring frequencies $F$ enhance ISR at intermediate (weak) synaptic noise intensities, while values of $P$ consistent with depression-domination (potentiation-domination) enhance (deteriorate) ISR. Moreover, although STDP and HSP parameters may jointly enhance ISR, $P$ has a greater impact on ISR compared to $F$. Our findings inform future ISR enhancement strategies in noisy artificial neural circuits, aiming to optimize information transfer between input and output spike trains in neuromorphic systems, and prompt venues for experiments in neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d7c658287ef239a4055ba2bf2436639fd02518af" target='_blank'>
              Inverse stochastic resonance in adaptive small-world neural networks
              </a>
            </td>
          <td>
            Marius E. Yamakou, Jinjie Zhu, Erik A. Martens
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>9</td>
        </tr>

        <tr id="Transformations are a key tool in the qualitative study of dynamical systems: transformations to a normal form, for example, underpin the study of instabilities and bifurcations. In this work, we test, and when possible establish, an equivalence between two different artificial neural networks by attempting to construct a data-driven transformation between them, using diffusion maps with a Mahalanobis-like metric. If the construction succeeds, the two networks can be thought of as belonging to the same equivalence class. We first discuss transformation functions between only the outputs of the two networks; we then also consider transformations that take into account outputs (activations) of a number of internal neurons from each network. Whitney's theorem dictates the number of (generic) measurements from one of the networks required to reconstruct each and every feature of the second network. The construction of the transformation function relies on a consistent, intrinsic representation of the network input space. We illustrate our algorithm by matching neural network pairs trained to learn (a) observations of scalar functions, (b) observations of two-dimensional vector fields, and (c) representations of images of a moving three-dimensional object (a rotating horse). We also demonstrate reconstruction of a network's input (and output) from minimal partial observations of intermediate neuron activations. The construction of equivalences across different network instantiations clearly relates to transfer learning and will also be valuable in establishing equivalence between different machine learning-based tools.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e0effe6704619a849e7613b7f6a123884ce613df" target='_blank'>
              Transformations establishing equivalence across neural networks: When have two networks learned the same task?
              </a>
            </td>
          <td>
            Tom S. Bertalan, Felix Dietrich, I. Kevrekidis
          </td>
          <td>2024-07-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Areas of computational mechanics such as uncertainty quantification and optimization usually involve repeated evaluation of numerical models that represent the behavior of engineering systems. In the case of complex nonlinear systems however, these models tend to be expensive to evaluate, making surrogate models quite valuable. Artificial neural networks approximate systems very well by taking advantage of the inherent information of its given training data. In this context, this paper investigates the improvement of the training process by including sensitivity information, which are partial derivatives w.r.t. inputs, as outlined by Sobolev training. In computational mechanics, sensitivities can be applied to neural networks by expanding the training loss function with additional loss terms, thereby improving training convergence resulting in lower generalisation error. This improvement is shown in two examples of linear and non-linear material behavior. More specifically, the Sobolev designed loss function is expanded with residual weights adjusting the effect of each loss on the training step. Residual weighting is the given scaling to the different training data, which in this case are response and sensitivities. These residual weights are optimized by an adaptive scheme, whereby varying objective functions are explored, with some showing improvements in accuracy and precision of the general training convergence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bc84ce2dba59d576dd4d3f10a9b0f7b62826171a" target='_blank'>
              Sobolev neural network with residual weighting as a surrogate in linear and non-linear mechanics
              </a>
            </td>
          <td>
            A.O.M. Kilicsoy, J. Liedmann, M. Valdebenito, F. Barthold, M. Faes
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Skilful Machine Learned weather forecasts have challenged our approach to numerical weather prediction, demonstrating competitive performance compared to traditional physics-based approaches. Data-driven systems have been trained to forecast future weather by learning from long historical records of past weather such as the ECMWF ERA5. These datasets have been made freely available to the wider research community, including the commercial sector, which has been a major factor in the rapid rise of ML forecast systems and the levels of accuracy they have achieved. However, historical reanalyses used for training and real-time analyses used for initial conditions are produced by data assimilation, an optimal blending of observations with a physics-based forecast model. As such, many ML forecast systems have an implicit and unquantified dependence on the physics-based models they seek to challenge. Here we propose a new approach, training a neural network to predict future weather purely from historical observations with no dependence on reanalyses. We use raw observations to initialise a model of the atmosphere (in observation space) learned directly from the observations themselves. Forecasts of crucial weather parameters (such as surface temperature and wind) are obtained by predicting weather parameter observations (e.g. SYNOP surface data) at future times and arbitrary locations. We present preliminary results on forecasting observations 12-hours into the future. These already demonstrate successful learning of time evolutions of the physical processes captured in real observations. We argue that this new approach, by staying purely in observation space, avoids many of the challenges of traditional data assimilation, can exploit a wider range of observations and is readily expanded to simultaneous forecasting of the full Earth system (atmosphere, land, ocean and composition).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f12110407568e11614a4bec8a9d4a303c2c72458" target='_blank'>
              Data driven weather forecasts trained and initialised directly from observations
              </a>
            </td>
          <td>
            Anthony McNally, Christian Lessig, Peter Lean, Eulalie Boucher, Mihai Alexe, E. Pinnington, M. Chantry, Simon Lang, Chris Burrows, Marcin Chrust, F. Pinault, Ethel Villeneuve, Niels Bormann, Sean Healy
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="The Path-dependent Neural Jump ODE (PD-NJ-ODE) is a model for online prediction of generic (possibly non-Markovian) stochastic processes with irregular (in time) and potentially incomplete (with respect to coordinates) observations. It is a model for which convergence to the $L^2$-optimal predictor, which is given by the conditional expectation, is established theoretically. Thereby, the training of the model is solely based on a dataset of realizations of the underlying stochastic process, without the need of knowledge of the law of the process. In the case where the underlying process is deterministic, the conditional expectation coincides with the process itself. Therefore, this framework can equivalently be used to learn the dynamics of ODE or PDE systems solely from realizations of the dynamical system with different initial conditions. We showcase the potential of our method by applying it to the chaotic system of a double pendulum. When training the standard PD-NJ-ODE method, we see that the prediction starts to diverge from the true path after about half of the evaluation time. In this work we enhance the model with two novel ideas, which independently of each other improve the performance of our modelling setup. The resulting dynamics match the true dynamics of the chaotic system very closely. The same enhancements can be used to provably enable the PD-NJ-ODE to learn long-term predictions for general stochastic datasets, where the standard model fails. This is verified in several experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba851d147a33cf0c6fcf639b5d4990df283a0b23" target='_blank'>
              Learning Chaotic Systems and Long-Term Predictions with Neural Jump ODEs
              </a>
            </td>
          <td>
            F. Krach, Josef Teichmann
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="The interdisciplinary time-series analysis literature encompasses thousands of statistical features for quantifying interpretable properties of dynamical data. But for any given application, it is likely that just a small subset of informative time-series features is required to capture the dynamical quantities of interest. So, while comprehensive libraries of time-series features have been developed, it is useful to construct reduced and computationally efficient subsets for specific applications. In this work, we demonstrate a systematic process to deduce such a reduced set, focused on the problem of distinguishing changes to functional Magnetic Resonance Imaging (fMRI) time series caused by a range of experimental manipulations of excitatory and inhibitory neural activity in mouse cortical circuits. We reduce a comprehensive library of over 7000 candidate time-series features down to a subset of 16 features, which we call catchaMouse16, that aims to both: (i) accurately characterize biologically relevant properties of fMRI time series; and (ii) minimize inter-feature redundancy. The catchaMouse16 feature set accurately classifies experimental perturbations of neuronal activity from fMRI recordings, and also shows strong generalization performance on an unseen mouse and human resting-state fMRI data where it tracks spatial variations in excitatory and inhibitory cortical cell densities, often with greater statistical power than the full hctsa feature set. We provide an efficient, open-source implementation of the catchaMouse16 feature set in C (achieving an approximately 60 times speed-up relative to the native Matlab code of the same features), with wrappers for Python and Matlab. This work demonstrates a procedure to reduce a large candidate time-series feature set down to the key statistical properties of mouse fMRI dynamics that can be used to efficiently quantify and interpret informative dynamical patterns in neural time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e685be5f1377cb0acf6421ed0e2a179e1e4372df" target='_blank'>
              Canonical time-series features for characterizing biologically informative dynamical patterns in fMRI
              </a>
            </td>
          <td>
            Imran Alam, Brendan Harris, Patrick Cahill, Oliver Cliff, M. Markicevic, Valerio Zerbi, BD Fulcher
          </td>
          <td>2024-07-17</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Deep Gaussian Processes (DGPs) leverage a compositional structure to model non-stationary processes. DGPs typically rely on local inducing point approximations across intermediate GP layers. Recent advances in DGP inference have shown that incorporating global Fourier features from Reproducing Kernel Hilbert Space (RKHS) can enhance the DGPs' capability to capture complex non-stationary patterns. This paper extends the use of these features to compositional GPs involving linear transformations. In particular, we introduce Ordinary Differential Equation (ODE) -based RKHS Fourier features that allow for adaptive amplitude and phase modulation through convolution operations. This convolutional formulation relates our work to recently proposed deep latent force models, a multi-layer structure designed for modelling nonlinear dynamical systems. By embedding these adjustable RKHS Fourier features within a doubly stochastic variational inference framework, our model exhibits improved predictive performance across various regression tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6828a61a888094de685450ee2ce84e8f6bcfb2a8" target='_blank'>
              Adaptive RKHS Fourier Features for Compositional Gaussian Process Models
              </a>
            </td>
          <td>
            Xinxing Shi, Thomas Baldwin-McDonald, Mauricio A. 'Alvarez
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Understanding the dynamics of open quantum systems in strong coupling and non-Markovian regimes remains a formidable theoretical challenge. One popular and well-established method of approximation in these circumstances is provided by the polaron master equation (PME). In this work we reevaluate and extend the validity of the PME to capture the impact of non-Markovian polaron dressing, induced by non-equilibrium open system dynamics. By comparing with numerically exact techniques, we confirm that while the standard PME successfully predicts the dynamics of system observables that commute with the polaron transformation (e.g. populations in the Pauli z-basis), it can struggle to fully capture those that do not (e.g. coherences). This limitation stems from the mixing of system and environment degrees of freedom inherent to the polaron transformation, which affects the accuracy of calculated expectation values within the polaron frame. Employing the Nakajima-Zwanzig projection operator formalism, we introduce correction terms that provide an accurate description of observables that do not commute with the transformation. We demonstrate the significance of the correction terms in two cases, the canonical spin-boson model and a dissipative time-dependent Landau-Zener protocol, where they are shown to impact the system dynamics on both short and long timescales.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97eb69ef1b908a5b349cac016b071bc744ed7afd" target='_blank'>
              Capturing non-Markovian polaron dressing with the master equation formalism
              </a>
            </td>
          <td>
            Jake Iles-Smith, Owen Diba, A. Nazir
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="Following the discovery of the least squares method in 1805 by Legendre and later in 1809 by Gauss, surrogate modeling and machine learning have come a long way. From identifying patterns and trends in process data to predictive modeling, optimization, fault detection, reaction network discovery, and process operations, machine learning became an integral part of all aspects of process design and process systems engineering. This is enabled, at the same time necessitated, by the vast amounts of data that are readily available from processes, increased digitalization, automation, increasing computation power, and simulation software that can model complex phenomena that span over several temporal and spatial scales. Although this paper is not a comprehensive review, it gives an overview of the recent history of machine learning models that we use every day and how they shaped process design problems from the recent advances to the exploration of their prospects.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4ca2456c9116fb14b2abeebf2076d63a64574d2d" target='_blank'>
              From Then to Now and Beyond: Exploring How Machine Learning Shapes Process Design Problems
              </a>
            </td>
          <td>
            Burcu Beykal
          </td>
          <td>2024-07-09</td>
          <td>Systems and Control Transactions</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and identify near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method in 1D Burgers' equation and 2D jellyfish movement control in a fluid environment. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ed151b432bd4c9d4bafa7399dadc48d42ebe3d6" target='_blank'>
              A Generative Approach to Control Complex Physical Systems
              </a>
            </td>
          <td>
            Long Wei, Peiyan Hu, Ruiqi Feng, Haodong Feng, Yixuan Du, Tao Zhang, Rui Wang, Yue Wang, Zhi-Ming Ma, Tailin Wu
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Verification of uncertain, complex dynamical systems is crucial in the modern day world. An increasingly common method to verify complex logic specifications for dynamical systems involves symbolic abstractions: simpler, finite-state models whose behaviour mimics the one of the systems of interest. By sampling trajectories of the concrete, unknown system and via robust analysis, we build a data-driven abstraction, related to the underlying model through a probabilistic behavioural inclusion relation. As the distribution from which the trajectories are drawn is unknown, we adopt two distinct distribution-free theories, namely scenario optimization and conformal prediction. We compare and discuss the differences between the two approaches in terms of the type of guarantees that they are able to provide. Furthermore, via experimental benchmarks we outline the efficiency of the two methods with respect to the number of samples available and the tightness of the guarantees.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4bc6fac276ef549b5a06e10e0b424eaf5ed1fd7" target='_blank'>
              Scenario Approach and Conformal Prediction for Verification of Unknown Systems via Data-Driven Abstractions
              </a>
            </td>
          <td>
            Rudi Coppola, Andrea Peruffo, Lars Lindemann, Manuel Mazo
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Encoding frequency stability constraints in the operation problem is challenging due to its complex dynamics. Recently, data-driven approaches have been proposed to learn the stability criteria offline with the trained model embedded as a constraint of online optimization. However, random sampling of stationary operation points is less efficient in generating balanced stable and unstable samples. Meanwhile, the performance of such a model is strongly dependent on the quality of the training dataset. Observing this research gap, we propose a gradient-based data generation method via forward-mode automatic differentiation. In this method, the original dynamic system is augmented with new states that represent the dynamic of sensitivities of the original states, which can be solved by invoking any ODE solver for a single time. To compensate for the contradiction between the gradient of various frequency stability criteria, gradient surgery is proposed by projecting the gradient on the normal plane of the other. In the end, we demonstrate the superior performance of the proposed sampling algorithm, compared with the unrolling differentiation and finite difference. All codes are available at https://github.com/xuwkk/frequency_sample_ad.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2a8a10d25fc8962321b615ea83477e1fb4d6310" target='_blank'>
              Efficient Sampling for Data-Driven Frequency Stability Constraint via Forward-Mode Automatic Differentiation
              </a>
            </td>
          <td>
            Wangkun Xu, Qian Chen, Pudong Ge, Zhongda Chu, Fei Teng
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Deep learning-based partial differential equation(PDE) solvers have received much attention in the past few years. Methods of this category can solve a wide range of PDEs with high accuracy, typically by transforming the problems into highly nonlinear optimization problems of neural network parameters. This work reviews several deep learning solvers proposed a few years ago, including PINN, WAN, DRM, and VPINN. Numerical results are provided to make comparisons amongst them and address the importance of loss formulation and the optimization method. A rigorous error analysis for PINN is also presented. Finally, we discuss the current limitations and bottlenecks of these methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e8ab3da28291cf15c3b44c4f064373f4597eb015" target='_blank'>
              A Review of Neural Network Solvers for Second-order Boundary Value Problems
              </a>
            </td>
          <td>
            Ramesh Chandra Sau, Luowei Yin
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Symmetries play a pivotal role in our understanding of the properties of quantum many-body systems. While there are theorems and a well-established toolbox for systems in thermal equilibrium, much less is known about the role of symmetries and their connection to dynamics out of equilibrium. This arises due to the direct link between a system's thermal state and its Hamiltonian, which is generally not the case for nonequilibrium dynamics. Here we present a pathway to identify the effective symmetries and to extract them from data in nonequilibrium quantum many-body systems. Our approach is based on exact relations between correlation functions involving different numbers of spatial points, which can be viewed as nonequilibrium versions of (equal-time) Ward identities encoding the symmetries of the system. We derive symmetry witnesses, which are particularly suitable for the analysis of measured or simulated data at different snapshots in time. To demonstrate the potential of the approach, we apply our method to numerical and experimental data for a spinor Bose gas. We investigate the important question of a dynamical restoration of an explicitly broken symmetry of the Hamiltonian by the initial state. Remarkably, it is found that effective symmetry restoration can occur long before the system equilibrates. We also use the approach to define and identify spontaneous symmetry breaking far from equilibrium, which is of great relevance for applications to nonequilibrium phase transitions. Our work opens new avenues for the classification and analysis of quantum as well as classical many-body dynamics in a large variety of systems, ranging from ultracold quantum gases to cosmology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8a8950d1ce041653ba3716007a86405364e34167" target='_blank'>
              Extracting the symmetries of nonequilibrium quantum many-body systems
              </a>
            </td>
          <td>
            A. Mikheev, Viktoria Noel, Ido Siovitz, H. Strobel, M. Oberthaler, Jurgen Berges
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>48</td>
        </tr>

        <tr id="We consider quantum or classical many-body Hamiltonian systems, whose dynamics is given by an integrable, contact interactions, plus another, possibly long-range, generic two-body potential. We show how the dynamics of local observables is given in terms of a generalised version of Bogoliubov-Born-Green-Kirkwood-Yvon hierarchy, which we denote as gBBGKY, which is built for the densities, and their correlations, of the quasiparticles of the underlying integrable model. Unlike the usual cases of perturbation theory from free gases, the presence of local interactions in the integrable model"lifts"the so-called kinetic blocking, and the second layer of the hierarchy reproduces the dynamics at all time-scales. The latter consists of a fast pre-equilibration to a non-thermal steady state, and its subsequent thermalisation to a Gibbs ensemble. We show how the final relaxation is encoded into a Boltzmann scattering integral involving three or higher body-scatterings, and which, remarkably, is entirely determined by the diffusion constants of the underlying integrable model. We check our results with exact molecular dynamics simulations, finding perfect agreement. Our results show how gBBGKY can be successfully employed in quantum systems to compute scattering integrals and Fermi's golden rule transition rates.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e0ea32035f5732ebc13f1df5f3d70be5820746ea" target='_blank'>
              Generalised BBGKY hierarchy for near-integrable dynamics
              </a>
            </td>
          <td>
            Leonardo Biagetti, Maciej Lebek, M. Panfil, J. D. Nardis
          </td>
          <td>2024-08-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="We propose physics-informed holomorphic neural networks (PIHNNs) as a method to solve boundary value problems where the solution can be represented via holomorphic functions. Specifically, we consider the case of plane linear elasticity and, by leveraging the Kolosov-Muskhelishvili representation of the solution in terms of holomorphic potentials, we train a complex-valued neural network to fulfill stress and displacement boundary conditions while automatically satisfying the governing equations. This is achieved by designing the network to return only approximations that inherently satisfy the Cauchy-Riemann conditions through specific choices of layers and activation functions. To ensure generality, we provide a universal approximation theorem guaranteeing that, under basic assumptions, the proposed holomorphic neural networks can approximate any holomorphic function. Furthermore, we suggest a new tailored weight initialization technique to mitigate the issue of vanishing/exploding gradients. Compared to the standard PINN approach, noteworthy benefits of the proposed method for the linear elasticity problem include a more efficient training, as evaluations are needed solely on the boundary of the domain, lower memory requirements, due to the reduced number of training points, and $C^\infty$ regularity of the learned solution. Several benchmark examples are used to verify the correctness of the obtained PIHNN approximations, the substantial benefits over traditional PINNs, and the possibility to deal with non-trivial, multiply-connected geometries via a domain-decomposition strategy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e71714f3dda61e6e50ace4788915467f7d4f810" target='_blank'>
              Physics-Informed Holomorphic Neural Networks (PIHNNs): Solving Linear Elasticity Problems
              </a>
            </td>
          <td>
            Matteo Calafa, Emil Hovad, A. Engsig-Karup, T. Andriollo
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="We present an expansion of a many-body correlation function in a sum of pseudomodes - exponents with complex frequencies that encompass both decay and oscillations. We demonstrate that, typically, it is enough to take a few first terms of this infinite sum to obtain an excellent approximation to the correlation function at any time, with the large time behavior being determined solely by the first pseudomode. The pseudomode expansion emerges in the framework of the Heisenberg version of the recursion method. This method essentially solves Heisenberg equations in a Lanczos tridiagonal basis constructed in the Krylov space of a given observable. To obtain pseudomodes, we first add artificial dissipation satisfying the dissipative generalization of the universal operator growth hypothesis, and then take the limit of the vanishing dissipation strength. Fast convergence of the pseudomode expansion is facilitated by the localization in the Krylov space, which is generic in the presence of dissipation and can survive the limit of the vanishing dissipation strength. As an illustration, we apply the pseudomode expansion to calculate infinite-temperature autocorrelation functions in the quantum Ising and $XX$ spin-$1/2$ models on the square lattice.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c0e1c45eedb9a16b0066dded4a252c6974da710" target='_blank'>
              Pseudomode expansion of many-body correlation functions
              </a>
            </td>
          <td>
            Alexander Teretenkov, F. Uskov, Oleg Lychkovskiy
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Studying the temporal evolution of complex systems requires tools able to detect the presence and quantify the strength of predictable dynamics within their output signals. Information theory aids in such a description, particularly through information storage (IS), which reflects the regularity of system dynamics by measuring the information shared between the present and the past system states. While the conventional IS computation provides an overall measure of predictable information, transient behaviors of predictability occurring during system transitions can be assessed by time-resolved measures such as the local information storage (L-IS), assuming stationarity, and the time-varying information storage (TV-IS), without stationarity assumptions. In this work, through a comparative analysis in simulated and real contexts, we aim to demonstrate how these methods complement each other and reveal dynamic changes of the system behavior associated to state transitions. In simulations, we show that the TV-IS can effectively track sudden changes of the information stored in the system, which is reflected in its average value computed over specific time intervals; on the other hand, the surprise originated by the emergence of a change in the predictability of the system is reflected in the variance of the L-IS computed within specific time intervals. In neurophysiological applications, the distinct phenomena of respiratory activity in sleep apnea and brain activity during somatosensory stimulation both reveal a significant decrease of IS evoked by state transitions, highlighting how such transitions can inject new information in physiological systems, affecting significantly their internal dynamics. Overall, TV-IS and L-IS appear to provide different and complementary information about the behavior of the systems under investigation, thereby offering valuable tools for the study of complex physiological systems where both stationary and non-stationary conditions may be present.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c3b648f9e9d48cea5dda06ced2a7fa4a474d58a" target='_blank'>
              Exploring transient neurophysiological states through local and time-varying measures of Information Dynamics
              </a>
            </td>
          <td>
            Y. Antonacci, C. Barà, G. De Felice, A. Sferlazza, R. Pernice, L. Faes
          </td>
          <td>2024-06-24</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="In the central nervous system, sequences of neural activity form trajectories on low dimensional neural manifolds. The neural computation underlying flexible cognition and behavior relies on dynamic control of these structures. For example different tasks or behaviors are represented on different subspaces, requiring fast timescale subspace rotation to move from one behavior to the next. For flexibility in a particular behavior, the neural trajectory must be dynamically controllable within that behaviorally determined subspace. To understand how dynamic control of neural trajectories and their underlying subspaces may be implemented in neural circuits, we first characterized the relationship between features of neural activity sequences and aspects of the low dimensional projection. Based on this, we propose neural mechanisms that can act within local circuits to modulate activity sequences thereby controlling neural trajectories in low dimensional subspaces. In particular, we show that gain modulation and transient synaptic currents control the speed and path of neural trajectories and clustered inhibition determines manifold orientation. Together, these neural mechanisms may enable a substrate for fast timescale computation on neural manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0a6120a5f2c70336b63e76de5fc920ea9b4af59d" target='_blank'>
              Dynamic control of neural manifolds
              </a>
            </td>
          <td>
            Andrew B. Lehr, Arvind Kumar, Christian Tetzlaff
          </td>
          <td>2024-07-11</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Piecewise-deterministic Markov processes (PDMPs) are often used to model abrupt changes in the global environment or capabilities of a controlled system. This is typically done by considering a set of"operating modes"(each with its own system dynamics and performance metrics) and assuming that the mode can switch stochastically while the system state evolves. Such models have a broad range of applications in engineering, economics, manufacturing, robotics, and biological sciences. Here, we introduce and analyze an"occasionally observed"version of mode-switching PDMPs. We show how such systems can be controlled optimally if the planner is not alerted to mode-switches as they occur but may instead have access to infrequent mode observations. We first develop a general framework for handling this through dynamic programming on a higher-dimensional mode-belief space. While quite general, this method is rarely practical due to the curse of dimensionality. We then discuss assumptions that allow for solving the same problem much more efficiently, with the computational costs growing linearly (rather than exponentially) with the number of modes. We use this approach to derive Hamilton-Jacobi-Bellman PDEs and quasi-variational inequalities encoding the optimal behavior for a variety of planning horizons (fixed, infinite, indefinite, random) and mode-observation schemes (at fixed times or on-demand). We discuss the computational challenges associated with each version and illustrate the resulting methods on test problems from surveillance-evading path planning. We also include an example based on robotic navigation: a Mars rover that minimizes the expected time to target while accounting for the possibility of unobserved/incremental damages and dynamics-altering breakdowns.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/96c947b88c4459fcab4ac11d8b03e77f99a42182" target='_blank'>
              Occasionally Observed Piecewise-deterministic Markov Processes
              </a>
            </td>
          <td>
            Marissa Gee, Alexander Vladimirsky
          </td>
          <td>2024-08-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this article we study if a Deep Learning technique can be used to obtain an approximated value of the Lyapunov exponents of a dynamical system. Moreover, we want to know if Machine Learning techniques are able, once trained, to provide the complete Lyapunov exponents spectrum with just single-variable time series. We train a Convolutional Neural Network and we use the resulting network to approximate the complete spectrum using the time series of just one variable from the studied systems (Lorenz system and coupled Lorenz system). The results are quite stunning as all the values are well approximated with only partial data. This strategy permits to speed up the complete analysis of the systems and also to study the hyperchaotic dynamics in the coupled Lorenz system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/09290aec6c4160820790c56611a3f3eca51095dc" target='_blank'>
              Full Lyapunov Exponents spectrum with Deep Learning from single-variable time series
              </a>
            </td>
          <td>
            C. Mayora-Cebollero, A. Mayora-Cebollero, 'Alvaro Lozano, Roberto Barrio
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="
 Reservoir computing is an efficient artificial neural network for model-free prediction and analysis of dynamical systems time series. As a data-based method, the capacity of reservoir computing is stongly affected by the time sampling interval of training data. In this paper, taking Lorenz system as an example, we explore the influence of this sampling interval on the performance of reservoir computing in predicting chaotic sequences. When the sampling interval increases, the prediction capacity of reservoir computing is first enhanced then weakened, presenting a bell-shaped curve. By slightly revising the calculation method of the output matrix, the prediction performance of reservoir computing with small sampling interval can be improved. Furthermore, reservoir computing can learn and reproduce the state of chaotic system with a large time interval, which is almost five times larger than that of the classic fourth-order Runge-Kutta method. Our results show the capacity of reservoir computing in the applications where the time sampling intervals are constrained and laid the foundation for building a fast algorithm with larger time iteration steps.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2299dd61e3d42dbb604ec45370f99d03bbd6e6f9" target='_blank'>
              Large sampling intervals for learning and predicting chaotic systems with reservoir computing
              </a>
            </td>
          <td>
            Qingyan Xie, Zixiang Yan, Hui Zhao, Jian Gao, Jinghua Xiao
          </td>
          <td>2024-06-28</td>
          <td>Journal of Physics A: Mathematical and Theoretical</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Many problems in Physics and Chemistry are formulated as the minimization of a functional. Therefore, methods for solving these problems typically require differentiating maps whose input and/or output are functions -- commonly referred to as variational differentiation. Such maps are not addressed at the mathematical level by the chain rule, which underlies modern symbolic and algorithmic differentiation (AD) systems. Although there are algorithmic solutions such as tracing and reverse accumulation, they do not provide human readability and introduce strict programming constraints that bottleneck performance, especially in high-performance computing (HPC) environments. In this manuscript, we propose a new computer theoretic model of differentiation by combining the pullback of the $\mathbf{B}$ and $\mathbf{C}$ combinators from the combinatory logic. Unlike frameworks based on the chain rule, this model differentiates a minimal complete basis for the space of computable functions. Consequently, the model is capable of analytic backpropagation and variational differentiation while supporting complex numbers. To demonstrate the generality of this approach we build a system named CombDiff, which can differentiate nontrivial variational problems such as Hartree-Fock (HF) theory and multilayer perceptrons.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c640b0ba0b06a2a81e8f349b2681297670ab04e" target='_blank'>
              Automating Variational Differentiation
              </a>
            </td>
          <td>
            Kangbo Li, Anil Damle
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Stochastic systems are used to model a variety of phenomena in which noise plays an essential role. In these models, one potential goal is to determine if noise can induce transitions between states, and if so, to calculate the most probable escape path from an attractor. In the small noise limit, the Freidlin-Wentzell theory of large deviations provides a variational framework to calculate these paths. This work focuses on using large deviation theory to calculate such paths for stochastic gradient systems with non-gradient perturbations. While for gradient systems the most probable escape paths consist of time-reversed heteroclinic orbits, for general systems it can be a challenging calculation. By applying Melnikov theory to the resulting Euler-Lagrange equations recast in Hamiltonian form, we determine a condition for when the optimal escape path is the heteroclinic orbit for the perturbed system. We provide a numerical example to illustrate how the computed most probable escape path compares with the theoretical result.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/330a794bfa885f5551f29fba1960f92ebc6ccf55" target='_blank'>
              Most probable escape paths in perturbed gradient systems
              </a>
            </td>
          <td>
            Katherine Slyman, Mackenzie Simper, John A Gemmer, Bjorn Sandstede
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector $\theta \in \mathbb{R}^n$, aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with $n$ is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator--the generator gradient estimator--for which the computation time remains stable in $n$. In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as $n$ increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6fc1545e20fb78c9271560d34a3512c20ad26286" target='_blank'>
              An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations
              </a>
            </td>
          <td>
            Shengbo Wang, Jose Blanchet, Peter Glynn
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Diffusion models (DMs) are a class of generative machine learning methods that sample a target distribution by transforming samples of a trivial (often Gaussian) distribution using a learned stochastic differential equation. In standard DMs, this is done by learning a ``score function'' that reverses the effect of adding diffusive noise to the distribution of interest. Here we consider the generalisation of DMs to lattice systems with discrete degrees of freedom, and where noise is added via Markov chain jump dynamics. We show how to use tensor networks (TNs) to efficiently define and sample such ``discrete diffusion models'' (DDMs) without explicitly having to solve a stochastic differential equation. We show the following: (i) by parametrising the data and evolution operators as TNs, the denoising dynamics can be represented exactly; (ii) the auto-regressive nature of TNs allows to generate samples efficiently and without bias; (iii) for sampling Boltzmann-like distributions, TNs allow to construct an efficient learning scheme that integrates well with Monte Carlo. We illustrate this approach to study the equilibrium of two models with non-trivial thermodynamics, the $d=1$ constrained Fredkin chain and the $d=2$ Ising model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4033d59c9ab7aac9bd3ba446669a55ca00e0800c" target='_blank'>
              Discrete generative diffusion models without stochastic differential equations: a tensor network approach
              </a>
            </td>
          <td>
            Luke Causer, Grant M. Rotskoff, J. P. Garrahan
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>54</td>
        </tr>

        <tr id="In this study, we considered the problem of estimating epidemiological parameters based on physics-informed neural networks (PINNs). In practice, not all trajectory data corresponding to the population estimated by epidemic models can be obtained, and some observed trajectories are noisy. Learning PINNs to estimate unknown epidemiological parameters using such partial observations is challenging. Accordingly, we introduce the concept of algebraic observability into PINNs. The validity of the proposed PINN, named as an algebraically observable PINNs, in terms of estimation parameters and prediction of unobserved variables, is demonstrated through numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0f7cf742ff57aaaa10202e43c7d6149e073d6568" target='_blank'>
              Estimate Epidemiological Parameters given Partial Observations based on Algebraically Observable PINNs
              </a>
            </td>
          <td>
            Mizuka Komatsu
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Identifying the causal relations between interested variables plays a pivotal role in representation learning as it provides deep insights into the dataset. Identifiability, as the central theme of this approach, normally hinges on leveraging data from multiple distributions (intervention, distribution shift, time series, etc.). Despite the exciting development in this field, a practical but often overlooked problem is: what if those distribution shifts happen sequentially? In contrast, any intelligence possesses the capacity to abstract and refine learned knowledge sequentially -- lifelong learning. In this paper, with a particular focus on the nonlinear independent component analysis (ICA) framework, we move one step forward toward the question of enabling models to learn meaningful (identifiable) representations in a sequential manner, termed continual causal representation learning. We theoretically demonstrate that model identifiability progresses from a subspace level to a component-wise level as the number of distributions increases. Empirically, we show that our method achieves performance comparable to nonlinear ICA methods trained jointly on multiple offline distributions and, surprisingly, the incoming new distribution does not necessarily benefit the identification of all latent variables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d9dd025ff21a417d50c5564f864be35cbb2325c6" target='_blank'>
              Continual Learning of Nonlinear Independent Representations
              </a>
            </td>
          <td>
            Boyang Sun, Ignavier Ng, Guan-Hong Chen, Yifan Shen, Qirong Ho, Kun Zhang
          </td>
          <td>2024-08-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="In this paper we introduce a Meshfree Variational Physics Informed Neural Network. It is a Variational Physics Informed Neural Network that does not require the generation of a triangulation of the entire domain and that can be trained with an adaptive set of test functions. In order to generate the test space we exploit an a posteriori error indicator and add test functions only where the error is higher. Four training strategies are proposed and compared. Numerical results show that the accuracy is higher than the one of a Variational Physics Informed Neural Network trained with the same number of test functions but defined on a quasi-uniform mesh.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8a9a2ceba8c6c319fce3b13aa5f1c315e14309ac" target='_blank'>
              Meshfree Variational Physics Informed Neural Networks (MF-VPINN): an adaptive training strategy
              </a>
            </td>
          <td>
            S. Berrone, Moreno Pintore
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Over the course of four decades, model predictive control (MPC) has become one of the great success stories in systems and control. It has grown from its native habitat (chemical process control) into all domains of control applications—power and energy systems, mechatronics and robotics, as well as aerospace and aeronautics. Hence, in a modern systems and control curriculum, MPC triggers not so much the question of if it should be taught. In fact, industrial demand for and the continued research potential of MPC suggest that one should rather ask the Aristotelian 5W1H (what, when, where, why, who, and how?) about teaching MPC. This article presents insights into the 5Ws distilled from the results of a survey on teaching MPC conducted in the systems and control community. Moreover, the how is approached through blueprint suggestions for curricula for an undergraduate discrete-time linear-quadratic MPC course and for graduate courses covering the continuous-time nonlinear avenue and the learning-based route.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/95cbb4bf5304c328ffa01fa4a5e73c270ccce7a8" target='_blank'>
              Teaching Model Predictive Control: What, When, Where, Why, Who, and How? [Focus on Education]
              </a>
            </td>
          <td>
            T. Faulwasser, Eric C. Kerrigan, Filip Logist, Sergio Lucia, M. Mönnigmann, Alessandra Parisio, M. S. Darup
          </td>
          <td>2024-08-01</td>
          <td>IEEE Control Systems</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Wild animals are commonly fitted with trackers that record their position through time, to learn about their behaviour. Broadly, statistical models for tracking data often fall into two categories: local models focus on describing small-scale movement decisions, and global models capture large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals' distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal's movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, e.g., to account for daily cycles in an animal's activity. We formulate the model as a state-space model and present a method of inference based on the Kalman filter. The approach requires discretising the continuous-time process, and we use simulations to investigate performance for various time resolutions of observation. The approach works well at fine resolutions, though the estimated stationary distribution tends to be too flat when time intervals between observations are very long.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9df6a9afdb8773bfa193d6bae41f91e4a44b70ca" target='_blank'>
              Multiscale modelling of animal movement with persistent dynamics
              </a>
            </td>
          <td>
            T. Michelot
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="The ubiquity of multiscale interactions in complex systems is well-recognized, with development and heredity serving as a prime example of how processes at different temporal scales influence one another. This work introduces a novel multiscale state-space model to explore the dynamic interplay between systems interacting across different time scales, with feedback between each scale. We propose a Bayesian learning framework to estimate unknown states by learning the unknown process noise covariances within this multiscale model. We develop a Particle Gibbs with Ancestor Sampling (PGAS) algorithm for inference and demonstrate through simulations the efficacy of our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a4108fef8d82f9c5401baeb3eaafc732ca1c2a3" target='_blank'>
              Bayesian Learning in a Nonlinear Multiscale State-Space Model
              </a>
            </td>
          <td>
            Nayely V'elez-Cruz, Manfred D. Laubichler
          </td>
          <td>2024-08-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="


Ecosystems contain numerous species interacting with each other and the environment. However, data on all relevant state variables is rarely available, hampering inference and prediction. Empirical dynamic modelling (EDM) is a valuable tool for prediction, inference and control in such partially observed systems.

However, EDM typically assumes that the available time series are observed without error. Failing to account for observation noise strongly biases estimates of Lyapunov exponents and reduces forecast accuracy. To address this limitation, we propose incorporating EDM into a hidden Markov framework and using an iterative scheme based on the expectation maximization (EM) algorithm to obtain filtered state and parameter estimates.

We evaluate the performance of this approach on several simulated dynamical systems with a range of additive noise levels, as well as on insect population time series.

Accounting for observation noise improved accuracy of population forecasts and estimates of Lyapunov exponents (LE) over a wide range of noise levels relevant to ecological time series.

">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e9c4d6519c0328c66d02635359f84c4d6cf2f4d" target='_blank'>
              Accounting for observation noise in equation‐free forecasting: The hidden‐Markov S‐map
              </a>
            </td>
          <td>
            Dylan Esguerra, Stephan B. Munch
          </td>
          <td>2024-07-01</td>
          <td>Methods in Ecology and Evolution</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this work, we introduce a novel learning-based controller tailored for autonomous control of a batch-type precipitation process involving calcium and magnesium carbonates. The process takes in fluid containing valuable materials such as Ca+2and Mg+2 ions, along with impurities and seed particles, to facilitate the sequential precipitation of these ions into their respective carbonates. The controller's goal is to attain a specified size of the precipitated particles under different process uncertainties. Here the residence time, i.e. the time allowed for the ions to remain in fluid phase, is used as the manipulation variable. The controller is designed as a solution to a stochastic optimal control problem and implemented using machine learning techniques. For the prediction model, we use convolutional neural networks (CNN) and for the control synthesis, we use a type of recurrent neural networks (RNNs). The designed control is learnable, adaptable to varying process dynamics and robust to random disturbances in the process, thus resulting in a learnable adaptive, and robust controller (LARC). The effectiveness of LARC is validated through different simulation-based tests.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ca52c12e0ce9a97c03f851037abfed99e3a3847" target='_blank'>
              Learnable Adaptive and Robust Controller for a Two Particle Carbonate Precipitation Process
              </a>
            </td>
          <td>
            Mikhail Kakanov, S. Hiremath, Andreas Voigt, Kai Sundmacher, N. Bajçinca
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Compartment models with delay terms are widely used across a range of disciplines. The motivation to include delay terms varies across different contexts. In epidemiological and pharmacokinetic models, the delays are often used to represent an incubation period. In this work, we derive a compartment model with delay terms from an underlying non-Markov stochastic process. Delay terms arise when waiting times are drawn from a delay exponential distribution. This stochastic process approach allows us to preserve the physicality of the model, gaining understanding into the conditions under which delay terms can arise. By providing the conditions under which the delay exponential function is a probability distribution, we establish a critical value for the delay terms. An exact stochastic simulation method is introduced for the generalized model, enabling us to utilize the simulation in scenarios where intrinsic stochasticity is significant, such as when the population size is small. We illustrate the applications of the model and validate our simulation algorithm on examples drawn from epidemiology and pharmacokinetics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15bad9e29f43321e2682f7822900b1143d36ab9c" target='_blank'>
              Delay compartment models from a stochastic process
              </a>
            </td>
          <td>
            C. Angstmann, A. V. McGann, Zhuang Xu
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>16</td>
        </tr>

        <tr id="Modern climate projections often suffer from inadequate spatial and temporal resolution due to computational limitations, resulting in inaccurate representations of sub-grid processes. A promising technique to address this is the Multiscale Modeling Framework (MMF), which embeds a kilometer-resolution cloud-resolving model within each atmospheric column of a host climate model to replace traditional convection and cloud parameterizations. Machine learning (ML) offers a unique opportunity to make MMF more accessible by emulating the embedded cloud-resolving model and reducing its substantial computational cost. Although many studies have demonstrated proof-of-concept success of achieving stable hybrid simulations, it remains a challenge to achieve near operational-level success with real geography and comprehensive variable emulation that includes, for example, explicit cloud condensate coupling. In this study, we present a stable hybrid model capable of integrating for at least 5 years with near operational-level complexity, including real geography, seasonality, explicit cloud condensate predictions, and land coupling. Our model demonstrates skillful online performance in metrics such as 5-year zonal mean biases compared to previous MMF emulation studies. The monthly error against reference MMF simulations with the same initial condition approaches the fundamental predictability limit. Key factors contributing to our online performance include an expressive U-Net architecture, additional input features that include large-scale forcings and convection memory, and physical thermodynamic constraints for microphysics. With microphysical constraints mitigating unrealistic cloud formation, our work is the first to demonstrate realistic multi-year cloud condensate climatology under the MMF framework. Our work showcases ML parameterization's potential for operational-level climate simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71dfebd2303c30bf8e52df7f42375cda87b276cf" target='_blank'>
              Stable Machine-Learning Parameterization of Subgrid Processes with Real Geography and Full-physics Emulation
              </a>
            </td>
          <td>
            Zeyuan Hu, Akshay Subramaniam, Zhiming Kuang, Jerry Lin, Sungduk Yu, Walter M. Hannah, Noah D. Brenowitz, Josh Romero, Michael S. Pritchard
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="This paper studies the stability and convergence properties of a class of multi-agent concurrent learning (CL) algorithms with momentum and restart. Such algorithms can be integrated as part of the estimation pipelines of data-enabled multi-agent control systems to enhance transient performance while maintaining stability guarantees. However, characterizing restarting policies that yield stable behaviors in decentralized CL systems, especially when the network topology of the communication graph is directed, has remained an open problem. In this paper, we provide an answer to this problem by synergistically leveraging tools from graph theory and hybrid dynamical systems theory. Specifically, we show that under a cooperative richness condition on the overall multi-agent system's data, and by employing coordinated periodic restart with a frequency that is tempered by the level of asymmetry of the communication graph, the resulting decentralized dynamics exhibit robust asymptotic stability properties, characterized in terms of input-to-state stability bounds, and also achieve a desirable transient performance. To demonstrate the practical implications of the theoretical findings, three applications are also presented: cooperative parameter estimation over networks with private data sets, cooperative model-reference adaptive control, and cooperative data-enabled feedback optimization of nonlinear plants.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/567d4ebaa08c236655c7c5193532c8e7f8713fc7" target='_blank'>
              Decentralized Concurrent Learning with Coordinated Momentum and Restart
              </a>
            </td>
          <td>
            Daniel E. Ochoa, Muhammad U. Javed, Xudong Chen, Jorge I. Poveda
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="This paper is concerned with the fundamental limits of nonlinear dynamical system learning from input-output traces. Specifically, we show that recurrent neural networks (RNNs) are capable of learning nonlinear systems that satisfy a Lipschitz property and forget past inputs fast enough in a metric-entropy optimal manner. As the sets of sequence-to-sequence maps realized by the dynamical systems we consider are significantly more massive than function classes generally considered in deep neural network approximation theory, a refined metric-entropy characterization is needed, namely in terms of order, type, and generalized dimension. We compute these quantities for the classes of exponentially-decaying and polynomially-decaying Lipschitz fading-memory systems and show that RNNs can achieve them.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/14056322161789dd48312016e9d0e76682856a25" target='_blank'>
              Metric-Entropy Limits on Nonlinear Dynamical System Learning
              </a>
            </td>
          <td>
            Yang Pan, Clemens Hutter, Helmut Bölcskei
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Prolonged contact between a corrosive liquid and metal alloys can cause progressive dealloying. For such liquid-metal dealloying (LMD) process, phase field models have been developed. However, the governing equations often involve coupled non-linear partial differential equations (PDE), which are challenging to solve numerically. In particular, stiffness in the PDEs requires an extremely small time steps (e.g. $10^{-12}$ or smaller). This computational bottleneck is especially problematic when running LMD simulation until a late time horizon is required. This motivates the development of surrogate models capable of leaping forward in time, by skipping several consecutive time steps at-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators (U-AFNO), a machine learning (ML) model inspired by recent advances in neural operator learning. U-AFNO employs U-Nets for extracting and reconstructing local features within the physical fields, and passes the latent space through a vision transformer (ViT) implemented in the Fourier space (AFNO). We use U-AFNOs to learn the dynamics mapping the field at a current time step into a later time step. We also identify global quantities of interest (QoI) describing the corrosion process (e.g. the deformation of the liquid-metal interface) and show that our proposed U-AFNO model is able to accurately predict the field dynamics, in-spite of the chaotic nature of LMD. Our model reproduces the key micro-structure statistics and QoIs with a level of accuracy on-par with the high-fidelity numerical solver. We also investigate the opportunity of using hybrid simulations, in which we alternate forward leap in time using the U-AFNO with high-fidelity time stepping. We demonstrate that while advantageous for some surrogate model design choices, our proposed U-AFNO model in fully auto-regressive settings consistently outperforms hybrid schemes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/868bad8571187ae2f328db235d6feb8d496140d9" target='_blank'>
              Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier Neural Operator with U-Net Backbone
              </a>
            </td>
          <td>
            Christophe Bonneville, N. Bieberdorf, Arun Hegde, Mark Asta, H. Najm, Laurent Capolungo, C. Safta
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>44</td>
        </tr>

        <tr id="Data-driven deep learning models are on the verge of transforming global weather forecasting. It is an open question if this success can extend to climate modeling, where long inference rollouts and data complexity pose significant challenges. Here, we present the first conditional generative model able to produce global climate ensemble simulations that are accurate and physically consistent. Our model runs at 6-hourly time steps and is shown to be stable for 10-year-long simulations. Our approach beats relevant baselines and nearly reaches a gold standard for successful climate model emulation. We discuss the key design choices behind our dynamics-informed diffusion model-based approach which enables this significant step towards efficient, data-driven climate simulations that can help us better understand the Earth and adapt to a changing climate.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d369ffa4e13e39e6b7c4b96ff3ad94b4aa9a3c23" target='_blank'>
              Probabilistic Emulation of a Global Climate Model with Spherical DYffusion
              </a>
            </td>
          <td>
            Salva Rühling Cachay, Brian Henn, Oliver Watt‐Meyer, Christopher S. Bretherton, Rose Yu
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Analyzing complex experimental data with multiple parameters is challenging. We propose using Singular Value Decomposition (SVD) as an effective solution. This method, demonstrated through real experimental data analysis, surpasses conventional approaches in understanding complex physics data. Singular values and vectors distinguish and highlight various physical mechanisms and scales, revealing previously challenging elements. SVD emerges as a powerful tool for navigating complex experimental landscapes, showing promise for diverse experimental measurements.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/24e7da06892289a90ca7ca4e5171ea48a7012501" target='_blank'>
              Unraveling Complexity: Singular Value Decomposition in Complex Experimental Data Analysis
              </a>
            </td>
          <td>
            Judith F. Stein, Aviad Frydman, R. Berkovits
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="The spin-vector Monte Carlo model is widely used as a benchmark for the classicality of quantum annealers but severely restricts the time evolution. The spin-vector Langevin (SVL) model has been proposed and tested as an alternative, closely reproducing the real-time dynamics of physical quantum annealers such as D-Wave machines in the dissipative regime. We investigate the SVL annealing dynamics of classical O(2) rotors on regular graphs, identifying universal features in the nonequilibrium dynamics when changing the range of interactions and the topology of the graph. Regular graphs with low connectance or edge density exhibit universal scaling dynamics consistent with the Kibble-Zurek mechanism, which leads to a power-law dependence of the density of defects and the residual energy as a function of the annealing time. As the interaction range is increased, the power-law scaling is suppressed, and an exponential scaling with the annealing time sets in. Our results establish a universal breakdown of the Kibble-Zurek mechanism in classical systems characterized by long-range interactions, in sharp contrast with previous findings in the quantum domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e358e334c41ef5944c2b2f71d0941cafc762ea6d" target='_blank'>
              Annealing Dynamics of Regular Rotor Networks: Universality and Its Breakdown
              </a>
            </td>
          <td>
            Andr'as Grabarits, Gaetano Sammartino, Adolfo del Campo
          </td>
          <td>2024-07-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This work addresses the synthesis of optimal feedback control laws via machine learning. In particular, the Averaged Feedback Learning Scheme (AFLS) and a data driven method are considered. Hypotheses for each method ensuring the convergence of the evaluation of the objective function of the underlying control problem at the obtained feedback-laws towards the optimal value function are provided. These hypotheses are connected to the regularity of the value function and the stability of the dynamics. In the case of AFLS these hypotheses only require H\"older continuity of the value function, whereas for the data driven method the value function must be at least $C^2$. It is demonstrated that these methods are connected via their optimality conditions. Additionally, numerical experiments are provided by applying both methods to a family control problems, parameterized by a positive real number which controls the regularity of the value function. For small parameters the value function is smooth and in contrast for large parameters it is non-differentiable, but semi-concave. The results of the experiments indicate that both methods have a similar performance for the case that the value function is smooth. On the other hand, if the value function is not differentiable, AFLS has a better performance which is consistent with the obtained convergence results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/307b109ee8daf5810e48f9fba58490e24a131f80" target='_blank'>
              Convergence of machine learning methods for feedback control laws: averaged feedback learning scheme and data driven method
              </a>
            </td>
          <td>
            Karl Kunisch, Donato V'asquez-Varas
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Predicting the trajectories of systems with unknown dynamics (\textit{i.e.} the governing rules) is crucial in various research fields, including physics and biology. This challenge has gathered significant attention from diverse communities. Most existing works focus on learning fixed system dynamics within one single system. However, real-world applications often involve multiple systems with different types of dynamics or evolving systems with non-stationary dynamics (dynamics shifts). When data from those systems are continuously collected and sequentially fed to machine learning models for training, these models tend to be biased toward the most recently learned dynamics, leading to catastrophic forgetting of previously observed/learned system dynamics. To this end, we aim to learn system dynamics via continual learning. Specifically, we present a novel framework of Mode-switching Graph ODE (MS-GODE), which can continually learn varying dynamics and encode the system-specific dynamics into binary masks over the model parameters. During the inference stage, the model can select the most confident mask based on the observational data to identify the system and predict future trajectories accordingly. Empirically, we systematically investigate the task configurations and compare the proposed MS-GODE with state-of-the-art techniques. More importantly, we construct a novel benchmark of biological dynamic systems, featuring diverse systems with disparate dynamics and significantly enriching the research field of machine learning for dynamic systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb4045f3e6f4dfdaceedc6d8a70fc7a232722b3e" target='_blank'>
              Learning System Dynamics without Forgetting
              </a>
            </td>
          <td>
            Xikun Zhang, Dongjin Song, Yushan Jiang, Yixin Chen, Dacheng Tao
          </td>
          <td>2024-06-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Detecting and quantifying causality is a focal topic in the fields of science, engineering, and interdisciplinary studies. However, causal studies on non-intervention systems attract much attention but remain extremely challenging. To address this challenge, we propose a framework named Interventional Dynamical Causality (IntDC) for such non-intervention systems, along with its computational criterion, Interventional Embedding Entropy (IEE), to quantify causality. The IEE criterion theoretically and numerically enables the deciphering of IntDC solely from observational (non-interventional) time-series data, without requiring any knowledge of dynamical models or real interventions in the considered system. Demonstrations of performance showed the accuracy and robustness of IEE on benchmark simulated systems as well as real-world systems, including the neural connectomes of C. elegans, COVID-19 transmission networks in Japan, and regulatory networks surrounding key circadian genes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/731fd0ad43f607161c56e7dddae19516d57b53d3" target='_blank'>
              Deciphering interventional dynamical causality from non-intervention systems
              </a>
            </td>
          <td>
            Jifan Shi, Yang Li, Juan Zhao, Siyang Leng, Kazuyuki Aihara, Luonan Chen, Wei Lin
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We propose a novel reduced-order methodology to describe complex multi-frequency fluid dynamics from time-resolved snapshot data. Starting point is the Cluster-based Network Model (CNM) thanks to its fully automatable development and human interpretability. Our key innovation is to model the transitions from cluster to cluster much more accurately by replacing snapshot states with short-term trajectories ("orbits") over multiple clusters, thus avoiding nonphysical intra-cluster diffusion in the dynamic reconstruction. The proposed orbital CNM (oCNM) employs functional clustering to coarse-grain the short-term trajectories. Specifically, different filtering techniques, resulting in different temporal basis expansions, demonstrate the versatility and capability of the oCNM to adapt to diverse flow phenomena. The oCNM is illustrated on the Stuart-Landau oscillator and its post-transient solution with time-varying parameters to test its ability to capture the amplitude selection mechanism and multi-frequency behaviours. Then, the oCNM is applied to the fluidic pinball across varying flow regimes at different Reynolds numbers, including the periodic, quasi-periodic, and chaotic dynamics. This orbital-focused perspective enhances the understanding of complex temporal behaviours by incorporating high-frequency behaviour into the kinematics of short-time trajectories while modelling the dynamics of the lower frequencies. In analogy to Spectral Proper Orthogonal Decomposition, which marked the transition from spatial-only modes to spatio-temporal ones, this work advances from analysing temporal local states to examining piecewise short-term trajectories, or orbits. By merging advanced analytical methods, such as the functional representation of short-time trajectories with CNM, this study paves the way for new approaches to dissect the complex dynamics characterising turbulent systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/413941f904c49031a4ce7ee2d45cc551a5cabb12" target='_blank'>
              Orbital cluster-based network modelling
              </a>
            </td>
          <td>
            A. Colanera, Nan Deng, M. Chiatto, Luigi De Luca, Bernd R. Noack
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="In this paper, we apply the Fourier neural operator (FNO) paradigm to ocean circulation and prediction problems. We aim to show that the complicated non-linear dynamics of an ocean circulation can be captured by a flexible, efficient, and expressive structure of the FNO networks. The machine learning model (FNO3D and the recurrent FNO2D networks) trained by simulated data as well as real data takes spatiotemporal input and predicts future ocean states (sea surface current and sea surface height). For this, the double gyre ocean circulation model driven by stochastic wind stress is considered to represent an ideal ocean circulation. In order to generate the training and test data that exhibits rich spatiotemporal variability, the initial states are perturbed by Gaussian random fields. Experimental results confirm that the trained models yield satisfactory prediction accuracy for both types of FNO models in this case. Second, as the training set, we used the HYCOM reanalysis data in a regional ocean. FNO2D experiments demonstrated that the 5-day input to 5-day prediction yields the averaged root mean square errors (RMSEs) of 5.0 cm/s, 6.7 cm/s, 7.9 cm/s, 8.9 cm/s, and 9.4 cm/s in surface current, calculated consecutively for each day, in a regional ocean circulation of the East/Japan Sea. Similarly, the RMSEs for sea surface height were 2.3 cm, 3.5 cm, 4.2 cm, 4.6 cm, and 4.9 cm, for each day. We also trained the model with 15-day input and 10-day prediction, resulting in comparable performance. Extensive numerical tests show that, once learned, the resolution-free FNO model instantly forecasts the ocean states and can be used as an alternative fast solver in various inference algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/66940e0cf48566a2e9e8617081952275da1fa484" target='_blank'>
              Applications of the Fourier neural operator in a regional ocean modeling and prediction
              </a>
            </td>
          <td>
            Byoung-Ju Choi, H. Jin, Bataa Lkhagvasuren
          </td>
          <td>2024-07-02</td>
          <td>Frontiers in Marine Science</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In this paper, we design a transfer sparse identification algorithm under the Bayesian framework through introducing other system knowledge into the system to be identified. This method provides a new identification solution for a nonlinear autoregressive model with exogenous inputs (NARX). The estimates of the transferred parameters are calculated by adding the transfer correction term to the un‐transferred estimates. To achieve this, a joint prior distribution is devised for the parameters, ultimately enhancing the efficient utilization of existing data, reducing the reliance on new data, and achieving more accurate identification. The maximized marginal likelihood method is used to find the transfer gain and the transfer information matrix in the transfer correction term. Meanwhile, in order to make the algorithm automatically adapt to different data, we design an automatic structure detection method based on the transfer framework. The method automatically determines the sparsity threshold based on the maximum inter‐class variance. Two examples are provided to demonstrate the advantages of our algorithm.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/275895802c6290128f34f9549d5e7ee7a46c6781" target='_blank'>
              A Bayesian transfer sparse identification method for nonlinear ARX systems
              </a>
            </td>
          <td>
            Kang Zhang, Xiaoli Luan, Feng Ding, Fei Liu
          </td>
          <td>2024-08-08</td>
          <td>International Journal of Adaptive Control and Signal Processing</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Engineers often utilize data-driven surrogate models in optimization to partially replace the costly computational simulations of physics-based models. However, different aspects affect the accuracy and prediction capability of the surrogate models, e.g., the dimensionality of the data and nonlinearities in the mapping between input and output. Learning well-performing models by selecting appropriate techniques to fit the data benefits from the machine learning and data science expertise of the engineer which may vary depending on the application domain. Recently, large language models (LLMs) have shown promising capabilities to support humans through natural language-based interfaces in approaching technical problems, as well as to co-develop software and to democratize domain knowledge. In this paper, we utilize ChatGPT 4 to co-develop a framework to select and train surrogate models for engineering optimization tasks. More specifically, we interact with ChatGPT to outline a process and software to support the selection and application of regression techniques based on characteristics of the available data and target application. We evaluate the developed methodology on synthetic and realistic engineering optimization data and problems. In our experiments, we demonstrate that the models obtained through the methodology developed with ChatGPT achieve comparable performance in regression and optimization tasks than observed in existing works in the literature. Hence, despite some limitations, such as missing updates of available software libraries, LLMs can support less experienced engineers to solve surrogate-assisted optimization problems more efficiently by providing insights on the application data and software for deploying surrogate models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b199732a0ef844fff7389cc7d27737b52409f2df" target='_blank'>
              Large Language Model-assisted Surrogate Modelling for Engineering Optimization
              </a>
            </td>
          <td>
            Thiago Rios, Felix Lanfermann, Stefan Menzel
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>1</td>
          <td>7</td>
        </tr>

        <tr id="We study the dynamics of a continuous-time model of the Stochastic Gradient Descent (SGD) for the least-square problem. Indeed, pursuing the work of Li et al. (2019), we analyze Stochastic Differential Equations (SDEs) that model SGD either in the case of the training loss (finite samples) or the population one (online setting). A key qualitative feature of the dynamics is the existence of a perfect interpolator of the data, irrespective of the sample size. In both scenarios, we provide precise, non-asymptotic rates of convergence to the (possibly degenerate) stationary distribution. Additionally, we describe this asymptotic distribution, offering estimates of its mean, deviations from it, and a proof of the emergence of heavy-tails related to the step-size magnitude. Numerical simulations supporting our findings are also presented.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecfeb578d6cbf8848e1912ad8e32e903a645ea7a" target='_blank'>
              Stochastic Differential Equations models for Least-Squares Stochastic Gradient Descent
              </a>
            </td>
          <td>
            Adrien Schertzer, Loucas Pillaud-Vivien
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="This paper presents a novel approach for propagating uncertainties in dynamical systems building on high-order Taylor expansions of the flow and moment-generating functions (MGFs). Unlike prior methods that focus on Gaussian distributions, our approach leverages the relationship between MGFs and distribution moments to extend high-order uncertainty propagation techniques to non-Gaussian scenarios. This significantly broadens the applicability of these methods to a wider range of problems and uncertainty types. High-order moment computations are performed one-off and symbolically, reducing the computational burden of the technique to the calculation of Taylor series coefficients around a nominal trajectory, achieved by efficiently integrating the system's variational equations. Furthermore, the use of the proposed approach in combination with event transition tensors, allows for accurate propagation of uncertainties at specific events, such as the landing surface of a celestial body, the crossing of a predefined Poincar\'e section, or the trigger of an arbitrary event during the propagation. Via numerical simulations we demonstrate the effectiveness of our method in various astrodynamics applications, including the unperturbed and perturbed two-body problem, and the circular restricted three-body problem, showing that it accurately propagates non-Gaussian uncertainties both at future times and at event manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6afd2423f192d0672dfad436cf9f3cd283ac1177" target='_blank'>
              Nonlinear Propagation of Non-Gaussian Uncertainties: Theory and Applications to Spacecraft Trajectory Design
              </a>
            </td>
          <td>
            Giacomo Acciarini, Nicola Baresi, David Lloyd, Dario Izzo
          </td>
          <td>2024-08-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024'],
    y: [32],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>