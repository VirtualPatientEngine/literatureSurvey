---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-15 06:05:24 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Continuous PDE Dynamics Forecasting with Implicit Neural Representations</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18fc14bf5e659e56f4c5cc3ccd593462dd28c383" target='_blank'>Learning Space-Time Continuous Neural PDEs from Partially Observed States</a></td>
          <td>
            V. Iakovlev, Markus Heinonen, H. Lähdesmäki
          </td>
          <td>2023-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>48</td>
        </tr>
    
        <tr id="Significance In many physical systems, the governing equations are known with high confidence, but direct numerical solution is prohibitively expensive. Often this situation is alleviated by writing effective equations to approximate dynamics below the grid scale. This process is often impossible to perform analytically and is often ad hoc. Here we propose data-driven discretization, a method that uses machine learning to systematically derive discretizations for continuous physical systems. On a series of model problems, data-driven discretization gives accurate solutions with a dramatic drop in required resolution. The numerical solution of partial differential equations (PDEs) is challenging because of the need to resolve spatiotemporal features over wide length- and timescales. Often, it is computationally intractable to resolve the finest features in the solution. The only recourse is to use approximate coarse-grained representations, which aim to accurately represent long-wavelength dynamics while properly accounting for unresolved small-scale physics. Deriving such coarse-grained equations is notoriously difficult and often ad hoc. Here we introduce data-driven discretization, a method for learning optimized approximations to PDEs based on actual solutions to the known underlying equations. Our approach uses neural networks to estimate spatial derivatives, which are optimized end to end to best satisfy the equations on a low-resolution grid. The resulting numerical methods are remarkably accurate, allowing us to integrate in time a collection of nonlinear equations in 1 spatial dimension at resolutions 4× to 8× coarser than is possible with standard finite-difference methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a43842478bf9d1aff71e964f584af4eeb196073" target='_blank'>Learning data-driven discretizations for partial differential equations</a></td>
          <td>
            Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, M. Brenner
          </td>
          <td>2018-08-15</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>427</td>
          <td>65</td>
        </tr>
    
        <tr id="In this paper, we consider the problem of learning prediction models for spatiotemporal physical processes driven by unknown partial differential equations (PDEs). We propose a deep learning framework that learns the underlying dynamics and predicts its evolution using sparsely distributed data sites. Deep learning has shown promising results in modeling physical dynamics in recent years. However, most of the existing deep learning methods for modeling physical dynamics either focus on solving known PDEs or require data in a dense grid when the governing PDEs are unknown. In contrast, our method focuses on learning prediction models for unknown PDE-driven dynamics only from sparsely observed data. The proposed method is spatial dimension-independent and geometrically flexible. We demonstrate our method in the forecasting task for the two-dimensional wave equation and the Burgers-Fisher equation in multiple geometries with different boundary conditions, and the ten-dimensional heat equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e00fa719a36548c833fe2abe1c2180ecaf3e8e0f" target='_blank'>A Deep Learning Approach for Predicting Spatiotemporal Dynamics From Sparsely Observed Data</a></td>
          <td>
            Priyabrata Saha, S. Mukhopadhyay
          </td>
          <td>2020-12-01</td>
          <td>IEEE Access</td>
          <td>4</td>
          <td>45</td>
        </tr>
    
        <tr id="Recent advances in incorporating physical knowledge into deep neural networks can estimate previously unknown governing partial differential equations (PDEs) in a data-driven way. They have shown promising results in spatiotemporal predictive learning. However, these methods typically assume universal governing PDEs across space, which is impractical for modeling complex spatiotemporal phenomena with high spatial variability (e.g., climate). Also, they cannot effectively model the evolution of potential errors in estimating the physical dynamics over time. This paper introduces a physics-guided neural network, SVPNet, which learns effective physical representations by estimating the error evolution in physics states for correction and modeling spatially varying physical dynamics to predict the next state. Experiments carried out in four scenarios, including benchmarks and real-world datasets, show that SVPNet outperforms state-of-the-art methods in spatiotemporal prediction tasks for natural processes and significantly improves prediction when training data are limited. Ablation studies also highlight that SVPNet is powerful in capturing physical dynamics in complex physical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a79653a75c7412044433ab2757edce70b964019a" target='_blank'>Modeling Spatially Varying Physical Dynamics for Spatiotemporal Predictive Learning</a></td>
          <td>
            Yijun Lin, Yao-Yi Chiang
          </td>
          <td>2023-11-13</td>
          <td>Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems</td>
          <td>0</td>
          <td>2</td>
        </tr>
    
        <tr id="Learning underlying dynamics from data is important and challenging in many real-world scenarios. Incorporating differential equations (DEs) to design continuous networks has drawn much attention recently, however, most prior works make specific assumptions on the type of DEs, making the model specialized for particular problems. This work presents a partial differential equation (PDE) based framework which improves the dynamics modeling capability. Building upon the recent Fourier neural operator, we propose a neural operator that can handle time continuously without requiring iterative operations or specific grids of temporal discretization. A theoretical result demonstrating its universality is provided. We also uncover an intrinsic property of neural operators that improves data efficiency and model generalization by ensuring stability. Our model achieves superior accuracy in dealing with time-dependent PDEs compared to existing models. Furthermore, several numerical pieces of evidence validate that our method better represents a wide range of dynamics and outperforms state-of-the-art DE-based models in real-time-series applications. Our framework opens up a new way for a continuous representation of neural networks that can be readily adopted for real-world applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1095fbfe9feb9117819a6d899faf624f2e943e73" target='_blank'>Learning PDE Solution Operator for Continuous Modeling of Time-Series</a></td>
          <td>
            Yesom Park, Jaemoo Choi, Changyeon Yoon, Changhoon Song, Myung-joo Kang
          </td>
          <td>2023-02-02</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>24</td>
        </tr>
    
        <tr id="Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions -- e.g. symmetries or boundary conditions -- in favour of modelling flexibility. Instead, we propose a space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space - respects known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency. We validated that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail - and improve over baselines in a number of challenging geometries.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c8295fdfb6de078027cabc7d49b319df8abb0ee" target='_blank'>Space-Time Continuous PDE Forecasting using Equivariant Neural Fields</a></td>
          <td>
            David M. Knigge, David R. Wessels, Riccardo Valperga, Samuele Papa, J. Sonke, E. Gavves, E. J. Bekkers
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>56</td>
        </tr>
    
        <tr id="We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ed3fa74ebc0adcad4aee915c4625b384eaddbc91" target='_blank'>AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields</a></td>
          <td>
            Louis Serrano, Thomas X. Wang, E. L. Naour, Jean-Noël Vittaut, P. Gallinari
          </td>
          <td>2024-06-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>48</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6645028d3aad0592fe663469862fa3a2b05e73c3" target='_blank'>STENCIL-NET for equation-free forecasting from data</a></td>
          <td>
            S. Maddu, D. Sturm, B. Cheeseman, Christian L. Müller, I. Sbalzarini
          </td>
          <td>2023-08-07</td>
          <td>Scientific Reports</td>
          <td>3</td>
          <td>38</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3d769578637c4eebda2ace167b51a0a3ff7698d" target='_blank'>NeuralPDE: Modelling Dynamical Systems from Data</a></td>
          <td>
            Andrzej Dulny, A. Hotho, Anna Krause
          </td>
          <td>2021-11-15</td>
          <td>ArXiv</td>
          <td>8</td>
          <td>53</td>
        </tr>
    
        <tr id="Numerical methods for approximately solving partial differential equations (PDE) are at the core of scientific computing. Often, this requires high-resolution or adaptive discretization grids to capture relevant spatio-temporal features in the PDE solution, e.g., in applications like turbulence, combustion, and shock propagation. Numerical approximation also requires knowing the PDE in order to construct problem-specific discretizations. Systematically deriving such solution-adaptive discrete operators, however, is a current challenge. Here we present STENCIL-NET, an artificial neural network architecture for data-driven learning of problem- and resolution-specific local discretizations of nonlinear PDEs. STENCIL-NET achieves numerically stable discretization of the operators in an unknown nonlinear PDE by spatially and temporally adaptive parametric pooling on regular Cartesian grids, and by incorporating knowledge about discrete time integration. Knowing the actual PDE is not necessary, as solution data is sufficient to train the network to learn the discrete operators. A once-trained STENCIL-NET model can be used to predict solutions of the PDE on larger spatial domains and for longer times than it was trained for, hence addressing the problem of PDE-constrained extrapolation from data. To support this claim, we present numerical experiments on long-term forecasting of chaotic PDE solutions on coarse spatio-temporal grids. We also quantify the speed-up achieved by substituting base-line numerical methods with equation-free STENCIL-NET predictions on coarser grids with little compromise on accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9892628c5207890a3987a196304fc32a2bbacafd" target='_blank'>STENCIL-NET: Data-driven solution-adaptive discretization of partial differential equations</a></td>
          <td>
            S. Maddu, D. Sturm, B. Cheeseman, Christian L. Müller, I. Sbalzarini
          </td>
          <td>2021-01-15</td>
          <td>ArXiv</td>
          <td>7</td>
          <td>38</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>