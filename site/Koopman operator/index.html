<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../PINNs/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Koopman operator
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Time-series%20forecasting/" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Symbolic%20regression/" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Neural%20ODEs/" class="md-tabs__link">
        
  
    
  
  Neural ODEs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Physics-based%20GNNs/" class="md-tabs__link">
        
  
    
  
  Physics-based GNNs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Latent%20space%20simulators/" class="md-tabs__link">
        
  
    
  
  Latent space simulators

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Parametrizing%20using%20ML/" class="md-tabs__link">
        
  
    
  
  Parametrizing using ML

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../PINNs/" class="md-tabs__link">
        
  
    
  
  PINNs

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Koopman operator

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Time-series%20forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Symbolic%20regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20ODEs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural ODEs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Physics-based%20GNNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physics-based GNNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Latent%20space%20simulators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent space simulators
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parametrizing%20using%20ML/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizing using ML
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../PINNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PINNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Koopman operator
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Koopman operator</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-05-27 07:04:41 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Koopman operator</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Koopman operator</a><br>
      <a href="#recommended_articles">3. Recommended articles on Koopman operator</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Koopman operator</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Koopman operator</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf657b5049c1a5c839369d3948ffb4c0584cd1d2" target='_blank'>
                Hamiltonian Systems and Transformation in Hilbert Space.
                </a>
              </td>
          <td>
            B. O. Koopman
          </td>
          <td>1931-05-01</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1617</td>
          <td>18</td>

            <td><a href='../recommendations/bf657b5049c1a5c839369d3948ffb4c0584cd1d2' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="A majority of methods from dynamical system analysis, especially those in applied settings, rely on Poincaré's geometric picture that focuses on "dynamics of states." While this picture has fueled our field for a century, it has shown difficulties in handling high-dimensional, ill-described, and uncertain systems, which are more and more common in engineered systems design and analysis of "big data" measurements. This overview article presents an alternative framework for dynamical systems, based on the "dynamics of observables" picture. The central object is the Koopman operator: an infinite-dimensional, linear operator that is nonetheless capable of capturing the full nonlinear dynamics. The first goal of this paper is to make it clear how methods that appeared in different papers and contexts all relate to each other through spectral properties of the Koopman operator. The second goal is to present these methods in a concise manner in an effort to make the framework accessible to researchers who would like to apply them, but also, expand and improve them. Finally, we aim to provide a road map through the literature where each of the topics was described in detail. We describe three main concepts: Koopman mode analysis, Koopman eigenquotients, and continuous indicators of ergodicity. For each concept, we provide a summary of theoretical concepts required to define and study them, numerical methods that have been developed for their analysis, and, when possible, applications that made use of them. The Koopman framework is showing potential for crossing over from academic and theoretical use to industrial practice. Therefore, the paper highlights its strengths in applied and numerical contexts. Additionally, we point out areas where an additional research push is needed before the approach is adopted as an off-the-shelf framework for analysis and design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c9be1e38f978f43427ea5293b3138e0c4fede71" target='_blank'>
                Applied Koopmanism.
                </a>
              </td>
          <td>
            M. Budišić, Ryan Mohr, I. Mezić
          </td>
          <td>2012-06-14</td>
          <td>Chaos</td>
          <td>713</td>
          <td>49</td>

            <td><a href='../recommendations/2c9be1e38f978f43427ea5293b3138e0c4fede71' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by ℓ1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3c279828af3621d2c16ac26e5900b970383f60e" target='_blank'>
                Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control
                </a>
              </td>
          <td>
            S. Brunton, Bingni W. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2015-10-11</td>
          <td>PLoS ONE</td>
          <td>443</td>
          <td>62</td>

            <td><a href='../recommendations/a3c279828af3621d2c16ac26e5900b970383f60e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6adeda1af8abc6bc3c17c0b39f635a845476cd9f" target='_blank'>
                Deep learning for universal linear embeddings of nonlinear dynamics
                </a>
              </td>
          <td>
            Bethany Lusch, J. Kutz, S. Brunton
          </td>
          <td>2017-12-27</td>
          <td>Nature Communications</td>
          <td>954</td>
          <td>62</td>

            <td><a href='../recommendations/6adeda1af8abc6bc3c17c0b39f635a845476cd9f' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We develop a deep autoencoder architecture that can be used to find a coordinate transformation which turns a non-linear partial differential equation (PDE) into a linear PDE. Our architecture is motivated by the linearising transformations provided by the Cole–Hopf transform for Burgers’ equation and the inverse scattering transform for completely integrable PDEs. By leveraging a residual network architecture, a near-identity transformation can be exploited to encode intrinsic coordinates in which the dynamics are linear. The resulting dynamics are given by a Koopman operator matrix K. The decoder allows us to transform back to the original coordinates as well. Multiple time step prediction can be performed by repeated multiplication by the matrix K in the intrinsic coordinates. We demonstrate our method on a number of examples, including the heat equation and Burgers’ equation, as well as the substantially more challenging Kuramoto–Sivashinsky equation, showing that our method provides a robust architecture for discovering linearising transforms for non-linear PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b" target='_blank'>
                Deep learning models for global coordinate transformations that linearise PDEs
                </a>
              </td>
          <td>
            Craig Gin, Bethany Lusch, S. Brunton, J. Kutz
          </td>
          <td>2019-11-07</td>
          <td>European Journal of Applied Mathematics</td>
          <td>30</td>
          <td>62</td>

            <td><a href='../recommendations/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural byproduct of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11df7f23f72703ceefccc6367a6a18719850c53e" target='_blank'>
                From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction
                </a>
              </td>
          <td>
            Henning Lange, S. Brunton, N. Kutz
          </td>
          <td>2020-04-01</td>
          <td>Journal of machine learning research, J. Mach. Learn. Res.</td>
          <td>53</td>
          <td>62</td>

            <td><a href='../recommendations/11df7f23f72703ceefccc6367a6a18719850c53e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68b6ca45a588d538b36335b23f6969c960cf2e6e" target='_blank'>
                Modern Koopman Theory for Dynamical Systems
                </a>
              </td>
          <td>
            S. Brunton, M. Budišić, E. Kaiser, J. Kutz
          </td>
          <td>2021-02-24</td>
          <td>SIAM Rev., SIAM Review</td>
          <td>256</td>
          <td>62</td>

            <td><a href='../recommendations/68b6ca45a588d538b36335b23f6969c960cf2e6e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/893768d957f8a46f0ba5bab11e5f2e2698ef1409" target='_blank'>
                Parsimony as the ultimate regularizer for physics-informed machine learning
                </a>
              </td>
          <td>
            J. Kutz, S. Brunton
          </td>
          <td>2022-01-20</td>
          <td>Nonlinear Dynamics</td>
          <td>21</td>
          <td>62</td>

            <td><a href='../recommendations/893768d957f8a46f0ba5bab11e5f2e2698ef1409' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Koopman operator'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Koopman operator</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators. By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis. While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra. Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution. At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing. This provides a robust decomposition encompassing both discrete and continuous spectral elements. We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures. Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability. We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility. This work paves the way for future research and applications of decompositions with continuous spectra.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7d70b9446ba0535cfb98ba6fff01f71c3423ae3" target='_blank'>
              Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators
              </a>
            </td>
          <td>
            Matthew J. Colbrook, Catherine Drysdale, Andrew Horning
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>16</td>
        </tr>

        <tr id="Koopman operators are infinite-dimensional operators that linearize nonlinear dynamical systems, facilitating the study of their spectral properties and enabling the prediction of the time evolution of observable quantities. Recent methods have aimed to approximate Koopman operators while preserving key structures. However, approximating Koopman operators typically requires a dictionary of observables to capture the system's behavior in a finite-dimensional subspace. The selection of these functions is often heuristic, may result in the loss of spectral information, and can severely complicate structure preservation. This paper introduces Multiplicative Dynamic Mode Decomposition (MultDMD), which enforces the multiplicative structure inherent in the Koopman operator within its finite-dimensional approximation. Leveraging this multiplicative property, we guide the selection of observables and define a constrained optimization problem for the matrix approximation, which can be efficiently solved. MultDMD presents a structured approach to finite-dimensional approximations and can more accurately reflect the spectral properties of the Koopman operator. We elaborate on the theoretical framework of MultDMD, detailing its formulation, optimization strategy, and convergence properties. The efficacy of MultDMD is demonstrated through several examples, including the nonlinear pendulum, the Lorenz system, and fluid dynamics data, where we demonstrate its remarkable robustness to noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0cc59f7258cc0c3e219e7edb0f6cbaf13b67c680" target='_blank'>
              Multiplicative Dynamic Mode Decomposition
              </a>
            </td>
          <td>
            Nicolas Boull'e, Matthew J. Colbrook
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world. Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions. In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes. In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems. The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system. Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/253489dec37ed05e844568d9dae4237b151b936f" target='_blank'>
              Koopman-based Deep Learning for Nonlinear System Estimation
              </a>
            </td>
          <td>
            Zexin Sun, Mingyu Chen, John Baillieul
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Several related works have introduced Koopman-based Machine Learning architectures as a surrogate model for dynamical systems. These architectures aim to learn non-linear measurements (also known as observables) of the system's state that evolve by a linear operator and are, therefore, amenable to model-based linear control techniques. So far, mainly simple systems have been targeted, and Koopman architectures as reduced-order models for more complex dynamics have not been fully explored. Hence, we use a Koopman-inspired architecture called the Linear Recurrent Autoencoder Network (LRAN) for learning reduced-order dynamics in convection flows of a Rayleigh B\'enard Convection (RBC) system at different amounts of turbulence. The data is obtained from direct numerical simulations of the RBC system. A traditional fluid dynamics method, the Kernel Dynamic Mode Decomposition (KDMD), is used to compare the LRAN. For both methods, we performed hyperparameter sweeps to identify optimal settings. We used a Normalized Sum of Square Error measure for the quantitative evaluation of the models, and we also studied the model predictions qualitatively. We obtained more accurate predictions with the LRAN than with KDMD in the most turbulent setting. We conjecture that this is due to the LRAN's flexibility in learning complicated observables from data, thereby serving as a viable surrogate model for the main structure of fluid dynamics in turbulent convection settings. In contrast, KDMD was more effective in lower turbulence settings due to the repetitiveness of the convection flow. The feasibility of Koopman-based surrogate models for turbulent fluid flows opens possibilities for efficient model-based control techniques useful in a variety of industrial settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b14d40f8f539b3f8e07c3779360a96930d9f97db" target='_blank'>
              Koopman-Based Surrogate Modelling of Turbulent Rayleigh-B\'enard Convection
              </a>
            </td>
          <td>
            Thorben Markmann, Michiel Straat, Barbara Hammer
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Linearity of Koopman operators and simplicity of their estimators coupled with model-reduction capabilities has lead to their great popularity in applications for learning dynamical systems. While nonparametric Koopman operator learning in infinite-dimensional reproducing kernel Hilbert spaces is well understood for autonomous systems, its control system analogues are largely unexplored. Addressing systems with control inputs in a principled manner is crucial for fully data-driven learning of controllers, especially since existing approaches commonly resort to representational heuristics or parametric models of limited expressiveness and scalability. We address the aforementioned challenge by proposing a universal framework via control-affine reproducing kernels that enables direct estimation of a single operator even for control systems. The proposed approach, called control-Koopman operator regression (cKOR), is thus completely analogous to Koopman operator regression of the autonomous case. First in the literature, we present a nonparametric framework for learning Koopman operator representations of nonlinear control-affine systems that does not suffer from the curse of control input dimensionality. This allows for reformulating the infinite-dimensional learning problem in a finite-dimensional space based solely on data without apriori loss of precision due to a restriction to a finite span of functions or inputs as in other approaches. For enabling applications to large-scale control systems, we also enhance the scalability of control-Koopman operator estimators by leveraging random projections (sketching). The efficacy of our novel cKOR approach is demonstrated on both forecasting and control tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6527db73e0af15e2dff15cbf1ecfc8adfcdd5716" target='_blank'>
              Nonparametric Control-Koopman Operator Learning: Flexible and Scalable Models for Prediction and Control
              </a>
            </td>
          <td>
            Petar Bevanda, Bas Driessen, Lucian-Cristian Iacob, Roland Toth, Stefan Sosnowski, Sandra Hirche
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="The discovery of linear embedding is the key to the synthesis of linear control techniques for nonlinear systems. In recent years, while Koopman operator theory has become a prominent approach for learning these linear embeddings through data-driven methods, these algorithms often exhibit limitations in generalizability beyond the distribution captured by training data and are not robust to changes in the nominal system dynamics induced by intrinsic or environmental factors. To overcome these limitations, this study presents an adaptive Koopman architecture capable of responding to the changes in system dynamics online. The proposed framework initially employs an autoencoder-based neural network that utilizes input-output information from the nominal system to learn the corresponding Koopman embedding offline. Subsequently, we augment this nominal Koopman architecture with a feed-forward neural network that learns to modify the nominal dynamics in response to any deviation between the predicted and observed lifted states, leading to improved generalization and robustness to a wide range of uncertainties and disturbances compared to contemporary methods. Extensive tracking control simulations, which are undertaken by integrating the proposed scheme within a Model Predictive Control framework, are used to highlight its robustness against measurement noise, disturbances, and parametric variations in system dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/302db61f58f8a2e62340fcfaacbceec2620e551a" target='_blank'>
              Adaptive Koopman Embedding for Robust Control of Complex Nonlinear Dynamical Systems
              </a>
            </td>
          <td>
            Rajpal Singh, Chandan Kumar Sah, J. Keshavan
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Data-driven modelling techniques provide a method for deriving models of dynamical systems directly from complicated data streams. However, tracking and forecasting such data streams poses a significant challenge to most methods, as they assume the underlying process and model does not change over time. In this paper, we apply one such data-driven method, the Koopman autoencoder (KAE), to high-dimensional oscillatory data to generate a low-dimensional latent space and model, where the system's dynamics appear linear. This allows one to accurately track and forecast systems where the underlying model may change over time. States and the model in the reduced order latent space can then be efficiently updated as new data becomes available, using data assimilation techniques such as the ensemble Kalman filter (EnKF), in a technique we call the KAE EnKF. We demonstrate that this approach is able to effectively track and forecast time-varying, nonlinear dynamical systems in synthetic examples. We then apply the KAE EnKF to a video of a physical pendulum, and achieve a significant improvement over current state-of-the-art methods. By generating effective latent space reconstructions, we find that we are able to construct accurate short-term forecasts and efficient adaptations to externally forced changes to the pendulum's frequency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be5728bd13fd3650c966df73ef5772bf8b905261" target='_blank'>
              Tracking and forecasting oscillatory data streams using Koopman autoencoders and Kalman filtering
              </a>
            </td>
          <td>
            Stephen A Falconer, David J. B. Lloyd, N. Santitissadeekorn
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="In this paper, we present a novel sufficient condition for the stability of discrete-time linear systems that can be represented as a set of piecewise linear constraints, which make them suitable for quadratic programming optimization problems. More specifically, we tackle the problem of imposing asymptotic stability to a Koopman matrix learned from data during iterative gradient descent optimization processes. We show that this sufficient condition can be decoupled by rows of the system matrix, and propose a control barrier function-based projected gradient descent to enforce gradual evolution towards the stability set by running an optimization-in-the-loop during the iterative learning process. We compare the performance of our algorithm with other two recent approaches in the literature, and show that we get close to state-of-the-art performance while providing the added flexibility of allowing the optimization problem to be further customized for specific applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04474198011a7b91ea2d87230d979871d8f91839" target='_blank'>
              Learning deep Koopman operators with convex stability constraints
              </a>
            </td>
          <td>
            M. Mitjans, Liangting Wu, Roberto Tron
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Global information about dynamical systems can be extracted by analysing associated infinite-dimensional transfer operators, such as Perron-Frobenius and Koopman operators as well as their infinitesimal generators. In practice, these operators typically need to be approximated from data. Popular approximation methods are extended dynamic mode decomposition (EDMD) and generator extended mode decomposition (gEDMD). We propose a unified framework that leverages Monte Carlo sampling to approximate the operator of interest on a finite-dimensional space spanned by a set of basis functions. Our framework contains EDMD and gEDMD as special cases, but can also be used to approximate more general operators. Our key contributions are proofs of the convergence of the approximating operator and its spectrum under non-restrictive conditions. Moreover, we derive explicit convergence rates and account for the presence of noise in the observations. Whilst all these results are broadly applicable, they also refine previous analyses of EDMD and gEDMD. We verify the analytical results with the aid of several numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4fd97793c90e0875a903bea631c29c0d483308d" target='_blank'>
              Data-driven approximation of Koopman operators and generators: Convergence rates and error bounds
              </a>
            </td>
          <td>
            Liam Llamazares-Elias, Samir Llamazares-Elias, Jonas Latz, Stefan Klus
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We introduce the optimized dynamic mode decomposition algorithm for constructing an adaptive and computationally efficient reduced order model and forecasting tool for global atmospheric chemistry dynamics. By exploiting a low-dimensional set of global spatio-temporal modes, interpretable characterizations of the underlying spatial and temporal scales can be computed. Forecasting is also achieved with a linear model that uses a linear superposition of the dominant spatio-temporal features. The DMD method is demonstrated on three months of global chemistry dynamics data, showing its significant performance in computational speed and interpretability. We show that the presented decomposition method successfully extracts known major features of atmospheric chemistry, such as summertime surface pollution and biomass burning activities. Moreover, the DMD algorithm allows for rapid reconstruction of the underlying linear model, which can then easily accommodate non-stationary data and changes in the dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eafb6d2897f54e1cb6786f47eb557bb66835ba35" target='_blank'>
              Optimized Dynamic Mode Decomposition for Reconstruction and Forecasting of Atmospheric Chemistry Data
              </a>
            </td>
          <td>
            Meghana Velegar, Christoph Keller, J. Kutz
          </td>
          <td>2024-04-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.This paper investigates how the estimation process is affected when the data is quantized. Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from quantized data. Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the quantized estimate can be considered a regularized version of the unquantized estimate. This key theoretical finding paves the way to accurately recover the unquantized estimate from quantized data. We also explore the relationship between the two estimates in the finite data regime. The theory is validated through repeated numerical experiments conducted on three different dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca8743a3b7438c0b66cccbd2244dc69b48383c58" target='_blank'>
              On the Effect of Quantization on Dynamic Mode Decomposition
              </a>
            </td>
          <td>
            Dipankar Maity, Debdipta Goswami, Sriram Narayanan
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Deep learning algorithms provide a new paradigm to study high-dimensional dynamical behaviors, such as those in fusion plasma systems. Development of novel model reduction methods, coupled with detection of abnormal modes with plasma physics, opens a unique opportunity for building efficient models to identify plasma instabilities for real-time control. Our Fusion Transfer Learning (FTL) model demonstrates success in reconstructing nonlinear kink mode structures by learning from a limited amount of nonlinear simulation data. The knowledge transfer process leverages a pre-trained neural encoder-decoder network, initially trained on linear simulations, to effectively capture nonlinear dynamics. The low-dimensional embeddings extract the coherent structures of interest, while preserving the inherent dynamics of the complex system. Experimental results highlight FTL's capacity to capture transitional behaviors and dynamical features in plasma dynamics -- a task often challenging for conventional methods. The model developed in this study is generalizable and can be extended broadly through transfer learning to address various magnetohydrodynamics (MHD) modes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c07715bd86e219597bf8c42b1d974b4316c9ebd8" target='_blank'>
              FTL: Transfer Learning Nonlinear Plasma Dynamic Transitions in Low Dimensional Embeddings via Deep Neural Networks
              </a>
            </td>
          <td>
            Zhe Bai, Xishuo Wei, William Tang, L. Oliker, Zhihong Lin, Samuel Williams
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>45</td>
        </tr>

        <tr id="The Koopman operator has entered and transformed many research areas over the last years. Although the underlying concept$\unicode{x2013}$representing highly nonlinear dynamical systems by infinite-dimensional linear operators$\unicode{x2013}$has been known for a long time, the availability of large data sets and efficient machine learning algorithms for estimating the Koopman operator from data make this framework extremely powerful and popular. Koopman operator theory allows us to gain insights into the characteristic global properties of a system without requiring detailed mathematical models. We will show how these methods can also be used to analyze complex networks and highlight relationships between Koopman operators and graph Laplacians.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71e77a5371257de746caf49b4ba2c4de559e5197" target='_blank'>
              Dynamical systems and complex networks: A Koopman operator perspective
              </a>
            </td>
          <td>
            Stefan Klus, Natavsa Djurdjevac Conrad
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/39a4abed8031137161277512e595c8105316e865" target='_blank'>
              Dominant subspaces of high-fidelity polynomial structured parametric dynamical systems and model reduction
              </a>
            </td>
          <td>
            P. Goyal, I. P. Duff, Peter Benner
          </td>
          <td>2024-05-03</td>
          <td>Adv. Comput. Math.</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models. Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space. This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors. To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state. As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system. The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf5ac27ebe1f37285c1d7a4518dacc14d7c81c12" target='_blank'>
              Koopman Data-Driven Predictive Control with Robust Stability and Recursive Feasibility Guarantees
              </a>
            </td>
          <td>
            T. D. Jong, V. Breschi, Maarten Schoukens, Mircea Lazar
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression. However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally. In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification. To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem. First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints. Instead, we implement soft constraints by modifying the cost function to be minimized. This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model. Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0c03c126b4d641a81099470f03a7d5215a2a6820" target='_blank'>
              Multi-objective SINDy for parameterized model discovery from single transient trajectory data
              </a>
            </td>
          <td>
            Javier A. Lemus, Benjamin Herrmann
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Koopman operator theory is a kind of data-driven modelling approach that accurately captures the nonlinearities of mechatronic systems such as vehicles against physics-based methods. However, the infinite-dimensional Koopman operator is impossible to implement in real-world applications. To approximate the infinite-dimensional Koopman operator through collection dataset rather than manual trial and error, we adopt deep neural networks (DNNs) to extract basis functions by offline training and map the nonlinearities of vehicle planar dynamics into a linear form in the lifted space. Besides, the effects of the dimensions of basis functions on the model accuracy are explored. Further, the extended state observer (ESO) is introduced to online estimate the total disturbance in the lifted space and compensate for the modelling errors and residuals of the learned deep Koopman operator (DK) while also improving its generalization. Then, the proposed model is applied to predict vehicle states within prediction horizons and later formulates the constrained finite-time optimization problem of model predictive control (MPC), i.e., ESO-DKMPC. In terms of the trajectory tracking of autonomous vehicles, the ESO-DKMPC generates the wheel steering angle to govern lateral motions based on the decoupling control structure. The various conditions under the double-lane change scenarios are built on the CarSim/Simulink co-simulation platform, and extensive comparisons are conducted with the linear MPC (LMPC) and nonlinear MPC (NMPC) informed by the physics-based model. The results indicate that the proposed ESO-DKMPC has better tracking performance and moderate efficacy both within linear and nonlinear regions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f35ec41629e3225b938d93d6beae5fc75e89733" target='_blank'>
              Incorporating ESO into Deep Koopman Operator Modeling for Control of Autonomous Vehicles
              </a>
            </td>
          <td>
            Hao Chen, Chengqi Lv
          </td>
          <td>2024-05-16</td>
          <td>IEEE Transactions on Control Systems Technology</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Turbulent flows are chaotic and multi-scale dynamical systems, which have large numbers of degrees of freedom. Turbulent flows, however, can be modelled with a smaller number of degrees of freedom when using the appropriate coordinate system, which is the goal of dimensionality reduction via nonlinear autoencoders. Autoencoders are expressive tools, but they are difficult to interpret. The goal of this paper is to propose a method to aid the interpretability of autoencoders. This is the decoder decomposition. First, we propose the decoder decomposition, which is a post-processing method to connect the latent variables to the coherent structures of flows. Second, we apply the decoder decomposition to analyse the latent space of synthetic data of a two-dimensional unsteady wake past a cylinder. We find that the dimension of latent space has a significant impact on the interpretability of autoencoders. We identify the physical and spurious latent variables. Third, we apply the decoder decomposition to the latent space of wind-tunnel experimental data of a three-dimensional turbulent wake past a bluff body. We show that the reconstruction error is a function of both the latent space dimension and the decoder size, which are correlated. Finally, we apply the decoder decomposition to rank and select latent variables based on the coherent structures that they represent. This is useful to filter unwanted or spurious latent variables, or to pinpoint specific coherent structures of interest. The ability to rank and select latent variables will help users design and interpret nonlinear autoencoders.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b0d533f9a79dba8eb27c8b9aeb89ecbe15955407" target='_blank'>
              Decoder Decomposition for the Analysis of the Latent Space of Nonlinear Autoencoders With Wind-Tunnel Experimental Data
              </a>
            </td>
          <td>
            Yaxin Mo, Tullio Traverso, L. Magri
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations. The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis. In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems. Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation. For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders. The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3105448eec3c079d49fb5005676d95696e680c56" target='_blank'>
              Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction
              </a>
            </td>
          <td>
            N. R. Franco, Andrea Manzoni, P. Zunino, J. Hesthaven
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>63</td>
        </tr>

        <tr id="The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles. In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models. To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy. Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning. Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations. Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers. Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions. Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fa0602282d2da4dfe0c28a52a63f971fa7313c78" target='_blank'>
              Machine Learning in Viscoelastic Fluids via Energy-Based Kernel Embedding
              </a>
            </td>
          <td>
            Samuel E. Otto, C. Oishi, Fabio Amaral, S. Brunton, J. Kutz
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>62</td>
        </tr>

        <tr id="The Koopman operator framework holds promise for spectral analysis of nonlinear dynamical systems based on linear operators. Eigenvalues and eigenfunctions of the Koopman operator, so-called Koopman eigenvalues and Koopman eigenfunctions, respectively, mirror global properties of the system's flow. In this paper we perform the Koopman analysis of the singularly-perturbed van der Pol system. First, we show the spectral signature depending on singular perturbation: how two Koopman principle eigenvalues are ordered and what distinct shapes emerge in their associated Koopman eigenfunctions. Second, we discuss the singular limit of the Koopman operator, which is derived through the concatenation of Koopman operators for the fast and slow subsystems. From the spectral properties of the Koopman operator for the singularl-perturbed system and the singular limit, we suggest that the Koopman eigenfunctions inherit geometric properties of the singularly-perturbed system. These results are applicable to general planar singularly-perturbed systems with stable limit cycles.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44832568206bd357d00878ddf822a227a13fd679" target='_blank'>
              Koopman Analysis of the Singularly-Perturbed van der Pol Oscillator
              </a>
            </td>
          <td>
            Natsuki Katayama, Yoshihiko Susuki
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="An iterated multistep forecasting scheme based on recurrent neural networks (RNN) is proposed for the time series generated by causal chains with infinite memory. This forecasting strategy contains, as a particular case, the iterative prediction strategies for dynamical systems that are customary in reservoir computing. Readily computable error bounds are obtained as a function of the forecasting horizon, functional and dynamical features of the specific RNN used, and the approximation error committed by it. The framework in the paper circumvents difficult-to-verify embedding hypotheses that appear in previous references in the literature and applies to new situations like the finite-dimensional observations of functional differential equations or the deterministic parts of stochastic processes to which standard embedding techniques do not necessarily apply.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/08c01d5b50fd31d1eec0663fad8a6c82d4655e41" target='_blank'>
              Forecasting causal dynamics with universal reservoirs
              </a>
            </td>
          <td>
            Lyudmila Grigoryeva, James Louw, Juan-Pablo Ortega
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="We present a data-driven pipeline for model building that combines interpretable machine learning, hydrodynamic theories, and microscopic models. The goal is to uncover the underlying processes governing nonlinear dynamics experiments. We exemplify our method with data from microfluidic experiments where crystals of streaming droplets support the propagation of nonlinear waves absent in passive crystals. By combining physics-inspired neural networks, known as neural operators, with symbolic regression tools, we generate the solution, as well as the mathematical form, of a nonlinear dynamical system that accurately models the experimental data. Finally, we interpret this continuum model from fundamental physics principles. Informed by machine learning, we coarse grain a microscopic model of interacting droplets and discover that non-reciprocal hydrodynamic interactions stabilise and promote nonlinear wave propagation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/687a2bdf4046a679f876d2b660bf758b24b136f2" target='_blank'>
              Interpreting neural operators: how nonlinear waves propagate in non-reciprocal solids
              </a>
            </td>
          <td>
            Jonathan Colen, Alexis Poncet, Denis Bartolo, Vincenzo Vitelli
          </td>
          <td>2024-04-19</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>6</td>
        </tr>

        <tr id="Operator learning is an emerging area of machine learning which aims to learn mappings between infinite dimensional function spaces. Here we uncover a connection between operator learning architectures and conditioned neural fields from computer vision, providing a unified perspective for examining differences between popular operator learning models. We find that many commonly used operator learning models can be viewed as neural fields with conditioning mechanisms restricted to point-wise and/or global information. Motivated by this, we propose the Continuous Vision Transformer (CViT), a novel neural operator architecture that employs a vision transformer encoder and uses cross-attention to modulate a base field constructed with a trainable grid-based positional encoding of query coordinates. Despite its simplicity, CViT achieves state-of-the-art results across challenging benchmarks in climate modeling and fluid dynamics. Our contributions can be viewed as a first step towards adapting advanced computer vision architectures for building more flexible and accurate machine learning models in physical sciences.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab2cf2094210ca963bfcbe02f2b55aea2a795919" target='_blank'>
              Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective
              </a>
            </td>
          <td>
            , Jacob H. Seidman, Shyam Sankaran, Hanwen Wang, George J. Pappas, P. Perdikaris
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>43</td>
        </tr>

        <tr id="The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data. However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets. Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms. To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework. SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization. In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit. In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm. In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9f2e0f138fdb706edb87999a79e0c8ba055c75b7" target='_blank'>
              GN-SINDy: Greedy Sampling Neural Network in Sparse Identification of Nonlinear Partial Differential Equations
              </a>
            </td>
          <td>
            A. Forootani, Peter Benner
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="We utilize extreme learning machines for the prediction of partial differential equations (PDEs). Our method splits the state space into multiple windows that are predicted individually using a single model. Despite requiring only few data points (in some cases, our method can learn from a single full-state snapshot), it still achieves high accuracy and can predict the flow of PDEs over long time horizons. Moreover, we show how additional symmetries can be exploited to increase sample efficiency and to enforce equivariance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a4edc85da3f3309576bf3b09310e6de02a94929" target='_blank'>
              Predicting PDEs Fast and Efficiently with Equivariant Extreme Learning Machines
              </a>
            </td>
          <td>
            Hans Harder, Sebastian Peitz
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The growing scale and complexity of safety-critical control systems underscore the need to evolve current control architectures aiming for the unparalleled performances achievable through state-of-the-art optimization and machine learning algorithms. However, maintaining closed-loop stability while boosting the performance of nonlinear control systems using data-driven and deep-learning approaches stands as an important unsolved challenge. In this paper, we tackle the performance-boosting problem with closed-loop stability guarantees. Specifically, we establish a synergy between the Internal Model Control (IMC) principle for nonlinear systems and state-of-the-art unconstrained optimization approaches for learning stable dynamics. Our methods enable learning over arbitrarily deep neural network classes of performance-boosting controllers for stable nonlinear systems; crucially, we guarantee Lp closed-loop stability even if optimization is halted prematurely, and even when the ground-truth dynamics are unknown, with vanishing conservatism in the class of stabilizing policies as the model uncertainty is reduced to zero. We discuss the implementation details of the proposed control schemes, including distributed ones, along with the corresponding optimization procedures, demonstrating the potential of freely shaping the cost functions through several numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e51cb2795804c20221e3c822bebda0752242996d" target='_blank'>
              Learning to Boost the Performance of Stable Nonlinear Systems
              </a>
            </td>
          <td>
            Luca Furieri, C. Galimberti, Giancarlo Ferrari-Trecate
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Adjoint methods have been the pillar of gradient-based optimization for decades. They enable the accurate computation of a gradient (sensitivity) of a quantity of interest with respect to all system's parameters in one calculation. When the gradient is embedded in an optimization routine, the quantity of interest can be optimized for the system to have the desired behaviour. Adjoint methods require the system's Jacobian, whose computation can be cumbersome, and is problem dependent. We propose a computational strategy to infer the adjoint sensitivities from data (observables), which bypasses the need of the Jacobian of the physical system. The key component of this strategy is an echo state network, which learns the dynamics of nonlinear regimes with varying parameters, and evolves dynamically via a hidden state. Although the framework is general, we focus on thermoacoustics governed by nonlinear and time-delayed systems. First, we show that a parameter-aware Echo State Network (ESN) infers the parameterized dynamics. Second, we derive the adjoint of the ESN to compute the sensitivity of time-averaged cost functionals. Third, we propose the Thermoacoustic Echo State Network (T-ESN), which hard constrains the physical knowledge in the network architecture. Fourth, we apply the framework to a variety of nonlinear thermoacoustic regimes of a prototypical system. We show that the T-ESN accurately infers the correct adjoint sensitivities of the time-averaged acoustic energy with respect to the flame parameters. The results are robust to noisy data, from periodic, through quasiperiodic, to chaotic regimes. A single network predicts the nonlinear bifurcations on unseen scenarios, and so the inferred adjoint sensitivities are employed to suppress an instability via steepest descent. This work opens new possibilities for gradient-based data-driven design optimization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0daae2897c821b15dac81a757826d1009a038ffb" target='_blank'>
              Data-driven computation of adjoint sensitivities without adjoint solvers: An application to thermoacoustics
              </a>
            </td>
          <td>
            D. E. Ozan, Luca Magri
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Identifying differential operators from data is essential for the mathematical modeling of complex physical and biological systems where massive datasets are available. These operators must be stable for accurate predictions for dynamics forecasting problems. In this article, we propose a novel methodology for learning sparse differential operators that are theoretically linearly stable by solving a constrained regression problem. These underlying constraints are obtained following linear stability for dynamical systems. We further extend this approach for learning nonlinear differential operators by determining linear stability constraints for linearized equations around an equilibrium point. The applicability of the proposed method is demonstrated for both linear and nonlinear partial differential equations such as 1-D scalar advection-diffusion equation, 1-D Burgers equation and 2-D advection equation. The results indicated that solutions to constrained regression problems with linear stability constraints provide accurate and linearly stable sparse differential operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4ff687c7535d984211e6b3fc207c4c872443a9a0" target='_blank'>
              Data-driven identification of stable differential operators using constrained regression
              </a>
            </td>
          <td>
            Aviral Prakash, Yongjie Jessica Zhang
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Fluid dynamics systems driven by dominant, nearly periodic large-scale dynamics are common across wakes, jets, rotating machinery, and high-speed flows. Traditional decomposition techniques such as proper orthogonal decomposition and dynamic mode decomposition have been used to gain insight into these flows, but can require many modes to represent physical processes. With the aim of generating modes that intuitively convey the underlying physical mechanisms, we propose an intrinsic phase-based proper orthogonal decomposition (IPhaB POD) method. IPhaB POD creates energetically ranked modes that evolve along a characteristic cycle of a driving near-periodic large scale. Our proposed formulation is set in the time domain, which is particularly useful in cases where the large-scale dynamics are imperfectly periodic. We formally derive IPhaB POD within a proper orthogonal decomposition framework, and it inherits the optimal representation inherent to POD. As part of this derivation, a dynamical systems representation of the large scale is utilized, facilitating a definition of phase relative to the large scale within the time domain. An expectation operator and inner product are also constructed relative to this definition of phase in a manner that allows for the various cycles within the data to demonstrate imperfect periodicity. The formulation is tested on two sample problems: a simple, low Reynolds number airfoil wake, and a complex, high-speed pulsating shock wave problem. The resulting modes are shown to better isolate the large-scale dynamics in the first mode than space-only proper orthogonal decomposition, and to highlight meaningful small-scale dynamics in higher modes for the shock flow problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8599bb13ba943daa8faca82ff974b128401e0ae4" target='_blank'>
              Intrinsic phase based proper orthogonal decomposition (IPhaB POD): a method for physically interpretable modes in near-periodic systems
              </a>
            </td>
          <td>
            A. Borra, Zoey Flynn, Andres Goza, T. Saxton-Fox
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="A problem in nonlinear and complex dynamical systems with broad applications is forecasting the occurrence of a critical transition based solely on data without knowledge about the system equations. When such a transition leads to system collapse, as often is the case, all the available data are from the pre-critical regime where the system still functions normally, making the prediction problem challenging. In recent years, a machine-learning based approach tailored to solving this difficult prediction problem, adaptable reservoir computing, has been articulated. This Perspective introduces the basics of this machine-learning scheme and describes representative results. The general setting is that the system dynamics live on a normal attractor with oscillatory dynamics at the present time and, as a bifurcation parameter changes into the future, a critical transition can occur after which the system switches to a completely different attractor, signifying system collapse. To predict a critical transition, it is essential that the reservoir computer not only learns the dynamical "climate" of the system of interest at some specific parameter value but, more importantly, discovers how the system dynamics changes with the bifurcation parameter. It is demonstrated that this capability can be endowed into the machine through a training process with time series from a small number of distinct, pre-critical parameter values, thereby enabling accurate and reliable prediction of the catastrophic critical transition. Three applications are presented: predicting crisis, forecasting amplitude death, and creating digital twins of nonlinear dynamical systems. Limitations and future perspectives are discussed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f1749f284391bc6f9be0f2b8f564c797144a8e75" target='_blank'>
              Adaptable reservoir computing: A paradigm for model-free data-driven prediction of critical transitions in nonlinear dynamical systems.
              </a>
            </td>
          <td>
            Shirin Panahi, Ying-Cheng Lai
          </td>
          <td>2024-05-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. One main innovation established here is the construction of data-based realizations for any number of parameters. Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets. To achieve this, a new generalized multivariate rational function realization is defined. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations. The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b36130682d6d70c9ceb26aefd1dd20ecbe5695c2" target='_blank'>
              The Loewner framework for parametric systems: Taming the curse of dimensionality
              </a>
            </td>
          <td>
            A. Antoulas, I. V. Gosea, Charles Poussot-Vassal
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d99de93bf2840cc46f345bc79ece190229de9b97" target='_blank'>
              Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations
              </a>
            </td>
          <td>
            Huan Zhang, Yifan Chen, Eric Vanden-Eijnden, Benjamin Peherstorfer
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We present a unified constructive universal approximation theorem covering a wide range of learning machines including both shallow and deep neural networks based on the group representation theory. Constructive here means that the distribution of parameters is given in a closed-form expression (called the ridgelet transform). Contrary to the case of shallow models, expressive power analysis of deep models has been conducted in a case-by-case manner. Recently, Sonoda et al. (2023a,b) developed a systematic method to show a constructive approximation theorem from scalar-valued joint-group-invariant feature maps, covering a formal deep network. However, each hidden layer was formalized as an abstract group action, so it was not possible to cover real deep networks defined by composites of nonlinear activation function. In this study, we extend the method for vector-valued joint-group-equivariant feature maps, so to cover such real networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fba68fd7e1603644fcf102c1af54f9e73852f93f" target='_blank'>
              Constructive Universal Approximation Theorems for Deep Joint-Equivariant Networks by Schur's Lemma
              </a>
            </td>
          <td>
            Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="The conditional mean embedding (CME) encodes Markovian stochastic kernels through their actions on probability distributions embedded within the reproducing kernel Hilbert spaces (RKHS). The CME plays a key role in several well-known machine learning tasks such as reinforcement learning, analysis of dynamical systems, etc. We present an algorithm to learn the CME incrementally from data via an operator-valued stochastic gradient descent. As is well-known, function learning in RKHS suffers from scalability challenges from large data. We utilize a compression mechanism to counter the scalability challenge. The core contribution of this paper is a finite-sample performance guarantee on the last iterate of the online compressed operator learning algorithm with fast-mixing Markovian samples, when the target CME may not be contained in the hypothesis space. We illustrate the efficacy of our algorithm by applying it to the analysis of an example dynamical system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/151de1508e748acb71af40180cea95758cfaa6c9" target='_blank'>
              Compressed Online Learning of Conditional Mean Embedding
              </a>
            </td>
          <td>
            Boya Hou, Sina Sanjari, Alec Koppel, S. Bose
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data. The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design. A rigorous proof of convergence as the distance between observation data points converges to zero is provided. Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques. We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification. The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/62f5043fbad579fdcc9ee3d3f2353a98feac5ae8" target='_blank'>
              Machine learning of continuous and discrete variational ODEs with convergence guarantee and uncertainty quantification
              </a>
            </td>
          <td>
            Christian Offen
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Identifying Ordinary Differential Equations (ODEs) from measurement data requires both fitting the dynamics and assimilating, either implicitly or explicitly, the measurement data. The Sparse Identification of Nonlinear Dynamics (SINDy) method involves a derivative estimation step (and optionally, smoothing) and a sparse regression step on a library of candidate ODE terms. Kalman smoothing is a classical framework for assimilating the measurement data with known noise statistics. Previously, derivatives in SINDy and its python package, pysindy, had been estimated by finite difference, L1 total variation minimization, or local filters like Savitzky-Golay. In contrast, Kalman allows discovering ODEs that best recreate the essential dynamics in simulation, even in cases when it does not perform as well at recovering coefficients, as measured by their F1 score and mean absolute error. We have incorporated Kalman smoothing, along with hyperparameter optimization, into the existing pysindy architecture, allowing for rapid adoption of the method. Numerical experiments on a number of dynamical systems show Kalman smoothing to be the most amenable to parameter selection and best at preserving problem structure in the presence of noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5435325a3ad3a24c95b8947ff93859f828f47937" target='_blank'>
              Learning Nonlinear Dynamics Using Kalman Smoothing
              </a>
            </td>
          <td>
            Jacob Stevens-Haas, Yash Bhangale, Aleksandr Y Aravkin, Nathan Kutz
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We propose a self-supervised approach for learning physics-based subspaces for real-time simulation. Existing learning-based methods construct subspaces by approximating pre-defined simulation data in a purely geometric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space dimensions, and generalizes poorly beyond the training set. To overcome these limitations, we propose a self-supervised approach that directly minimizes the system's mechanical energy during training. We show that our method leads to learned subspaces that reflect physical equilibrium constraints, resolve overfitting issues of previous methods, and offer interpretable latent space parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8af663bff69f1f9808eaf47048759e64491d65cf" target='_blank'>
              Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces
              </a>
            </td>
          <td>
            Jiahong Wang, Yinwei Du, Stelian Coros, Bernhard Thomaszewski
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Observed data from a dynamic system can be assimilated into a predictive model by means of Kalman filters. Nonlinear extensions of the Kalman filter, such as the Extended Kalman Filter (EKF), are required to enable the joint estimation of (possibly nonlinear) system dynamics and of input parameters. To construct the evolution model used in the prediction phase of the EKF, we propose to rely on the Sparse Identification of Nonlinear Dynamics (SINDy). The numerical integration of a SINDy model leads to great computational savings compared to alternate strategies based on, e.g., finite elements. Indeed, SINDy allows for the immediate definition of the Jacobian matrices required by the EKF to identify system dynamics and properties, a derivation that is usually extremely involved with physical models. As a result, combining the EKF with SINDy provides a computationally efficient, easy-to-apply approach for the identification of nonlinear systems, capable of robust operation even outside the range of training of SINDy. To demonstrate the potential of the approach, we address the identification of a linear non-autonomous system consisting of a shear building model excited by real seismograms, and the identification of a partially observed nonlinear system. The challenge arising from applying SINDy when the system state is not accessible has been relieved by means of time-delay embedding. The great accuracy and the small uncertainty associated with the state identification, where the state has been augmented to include system properties, underscores the great potential of the proposed strategy, paving the way for the development of predictive digital twins in different fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2bef5ad9ed2a3bdfdaf5ec8957c692fcbb9132f7" target='_blank'>
              EKF-SINDy: Empowering the extended Kalman filter with sparse identification of nonlinear dynamics
              </a>
            </td>
          <td>
            Luca Rosafalco, Paolo Conti, Andrea Manzoni, Stefano Mariani, A. Frangi
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>7</td>
        </tr>

        <tr id="Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the $\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecc56a1b67361349c21c1fd5c588dc93f8ce39fc" target='_blank'>
              TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net
              </a>
            </td>
          <td>
            Zhuo Chen, Jacob McCarran, Esteban Vizcaino, Marin Soljacic, Di Luo
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Recurrent Neural Networks excel at predicting and generating complex high-dimensional temporal patterns. Due to their inherent nonlinear dynamics and memory, they can learn unbounded temporal dependencies from data. In a Machine Learning setting, the network's parameters are adapted during a training phase to match the requirements of a given task/problem increasing its computational capabilities. After the training, the network parameters are kept fixed to exploit the learned computations. The static parameters thereby render the network unadaptive to changing conditions, such as external or internal perturbation. In this manuscript, we demonstrate how keeping parts of the network adaptive even after the training enhances its functionality and robustness. Here, we utilize the conceptor framework and conceptualize an adaptive control loop analyzing the network's behavior continuously and adjusting its time-varying internal representation to follow a desired target. We demonstrate how the added adaptivity of the network supports the computational functionality in three distinct tasks: interpolation of temporal patterns, stabilization against partial network degradation, and robustness against input distortion. Our results highlight the potential of adaptive networks in machine learning beyond training, enabling them to not only learn complex patterns but also dynamically adjust to changing environments, ultimately broadening their applicability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/81d77920a1f3057b33f9ab48db38a16dc2b0f292" target='_blank'>
              Adaptive control of recurrent neural networks using conceptors
              </a>
            </td>
          <td>
            Guillaume Pourcel, Mirko Goldmann, Ingo Fischer, Miguel C. Soriano
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Modeling complex systems using standard neural ordinary differential equations (NODEs) often faces some essential challenges, including high computational costs and susceptibility to local optima. To address these challenges, we propose a simulation-free framework, called Fourier NODEs (FNODEs), that effectively trains NODEs by directly matching the target vector field based on Fourier analysis. Specifically, we employ the Fourier analysis to estimate temporal and potential high-order spatial gradients from noisy observational data. We then incorporate the estimated spatial gradients as additional inputs to a neural network. Furthermore, we utilize the estimated temporal gradient as the optimization objective for the output of the neural network. Later, the trained neural network generates more data points through an ODE solver without participating in the computational graph, facilitating more accurate estimations of gradients based on Fourier analysis. These two steps form a positive feedback loop, enabling accurate dynamics modeling in our framework. Consequently, our approach outperforms state-of-the-art methods in terms of training time, dynamics prediction, and robustness. Finally, we demonstrate the superior performance of our framework using a number of representative complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/96bedb3203006239c598b64a69777f9f9b9613ed" target='_blank'>
              From Fourier to Neural ODEs: Flow Matching for Modeling Complex Systems
              </a>
            </td>
          <td>
            Xin Li, Jingdong Zhang, Qunxi Zhu, Chengli Zhao, Xue Zhang, Xiaojun Duan, Wei Lin
          </td>
          <td>2024-05-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/810eaf286a745319d778b46fae72d2d33882824b" target='_blank'>
              Learning the Infinitesimal Generator of Stochastic Diffusion Processes
              </a>
            </td>
          <td>
            Vladimir Kostic, Karim Lounici, Helene Halconruy, Timothee Devergne, M. Pontil
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>70</td>
        </tr>

        <tr id="A new knowledge-based and machine learning hybrid modeling approach, called conditional Gaussian neural stochastic differential equation (CGNSDE), is developed to facilitate modeling complex dynamical systems and implementing analytic formulae of the associated data assimilation (DA). In contrast to the standard neural network predictive models, the CGNSDE is designed to effectively tackle both forward prediction tasks and inverse state estimation problems. The CGNSDE starts by exploiting a systematic causal inference via information theory to build a simple knowledge-based nonlinear model that nevertheless captures as much explainable physics as possible. Then, neural networks are supplemented to the knowledge-based model in a specific way, which not only characterizes the remaining features that are challenging to model with simple forms but also advances the use of analytic formulae to efficiently compute the nonlinear DA solution. These analytic formulae are used as an additional computationally affordable loss to train the neural networks that directly improve the DA accuracy. This DA loss function promotes the CGNSDE to capture the interactions between state variables and thus advances its modeling skills. With the DA loss, the CGNSDE is more capable of estimating extreme events and quantifying the associated uncertainty. Furthermore, crucial physical properties in many complex systems, such as the translate-invariant local dependence of state variables, can significantly simplify the neural network structures and facilitate the CGNSDE to be applied to high-dimensional systems. Numerical experiments based on chaotic systems with intermittency and strong non-Gaussian features indicate that the CGNSDE outperforms knowledge-based regression models, and the DA loss further enhances the modeling skills of the CGNSDE.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a639323a3ab8c39800f9e9f42ae3d95438cb1ec6" target='_blank'>
              CGNSDE: Conditional Gaussian Neural Stochastic Differential Equation for Modeling Complex Systems and Data Assimilation
              </a>
            </td>
          <td>
            Chuanqi Chen, Nan Chen, Jingbo Wu
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) in Euclidean space with closed-form symbolic solutions has long been a dream for mathematicians. Inspired by deep learning, Physics-Informed Neural Networks (PINNs) have shown great promise in numerically solving PDEs. However, since PINNs essentially approximate solutions within the continuous function space, their numerical solutions fall short in both precision and interpretability compared to symbolic solutions. This paper proposes a novel framework: a closed-form \textbf{Sym}bolic framework for \textbf{PDE}s (SymPDE), exploring the use of deep reinforcement learning to directly obtain symbolic solutions for PDEs. SymPDE alleviates the challenges PINNs face in fitting high-frequency and steeply changing functions. To our knowledge, no prior work has implemented this approach. Experiments on solving the Poisson's equation and heat equation in time-independent and spatiotemporal dynamical systems respectively demonstrate that SymPDE can provide accurate closed-form symbolic solutions for various types of PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21f2e9b563c24b42d70f04813caf69fba9a40ef7" target='_blank'>
              Closed-form Symbolic Solutions: A New Perspective on Solving Partial Differential Equations
              </a>
            </td>
          <td>
            , Yanjie Li, Lina Yu, Min Wu, Weijun Li, Meilan Hao, Wenqiang Li, Jingyi Liu, Yusong Deng
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5e18a33ab8b574bef024af1692a95b86dffe47ac" target='_blank'>
              Statistical algorithms for low-frequency diffusion data: A PDE approach
              </a>
            </td>
          <td>
            Matteo Giordano, Sven Wang
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We develop time-series machine learning (ML) methods for closure modeling of the Unsteady Reynolds Averaged Navier Stokes (URANS) equations applied to stably stratified turbulence (SST). SST is strongly affected by fine balances between forces and becomes more anisotropic in time for decaying cases. Moreover, there is a limited understanding of the physical phenomena described by some of the terms in the URANS equations. Rather than attempting to model each term separately, it is attractive to explore the capability of machine learning to model groups of terms, i.e., to directly model the force balances. We consider decaying SST which are homogeneous and stably stratified by a uniform density gradient, enabling dimensionality reduction. We consider two time-series ML models: Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equation (NODE). Both models perform accurately and are numerically stable in a posteriori tests. Furthermore, we explore the data requirements of the ML models by extracting physically relevant timescales of the complex system. We find that the ratio of the timescales of the minimum information required by the ML models to accurately capture the dynamics of the SST corresponds to the Reynolds number of the flow. The current framework provides the backbone to explore the capability of such models to capture the dynamics of higher-dimensional complex SST flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b6cca1a9a38a3cab38adaf5f136dbc8ce93c3f9" target='_blank'>
              Machine-Learned Closure of URANS for Stably Stratified Turbulence: Connecting Physical Timescales&Data Hyperparameters of Deep Time-Series Models
              </a>
            </td>
          <td>
            Muralikrishnan Gopalakrishnan Meena, Demetri Liousas, Andrew D. Simin, Aditya Kashi, Wesley Brewer, James J. Riley, S. D. B. Kops
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We introduce Geometric Neural Operators (GNPs) for accounting for geometric contributions in data-driven deep learning of operators. We show how GNPs can be used (i) to estimate geometric properties, such as the metric and curvatures, (ii) to approximate Partial Differential Equations (PDEs) on manifolds, (iii) learn solution maps for Laplace-Beltrami (LB) operators, and (iv) to solve Bayesian inverse problems for identifying manifold shapes. The methods allow for handling geometries of general shape including point-cloud representations. The developed GNPs provide approaches for incorporating the roles of geometry in data-driven learning of operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44f2c8d5ee8bf6e5107f615a0cdc8afd7cb4c7a4" target='_blank'>
              Geometric Neural Operators (GNPs) for Data-Driven Deep Learning of Non-Euclidean Operators
              </a>
            </td>
          <td>
            Blaine Quackenbush, P. Atzberger
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f116a6da3fbd691c6355c87e9f75c42ef5145170" target='_blank'>
              Zero-Shot Transfer of Neural ODEs
              </a>
            </td>
          <td>
            Tyler Ingebrand, Adam J. Thorpe, U. Topcu
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="We study the problem of stabilizing infinite-dimensional systems with input and output quantization. The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded. Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters. We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions. Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/74a8b97a85ed1412fab3f946b36e87644f036b76" target='_blank'>
              Stabilization of infinite-dimensional systems under quantization and packet loss
              </a>
            </td>
          <td>
            Masashi Wakaiki
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/819bc0d5e0ea2efadac1364064e40b76cf3a3a11" target='_blank'>
              Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation
              </a>
            </td>
          <td>
            Jingmin Sun, Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems. Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging. Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process. Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators. Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions. We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f005c8b19ca1d9171afc191fc78b81f136ddfb8c" target='_blank'>
              LINOCS: Lookahead Inference of Networked Operators for Continuous Stability
              </a>
            </td>
          <td>
            Noga Mudrik, Eva Yezerets, Yenho Chen, Christopher Rozell, Adam Charles
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The Singular Value Decomposition (SVD) of linear functions facilitates the calculation of their 2-induced norm and row and null spaces, hallmarks of linear control theory. In this work, we present a function representation that, similar to SVD, provides an upper bound on the 2-induced norm of bounded-input bounded-output functions, as well as facilitates the computation of generalizations of the notions of row and null spaces. Borrowing from the notion of"lifting"in Koopman operator theory, we construct a finite-dimensional lifting of inputs that relaxes the unitary property of the right-most matrix in traditional SVD, $V^*$, to be an injective, norm-preserving mapping to a slightly higher-dimensional space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/535509f18e90d04fe23ee79a1110a2c4936a88d3" target='_blank'>
              An SVD-like Decomposition of Bounded-Input Bounded-Output Functions
              </a>
            </td>
          <td>
            Brian Charles Brown, Michael King, Sean Warnick, Enoch Yeung, David Grimsman
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Physics-informed neural networks (PINNs) have recently emerged as a novel and popular approach for solving forward and inverse problems involving partial differential equations (PDEs). However, achieving stable training and obtaining correct results remain a challenge in many cases, often attributed to the ill-conditioning of PINNs. Nonetheless, further analysis is still lacking, severely limiting the progress and applications of PINNs in complex engineering problems. Drawing inspiration from the ill-conditioning analysis in traditional numerical methods, we establish a connection between the ill-conditioning of PINNs and the ill-conditioning of the Jacobian matrix of the PDE system. Specifically, for any given PDE system, we construct its controlled system. This controlled system allows for adjustment of the condition number of the Jacobian matrix while retaining the same solution as the original system. Our numerical findings suggest that the ill-conditioning observed in PINNs predominantly stems from that of the Jacobian matrix. As the condition number of the Jacobian matrix decreases, the controlled systems exhibit faster convergence rates and higher accuracy. Building upon this understanding and the natural extension of controlled systems, we present a general approach to mitigate the ill-conditioning of PINNs, leading to successful simulations of the three-dimensional flow around the M6 wing at a Reynolds number of 5,000. To the best of our knowledge, this is the first time that PINNs have been successful in simulating such complex systems, offering a promising new technique for addressing industrial complexity problems. Our findings also offer valuable insights guiding the future development of PINNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/786bc6af5f479b59f91b560760a8ed56ed691f9f" target='_blank'>
              An analysis and solution of ill-conditioning in physics-informed neural networks
              </a>
            </td>
          <td>
            W. Cao, Weiwei Zhang
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="We develop a fast and scalable numerical approach to solve Wasserstein gradient flows (WGFs), particularly suitable for high-dimensional cases. Our approach is to use general reduced-order models, like deep neural networks, to parameterize the push-forward maps such that they can push a simple reference density to the one solving the given WGF. The new dynamical system is called parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter space equipped with a pullback Wasserstein metric. Our numerical scheme can approximate the solutions of WGFs for general energy functionals effectively, without requiring spatial discretization or nonconvex optimization procedures, thus avoiding some limitations of classical numerical methods and more recent deep-learning-based approaches. A comprehensive analysis of the approximation errors measured by Wasserstein distance is also provided in this work. Numerical experiments show promising computational efficiency and verified accuracy on various WGF examples using our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f4a9e55509ca4e5d72a6f1b3b99f8552bd505b8" target='_blank'>
              Parameterized Wasserstein Gradient Flow
              </a>
            </td>
          <td>
            Yijie Jin, Shu Liu, Hao Wu, Xiaojing Ye, Haomin Zhou
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan. Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter. While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored. We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\'e rotation numbers, and canonical action-angle variables. In the second part, we establish connections between these maps and general chaotic maps in standard form. Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization). Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude. This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude. Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a8d139a939d3e8f40522265c89f6fb72d4cb1c3" target='_blank'>
              Dynamics of McMillan mappings I. McMillan multipoles
              </a>
            </td>
          <td>
            T. Zolkin, Sergei Nagaitsev, Ivan Morozov
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs). Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases. The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning. The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab062559eb3a22087cb23a00b82a93b0fff80ee0" target='_blank'>
              Convergence Rates of Online Critic Value Function Approximation in Native Spaces
              </a>
            </td>
          <td>
            Shengyuan Niu, Ali Bouland, Haoran Wang, Filippos Fotiadis, Andrew J. Kurdila, Andrea L'Afflitto, S. Paruchuri, K. Vamvoudakis
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>34</td>
        </tr>

        <tr id="Transport phenomena (e.g., fluid flows) are governed by time-dependent partial differential equations (PDEs) describing mass, momentum, and energy conservation, and are ubiquitous in many engineering applications. However, deep learning architectures are fundamentally incompatible with the simulation of these PDEs. This paper clearly articulates and then solves this incompatibility. The local-dependency of generic transport PDEs implies that it only involves local information to predict the physical properties at a location in the next time step. However, the deep learning architecture will inevitably increase the scope of information to make such predictions as the number of layers increases, which can cause sluggish convergence and compromise generalizability. This paper aims to solve this problem by proposing a distributed data scoping method with linear time complexity to strictly limit the scope of information to predict the local properties. The numerical experiments over multiple physics show that our data scoping method significantly accelerates training convergence and improves the generalizability of benchmark models on large-scale engineering simulations. Specifically, over the geometries not included in the training data for heat transferring simulation, it can increase the accuracy of Convolutional Neural Networks (CNNs) by 21.7 \% and that of Fourier Neural Operators (FNOs) by 38.5 \% on average.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5355d8f1cafbe46f9c3a9a1f31193f495099644" target='_blank'>
              Data Scoping: Effectively Learning the Evolution of Generic Transport PDEs
              </a>
            </td>
          <td>
            Jiangce Chen, Wenzhuo Xu, Zeda Xu, Noelia Grande Guti'errez, S. Narra, Christopher McComb
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="The process of training an artificial neural network involves iteratively adapting its parameters so as to minimize the error of the network’s prediction, when confronted with a learning task. This iterative change can be naturally interpreted as a trajectory in network space–a time series of networks–and thus the training algorithm (e.g., gradient descent optimization of a suitable loss function) can be interpreted as a dynamical system in graph space. In order to illustrate this interpretation, here we study the dynamical properties of this process by analyzing through this lens the network trajectories of a shallow neural network, and its evolution through learning a simple classification task. We systematically consider different ranges of the learning rate and explore both the dynamical and orbital stability of the resulting network trajectories, finding hints of regular and chaotic behavior depending on the learning rate regime. Our findings are put in contrast to common wisdom on convergence properties of neural networks and dynamical systems theory. This work also contributes to the cross-fertilization of ideas between dynamical systems theory, network theory and machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/72e81727419ca3da67887cc9cd6a76a2a0394b00" target='_blank'>
              Dynamical stability and chaos in artificial neural network trajectories along training
              </a>
            </td>
          <td>
            Kaloyan Danovski, Miguel C. Soriano, Lucas Lacasa
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Reduced order modeling (ROM) techniques, such as proper orthogonal decomposition (POD) and dynamic mode decomposition (DMD), have been widely used to analyze stationary flows. Neural networks such as autoencoders are effective to reduce the dimension of non-stationary flow, but their larger storage requirements compared to POD and DMD compromise the expectations on ROM. The present work aims at compressing the autoencoder model via two distinctively different approaches, i.e., pruning and singular value decomposition (SVD). The developed algorithm is then applied to reconstruct the flow fields of typical stationary (i.e., a laminar cylinder flow and two turbulent channel flows) and non-stationary (i.e., a laminar co-rotating vortex pair) examples. It is shown that pruning and SVD reduce the size of the autoencoder network to 6% and 3% for the two simple laminar cases (or 18% and 13%, 20%, and 10% for the two complex turbulent channel flow cases), respectively, with approximately the same order of accuracy. Therefore, the proposed autoencoders optimized by the network pruning and SVD lead to effective ROM of both stationary and non-stationary flows although they require more iterations to converge than conventional methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5874cd062cb9b26e45105dc15a96af9d3cb80370" target='_blank'>
              Compressed neural networks for reduced order modeling
              </a>
            </td>
          <td>
            Yin Zhu, Qiangqiang Sun, Dandan Xiao, Jie Yao, Xuerui Mao
          </td>
          <td>2024-05-01</td>
          <td>Physics of Fluids</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="
 In this work, we consider the problem of learning a reduced-order model of a high-dimensional stochastic nonlinear system with control inputs from noisy data. In particular, we develop a hybrid parametric/non-parametric model that learns the “average” linear dynamics in the data using dynamic mode decomposition with control (DMDc) and the nonlinearities and model uncertainties using Gaussian process (GP) regression and compare it with total least squares dynamic mode decomposition, extended here to systems with control inputs (tlsDMDc). The proposed approach is also compared with existing methods, such as DMDc-only and GP-only models, in two tasks: controlling the stochastic nonlinear Stuart-Landau equation and predicting the flowfield induced by a jet-like body force field in a turbulent boundary layer using data from large-scale numerical simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3159326ad063440b49d097ce814e17712e736c69" target='_blank'>
              Dynamic Mode Decomposition with Gaussian Process Regression for Control of High-Dimensional Nonlinear Systems
              </a>
            </td>
          <td>
            Alexandros Tsolovikos, E. Bakolas, David Goldstein
          </td>
          <td>2024-05-24</td>
          <td>Journal of Dynamic Systems, Measurement and Control</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Duality between estimation and control is a foundational concept in Control Theory. Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory. Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete. Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors. A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control. The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models. The explanation is used to motivate the difference between the linear and the nonlinear models. Once the difference is understood, duality for HMMs is described based on our recent work. The article also includes a comparison and discussion of the different types of duality considered in literature.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cff50920a2cdf0632a55e8e52a013a32a3926e8a" target='_blank'>
              Arrow of Time in Estimation and Control: Duality Theory Beyond the Linear Gaussian Model
              </a>
            </td>
          <td>
            J. W. Kim, Prashant G. Mehta
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="This paper is devoted to the estimation of the Lipschitz constant of neural networks using semidefinite programming. For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of neural networks with a dynamic programming recursion. Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints. If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well. We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures. To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a76d04075753704069e46f5f5e4d7d5b55ae992b" target='_blank'>
              Lipschitz constant estimation for general neural network architectures using control tools
              </a>
            </td>
          <td>
            Patricia Pauli, Dennis Gramlich, Frank Allgower
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="We investigate the geometric structure of adjoint systems associated with evolutionary partial differential equations at the fully continuous, semi-discrete, and fully discrete levels and the relations between these levels. We show that the adjoint system associated with an evolutionary partial differential equation has an infinite-dimensional Hamiltonian structure, which is useful for connecting the fully continuous, semi-discrete, and fully discrete levels. We subsequently address the question of discretize-then-optimize versus optimize-then-discrete for both semi-discretization and time integration, by characterizing the commutativity of discretize-then-optimize methods versus optimize-then-discretize methods uniquely in terms of an adjoint-variational quadratic conservation law. For Galerkin semi-discretizations and one-step time integration methods in particular, we explicitly construct these commuting methods by using structure-preserving discretization techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/241adc8957796aaeb4f587c9104839c6c62a858f" target='_blank'>
              On Properties of Adjoint Systems for Evolutionary PDEs
              </a>
            </td>
          <td>
            Brian K. Tran, Benjamin Southworth, Melvin Leok
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Formulating dynamical models for physical phenomena is essential for understanding the interplay between the different mechanisms and predicting the evolution of physical states. However, a dynamical model alone is often insufficient to address these fundamental tasks, as it suffers from model errors and uncertainties. One common remedy is to rely on data assimilation, where the state estimate is updated with observations of the true system. Ensemble filters sequentially assimilate observations by updating a set of samples over time. They operate in two steps: a forecast step that propagates each sample through the dynamical model and an analysis step that updates the samples with incoming observations. For accurate and robust predictions of dynamical systems, discrete solutions must preserve their critical invariants. While modern numerical solvers satisfy these invariants, existing invariant-preserving analysis steps are limited to Gaussian settings and are often not compatible with classical regularization techniques of ensemble filters, e.g., inflation and covariance tapering. The present work focuses on preserving linear invariants, such as mass, stoichiometric balance of chemical species, and electrical charges. Using tools from measure transport theory (Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear ensemble filters that automatically preserve desired linear invariants in non-Gaussian filtering problems. By specializing this framework to the Gaussian setting, we recover a constrained formulation of the Kalman filter. Then, we show how to combine existing regularization techniques for the ensemble Kalman filter (Evensen, 1994, J. Geophys. Res.) with the preservation of the linear invariants. Finally, we assess the benefits of preserving linear invariants for the ensemble Kalman filter and nonlinear ensemble filters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d141c58886abcc457131832eb29eed4ce25f26e6" target='_blank'>
              Preserving linear invariants in ensemble filtering methods
              </a>
            </td>
          <td>
            M. Provost, Jan Glaubitz, Youssef Marzouk
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Observable operator models (OOMs) offer a powerful framework for modelling stochastic processes, surpassing the traditional hidden Markov models (HMMs) in generality and efficiency. However, using OOMs to model infinite-dimensional processes poses significant theoretical challenges. This article explores a rigorous approach to developing an approximation theory for OOMs of infinite-dimensional processes. Building upon foundational work outlined in an unpublished tutorial [Jae98], an inner product structure on the space of future distributions is rigorously established and the continuity of observable operators with respect to the associated 2-norm is proven. The original theorem proven in this thesis describes a fundamental obstacle in making an infinite-dimensional space of future distributions into a Hilbert space. The presented findings lay the groundwork for future research in approximating observable operators of infinite-dimensional processes, while a remedy to the encountered obstacle is suggested.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87e6a7d4e9af5babec0554b7f23c6a6ebe8ad5cd" target='_blank'>
              Towards an Approximation Theory of Observable Operator Models
              </a>
            </td>
          <td>
            Wojciech Anyszka
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The article presents a systematic study of the problem of conditioning a Gaussian random variable $\xi$ on nonlinear observations of the form $F \circ \phi(\xi)$ where $\phi: \mathcal{X} \to \mathbb{R}^N$ is a bounded linear operator and $F$ is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $\xi \mid F\circ \phi(\xi)$, stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bcb85cb53348b1edc2b5e752a2433055403a6fd7" target='_blank'>
              Gaussian Measures Conditioned on Nonlinear Observations: Consistency, MAP Estimators, and Simulation
              </a>
            </td>
          <td>
            Yifan Chen, Bamdad Hosseini, H. Owhadi, Andrew M Stuart
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="In this work we introduce the notion of an angular spectrum for a linear discrete time nonautonomous dynamical system. The angular spectrum comprises all accumulation points of longtime averages formed by maximal principal angles between successive subspaces generated by the dynamical system. The angular spectrum is bounded by angular values which have previously been investigated by the authors. In this contribution we derive explicit formulas for the angular spectrum of some autonomous and specific nonautonomous systems. Based on a reduction principle we set up a numerical method for the general case; we investigate its convergence and apply the method to systems with a homoclinic orbit and a strange attractor. Our main theoretical result is a theorem on the invariance of the angular spectrum under summable perturbations of the given matrices (roughness theorem). It applies to systems with a so-called complete exponential dichotomy (CED), a concept which we introduce in this paper and which imposes more stringent conditions than those underlying the exponential dichotomy spectrum.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4c11794957c48141b4f2177d97d8670cee0ef62" target='_blank'>
              Angular spectra of linear dynamical systems in discrete time
              </a>
            </td>
          <td>
            W. Beyn, Thorsten Huls
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8e6ba94461e38ca92209eaa1e802d6a39c777186" target='_blank'>
              Automating the Discovery of Partial Differential Equations in Dynamical Systems
              </a>
            </td>
          <td>
            Weizhen Li, Rui Carvalho
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22a46186cfa337e51d671a8ac45c149e5f0bf01b" target='_blank'>
              Data-driven modeling of interrelated dynamical systems
              </a>
            </td>
          <td>
            Yonatan Elul, Eyal Rozenberg, Amit Boyarski, Yael Yaniv, Assaf Schuster, A. M. Bronstein
          </td>
          <td>2024-05-01</td>
          <td>Communications Physics</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In chaos control, one usually seeks to stabilize the unstable periodic orbits (UPOs) that densely inhabit the attractors of many chaotic dynamical systems. These orbits collectively play a significant role in determining the dynamics and properties of chaotic systems and are said to form the skeleton of the associated attractors. While UPOs are insightful tools for analysis, they are naturally unstable and, as such, are difficult to find and computationally expensive to stabilize. An alternative to using UPOs is to approximate them using cupolets. Cupolets, a name derived from chaotic, unstable, periodic, orbit-lets, are a relatively new class of waveforms that represent highly accurate approximations to the UPOs of chaotic systems, but which are generated via a particular control scheme that applies tiny perturbations along Poincaré sections. Originally discovered in an application of secure chaotic communications, cupolets have since gone on to play pivotal roles in a number of theoretical and practical applications. These developments include using cupolets as wavelets for image compression, targeting in dynamical systems, a chaotic analog to quantum entanglement, an abstract reducibility classification, a basis for audio and video compression, and, most recently, their detection in a chaotic neuron model. This review will detail the historical development of cupolets, how they are generated, and their successful integration into theoretical and computational science and will also identify some unanswered questions and future directions for this work.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a248090592163d02636c115d39431e2d9ab4685a" target='_blank'>
              Cupolets: History, Theory, and Applications
              </a>
            </td>
          <td>
            Matthew A. Morena, K. Short
          </td>
          <td>2024-05-13</td>
          <td>Dynamics</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this paper, we consider the problem of reference tracking in uncertain nonlinear systems. A neural State-Space Model (NSSM) is used to approximate the nonlinear system, where a deep encoder network learns the nonlinearity from data, and a state-space component captures the temporal relationship. This transforms the nonlinear system into a linear system in a latent space, enabling the application of model predictive control (MPC) to determine effective control actions. Our objective is to design the optimal controller using limited data from the \textit{target system} (the system of interest). To this end, we employ an implicit model-agnostic meta-learning (iMAML) framework that leverages information from \textit{source systems} (systems that share similarities with the target system) to expedite training in the target system and enhance its control performance. The framework consists of two phases: the (offine) meta-training phase learns a aggregated NSSM using data from source systems, and the (online) meta-inference phase quickly adapts this aggregated model to the target system using only a few data points and few online training iterations, based on local loss function gradients. The iMAML algorithm exploits the implicit function theorem to exactly compute the gradient during training, without relying on the entire optimization path. By focusing solely on the optimal solution, rather than the path, we can meta-train with less storage complexity and fewer approximations than other contemporary meta-learning algorithms. We demonstrate through numerical examples that our proposed method can yield accurate predictive models by adaptation, resulting in a downstream MPC that outperforms several baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2140b914c25c6c89e81a3b8e30f2c87f5d4bcd5d" target='_blank'>
              MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast Adaptation of Neural Predictive Models
              </a>
            </td>
          <td>
            Jiaqi Yan, Ankush Chakrabarty, Alisa Rupenyan, John Lygeros
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, We propose a novel and flexible formulation of the operator learning problem that allows jointly predicting continuous quantities and inferring distributions of discrete parameters, and thus amortizing the cost of both the inverse and the surrogate models to a joint pre-training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information. We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the systems conditions. We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e0710094b90aeeb6ed231170f016ff0f9672c27" target='_blank'>
              FUSE: Fast Unified Simulation and Estimation for PDEs
              </a>
            </td>
          <td>
            Levi E. Lingsch, Dana Grund, Siddhartha Mishra, Georgios Kissas
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Combustion instability in gas turbines and rocket engines, as one of the most challenging problems in combustion research, arises from the complex interactions among flames, which are also influenced by chemical reactions, heat and mass transfer, and acoustics. Identifying and understanding combustion instability is essential to ensure the safe and reliable operation of many combustion systems, where exploring and classifying the dynamical behaviors of complex flame systems is a core take. To facilitate fundamental studies, the present work concerns dynamical mode recognition of coupled flame oscillators made of flickering buoyant diffusion flames, which have gained increasing attention in recent years but are not sufficiently understood. The time series data of flame oscillators are generated by fully validated reacting flow simulations. Due to limitations of expertise-based models, a data-driven approach is adopted. In this study, a nonlinear dimensional reduction model of variational autoencoder (VAE) is used to project the simulation data onto a 2-dimensional latent space. Based on the phase trajectories in latent space, both supervised and unsupervised classifiers are proposed for datasets with well known labeling and without, respectively. For labeled datasets, we establish the Wasserstein-distance-based classifier (WDC) for mode recognition; for unlabeled datasets, we develop a novel unsupervised classifier (GMM-DTWC) combining dynamic time warping (DTW) and Gaussian mixture model (GMM). Through comparing with conventional approaches for dimensionality reduction and classification, the proposed supervised and unsupervised VAE-based approaches exhibit a prominent performance for distinguishing dynamical modes, implying their potential extension to dynamical mode recognition of complex combustion problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/022f51632e3880e4c0769c40770e74fe7fba9c2c" target='_blank'>
              Dynamical Mode Recognition of Coupled Flame Oscillators by Supervised and Unsupervised Learning Approaches
              </a>
            </td>
          <td>
            Weiming Xu, Tao Yang, Peng Zhang
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The nonlinear sine-Gordon equation is a prevalent feature in numerous scientific and engineering problems. In this paper, we propose a machine learning-based approach, physics-informed neural networks (PINNs), to investigate and explore the solution of the generalized non-linear sine-Gordon equation, encompassing Dirichlet and Neumann boundary conditions. To incorporate physical information for the sine-Gordon equation, a multiobjective loss function has been defined consisting of the residual of governing partial differential equation (PDE), initial conditions, and various boundary conditions. Using multiple densely connected independent artificial neural networks (ANNs), called feedforward deep neural networks designed to handle partial differential equations, PINNs have been trained through automatic differentiation to minimize a loss function that incorporates the given PDE that governs the physical laws of phenomena. To illustrate the effectiveness, validity, and practical implications of our proposed approach, two computational examples from the nonlinear sine-Gordon are presented. We have developed a PINN algorithm and implemented it using Python software. Various experiments were conducted to determine an optimal neural architecture. The network training was employed by using the current state-of-the-art optimization methods in machine learning known as Adam and L-BFGS-B minimization techniques. Additionally, the solutions from the proposed method are compared with the established analytical solutions found in the literature. The findings show that the proposed method is a computational machine learning approach that is accurate and efficient for solving nonlinear sine-Gordon equations with a variety of boundary conditions as well as any complex nonlinear physical problems across multiple disciplines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2fa94192e360572ae7bfd3cff18118b6634e19da" target='_blank'>
              Exploring Physics-Informed Neural Networks for the Generalized Nonlinear Sine-Gordon Equation
              </a>
            </td>
          <td>
            Alemayehu Tamirie Deresse, T. T. Dufera
          </td>
          <td>2024-04-30</td>
          <td>Applied Computational Intelligence and Soft Computing</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This article develops mathematical formalisms and provides numerical methods for studying the evolution of measures in nonsmooth dynamical systems using the continuity equation. The nonsmooth dynamical system is described by an evolution variational inequality and we derive the continuity equation associated with this system class using three different formalisms. The first formalism consists of using the {superposition principle} to describe the continuity equation for a measure that disintegrates into a probability measure supported on the set of vector fields and another measure representing the distribution of system trajectories at each time instant. The second formalism is based on the regularization of the nonsmooth vector field and describing the measure as the limit of a sequence of measures associated with the regularization parameter. In doing so, we obtain quantitative bounds on the Wasserstein metric between measure solutions of the regularized vector field and the limiting measure associated with the nonsmooth vector field. The third formalism uses a time-stepping algorithm to model a time-discretized evolution of the measures and show that the absolutely continuous trajectories associated with the continuity equation are recovered in the limit as the sampling time goes to zero. We also validate each formalism with numerical examples. For the first formalism, we use polynomial optimization techniques and the moment-SOS hierarchy to obtain approximate moments of the measures. For the second formalism, we illustrate the bounds on the Wasserstein metric for an academic example for which the closed-form expression of the Wasserstein metric can be calculated. For the third formalism, we illustrate the time-stepping based algorithm for measure evolution on an example that shows the effect of the concentration of measures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e9e4499422a19e67ffdc1502cf5b778af6e4bb7" target='_blank'>
              Evolution of Measures in Nonsmooth Dynamical Systems: Formalisms and Computation
              </a>
            </td>
          <td>
            S. Chhatoi, A. Tanwani, Didier Henrion
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise. The resulting motion can only be described as a random process instead of a solution curve. Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow. This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods. It presents a numerical procedure based on kernel integral operators, that computes this expectation. In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c51278ccd72965de531b2582e9c1c49ab661474a" target='_blank'>
              Drift estimation in stochastic flows using kernel integral operators
              </a>
            </td>
          <td>
            Suddhasattwa Das
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics simulations, where backpropagating feedback through many unrolled time steps is considered crucial to acquiring temporally coherent behavior. The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow, and certain modifications to the backpropagation pass leave the positions of the original minima unchanged. As any modification of backpropagation decouples forward and backward pass, the rotation-free character of the gradient field is lost. Therefore, we discuss the negative implications of using such a rotational vector field for optimization and how to counteract them. Our final procedure is easily implementable via a sequence of gradient stopping and component-wise comparison operations, which do not negatively affect scalability. Our experiments on three control problems show that especially as we increase the complexity of each task, the unbalanced updates from the gradient can no longer provide the precise control signals necessary while our method still solves the tasks. Our code can be found at https://github.com/tum-pbs/StableBPTT.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/548ed7572cbe5de1e13cfba73e4cb22db79e14a2" target='_blank'>
              Stabilizing Backpropagation Through Time to Learn Complex Physics
              </a>
            </td>
          <td>
            Patrick Schnell, Nils Thuerey
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time-dependent flow fields are typically generated by a computational fluid dynamics (CFD) method, which is an extremely time-consuming process. However, the latent relationship between the flow fields is governed by the Navier-Stokes equations and can be described by an operator. We therefore train a deep operator network, or simply DeepONet, to learn the temporal evolution between flow snapshots. Once properly trained, given a few consecutive snapshots as input, the network has a great potential to generate the next snapshot accurately and quickly. Using the output as a new input, the network iterates the process, generating a series of successive snapshots with little wall time. Specifically, we consider 2D flow around a circular cylinder at Reynolds number 1000, and prepare a set of high-fidelity data using a high-order spectral/hp element method as ground truth. Although the flow fields are periodic, there are many small-scale features in the wake flow that are difficult to generate accurately. Furthermore, any discrepancy between the prediction and the ground truth for the first snapshots can easily accumulate during the iterative process, which eventually amplifies the overall deviations. Therefore, we propose two alternative techniques to improve the training of DeepONet. The first one enhances the feature extraction of the network by harnessing the"multi-head non-local block". The second one refines the network parameters by leveraging the local smooth optimization technique. Both techniques prove to be highly effective in reducing the cumulative errors and our results outperform those of the dynamic mode decomposition method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb1100bf83e3489a2cc3c232b47b40267da64f5f" target='_blank'>
              Data-driven modeling of unsteady flow based on deep operator network
              </a>
            </td>
          <td>
            Heming Bai, Zhicheng Wang, Xuesen Chu, J.Q. Deng, Xin Bian
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations. Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself. We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space. Moreover, we prove a rigorous convergence result with a convergence rate. As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution. One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward. Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c247154ceefdafb75f75cef53cbf294aada8d1d9" target='_blank'>
              Finite-dimensional approximations of push-forwards on locally analytic functionals and truncation of least-squares polynomials
              </a>
            </td>
          <td>
            Isao Ishikawa
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data. It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications. While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required. Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology. To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems. By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set. Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b989117896302bd39581399a3a8523e2427d1a9" target='_blank'>
              Deep Learning as Ricci Flow
              </a>
            </td>
          <td>
            Anthony Baptista, Alessandro Barp, Tapabrata Chakraborti, Chris Harbron, Ben D. MacArthur, Christopher R. S. Banerji
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="There is a growing attention given to utilizing Lagrangian and Hamiltonian mechanics with network training in order to incorporate physics into the network. Most commonly, conservative systems are modeled, in which there are no frictional losses, so the system may be run forward and backward in time without requiring regularization. This work addresses systems in which the reverse direction is ill-posed because of the dissipation that occurs in forward evolution. The novelty is the use of Morse-Feshbach Lagrangian, which models dissipative dynamics by doubling the number of dimensions of the system in order to create a mirror latent representation that would counterbalance the dissipation of the observable system, making it a conservative system, albeit embedded in a larger space. We start with their formal approach by redefining a new Dissipative Lagrangian, such that the unknown matrices in the Euler-Lagrange's equations arise as partial derivatives of the Lagrangian with respect to only the observables. We then train a network from simulated training data for dissipative systems such as Fickian diffusion that arise in materials sciences. It is shown by experiments that the systems can be evolved in both forward and reverse directions without regularization beyond that provided by the Morse-Feshbach Lagrangian. Experiments of dissipative systems, such as Fickian diffusion, demonstrate the degree to which dynamics can be reversed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4a579ed4a18e0a0b6f52b6908e3f00349d0e966e" target='_blank'>
              Lagrangian Neural Networks for Reversible Dissipative Evolution
              </a>
            </td>
          <td>
            V. Sundararaghavan, Megna N. Shah, Jeff P. Simmons
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="This study utilized the Gaussian Processes (GPs) regression framework to establish stochastic error bounds between the actual and predicted state evolution of nonlinear systems. These systems are embedded in the linear parameter-varying (LPV) formulation and controlled using model predictive control (MPC). Our main focus is quantifying the uncertainty of the LPVMPC framework's forward error resulting from scheduling signal estimation mismatch. We compared our stochastic approach with a recent deterministic approach and observed improvements in conservatism and robustness. To validate our analysis and method, we solved the regulator problem of an unbalanced disk.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff8595a6a1aaeecc229b93d39c77d7cfe4f41df1" target='_blank'>
              Stochastic Error Bounds in Nonlinear Model Predictive Control with Gaussian Processes via Parameter-Varying Embeddings
              </a>
            </td>
          <td>
            Dimitrios S. Karachalios, Hossameldin S. Abbas
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The $L^2$ gradient flow of the Ginzburg-Landau free energy functional leads to the Allen Cahn equation that is widely used for modeling phase separation. Machine learning methods for solving the Allen-Cahn equation in its strong form suffer from inaccuracies in collocation techniques, errors in computing higher-order spatial derivatives through automatic differentiation, and the large system size required by the space-time approach. To overcome these limitations, we propose a separable neural network-based approximation of the phase field in a minimizing movement scheme to solve the aforementioned gradient flow problem. At each time step, the separable neural network is used to approximate the phase field in space through a low-rank tensor decomposition thereby accelerating the derivative calculations. The minimizing movement scheme naturally allows for the use of Gauss quadrature technique to compute the functional. A `$tanh$' transformation is applied on the neural network-predicted phase field to strictly bounds the solutions within the values of the two phases. For this transformation, a theoretical guarantee for energy stability of the minimizing movement scheme is established. Our results suggest that bounding the solution through this transformation is the key to effectively model sharp interfaces through separable neural network. The proposed method outperforms the state-of-the-art machine learning methods for phase separation problems and is an order of magnitude faster than the finite element method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b9587ef56ec1c5bcfe5410cb08c0d1d13eebeee3" target='_blank'>
              Gradient Flow Based Phase-Field Modeling Using Separable Neural Networks
              </a>
            </td>
          <td>
            R. Mattey, Susanta Ghosh
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Neural Ordinary Differential Equations typically struggle to generalize to new dynamical behaviors created by parameter changes in the underlying system, even when the dynamics are close to previously seen behaviors. The issue gets worse when the changing parameters are unobserved, i.e., their value or influence is not directly measurable when collecting data. We introduce Neural Context Flow (NCF), a framework that encodes said unobserved parameters in a latent context vector as input to a vector field. NCFs leverage differentiability of the vector field with respect to the parameters, along with first-order Taylor expansion to allow any context vector to influence trajectories from other parameters. We validate our method and compare it to established Multi-Task and Meta-Learning alternatives, showing competitive performance in mean squared error for in-domain and out-of-distribution evaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scott problems. This study holds practical implications for foundational models in science and related areas that benefit from conditional neural ODEs. Our code is openly available at https://github.com/ddrous/ncflow.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c738e0450649ed6c04ff7e82d993987c381e35a" target='_blank'>
              Neural Context Flows for Learning Generalizable Dynamical Systems
              </a>
            </td>
          <td>
            Roussel Desmond Nzoyem, David A.W. Barton, Tom Deakin
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile. Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows. This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles. Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator. Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD. This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method. We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis. This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient. We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f03e85eaeca443d0e3afa26fb6b312ac36136ef" target='_blank'>
              Sparse space-time resolvent analysis for statistically-stationary and time-varying flows
              </a>
            </td>
          <td>
            Barbara Lopez-Doriga, Eric Ballouz, H. J. Bae, Scott T. M. Dawson
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Regression on function spaces is typically limited to models with Gaussian process priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3ae485c26a60310aa9df90e2ba95440cb064a85" target='_blank'>
              Universal Functional Regression with Neural Operator Flows
              </a>
            </td>
          <td>
            Yaozhong Shi, Angela F. Gao, Zachary E. Ross, K. Azizzadenesheli
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>30</td>
        </tr>

        <tr id="Machine learning techniques are being used as an alternative to traditional numerical discretization methods for solving hyperbolic partial differential equations (PDEs) relevant to fluid flow. Whilst numerical methods are higher fidelity, they are computationally expensive. Machine learning methods on the other hand are lower fidelity but provide significant speed-ups. The emergence of physics-informed neural networks (PINNs) in fluid dynamics has allowed scientists to directly use PDEs for evaluating loss functions in an unsupervised manner. The downfall of this approach is that the differential form of systems is invalid at regions of shock inherent in hyperbolic PDEs such as the compressible Euler equations. To circumvent this problem we propose a modification to PDE-based PINN losses by using a finite volume-based loss function that incorporates the flux of Godunov-type methods. These Godunov-type methods are also known as approximate Riemann solvers and evaluate intercell fluxes in an entropy-satisfying manner, yielding more physically accurate shocks. Our approach increases fidelity compared to using regularized PDE-based PINN losses, as tested on the 2D Riemann problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7f9de7e2760476db603d1b4eafe262b6e8cbba1d" target='_blank'>
              Godunov Loss Functions for Modelling of Hyperbolic Conservation Laws
              </a>
            </td>
          <td>
            R. G. Cassia, R. R. Kerswell
          </td>
          <td>2024-05-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Two-dimensional (2-D) incompressible, inviscid fluids produce fascinating patterns of swirling motion. How and why the patterns emerge are long-standing questions, first addressed in the 19th century by Helmholtz, Kirchhoff, and Kelvin. Countless researchers have since contributed to innovative techniques and results, but the overarching problem of swirling 2-D motion and its long-time behavior remains largely open. Here we advocate an alternative view-point that sheds light on this problem via a link to isospectral matrix flows. The link is established through V.\;Zeitlin's beautiful model for the numerical discretization of Euler's equations in 2-D. When considered on the sphere, Zeitlin's model enables a deep connection between 2-D hydrodynamics and unitary representation theory of Lie algebras as pursued in quantum theory. Consequently, it provides a dictionary that maps hydrodynamical concepts to matrix Lie theory, which in turn gives connections to matrix factorizations, random matrices, and integrability theory, for example. The transferal of outcomes, from finite-dimensional matrices to infinite-dimensional fluids, is then supported by hard-fought results in quantization theory -- the field which describes the limit between quantum and classical physics. We demonstrate how the dictionary is constructed and how it unveils techniques for 2-D hydrodynamics. We also give accompanying convergence results for Zeitlin's model on the sphere.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d9d8004d3bc109d0d5117139bbc172ded5ed7494" target='_blank'>
              Two-dimensional fluids via matrix hydrodynamics
              </a>
            </td>
          <td>
            K. Modin, M. Viviani
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="The use of model order reduction techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community. Methods such as the multi-fidelity ensemble Kalman filter (MF-EnKF) and the multi-level ensemble Kalman filter (ML-EnKF) are recognized as state-of-the-art techniques. However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient. In our work, we investigate the use of {\it{adaptive}} reduced basis techniques in which the approximation space is modified online based on the information that is extracted from a limited number of full order solutions and that is carried by the past models. This allows to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multi-fidelity and multi-level methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f96f0401ab767d0dd4023e4c19c8614c5dc2c2e7" target='_blank'>
              An adaptive hierarchical ensemble Kalman filter with reduced basis models
              </a>
            </td>
          <td>
            Francesco A. B. Silva, Cecilia Pagliantini, K. Veroy
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="This paper presents a new boundary-value problem formulation for quantifying uncertainty induced by the presence of small Brownian noise near transversally stable periodic orbits (limit cycles) and quasiperiodic invariant tori of the deterministic dynamical systems obtained in the absence of noise. The formulation uses adjoints to construct a continuous family of transversal hyperplanes that are invariant under the linearized deterministic flow near the limit cycle or quasiperiodic invariant torus. The intersections with each hyperplane of stochastic trajectories that remain near the deterministic cycle or torus over intermediate times may be approximated by a Gaussian distribution whose covariance matrix can be obtained from the solution to the corresponding boundary-value problem. In the case of limit cycles, the analysis improves upon results in the literature through the explicit use of state-space projections, transversality constraints, and symmetry-breaking parameters that ensure uniqueness of the solution despite the lack of hyperbolicity along the limit cycle. These same innovations are then generalized to the case of a quasiperiodic invariant torus of arbitrary dimension. In each case, a closed-form solution to the covariance boundary-value problem is found in terms of a convergent series. The methodology is validated against the results of numerical integration for two examples of stochastically perturbed limit cycles and one example of a stochastically perturbed two-dimensional quasiperiodic invariant torus. Finally, an implementation of the covariance boundary-value problem in the numerical continuation package coco is applied to analyze the small-noise limit near a two-dimensional quasiperiodic invariant torus in a nonlinear deterministic dynamical system in $\mathbb{R}^4$ that does not support closed-form analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3301de8fe4477b187bf60eaf9e1b46e7eb66f04f" target='_blank'>
              Adjoint-Based Projections for Uncertainty Quantification near Stochastically Perturbed Limit Cycles and Tori
              </a>
            </td>
          <td>
            Zaid Ahsan, Harry Dankowicz, Christian Kuehn
          </td>
          <td>2024-04-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model. The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space. Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints. This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution. The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis. To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE). Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics. The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15f8d797779bc932cdc0bb4020bb6cb273b8cfa5" target='_blank'>
              Preserving Nonlinear Constraints in Variational Flow Filtering Data Assimilation
              </a>
            </td>
          <td>
            Amit N. Subrahmanya, Andrey A. Popov, Reid J. Gomillion, Adrian Sandu
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Learning approximations to smooth target functions of many variables from finite sets of pointwise samples is an important task in scientific computing and its many applications in computational science and engineering. Despite well over half a century of research on high-dimensional approximation, this remains a challenging problem. Yet, significant advances have been made in the last decade towards efficient methods for doing this, commencing with so-called sparse polynomial approximation methods and continuing most recently with methods based on Deep Neural Networks (DNNs). In tandem, there have been substantial advances in the relevant approximation theory and analysis of these techniques. In this work, we survey this recent progress. We describe the contemporary motivations for this problem, which stem from parametric models and computational uncertainty quantification; the relevant function classes, namely, classes of infinite-dimensional, Banach-valued, holomorphic functions; fundamental limits of learnability from finite data for these classes; and finally, sparse polynomial and DNN methods for efficiently learning such functions from finite data. For the latter, there is currently a significant gap between the approximation theory of DNNs and the practical performance of deep learning. Aiming to narrow this gap, we develop the topic of practical existence theory, which asserts the existence of dimension-independent DNN architectures and training strategies that achieve provably near-optimal generalization errors in terms of the amount of training data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36a0439df646365cfb853ff77a6ee369e1b99325" target='_blank'>
              Learning smooth functions in high dimensions: from sparse polynomials to deep neural networks
              </a>
            </td>
          <td>
            Ben Adcock, S. Brugiapaglia, N. Dexter, S. Moraga
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Reduced order models based on the transport of a lower dimensional manifold representation of the thermochemical state, such as Principal Component (PC) transport and Machine Learning (ML) techniques, have been developed to reduce the computational cost associated with the Direct Numerical Simulations (DNS) of reactive flows. Both PC transport and ML normally require an abundance of data to exhibit sufficient predictive accuracy, which might not be available due to the prohibitive cost of DNS or experimental data acquisition. To alleviate such difficulties, similar data from an existing dataset or domain (source domain) can be used to train ML models, potentially resulting in adequate predictions in the domain of interest (target domain). This study presents a novel probabilistic transfer learning (TL) framework to enhance the trust in ML models in correctly predicting the thermochemical state in a lower dimensional manifold and a sparse data setting. The framework uses Bayesian neural networks, and autoencoders, to reduce the dimensionality of the state space and diffuse the knowledge from the source to the target domain. The new framework is applied to one-dimensional freely-propagating flame solutions under different data sparsity scenarios. The results reveal that there is an optimal amount of knowledge to be transferred, which depends on the amount of data available in the target domain and the similarity between the domains. TL can reduce the reconstruction error by one order of magnitude for cases with large sparsity. The new framework required 10 times less data for the target domain to reproduce the same error as in the abundant data scenario. Furthermore, comparisons with a state-of-the-art deterministic TL strategy show that the probabilistic method can require four times less data to achieve the same reconstruction error.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1b40d43f32053fa66703ccc372c93df9eb12e60d" target='_blank'>
              Probabilistic transfer learning methodology to expedite high fidelity simulation of reactive flows
              </a>
            </td>
          <td>
            Bruno S. Soriano, Kisung Jung, T. Echekki, Jacqueline H. Chen, Mohammad Khalil
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="There exist many examples of systems which have some symmetries, and which one may monitor with symmetry preserving controls. Since symmetries are preserved along the evolution, full controllability is not possible, and controllability has to be considered inside sets of states with same symmetries. We prove that generic systems with symmetries are controllable in this sense. This result has several applications, for instance: (i) generic controllability of particle systems when the kernel of interaction between particles plays the role of a mean-field control; (ii) generic controllability for families of vector fields on manifolds with boundary; (iii) universal interpolation for neural networks architectures with"generic"self attention-type layers - a type of layers ubiquitous in recent neural networks architectures, e.g., in the Transformers architecture. The tools we develop could help address various other questions of control of equivariant systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/27cc3c9d1854c9bebc495276317483bfc173510a" target='_blank'>
              Generic controllability of equivariant systems and applications to particle systems and neural networks
              </a>
            </td>
          <td>
            Andrei Agrachev, Cyril Letrouit
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="One of the main theoretical challenges in learning dynamical systems from data is providing upper bounds on the generalization error, that is, the difference between the expected prediction error and the empirical prediction error measured on some finite sample. In machine learning, a popular class of such bounds are the so-called Probably Approximately Correct (PAC) bounds. In this paper, we derive a PAC bound for stable continuous-time linear parameter-varying (LPV) systems. Our bound depends on the H2 norm of the chosen class of the LPV systems, but does not depend on the time interval for which the signals are considered.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06ee4d9bca15b8e5ff46993fc4598f015c786bb2" target='_blank'>
              A finite-sample generalization bound for stable LPV systems
              </a>
            </td>
          <td>
            Daniel Racz, Martin Gonzalez, M. Petreczky, A. Benczúr, B. Daróczy
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="State-of-the-art techniques in generative artificial intelligence are employed for the first time to construct a surrogate model for plasma turbulence that enables long time transport simulations. The proposed GAIT (Generative Artificial Intelligence Turbulence) model is based on the coupling of a convolutional variational auto-encoder, that encodes precomputed turbulence data into a reduce latent space, and a deep neural network and decoder that generate new turbulence states 400 times faster than the direct numerical integration. The model is applied to the Hasegawa-Wakatani (HW) plasma turbulence model, that is closely related to the quasigeostrophic model used in geophysical fluid dynamics. Very good agreement is found between the GAIT and the HW models in the spatio-temporal Fourier and Proper Orthogonal Decomposition spectra as well as in the flow topology characterized by the Okubo-Weiss decomposition. Agreement is also found in the probability distribution function of particle displacements and the effective turbulent diffusivity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c3f76358081c555aef0dcae162ce80dbb4c244b" target='_blank'>
              A generative machine learning surrogate model of plasma turbulence
              </a>
            </td>
          <td>
            B. Clavier, D. Zarzoso, D. del-Castillo-Negrete, E. Frenord
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Data-driven modelling and scientific machine learning have been responsible for significant advances in determining suitable models to describe data. Within dynamical systems, neural ordinary differential equations (ODEs), where the system equations are set to be governed by a neural network, have become a popular tool for this challenge in recent years. However, less emphasis has been placed on systems that are only partially-observed. In this work, we employ a hybrid neural ODE structure, where the system equations are governed by a combination of a neural network and domain-specific knowledge, together with symbolic regression (SR), to learn governing equations of partially-observed dynamical systems. We test this approach on two case studies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional model of the Lorenz system. We demonstrate that the method is capable of successfully learning the true underlying governing equations of unobserved states within these systems, with robustness to measurement noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8e5f91324b2ca816ed10e0d9a1d6565fb12a4a1f" target='_blank'>
              Learning Governing Equations of Unobserved States in Dynamical Systems
              </a>
            </td>
          <td>
            Gevik Grigorian, Sandip V. George, S. Arridge
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Model predictive control (MPC) for linear systems with quadratic costs and linear constraints is shown to admit an exact representation as an implicit neural network. A method to"unravel"the implicit neural network of MPC into an explicit one is also introduced. As well as building links between model-based and data-driven control, these results emphasize the capability of implicit neural networks for representing solutions of optimisation problems, as such problems are themselves implicitly defined functions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc3b642b6f39332a0a65d26c370754cf642847ba" target='_blank'>
              Mapping back and forth between model predictive control and neural networks
              </a>
            </td>
          <td>
            Ross Drummond, P. R. B. Monasterios, Giorgio Valmorbida
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Modern deep neural networks have achieved high performance across various tasks. Recently, researchers have noted occurrences of low-dimensional structure in the weights, Hessian's, gradients, and feature vectors of these networks, spanning different datasets and architectures when trained to convergence. In this analysis, we theoretically demonstrate these observations arising, and show how they can be unified within a generalized unconstrained feature model that can be considered analytically. Specifically, we consider a previously described structure called Neural Collapse, and its multi-layer counterpart, Deep Neural Collapse, which emerges when the network approaches global optima. This phenomenon explains the other observed low-dimensional behaviours on a layer-wise level, such as the bulk and outlier structure seen in Hessian spectra, and the alignment of gradient descent with the outlier eigenspace of the Hessian. Empirical results in both the deep linear unconstrained feature model and its non-linear equivalent support these predicted observations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b01285a2babda0ce3f8f9b2933f0c1a7dfa7368" target='_blank'>
              Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model
              </a>
            </td>
          <td>
            Connall Garrod, Jonathan P. Keating
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="The study of neural operators has paved the way for the development of efficient approaches for solving partial differential equations (PDEs) compared with traditional methods. However, most of the existing neural operators lack the capability to provide uncertainty measures for their predictions, a crucial aspect, especially in data-driven scenarios with limited available data. In this work, we propose a novel Neural Operator-induced Gaussian Process (NOGaP), which exploits the probabilistic characteristics of Gaussian Processes (GPs) while leveraging the learning prowess of operator learning. The proposed framework leads to improved prediction accuracy and offers a quantifiable measure of uncertainty. The proposed framework is extensively evaluated through experiments on various PDE examples, including Burger's equation, Darcy flow, non-homogeneous Poisson, and wave-advection equations. Furthermore, a comparative study with state-of-the-art operator learning algorithms is presented to highlight the advantages of NOGaP. The results demonstrate superior accuracy and expected uncertainty characteristics, suggesting the promising potential of the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0197704fc0e18ed51f63eda29b27b3cb01285ad6" target='_blank'>
              Neural Operator induced Gaussian Process framework for probabilistic solution of parametric partial differential equations
              </a>
            </td>
          <td>
            Sawan Kumar, R. Nayek, Souvik Chakraborty
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In one calculation, adjoint sensitivity analysis provides the gradient of a quantity of interest with respect to all system's parameters. Conventionally, adjoint solvers need to be implemented by differentiating computational models, which can be a cumbersome task and is code-specific. To propose an adjoint solver that is not code-specific, we develop a data-driven strategy. We demonstrate its application on the computation of gradients of long-time averages of chaotic flows. First, we deploy a parameter-aware echo state network (ESN) to accurately forecast and simulate the dynamics of a dynamical system for a range of system's parameters. Second, we derive the adjoint of the parameter-aware ESN. Finally, we combine the parameter-aware ESN with its adjoint version to compute the sensitivities to the system parameters. We showcase the method on a prototypical chaotic system. Because adjoint sensitivities in chaotic regimes diverge for long integration times, we analyse the application of ensemble adjoint method to the ESN. We find that the adjoint sensitivities obtained from the ESN match closely with the original system. This work opens possibilities for sensitivity analysis without code-specific adjoint solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/553a6afc439089894b231b44d32efab776f1e7b8" target='_blank'>
              Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A Data-Driven Approach
              </a>
            </td>
          <td>
            D. E. Ozan, Luca Magri
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data-driven modeling of dynamical systems often faces numerous data-related challenges. A fundamental requirement is the existence of a unique set of parameters for a chosen model structure, an issue commonly referred to as identifiability. Although this problem is well studied for ordinary differential equations (ODEs), few studies have focused on the more general class of systems described by differential-algebraic equations (DAEs). Examples of DAEs include dynamical systems with algebraic equations representing conservation laws or approximating fast dynamics. This work introduces a novel identifiability test for models characterized by nonlinear DAEs. Unlike previous approaches, our test only requires prior knowledge of the system equations and does not need nonlinear transformation, index reduction, or numerical integration of the DAEs. We employed our identifiability analysis across a diverse range of DAE models, illustrating how system identifiability depends on the choices of sensors, experimental conditions, and model structures. Given the added challenges involved in identifying DAEs when compared to ODEs, we anticipate that our findings will have broad applicability and contribute significantly to the development and validation of data-driven methods for DAEs and other structure-preserving models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37947bd9315cd7b127f395007520d352fb1efed0" target='_blank'>
              Identifiability of Differential-Algebraic Systems
              </a>
            </td>
          <td>
            A. Montanari, Franccois Lamoline, Robert Bereza, Jorge Gonccalves
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Data assimilation refers to a set of algorithms designed to compute the optimal estimate of a system's state by refining the prior prediction (known as background states) using observed data. Variational assimilation methods rely on the maximum likelihood approach to formulate a variational cost, with the optimal state estimate derived by minimizing this cost. Although traditional variational methods have achieved great success and have been widely used in many numerical weather prediction centers, they generally assume Gaussian errors in the background states, which limits the accuracy of these algorithms due to the inherent inaccuracies of this assumption. In this paper, we introduce VAE-Var, a novel variational algorithm that leverages a variational autoencoder (VAE) to model a non-Gaussian estimate of the background error distribution. We theoretically derive the variational cost under the VAE estimation and present the general formulation of VAE-Var; we implement VAE-Var on low-dimensional chaotic systems and demonstrate through experimental results that VAE-Var consistently outperforms traditional variational assimilation methods in terms of accuracy across various observational settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aff96780bad11d20091e6674a2c44fe4f05b0b20" target='_blank'>
              VAE-Var: Variational-Autoencoder-Enhanced Variational Assimilation
              </a>
            </td>
          <td>
            Yi Xiao, Qilong Jia, Wei Xue, Lei Bai
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Continuous time recurrent neural networks (CTRNNs) are systems of coupled ordinary differential equations (ODEs) inspired by the structure of neural networks in the brain. CTRNNs are known to be universal dynamical approximators: given a large enough system, the parameters of a CTRNN can be tuned to produce output that is arbitrarily close to that of any other dynamical system. However, in practice, both designing systems of CTRNN to have a certain output, and the reverse-understanding the dynamics of a given system of CTRNN-can be nontrivial. In this article, we describe a method for embedding any specified Turing machine in its entirety into a CTRNN. As such, we describe in detail a continuous time dynamical system that performs arbitrary discrete-state computations. We suggest that in acting as both a continuous time dynamical system and as a computer, the study of such systems can help refine and advance the debate concerning the Computational Hypothesis that cognition is a form of computation and the Dynamical Hypothesis that cognitive systems are dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4c18d67633c17f70ffc53a4f2853ff51348378c" target='_blank'>
              A Continuous Time Dynamical Turing Machine.
              </a>
            </td>
          <td>
            C. Postlethwaite, Peter Ashwin, Matthew Egbert
          </td>
          <td>2024-05-16</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Many complicated dynamical events may be broken down into simpler pieces and efficiently described by a system that shifts among a variety of conditionally dynamical modes. Building on switching linear dynamical systems, we develop a new model that extends the switching linear dynamical systems for better discovering these dynamical modes. In the proposed model, the linear dynamics of latent variables can be described by a higher-order vector autoregressive process, which makes it feasible to evaluate the higher-order dependency relationships in the dynamics. In addition, the transition of switching states is determined by a stick-breaking logistic regression, overcoming the limitation of a restricted geometric state duration and recovering the symmetric dependency between the switching states and the latent variables from asymmetric relationships. Furthermore, logistic regression evidence potentials can appear as conditionally Gaussian potentials by utilizing the Pólya-gamma augmentation strategy. Filtering and smoothing algorithms and Bayesian inference for parameter learning in the proposed model are presented. The utility and versatility of the proposed model are demonstrated on synthetic data and public functional magnetic resonance imaging data. Our model improves the current methods for learning the switching linear dynamical modes, which will facilitate the identification and assessment of the dynamics of complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f180f44aa0cab4e14882a44aef54b3e4ae757688" target='_blank'>
              Bayesian Inference of Recurrent Switching Linear Dynamical Systems with Higher-Order Dependence
              </a>
            </td>
          <td>
            Houxiang Wang, Jiaqing Chen
          </td>
          <td>2024-04-13</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The randomized singular value decomposition (SVD) has become a popular approach to computing cheap, yet accurate, low-rank approximations to matrices due to its efficiency and strong theoretical guarantees. Recent work by Boull\'e and Townsend (FoCM, 2023) presents an infinite-dimensional analog of the randomized SVD to approximate Hilbert-Schmidt operators. However, many applications involve computing low-rank approximations to symmetric positive semi-definite matrices. In this setting, it is well-established that the randomized Nystr{\"o}m approximation is usually preferred over the randomized SVD. This paper explores an infinite-dimensional analog of the Nystr{\"o}m approximation to compute low-rank approximations to non-negative self-adjoint trace-class operators. We present an analysis of the method and, along the way, improve the existing infinite-dimensional bounds for the randomized SVD. Our analysis yields bounds on the expected value and tail bounds for the Nystr{\"o}m approximation error in the operator, trace, and Hilbert-Schmidt norms. Numerical experiments for simple integral operators validate the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a99a1ae9a29584700d070c9ffbf466c118c5409a" target='_blank'>
              Randomized Nyström approximation of non-negative self-adjoint operators
              </a>
            </td>
          <td>
            David Persson, Nicolas Boull'e, Daniel Kressner
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/50cc0f3e7c994d26825a061369cf24c303e71e0a" target='_blank'>
              Learning spiking neuronal networks with artificial neural networks: neural oscillations.
              </a>
            </td>
          <td>
            Ruilin Zhang, Zhongyi Wang, Tianyi Wu, Yuhang Cai, Louis Tao, Zhuocheng Xiao, Yao Li
          </td>
          <td>2024-04-17</td>
          <td>Journal of mathematical biology</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Semi-Lagrangian (SL) schemes are highly efficient for simulating transport equations and are widely used across various applications. Despite their success, designing genuinely multi-dimensional and conservative SL schemes remains a significant challenge. Building on our previous work [Chen et al., J. Comput. Phys., V490 112329, (2023)], we introduce a conservative machine-learning-based SL finite difference (FD) method that allows for extra-large time step evolution. At the core of our approach is a novel dynamical graph neural network designed to handle the complexities associated with tracking accurately upstream points along characteristics. This proposed neural transport solver learns the conservative SL FD discretization directly from data, improving accuracy and efficiency compared to traditional numerical schemes, while significantly simplifying algorithm implementation. We validate the method' s effectiveness and efficiency through numerical tests on benchmark transport equations in both one and two dimensions, as well as the nonlinear Vlasov-Poisson system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cee2cc76a668b9e93d93f4d17468c991f33508bf" target='_blank'>
              Conservative semi-lagrangian finite difference scheme for transport simulations using graph neural networks
              </a>
            </td>
          <td>
            Yongsheng Chen, Wei Guo, Xinghui Zhong
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="
 Learned Primal-Dual (LPD) is a deep learning based method for composite optimization problems that is based on unrolling/unfolding the primal-dual hybrid gradient algorithm. While achieving great successes in applications, the mathematical interpretation of LPD as a truncated iterative scheme is not necessarily sufficient to fully understand its properties. In this paper, we study the LPD with a general linear operator. We model the forward propagation of LPD as a system of difference equations and a system of differential equations in discrete- and continuous-time settings (for primal and dual variables/trajectories), which are named discrete-time LPD and continuous-time LPD, respectively. Forward analyses such as stabilities and the convergence of the state variables of the discrete-time LPD to the solution of continuous-time LPD are given. Moreover, we analyze the learning problems with/without regularization terms of both discrete-time and continuous-time LPD from the optimal control viewpoint. We prove convergence results of their optimal solutions with respect to the network state initialization and training data, showing in some sense the topological stability of the learning problems. We also establish convergence from the solution of the discrete-time LPD learning problem to that of the continuous-time LPD learning problem through a piecewise linear extension, under some appropriate assumptions on the space of learnable parameters. This study demonstrates theoretically the robustness of the LPD structure and the associated training process, and can induce some future research and applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e385d7198f01073117e03c2ead3f15483eeca0b6" target='_blank'>
              On dynamical system modeling of Learned Primal-Dual with a linear operator $\mathcal{K}$: Stability and convergence properties
              </a>
            </td>
          <td>
            Jinshu Huang, Yiming Gao, Chunlin Wu
          </td>
          <td>2024-05-10</td>
          <td>Inverse Problems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
 Pavlov [Adv. Math.295 (2016), 250–270; Nonlinearity32 (2019), 2441–2466] studied the measures of maximal entropy for dynamical systems with weak versions of specification property and found the existence of intrinsic ergodicity would be influenced by the assumptions of the gap functions. Inspired by these, in this article, we study the dynamical systems with non-uniform specification property. We give some basic properties these systems have and give an assumption for the gap functions to ensure the systems have the following five properties: CO-measures are dense in invariant measures; for every non-empty compact connected subset of invariant measures, its saturated set is dense in the total space; ergodic measures are residual in invariant measures; ergodic measures are connected; and entropy-dense. In addition, we will give examples to show the assumption is optimal.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/08a09034ef73f2f78893e511c0d23416281c5aa8" target='_blank'>
              Similarities and differences between specification and non-uniform specification
              </a>
            </td>
          <td>
            Wanshan Lin, Xueting Tian, Chenwei Yu
          </td>
          <td>2024-04-15</td>
          <td>Ergodic Theory and Dynamical Systems</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Manifold learning is a field of study in machine learning and statistics that is closely associated with dimensionality reduction algorithmic techniques is gaining popularity these days. There are two types of manifold learning approaches: linear and nonlinear. Principal component analysis (PCA) and multidimensional scaling (MDS) are two examples of linear techniques that have long been staples in the statistician's arsenal for evaluating multivariate data. Nonlinear manifold learning, which encompasses diffusion maps, Laplacian Eigenmaps, Hessian Eigenmaps, Isomap, and local linear embedding, has seen a surge in research effort recently. A few of these methods are nonlinear extensions of linear approaches. A nearest search, the definition of distances or affinities between points (a crucial component of these methods' effectiveness), and an Eigen problem for embedding high-dimensional points into a lower dimensional space make up the algorithmic process of the majority of these techniques. The strengths and weaknesses of the new method are briefly reviewed in this article. In the field of computer graphics, we utilize a particular manifold learning method was first presented in statistics and machine learning to create a global, Spectral-based shape descriptor.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/82375ae6c235229b9849c79c260eab0aac25395d" target='_blank'>
              Analysis of Manifold and its Application
              </a>
            </td>
          <td>
            Gyanvendra Pratap Singh, Shristi Srivastav
          </td>
          <td>2024-05-30</td>
          <td>International Journal of Science and Research Archive</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents an overview and comparative study of the state of the art in State-Order Reduction (SOR) and Scheduling Dimension Reduction (SDR) for Linear Parameter-Varying (LPV) State-Space (SS) models, comparing and benchmarking their capabilities, limitations and performance. The use case chosen for these studies is an interconnected network of nonlinear coupled mass spring damper systems with three different configurations, where some spring coefficients are described by arbitrary user-defined static nonlinear functions. For SOR, the following methods are compared: Linear Time-Invariant (LTI), LPV and LFR-based balanced reductions, moment matching and parameter-varying oblique projection. For SDR, the following methods are compared: Principal Component Analysis (PCA), trajectory PCA, Kernel PCA and LTI balanced truncation, autoencoders and deep neural network. The comparison reveals the most suitable reduction methods for the different benchmark configurations, from which we provide use case SOR and SDR guidelines that can be used to choose the best reduction method for a given LPV-SS model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db521cd814b2c0ea6c5e89c34e192c7077ffe7eb" target='_blank'>
              On the reduction of Linear Parameter-Varying State-Space models
              </a>
            </td>
          <td>
            E. J. Olucha, Bogoljub Terzin, Amritam Das, Roland T'oth
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Fluid dynamics problems are characterized by being multidimensional and nonlinear, causing the experiments and numerical simulations being complex, time-consuming and monetarily expensive. In this sense, there is a need to find new ways to obtain data in a more economical manner. Thus, in this work we study the application of time series forecasting to fluid dynamics problems, where the aim is to predict the flow dynamics using only past information. We focus our study on models based on deep learning that do not require a high amount of data for training, as this is the problem we are trying to address. Specifically in this work we have tested three autoregressive models where two of them are fully based on deep learning and the other one is a hybrid model that combines modal decomposition with deep learning. We ask these models to generate $200$ time-ahead predictions of two datasets coming from a numerical simulation and experimental measurements, where the latter is characterized by being turbulent. We show how the hybrid model generates more reliable predictions in the experimental case, as it is physics-informed in the sense that the modal decomposition extracts the physics in a way that allows us to predict it.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7b99cbcae24e51709d38ebd55a9ffa77001107f" target='_blank'>
              Exploring the efficacy of a hybrid approach with modal decomposition over fully deep learning models for flow dynamics forecasting
              </a>
            </td>
          <td>
            Rodrigo Abad'ia-Heredia, A. Corrochano, Manuel López-Martín, S. L. Clainche
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions. MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD. We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3de16e91cc88fd0f9ba210bffa57fcd84c2368b6" target='_blank'>
              MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation
              </a>
            </td>
          <td>
            Akshay Thakur, Souvik Chakraborty
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics. To construct these biases, a metriplectic evolution of the system is assumed. This provides excellent results, when compared to uninformed, black box networks. While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks. In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks. We apply this framework for examples in the fields of solid and fluid mechanics. Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15fd8e32ee00bcac84f21f3d06d5c1ef1f8e9337" target='_blank'>
              Graph neural networks informed locally by thermodynamics
              </a>
            </td>
          <td>
            Alicia Tierz, Ic´ıar Alfaro, David Gonz'alez, Francisco Chinesta, Elías Cueto
          </td>
          <td>2024-05-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being well-suited for optimal control design, numerical simulations illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear benchmarks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b929d17a9033ce7fade68ef2c9fc4e0ab291210" target='_blank'>
              A neural network-based approach to hybrid systems identification for control
              </a>
            </td>
          <td>
            F. Fabiani, Bartolomeo Stellato, Daniele Masti, P. Goulart
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Standard approaches to controlling dynamical systems involve biologically implausible steps such as backpropagation of errors or intermediate model-based system representations. Recent advances in machine learning have shown that"imperfect"feedback of errors during training can yield test performance that is similar to using full backpropagated errors, provided that the two error signals are at least somewhat aligned. Inspired by such methods, we introduce an iterative, spatiotemporally local protocol to learn driving forces and control non-equilibrium dynamical systems using imperfect feedback signals. We present numerical experiments and theoretical justification for several examples. For systems in conservative force fields that are driven by external time-dependent protocols, our update rules resemble a dynamical version of contrastive divergence. We appeal to linear response theory to establish that our imperfect update rules are locally convergent for these conservative systems. For systems evolving under non-conservative dynamics, we derive a new theoretical result that makes possible the control of non-equilibrium steady-state probabilities through simple local update rules. Finally, we show that similar local update rules can also solve dynamical control problems for non-conservative systems, and we illustrate this in the non-trivial example of active nematics. Our updates allow learning spatiotemporal activity fields that pull topological defects along desired trajectories in the active nematic fluid. These imperfect feedback methods are information efficient and in principle biologically plausible, and they can help extend recent methods of decentralized training for physical materials into dynamical settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/82094b89578003d8660077eda173551c4969dba9" target='_blank'>
              Learning to control non-equilibrium dynamics using local imperfect gradients
              </a>
            </td>
          <td>
            Carlos Floyd, Aaron Dinner, Suriyanarayanan Vaikuntanathan
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="Partial differential equation parameter estimation is a mathematical and computational process used to estimate the unknown parameters in a partial differential equation model from observational data. This paper employs a greedy sampling approach based on the Discrete Empirical Interpolation Method to identify the most informative samples in a dataset associated with a partial differential equation to estimate its parameters. Greedy samples are used to train a physics-informed neural network architecture which maps the nonlinear relation between spatio-temporal data and the measured values. To prove the impact of greedy samples on the training of the physics-informed neural network for parameter estimation of a partial differential equation, their performance is compared with random samples taken from the given dataset. Our simulation results show that for all considered partial differential equations, greedy samples outperform random samples, i.e., we can estimate parameters with a significantly lower number of samples while simultaneously reducing the relative estimation error. A Python package is also prepared to support different phases of the proposed algorithm, including data prepossessing, greedy sampling, neural network training, and comparison.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9ec87e0f1b224783d185d7b296dcb80121b11493" target='_blank'>
              GS-PINN: Greedy Sampling for Parameter Estimation in Partial Differential Equations
              </a>
            </td>
          <td>
            A. Forootani, Harshit Kapadia, Sridhar Chellappa, P. Goyal, Peter Benner
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens. Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet. The main goal of our work is to close this gap and understand the limitations and potentials of this device. It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates. Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams. In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables. This provides an evaluation of the dynamical aperture, Poincar\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes. Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time. The second part investigates the particle dynamics as a function of system parameters. We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations. Each regime is considered in great detail, including the limiting cases of large and small amplitudes. In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb3718191ea7e071bb47fd0a739ce8e135459644" target='_blank'>
              Dynamics of McMillan mappings II. Axially symmetric map
              </a>
            </td>
          <td>
            T. Zolkin, Brandon Cathey, Sergei Nagaitsev
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large. Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces. To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation. Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles. Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fc40e443b0c81201f8864bff37a50d5eb1d645f" target='_blank'>
              Control of high-dimensional collective dynamics by deep neural feedback laws and kinetic modelling
              </a>
            </td>
          <td>
            G. Albi, Sara Bicego, D. Kalise
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/012edc12bb8f81586eba3ee451de916124498e06" target='_blank'>
              Marrying Causal Representation Learning with Dynamical Systems for Science
              </a>
            </td>
          <td>
            Dingling Yao, Caroline Muller, Francesco Locatello
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In this work, we consider a state estimation problem for large-scale nonlinear processes in the absence of first-principles process models. By exploiting process operation data, both process modeling and state estimation design are addressed within a distributed framework. By leveraging the Koopman operator concept, a parallel subsystem modeling approach is proposed to establish interactive linear subsystem process models in higher-dimensional subspaces, each of which correlates with the original nonlinear subspace of the corresponding process subsystem via a nonlinear mapping. The data-driven linear subsystem models can be used to collaboratively characterize and predict the dynamical behaviors of the entire nonlinear process. Based on the established subsystem models, local state estimators that can explicitly handle process operation constraints are designed using moving horizon estimation. The local estimators are integrated via information exchange to form a distributed estimation scheme, which provides estimates of the unmeasured/unmeasurable state variables of the original nonlinear process in a linear manner. The proposed framework is applied to a chemical process and an agro-hydrological process to illustrate its effectiveness and applicability. Good open-loop predictability of the linear subsystem models is confirmed, and accurate estimates of the process states are obtained without requiring a first-principles process model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/03eda988898fe86254e2029f4bc9aee75bf1c0e9" target='_blank'>
              Data-driven parallel Koopman subsystem modeling and distributed moving horizon state estimation for large-scale nonlinear processes
              </a>
            </td>
          <td>
            Xiaojie Li, Song Bo, Xuewen Zhang, Yan Qin, Xunyuan Yin
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Neural networks have become a widely adopted tool for tackling a variety of problems in machine learning and artificial intelligence. In this contribution we use the mathematical framework of local stability analysis to gain a deeper understanding of the learning dynamics of feed forward neural networks. Therefore, we derive equations for the tangent operator of the learning dynamics of three-layer networks learning regression tasks. The results are valid for an arbitrary numbers of nodes and arbitrary choices of activation functions. Applying the results to a network learning a regression task, we investigate numerically, how stability indicators relate to the final training-loss. Although the specific results vary with different choices of initial conditions and activation functions, we demonstrate that it is possible to predict the final training loss, by monitoring finite-time Lyapunov exponents or covariant Lyapunov vectors during the training process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/870465bd2bfb984efc8bc1293ffbd4dda5049468" target='_blank'>
              On the weight dynamics of learning networks
              </a>
            </td>
          <td>
            Nahal Sharafi, Christoph Martin, Sarah Hallerberg
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The present work provides a contribution to an overarching cross-methodological causality investigation, encompassing a methodological synergy among physical, analytical, information-theoretic and systems intelligence approaches to causal discovery and quantification in complex system dynamics. These efforts methodologically lead to the emergence of a broader causal framework, valid not only in classical recurrence-based dynamical systems, but also on the generalized information physics of non-ergodic coevolutionary spatiotemporal complexity. This study begins with a comprehensive cross-examination of causality metrics derived from these diverse domains, by synthesizing causality insights from information theory, which enables the quantification of information flow among variables; differential geometry, which captures the curvature and structure of causal relationships; dynamical systems, which analyze the temporal evolution of systems and associated kinematic geometric properties; and fundamental physical metrics, which elucidate causal connections in the physical world from fundamental thermodynamic principles. Through this analysis, we aim to deepen our understanding of causality in complex systems, with physical process understanding and geophysical applications in mind. Having laid out some of the key methodological flavours of causality, the present communication introduces new metrics further contributing to a broader non-Shannonian information theoretic causality pool of methods, along with additional advances on quantum thermodymamical, nonlinear statistical mechanical, differential geometric and topologic approaches on causality. Positioning ourselves in a broader nonlinear non-Gaussian non-ergodic setting to tackle far-from-equilibrium structural-functional coevolution and synergistic emergence in complex system dynamics, our derivations further contribute to a new generation of information theoretic, dynamical systems and non-equilibrium thermodynamic causality approaches, along with their synergistic articulation towards a unified framework. This brings out further cross-methodological comparability, portability and complementary insights on dealing with the intricate causality of complex multiscale far-from-equilibrium Earth system dynamic phenomena. By unveiling manifold flavours of causality and their interconnections, this study brings out their commonalities, synergies, and further potential synergistic applications across disciplines. This interdisciplinary approach not only enhances our theoretical understanding of causality but also provides practical implications for applications in fields such as data science, network theory, and complex systems analysis, with direct relevance across a plethora of scientific, technical and operational fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ddb8ffb2f5fd9c939b35034b7206fae818eae999" target='_blank'>
              Unfolding the Manifold Flavours of Causality
              </a>
            </td>
          <td>
            Rui A. P. Perdigão
          </td>
          <td>2024-04-15</td>
          <td>None</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We investigate the reconstruction of time series from dynamical networks that are partially observed. In particular, we address the extent to which the time series at a node of the network can be successfully reconstructed when measuring from another node, or subset of nodes, corrupted by observational noise. We will assume the dynamical equations of the network are known, and that the dynamics are not necessarily low-dimensional. The case of linear dynamics is treated first, and leads to a definition of observation error magnification factor (OEMF) that measures the magnification of noise in the reconstruction process. Subsequently, the definition is applied to nonlinear and chaotic dynamics. Comparison of OEMF for different target/observer combinations can lead to better understanding of how to optimally observe a network. As part of the study, a computational method for reconstructing time series from partial observations is presented and analyzed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b5532acea4ab11e2370e2a90bab5192572801c7e" target='_blank'>
              Reconstruction of network dynamics from partial observations
              </a>
            </td>
          <td>
            Tyrus Berry, Timothy Sauer
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="There exist endless examples of dynamical systems with vast available data and unsatisfying mathematical descriptions. Sparse regression applied to symbolic libraries has quickly emerged as a powerful tool for learning governing equations directly from data; these learned equations balance quantitative accuracy with qualitative simplicity and human interpretability. Here, I present a general purpose, model agnostic sparse regression algorithm that extends a recently proposed exhaustive search leveraging iterative Singular Value Decompositions (SVD). This accelerated scheme, Scalable Pruning for Rapid Identification of Null vecTors (SPRINT), uses bisection with analytic bounds to quickly identify optimal rank-1 modifications to null vectors. It is intended to maintain sensitivity to small coefficients and be of reasonable computational cost for large symbolic libraries. A calculation that would take the age of the universe with an exhaustive search but can be achieved in a day with SPRINT.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f8ccf22995f032ef4f3f34e6540383da7d52b4c" target='_blank'>
              Scalable Sparse Regression for Model Discovery: The Fast Lane to Insight
              </a>
            </td>
          <td>
            Matthew Golden
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Finite-state abstractions are widely studied for the automated synthesis of correct-by-construction controllers for stochastic dynamical systems. However, existing abstraction methods often lead to prohibitively large finite-state models. To address this issue, we propose a novel abstraction scheme for stochastic linear systems that exploits the system's stability to obtain significantly smaller abstract models. As a unique feature, we first stabilize the open-loop dynamics using a linear feedback gain. We then use a model-based approach to abstract a known part of the stabilized dynamics while using a data-driven method to account for the stochastic uncertainty. We formalize abstractions as Markov decision processes (MDPs) with intervals of transition probabilities. By stabilizing the dynamics, we can further constrain the control input modeled in the abstraction, which leads to smaller abstract models while retaining the correctness of controllers. Moreover, when the stabilizing feedback controller is aligned with the property of interest, then a good trade-off is achieved between the reduction in the abstraction size and the performance loss. The experiments show that our approach can reduce the size of the graph of abstractions by up to 90% with negligible performance loss.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9809a88032e97d462ae5655ba5e288a54c09cf12" target='_blank'>
              A Stability-Based Abstraction Framework for Reach-Avoid Control of Stochastic Dynamical Systems with Unknown Noise Distributions
              </a>
            </td>
          <td>
            Thom S. Badings, Licio Romao, Alessandro Abate, Nils Jansen
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Uncertainty quantification (UQ) in scientific machine learning (SciML) combines the powerful predictive power of SciML with methods for quantifying the reliability of the learned models. However, two major challenges remain: limited interpretability and expensive training procedures. We provide a new interpretation for UQ problems by establishing a new theoretical connection between some Bayesian inference problems arising in SciML and viscous Hamilton-Jacobi partial differential equations (HJ PDEs). Namely, we show that the posterior mean and covariance can be recovered from the spatial gradient and Hessian of the solution to a viscous HJ PDE. As a first exploration of this connection, we specialize to Bayesian inference problems with linear models, Gaussian likelihoods, and Gaussian priors. In this case, the associated viscous HJ PDEs can be solved using Riccati ODEs, and we develop a new Riccati-based methodology that provides computational advantages when continuously updating the model predictions. Specifically, our Riccati-based approach can efficiently add or remove data points to the training set invariant to the order of the data and continuously tune hyperparameters. Moreover, neither update requires retraining on or access to previously incorporated data. We provide several examples from SciML involving noisy data and \textit{epistemic uncertainty} to illustrate the potential advantages of our approach. In particular, this approach's amenability to data streaming applications demonstrates its potential for real-time inferences, which, in turn, allows for applications in which the predicted uncertainty is used to dynamically alter the learning process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4134838398ace7c1614eb021aca6dc028e137505" target='_blank'>
              Leveraging viscous Hamilton-Jacobi PDEs for uncertainty quantification in scientific machine learning
              </a>
            </td>
          <td>
            Zongren Zou, Tingwei Meng, Paula Chen, J. Darbon, G. Karniadakis
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>125</td>
        </tr>

        <tr id="Computer simulations of the reaction of neurons to electric stimulation can help to improve the understanding of the mechanisms behind deep brain stimulation. This is necessary to develop better treatments for patients who suffer from Parkinson’s disease, epilepsy, or other disorders. Since detailed and accurate computer simulations of neurons are computationally expensive, different methods are available to reduce this complexity. In this paper, we aim to reduce the computational complexity of a linear finite-element model of a neuron, which is placed atop a planar electrode, by applying three different model order reduction methods. Precisely, we use Krylov subspace-based model order reduction, proper orthogonal decomposition, and operator inference to obtain reduced models of different orders. Furthermore, we compare the quality of the obtained reduced-order models with the full-size finite-element model. Additionally, we compare the computational (CPU) time to construct the different reduced-order models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e9aeab7ff1de69e56e1ad557fc6e5f1251ecd69d" target='_blank'>
              Comparison of Model Order Reduction Methods for a Linear Finite Element Model of an Electrically Stimulated Neuron
              </a>
            </td>
          <td>
            Ulrike Fitzer, Pawan Goyal, Arwed Schuetz, I. Zawra, Dennis Hohlfeld, Tamara Bechtold
          </td>
          <td>2024-04-07</td>
          <td>2024 25th International Conference on Thermal, Mechanical and Multi-Physics Simulation and Experiments in Microelectronics and Microsystems (EuroSimE)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d40082e5be2a7157a0b7aec673fb42471fdc321a" target='_blank'>
              Capturing Shock Waves by Relaxation Neural Networks
              </a>
            </td>
          <td>
            Nan Zhou, Zheng Ma
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry. We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7dfed0d91681828f8554f5547f7cccf87a7151e7" target='_blank'>
              Designing Poisson Integrators Through Machine Learning
              </a>
            </td>
          <td>
            M. Vaquero, David Mart'in de Diego, Jorge Cort'es
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="State estimation of nonlinear dynamical systems has long aimed to balance accuracy, computational efficiency, robustness, and reliability. The rapid evolution of various industries has amplified the demand for estimation frameworks that satisfy all these factors. This study introduces a neuromorphic approach for robust filtering of nonlinear dynamical systems: SNN-EMSIF (spiking neural network-extended modified sliding innovation filter). SNN-EMSIF combines the computational efficiency and scalability of SNNs with the robustness of EMSIF, an estimation framework designed for nonlinear systems with zero-mean Gaussian noise. Notably, the weight matrices are designed according to the system model, eliminating the need for a learning process. The framework's efficacy is evaluated through comprehensive Monte Carlo simulations, comparing SNN-EMSIF with EKF and EMSIF. Additionally, it is compared with SNN-EKF in the presence of modeling uncertainties and neuron loss, using RMSEs as a metric. The results demonstrate the superior accuracy and robustness of SNN-EMSIF. Further analysis of runtimes and spiking patterns reveals an impressive reduction of 85% in emitted spikes compared to possible spikes, highlighting the computational efficiency of SNN-EMSIF. This framework offers a promising solution for robust estimation in nonlinear dynamical systems, opening new avenues for efficient and reliable estimation in various industries that can benefit from neuromorphic computing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4597b269ad69fee485fb527be1df63b665e3950d" target='_blank'>
              Neuromorphic Robust Estimation of Nonlinear Dynamical Systems Applied to Satellite Rendezvous
              </a>
            </td>
          <td>
            Reza Ahmadvand, S. S. Sharif, Y. Banad
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Solving high-dimensional dynamical systems in multi-query or real-time applications requires efficient surrogate modelling techniques, as e.g., achieved via model order reduction (MOR). If these systems are Hamiltonian systems their physical structure should be preserved during the reduction, which can be ensured by applying symplectic basis generation techniques such as the complex SVD (cSVD). Recently, randomized symplectic methods such as the randomized complex singular value decomposition (rcSVD) have been developed for a more efficient computation of symplectic bases that preserve the Hamiltonian structure during MOR. In the current paper, we present two error bounds for the rcSVD basis depending on the choice of hyperparameters and show that with a proper choice of hyperparameters, the projection error of rcSVD is at most a constant factor worse than the projection error of cSVD. We provide numerical experiments that demonstrate the efficiency of randomized symplectic basis generation and compare the bounds numerically.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/784185a6757f45f1fb6a75360f444570984fe1d6" target='_blank'>
              Error Analysis of Randomized Symplectic Model Order Reduction for Hamiltonian systems
              </a>
            </td>
          <td>
            Robin Herkert, Patrick Buchfink, B. Haasdonk, Johannes Rettberg, Jorg Fehr
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4302593601e57a1c4cb487d871723b9d11a8e2b2" target='_blank'>
              PID Control-Based Self-Healing to Improve the Robustness of Large Language Models
              </a>
            </td>
          <td>
            Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In this paper we consider adaptive deep neural network approximation for stochastic dynamical systems. Based on the Liouville equation associated with the stochastic dynamical systems, a new temporal KRnet (tKRnet) is proposed to approximate the probability density functions (PDFs) of the state variables. The tKRnet gives an explicit density model for the solution of the Liouville equation, which alleviates the curse of dimensionality issue that limits the application of traditional grid based numerical methods. To efficiently train the tKRnet, an adaptive procedure is developed to generate collocation points for the corresponding residual loss function, where samples are generated iteratively using the approximate density function at each iteration. A temporal decomposition technique is also employed to improve the long-time integration. Theoretical analysis of our proposed method is provided, and numerical examples are presented to demonstrate its performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/827723bda19209701daa5c4d36f6625034285087" target='_blank'>
              Adaptive deep density approximation for stochastic dynamical systems
              </a>
            </td>
          <td>
            Junjie He, Qifeng Liao, Xiaoliang Wan
          </td>
          <td>2024-05-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="A neural architecture with randomly initialized weights, in the infinite width limit, is equivalent to a Gaussian Random Field whose covariance function is the so-called Neural Network Gaussian Process kernel (NNGP). We prove that a reproducing kernel Hilbert space (RKHS) defined by the NNGP contains only functions that can be approximated by the architecture. To achieve a certain approximation error the required number of neurons in each layer is defined by the RKHS norm of the target function. Moreover, the approximation can be constructed from a supervised dataset by a random multi-layer representation of an input vector, together with training of the last layer's weights. For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in ${\mathbb R}^n$, we compare the number of neurons required by Barron's theorem and by the multi-layer features construction. We show that if eigenvalues of the integral operator of the NNGP decay slower than $k^{-n-\frac{2}{3}}$ where $k$ is an order of an eigenvalue, then our theorem guarantees a more succinct neural network approximation than Barron's theorem. We also make some computational experiments to verify our theoretical findings. Our experiments show that realistic neural networks easily learn target functions even when both theorems do not give any guarantees.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/adb6cbdad430505cc5ea30a38d475d45a308a489" target='_blank'>
              Multi-layer random features and the approximation power of neural networks
              </a>
            </td>
          <td>
            Rustem Takhanov
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Neural networks are playing a crucial role in everyday life, with the most modern generative models able to achieve impressive results. Nonetheless, their functioning is still not very clear, and several strategies have been adopted to study how and why these model reach their outputs. A common approach is to consider the data in an Euclidean settings: recent years has witnessed instead a shift from this paradigm, moving thus to more general framework, namely Riemannian Geometry. Two recent works introduced a geometric framework to study neural networks making use of singular Riemannian metrics. In this paper we extend these results to convolutional, residual and recursive neural networks, studying also the case of non-differentiable activation functions, such as ReLU. We illustrate our findings with some numerical experiments on classification of images and thermodynamic problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/880758e6bd878b8074fac2a2b2c0868c61d52a64" target='_blank'>
              A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes
              </a>
            </td>
          <td>
            A. Benfenati, A. Marta
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements. Their temporal evolution is often driven by transitions between a finite number of discrete states. Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner. AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1388550b7dbb3f7941311d9f11f9bec99880c0bf" target='_blank'>
              Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction
              </a>
            </td>
          <td>
            Beomseok Kang, H. Kumar, Minah Lee, Biswadeep Chakraborty, Saibal Mukhopadhyay
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>7</td>
        </tr>

        <tr id="Separating relevant and irrelevant information is key to any modeling process or scientific inquiry. Theoretical physics offers a powerful tool for achieving this in the form of the renormalization group (RG). Here we demonstrate a practical approach to performing Wilsonian RG in the context of Gaussian Process (GP) Regression. We systematically integrate out the unlearnable modes of the GP kernel, thereby obtaining an RG flow of the Gaussian Process in which the data plays the role of the energy scale. In simple cases, this results in a universal flow of the ridge parameter, which becomes input-dependent in the richer scenario in which non-Gaussianities are included. In addition to being analytically tractable, this approach goes beyond structural analogies between RG and neural networks by providing a natural connection between RG flow and learnable vs. unlearnable modes. Studying such flows may improve our understanding of feature learning in deep neural networks, and identify potential universality classes in these models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ffb0c50aedbc2b6f786ab5dbdc2e8f25674ee4d" target='_blank'>
              Wilsonian Renormalization of Neural Network Gaussian Processes
              </a>
            </td>
          <td>
            Jessica N. Howard, Ro Jefferson, Anindita Maiti, Z. Ringel
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Whilst the Universal Approximation Theorem guarantees the existence of approximations to Sobolev functions -- the natural function spaces for PDEs -- by Neural Networks (NNs) of sufficient size, low-regularity solutions may lead to poor approximations in practice. For example, classical fully-connected feed-forward NNs fail to approximate continuous functions whose gradient is discontinuous when employing strong formulations like in Physics Informed Neural Networks (PINNs). In this article, we propose the use of regularity-conforming neural networks, where a priori information on the regularity of solutions to PDEs can be employed to construct proper architectures. We illustrate the potential of such architectures via a two-dimensional (2D) transmission problem, where the solution may admit discontinuities in the gradient across interfaces, as well as power-like singularities at certain points. In particular, we formulate the weak transmission problem in a PINNs-like strong formulation with interface and continuity conditions. Such architectures are partially explainable; discontinuities are explicitly described, allowing the introduction of novel terms into the loss function. We demonstrate via several model problems in one and two dimensions the advantages of using regularity-conforming architectures in contrast to classical architectures. The ideas presented in this article easily extend to problems in higher dimensions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9722743a510621c20dcb04ae20c71976a40bb8e4" target='_blank'>
              Regularity-Conforming Neural Networks (ReCoNNs) for solving Partial Differential Equations
              </a>
            </td>
          <td>
            Jamie M. Taylor, David Pardo, J. Muñoz‐Matute
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Solving partial differential equations (PDEs) on fine spatio-temporal scales for high-fidelity solutions is critical for numerous scientific breakthroughs. Yet, this process can be prohibitively expensive, owing to the inherent complexities of the problems, including nonlinearity and multiscale phenomena. To speed up large-scale computations, a process known as downscaling is employed, which generates high-fidelity approximate solutions from their low-fidelity counterparts. In this paper, we propose a novel Physics-Guided Diffusion Model (PGDM) for downscaling. Our model, initially trained on a dataset comprising low-and-high-fidelity paired solutions across coarse and fine scales, generates new high-fidelity approximations from any new low-fidelity inputs. These outputs are subsequently refined through fine-tuning, aimed at minimizing the physical discrepancies as defined by the discretized PDEs at the finer scale. We evaluate and benchmark our model's performance against other downscaling baselines in three categories of nonlinear PDEs. Our numerical experiments demonstrate that our model not only outperforms the baselines but also achieves a computational acceleration exceeding tenfold, while maintaining the same level of accuracy as the conventional fine-scale solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3d0b0cd794ab2c1acb7acc00ad5b99cfb5de7ab9" target='_blank'>
              Generative downscaling of PDE solvers with physics-guided diffusion models
              </a>
            </td>
          <td>
            Yulong Lu, Wuzhe Xu
          </td>
          <td>2024-04-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2ebcd3b939bc9932486a07358e9a732083b19f0" target='_blank'>
              Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes
              </a>
            </td>
          <td>
            Paul G. Beckman, Christopher J. Geoga
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc227c83d593a317b47926de7d4a6905d2fc78a4" target='_blank'>
              Learning Deep Dynamical Systems using Stable Neural ODEs
              </a>
            </td>
          <td>
            Andreas Sochopoulos, M. Gienger, S. Vijayakumar
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>46</td>
        </tr>

        <tr id="Besides classical feed-forward neural networks, also neural ordinary differential equations (neural ODEs) gained particular interest in recent years. Neural ODEs can be interpreted as an infinite depth limit of feed-forward or residual neural networks. We study the input-output dynamics of finite and infinite depth neural networks with scalar output. In the finite depth case, the input is a state associated to a finite number of nodes, which maps under multiple non-linear transformations to the state of one output node. In analogy, a neural ODE maps a linear transformation of the input to a linear transformation of its time-$T$ map. We show that depending on the specific structure of the network, the input-output map has different properties regarding the existence and regularity of critical points. These properties can be characterized via Morse functions, which are scalar functions, where every critical point is non-degenerate. We prove that critical points cannot exist, if the dimension of the hidden layer is monotonically decreasing or the dimension of the phase space is smaller or equal to the input dimension. In the case that critical points exist, we classify their regularity depending on the specific architecture of the network. We show that each critical point is non-degenerate, if for finite depth neural networks the underlying graph has no bottleneck, and if for neural ODEs, the linear transformations used have full rank. For each type of architecture, the proven properties are comparable in the finite and in the infinite depth case. The established theorems allow us to formulate results on universal embedding, i.e.\ on the exact representation of maps by neural networks and neural ODEs. Our dynamical systems viewpoint on the geometric structure of the input-output map provides a fundamental understanding, why certain architectures perform better than others.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/94c846422a6136b9971fe1195656c66dd1af58fe" target='_blank'>
              Analysis of the Geometric Structure of Neural Networks and Neural ODEs via Morse Functions
              </a>
            </td>
          <td>
            Christian Kuehn, Sara-Viola Kuntz
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The focus of this paper is on linear system identification in the setting where it is known that the underlying partially-observed linear dynamical system lies within a finite collection of known candidate models. We first consider the problem of identification from a given trajectory, which in this setting reduces to identifying the index of the true model with high probability. We characterize the finite-time sample complexity of this problem by leveraging recent advances in the non-asymptotic analysis of linear least-square methods in the literature. In comparison to the earlier results that assume no prior knowledge of the system, our approach takes advantage of the smaller hypothesis class and leads to the design of a learner with a dimension-free sample complexity bound. Next, we consider the switching control of linear systems, where there is a candidate controller for each of the candidate models and data is collected through interaction of the system with a collection of potentially destabilizing controllers. We develop a dimension-dependent criterion that can detect those destabilizing controllers in finite time. By leveraging these results, we propose a data-driven switching strategy that identifies the unknown parameters of the underlying system. We then provide a non-asymptotic analysis of its performance and discuss its implications on the classical method of estimator-based supervisory control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb666fc9b65336bceb9666e5c2413016ca5c9d05" target='_blank'>
              A least-square method for non-asymptotic identification in linear switching control
              </a>
            </td>
          <td>
            Haoyuan Sun, A. Jadbabaie
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>64</td>
        </tr>

        <tr id="Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/14fdab35cc6288083a38a92392af3f1da0b95a90" target='_blank'>
              KAN: Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljavci'c, Thomas Y. Hou, Max Tegmark
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>13</td>
          <td>1</td>
        </tr>

        <tr id="This paper proposes a novel approach to improve the performance of distributed nonlinear control systems while preserving stability by leveraging Deep Neural Networks (DNNs). We build upon the Neural System Level Synthesis (Neur-SLS) framework and introduce a method to parameterize stabilizing control policies that are distributed across a network topology. A distinctive feature is that we iteratively minimize an arbitrary control cost function through an unconstrained optimization algorithm, all while preserving the stability of the overall network architecture by design. This is achieved through two key steps. First, we establish a method to parameterize interconnected Recurrent Equilibrium Networks (RENs) that guarantees a bounded $\mathcal{L}_2$ gain at the network level. This ensures stability. Second, we demonstrate how the information flow within the network is preserved, enabling a fully distributed implementation where each subsystem only communicates with its neighbors. To showcase the effectiveness of our approach, we present a simulation of a distributed formation control problem for a fleet of vehicles. The simulation demonstrates how the proposed neural controller enables the vehicles to maintain a desired formation while navigating obstacles and avoiding collisions, all while guaranteeing network stability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1545827d23594bf547bd8dd7f92215333604d340" target='_blank'>
              Optimal distributed control with stability guarantees by training a network of neural closed-loop maps
              </a>
            </td>
          <td>
            Danilo Saccani, L. Massai, Luca Furieri, Giancarlo Ferrari-Trecate
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>12</td>
        </tr>

        <tr id="When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/de88da10c9f10d06296454a012c45ac172bb6fe7" target='_blank'>
              A real-time digital twin of azimuthal thermoacoustic instabilities
              </a>
            </td>
          <td>
            Andrea N'ovoa, N. Noiray, James R. Dawson, Luca Magri
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Physical models often contain unknown functions and relations. The goal of our work is to answer the question of how one should excite or control a system under consideration in an appropriate way to be able to reconstruct an unknown nonlinear relation. To answer this question, we propose a greedy reconstruction algorithm within an offline-online strategy. We apply this strategy to a two-dimensional semilinear elliptic model. Our identification is based on the application of several space-dependent excitations (also called controls). These specific controls are designed by the algorithm in order to obtain a deeper insight into the underlying physical problem and a more precise reconstruction of the unknown relation. We perform numerical simulations that demonstrate the effectiveness of our approach which is not limited to the current type of equation. Since our algorithm provides not only a way to determine unknown operators by existing data but also protocols for new experiments, it is a holistic concept to tackle the problem of improving physical models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/444637f77c6bb46c9277165f2c7f0b97cf9c43d0" target='_blank'>
              Reconstruction of unknown nonlinear operators in semilinear elliptic models using optimal inputs
              </a>
            </td>
          <td>
            Jan Bartsch, S. Buchwald, G. Ciaramella, Stefan Volkwein
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems. Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role. Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods. However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms. We propose a general framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices. Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability. Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms. The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter. Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a17bd07c7d0086b89607f16256d71eb9567787cf" target='_blank'>
              A general error analysis for randomized low-rank approximation with application to data assimilation
              </a>
            </td>
          <td>
            Alexandre Scotto Di Perrotolo, Youssef Diouane, S. Gurol, Xavier Vasseur
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Physics-guided neural networks (PGNN) is an effective tool that combines the benefits of data-driven modeling with the interpretability and generalization of underlying physical information. However, for a classical PGNN, the penalization of the physics-guided part is at the output level, which leads to a conservative result as systems with highly similar state-transition functions, i.e. only slight differences in parameters, can have significantly different time-series outputs. Furthermore, the classical PGNN cost function regularizes the model estimate over the entire state space with a constant trade-off hyperparameter. In this paper, we introduce a novel model augmentation strategy for nonlinear state-space model identification based on PGNN, using a weighted function regularization (W-PGNN). The proposed approach can efficiently augment the prior physics-based state-space models based on measurement data. A new weighted regularization term is added to the cost function to penalize the difference between the state and output function of the baseline physics-based and final identified model. This ensures the estimated model follows the baseline physics model functions in regions where the data has low information content, while placing greater trust in the data when a high informativity is present. The effectiveness of the proposed strategy over the current PGNN method is demonstrated on a benchmark example.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9732330db3e9f3fa89caefb8ac538d9f0a8807e6" target='_blank'>
              Physics-Guided State-Space Model Augmentation Using Weighted Regularized Neural Networks
              </a>
            </td>
          <td>
            Yuhan Liu, Roland T'oth, M. Schoukens
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="For complex nonlinear systems, it is challenging to design algorithms that are fast, scalable, and give an accurate approximation of the stability region. This paper proposes a sampling-based approach to address these challenges. By extending the parametrization of quadratic Lyapunov functions with the system dynamics and formulating an $\ell_1$ optimization to maximize the invariant set over a grid of the state space, we arrive at a computationally efficient algorithm that estimates the domain of attraction (DOA) of nonlinear systems accurately by using only linear programming. The scalability of the Lyapunov function synthesis is further improved by combining the algorithm with ADMM-based parallelization. To resolve the inherent approximative nature of grid-based techniques, a small-scale nonlinear optimization is proposed. The performance of the algorithm is evaluated and compared to state-of-the-art solutions on several numerical examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0acaee5189452cb7a7c65946c5c7af1ad55613f4" target='_blank'>
              Computationally Efficient Sampling-Based Algorithm for Stability Analysis of Nonlinear Systems
              </a>
            </td>
          <td>
            P'eter Antal, , Roland T'oth
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC). Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear. We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework. We demonstrate this idea by examining the connection between $\gamma$-DDPC and the original DeePC formulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0746be015cf60990a7af8b7daa792611fc9aef35" target='_blank'>
              Towards a unifying framework for data-driven predictive control with quadratic regularization
              </a>
            </td>
          <td>
            Manuel Klädtke, M. S. Darup
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as different extremal cases of this general setup: dynamic programming, Monte Carlo methods, temporal difference learning, and deep RL. We see this as strong evidence that this approach is a natural one and believe that it will be a fruitful way to think about RL in the future.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b43e486e12f2687a86042a0bc22e1ba9a31dce5c" target='_blank'>
              Reinforcement Learning in Categorical Cybernetics
              </a>
            </td>
          <td>
            Jules Hedges, Riu Rodr'iguez Sakamoto
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In order to stabilize nonlinear systems modeled by stochastic differential equations, we design a Fast Exponentially Stable and Safe Neural Controller (FESSNC) for fast learning controllers. Our framework is parameterized by neural networks, and realizing both rigorous exponential stability and safety guarantees. Concretely, we design heuristic methods to learn the exponentially stable and the safe controllers, respectively, in light of the classic stochastic exponential stability theory and our established theorem on guaranteeing the almost-sure safety for stochastic dynamics. More significantly, to rigorously ensure the stability and the safety guarantees for the learned controllers, we develop a projection operator, projecting to the space of exponentially-stable and safe controllers. To reduce the high computation cost of solving the projection operation, approximate projection operators are delicately proposed with closed forms that map the learned controllers to the target controller space. Furthermore, we employ Hutchinson's trace estimator for a scalable unbiased estimate of the Hessian matrix that is used in the projection operator, which thus allows for computation cost reduction and therefore can accelerate the training and testing processes. More importantly, our approximate projection operations can be applied to the nonparametric control methods to improve their stability and safety performance. We empirically demonstrate the superiority of the FESSNC over the existing methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0914d87f5b5944f6cfbe07cbf4b4dddbce06f34d" target='_blank'>
              FESSNC: Fast Exponentially Stable and Safe Neural Controller
              </a>
            </td>
          <td>
            Jingdong Zhang, Luan Yang, Qunxi Zhu, Wei Lin
          </td>
          <td>2024-05-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Model-based reinforcement learning is an effective approach for controlling an unknown system. It is based on a longstanding pipeline familiar to the control community in which one performs experiments on the environment to collect a dataset, uses the resulting dataset to identify a model of the system, and finally performs control synthesis using the identified model. As interacting with the system may be costly and time consuming, targeted exploration is crucial for developing an effective control-oriented model with minimal experimentation. Motivated by this challenge, recent work has begun to study finite sample data requirements and sample efficient algorithms for the problem of optimal exploration in model-based reinforcement learning. However, existing theory and algorithms are limited to model classes which are linear in the parameters. Our work instead focuses on models with nonlinear parameter dependencies, and presents the first finite sample analysis of an active learning algorithm suitable for a general class of nonlinear dynamics. In certain settings, the excess control cost of our algorithm achieves the optimal rate, up to logarithmic factors. We validate our approach in simulation, showcasing the advantage of active, control-oriented exploration for controlling nonlinear systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7cf97c150f8a0cc3bcad648019e5d858ef78b7ef" target='_blank'>
              Active Learning for Control-Oriented Identification of Nonlinear Systems
              </a>
            </td>
          <td>
            Bruce D. Lee, Ingvar M. Ziemann, George J. Pappas, Nikolai Matni
          </td>
          <td>2024-04-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/29344aff1b6d6c53f66fc2ce3bc256a6dfd6fe51" target='_blank'>
              Optimal Parameter Estimation Techniques for Complex Nonlinear Systems
              </a>
            </td>
          <td>
            K. Kumar, Ekaterina Kostina
          </td>
          <td>2024-04-03</td>
          <td>Differential Equations and Dynamical Systems</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Widespread deployment of societal-scale machine learning systems necessitates a thorough understanding of the resulting long-term effects these systems have on their environment, including loss of trustworthiness, bias amplification, and violation of AI safety requirements. We introduce a repeated learning process to jointly describe several phenomena attributed to unintended hidden feedback loops, such as error amplification, induced concept drift, echo chambers and others. The process comprises the entire cycle of obtaining the data, training the predictive model, and delivering predictions to end-users within a single mathematical model. A distinctive feature of such repeated learning setting is that the state of the environment becomes causally dependent on the learner itself over time, thus violating the usual assumptions about the data distribution. We present a novel dynamical systems model of the repeated learning process and prove the limiting set of probability distributions for positive and negative feedback loop modes of the system operation. We conduct a series of computational experiments using an exemplary supervised learning problem on two synthetic data sets. The results of the experiments correspond to the theoretical predictions derived from the dynamical model. Our results demonstrate the feasibility of the proposed approach for studying the repeated learning processes in machine learning systems and open a range of opportunities for further research in the area.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bbdf526001c151c67507de963a0c683064b1f630" target='_blank'>
              A Mathematical Model of the Hidden Feedback Loop Effect in Machine Learning Systems
              </a>
            </td>
          <td>
            Andrey Veprikov, Alexander Afanasiev, Anton Khritankov
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems. At their core, they fulfill an energy balance relating stored, dissipated and supplied energy. To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation. The evaluation of the error estimator includes the computation of adjoint sensitivities. To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable. Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators. We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c957a644fac465407c4f14c351602c1ac548fdea" target='_blank'>
              Goal-oriented time adaptivity for port-Hamiltonian systems
              </a>
            </td>
          <td>
            Andreas Bartel, Manuel Schaller
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Various iterative eigenvalue solvers have been developed to compute parts of the spectrum for a large sparse matrix, including the power method, Krylov subspace methods, contour integral methods, and preconditioned solvers such as the so called LOBPCG method. All of these solvers rely on random matrices to determine, e.g., starting vectors that have, with high probability, a non-negligible overlap with the eigenvectors of interest. For this purpose, a safe and common choice are unstructured Gaussian random matrices. In this work, we investigate the use of random Khatri-Rao products in eigenvalue solvers. On the one hand, we establish a novel subspace embedding property that provides theoretical justification for the use of such structured random matrices. On the other hand, we highlight the potential algorithmic benefits when solving eigenvalue problems with Kronecker product structure, as they arise frequently from the discretization of eigenvalue problems for differential operators on tensor product domains. In particular, we consider the use of random Khatri-Rao products within a contour integral method and LOBPCG. Numerical experiments indicate that the gains for the contour integral method strongly depend on the ability to efficiently and accurately solve (shifted) matrix equations with low-rank right-hand side. The flexibility of LOBPCG to directly employ preconditioners makes it easier to benefit from Khatri-Rao product structure, at the expense of having less theoretical justification.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ee5c4c92e202d0b95268b77d6ca13dea320a5067" target='_blank'>
              Subspace embedding with random Khatri-Rao products and its application to eigensolvers
              </a>
            </td>
          <td>
            Zvonimir Bujanovi'c, Luka Grubivsi'c, Daniel Kressner, Hei Yin Lam
          </td>
          <td>2024-05-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In this work, we propose a novel backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations (BSDEs), where the deep neural network (DNN) models are trained not only on the inputs and labels but also the differentials of the corresponding labels. This is motivated by the fact that differential deep learning can provide an efficient approximation of the labels and their derivatives with respect to inputs. The BSDEs are reformulated as differential deep learning problems by using Malliavin calculus. The Malliavin derivatives of solution to a BSDE satisfy themselves another BSDE, resulting thus in a system of BSDEs. Such formulation requires the estimation of the solution, its gradient, and the Hessian matrix, represented by the triple of processes $\left(Y, Z, \Gamma\right).$ All the integrals within this system are discretized by using the Euler-Maruyama method. Subsequently, DNNs are employed to approximate the triple of these unknown processes. The DNN parameters are backwardly optimized at each time step by minimizing a differential learning type loss function, which is defined as a weighted sum of the dynamics of the discretized BSDE system, with the first term providing the dynamics of the process $Y$ and the other the process $Z$. An error analysis is carried out to show the convergence of the proposed algorithm. Various numerical experiments up to $50$ dimensions are provided to demonstrate the high efficiency. Both theoretically and numerically, it is demonstrated that our proposed scheme is more efficient compared to other contemporary deep learning-based methodologies, especially in the computation of the process $\Gamma$.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba20a6b309cbdd2ed5e0fd6cd389b9ae778ef03e" target='_blank'>
              A backward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations
              </a>
            </td>
          <td>
            Lorenc Kapllani, Long Teng
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The field of neuroscience and the development of artificial neural networks (ANNs) have mutually influenced each other, drawing from and contributing to many concepts initially developed in statistical mechanics. Notably, Hopfield networks and Boltzmann machines are versions of the Ising model, a model extensively studied in statistical mechanics for over a century. In the first part of this chapter, we provide an overview of the principles, models, and applications of ANNs, highlighting their connections to statistical mechanics and statistical learning theory. Artificial neural networks can be seen as high-dimensional mathematical functions, and understanding the geometric properties of their loss landscapes (i.e., the high-dimensional space on which one wishes to find extrema or saddles) can provide valuable insights into their optimization behavior, generalization abilities, and overall performance. Visualizing these functions can help us design better optimization methods and improve their generalization abilities. Thus, the second part of this chapter focuses on quantifying geometric properties and visualizing loss functions associated with deep ANNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/31fb5037b5fb3b6980ed528d67daf9ca9ae3513b" target='_blank'>
              Statistical Mechanics and Artificial Neural Networks: Principles, Models, and Applications
              </a>
            </td>
          <td>
            Lucas Bottcher, Gregory R. Wheeler
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper details how the Bayesian-network structure of the posterior distribution of state-space models can be exploited to build improved parameterizations for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution-smoother; each resulting in a different parameter estimation method. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization methods, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using and embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and be work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b901b2ba77f44c7b2e434be5f4559d0d932e3815" target='_blank'>
              Bayesian Networks for Variational System Identification
              </a>
            </td>
          <td>
            Dimas Abreu Archanjo Dutra
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov. Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality. It may be avoided by following a hierarchic probability flow from coarse to fine scales. This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis. For a $\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition. An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales. We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies. They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales. We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d65e2612cc3968688394a7660934b8780d0f7e26" target='_blank'>
              Hierarchic Flows to Estimate and Sample High-dimensional Probabilities
              </a>
            </td>
          <td>
            Etienne Lempereur, Stéphane Mallat
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks. The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions. The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions. We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks. One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation. To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy. We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/669fdb54ae870354c2070ef0ca8c6b182864e6a3" target='_blank'>
              Tensor neural networks for high-dimensional Fokker-Planck equations
              </a>
            </td>
          <td>
            Taorui Wang, Zheyuan Hu, Kenji Kawaguchi, Zhongqiang Zhang, G. Karniadakis
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>125</td>
        </tr>

        <tr id="The ability to automatically discover interpretable mathematical models from data could forever change how we model soft matter systems. For convex discovery problems with a unique global minimum, model discovery is well-established. It uses a classical top-down approach that first calculates a dense parameter vector, and then sparsifies the vector by gradually removing terms. For non-convex discovery problems with multiple local minima, this strategy is infeasible since the initial parameter vector is generally non-unique. Here we propose a novel bottom-up approach that starts with a sparse single-term vector, and then densifies the vector by systematically adding terms. Along the way, we discover models of gradually increasing complexity, a strategy that we call best-in-class modeling. To identify successful candidate terms, we reverse-engineer a library of sixteen functional building blocks that integrate a century of knowledge in material modeling with recent trends in machine learning and artificial intelligence. Yet, instead of solving the discrete combinatorial problem with 65,536 possible combinations of terms, best-in-class modeling starts with the best one-term model and iteratively repeats adding terms, until the objective function meets a user-defined convergence criterion. Strikingly, we achieve good convergence with only one or two terms. We illustrate the best-in-class one- and two-term models for a variety of soft matter systems including rubber, brain, artificial meat, skin, and arteries. Our discovered models display distinct and unexpected features for each family of materials, and suggest that best-in-class modeling is an efficient, robust, and easy-to-use strategy to discover the mechanical signatures of traditional and unconventional soft materials. We anticipate that our technology will generalize naturally to other classes of natural and man-made soft matter.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/200655dc975e870e69368b9f30fdb78bdcd351ea" target='_blank'>
              Best-in-class modeling: A novel strategy to discover constitutive models for soft matter systems
              </a>
            </td>
          <td>
            K. Linka, E. Kuhl
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="We develop a theory characterizing the fundamental capability of deep neural networks to learn, from evolution traces, the logical rules governing the behavior of cellular automata (CA). This is accomplished by first establishing a novel connection between CA and Lukasiewicz propositional logic. While binary CA have been known for decades to essentially perform operations in Boolean logic, no such relationship exists for general CA. We demonstrate that many-valued (MV) logic, specifically Lukasiewicz propositional logic, constitutes a suitable language for characterizing general CA as logical machines. This is done by interpolating CA transition functions to continuous piecewise linear functions, which, by virtue of the McNaughton theorem, yield formulae in MV logic characterizing the CA. Recognizing that deep rectified linear unit (ReLU) networks realize continuous piecewise linear functions, it follows that these formulae are naturally extracted from CA evolution traces by deep ReLU networks. A corresponding algorithm together with a software implementation is provided. Finally, we show that the dynamical behavior of CA can be realized by recurrent neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/929d08b40629ad84581461e76eb90880f77fea53" target='_blank'>
              Cellular automata, many-valued logic, and deep neural networks
              </a>
            </td>
          <td>
            Yani Zhang, Helmut Bölcskei
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many supervised learning tasks have intrinsic symmetries, such as translational and rotational symmetry in image classifications. These symmetries can be exploited to enhance performance. We formulate the symmetry constraints into a concise mathematical form. We design two ways to adopt the constraints into the cost function, thereby shaping the cost landscape in favour of parameter choices which respect the given symmetry. Unlike methods that alter the neural network circuit ansatz to impose symmetry, our method only changes the classical post-processing of gradient descent, which is simpler to implement. We call the method symmetry-guided gradient descent (SGGD). We illustrate SGGD in entanglement classification of Werner states and in a binary classification task in a 2-D feature space. In both cases, the results show that SGGD can accelerate the training, improve the generalization ability, and remove vanishing gradients, especially when the training data is biased.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e7d7dd7ced77cb9c35f85cea021da793348da645" target='_blank'>
              Symmetry-guided gradient descent for quantum neural networks
              </a>
            </td>
          <td>
            Ka Bian, Shitao Zhang, Fei Meng, Wen Zhang, Oscar Dahlsten
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets. These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups. Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory. They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries. In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups). Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning. We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles. The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6140d9bf7e31537e13dcf3cf1b9d37e9b71d1eec" target='_blank'>
              Kuramoto Oscillators and Swarms on Manifolds for Geometry Informed Machine Learning
              </a>
            </td>
          <td>
            Vladimir Jacimovic
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present a new class of equivariant neural networks, hereby dubbed Lattice-Equivariant Neural Networks (LENNs), designed to satisfy local symmetries of a lattice structure. Our approach develops within a recently introduced framework aimed at learning neural network-based surrogate models Lattice Boltzmann collision operators. Whenever neural networks are employed to model physical systems, respecting symmetries and equivariance properties has been shown to be key for accuracy, numerical stability, and performance. Here, hinging on ideas from group representation theory, we define trainable layers whose algebraic structure is equivariant with respect to the symmetries of the lattice cell. Our method naturally allows for efficient implementations, both in terms of memory usage and computational costs, supporting scalable training/testing for lattices in two spatial dimensions and higher, as the size of symmetry group grows. We validate and test our approach considering 2D and 3D flowing dynamics, both in laminar and turbulent regimes. We compare with group averaged-based symmetric networks and with plain, non-symmetric, networks, showing how our approach unlocks the (a-posteriori) accuracy and training stability of the former models, and the train/inference speed of the latter networks (LENNs are about one order of magnitude faster than group-averaged networks in 3D). Our work opens towards practical utilization of machine learning-augmented Lattice Boltzmann CFD in real-world simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fb4a930daf4488b3ac608656d8a8e2871211a97" target='_blank'>
              Enhancing lattice kinetic schemes for fluid dynamics with Lattice-Equivariant Neural Networks
              </a>
            </td>
          <td>
            Giulio Ortali, Alessandro Gabbana, Imre Atmodimedjo, Alessandro Corbetta
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Signature kernels are at the core of several machine learning algorithms for analysing multivariate time series. The kernel of two bounded variation paths (such as piecewise linear interpolations of time series data) is typically computed by solving a Goursat problem for a hyperbolic partial differential equation (PDE) in two independent time variables. However, this approach becomes considerably less practical for highly oscillatory input paths, as they have to be resolved at a fine enough scale to accurately recover their signature kernel, resulting in significant time and memory complexities. To mitigate this issue, we first show that the signature kernel of a broader class of paths, known as \emph{smooth rough paths}, also satisfies a PDE, albeit in the form of a system of coupled equations. We then use this result to introduce new algorithms for the numerical approximation of signature kernels. As bounded variation paths (and more generally geometric $p$-rough paths) can be approximated by piecewise smooth rough paths, one can replace the PDE with rapidly varying coefficients in the original Goursat problem by an explicit system of coupled equations with piecewise constant coefficients derived from the first few iterated integrals of the original input paths. While this approach requires solving more equations, they do not require looking back at the complex and fine structure of the initial paths, which significantly reduces the computational complexity associated with the analysis of highly oscillatory time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d466579deec9a59d524ac0981d5812dda5d235c7" target='_blank'>
              A High Order Solver for Signature Kernels
              </a>
            </td>
          <td>
            M. Lemercier, Terry Lyons
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>9</td>
        </tr>

        <tr id="A large part of modern research, especially in the broad field of complex systems, relies on the numerical integration of PDEs, with and without stochastic noise. This is usually done with eiher in- house made codes or external packages like MATLAB, Mathematica, Fenicsx, OpenFOAM, Dedalus, and others. These packages rarely offer a good combination of speed, generality, and the option to easily add stochasticity to the system, while in-house codes depend on certain expertise to obtain good performance, and are usually written for each specific use case, sacrificing modularity and reusability. This paper introduces a package written in CUDA C++, thus enabling by default gpu acceleration, that performs pseudo-spectral integration of generic stochastic PDEs in flat lattices in one, two and three dimensions. This manuscript describes how the basic functionality of cuPSS, with an example and benchmarking, showing that cuPSS offers a considerable improvement in speed over other popular finite-difference and spectral solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe3d36e5d7d4496386dd2bf557c35ebbea744e9f" target='_blank'>
              cuPSS: a package for pseudo-spectral integration of stochastic PDEs
              </a>
            </td>
          <td>
            Fernando Caballero
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We discuss Hamiltonian and Liouvillian learning for analog quantum simulation from non-equilibrium quench dynamics in the limit of weakly dissipative many-body systems. We present various strategies to learn the operator content of the Hamiltonian and the Lindblad operators of the Liouvillian. We compare different ans\"atze based on an experimentally accessible"learning error"which we consider as a function of the number of runs of the experiment. Initially, the learning error decreasing with the inverse square root of the number of runs, as the error in the reconstructed parameters is dominated by shot noise. Eventually the learning error remains constant, allowing us to recognize missing ansatz terms. A central aspect of our approach is to (re-)parametrize ans\"atze by introducing and varying the dependencies between parameters. This allows us to identify the relevant parameters of the system, thereby reducing the complexity of the learning task. Importantly, this (re-)parametrization relies solely on classical post-processing, which is compelling given the finite amount of data available from experiments. A distinguishing feature of our approach is the possibility to learn the Hamiltonian, without the necessity of learning the complete Liouvillian, thus further reducing the complexity of the learning task. We illustrate our method with two, experimentally relevant, spin models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/690545a3d51683d051059ba60a9d51c6979db019" target='_blank'>
              Hamiltonian and Liouvillian learning in weakly-dissipative quantum many-body systems
              </a>
            </td>
          <td>
            Tobias Olsacher, Tristan Kraft, C. Kokail, Barbara Kraus, Peter Zoller
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In this work, a hybrid physics-based data-driven surrogate model for the microscale analysis of heterogeneous material is investigated. The proposed model benefits from the physics-based knowledge contained in the constitutive models used in the full-order micromodel by embedding them in a neural network. Following previous developments, this paper extends the applicability of the physically recurrent neural network (PRNN) by introducing an architecture suitable for rate-dependent materials in a finite strain framework. In this model, the homogenized deformation gradient of the micromodel is encoded into a set of deformation gradients serving as input to the embedded constitutive models. These constitutive models compute stresses, which are combined in a decoder to predict the homogenized stress, such that the internal variables of the history-dependent constitutive models naturally provide physics-based memory for the network. To demonstrate the capabilities of the surrogate model, we consider a unidirectional composite micromodel with transversely isotropic elastic fibers and elasto-viscoplastic matrix material. The extrapolation properties of the surrogate model trained to replace such micromodel are tested on loading scenarios unseen during training, ranging from different strain-rates to cyclic loading and relaxation. Speed-ups of three orders of magnitude with respect to the runtime of the original micromodel are obtained.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5da7704174217b71ac7a0a58213335e136ca339d" target='_blank'>
              Physically recurrent neural network for rate and path-dependent heterogeneous materials in a finite strain framework
              </a>
            </td>
          <td>
            M. A. Maia, I. Rocha, D. Kovavcevi'c, F. P. Meer
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="In this article, we consider the correction of metric matrices in quasi-Newton methods (QNM) from the perspective of machine learning theory. Based on training information for estimating the matrix of the second derivatives of a function, we formulate a quality functional and minimize it by using gradient machine learning algorithms. We demonstrate that this approach leads us to the well-known ways of updating metric matrices used in QNM. The learning algorithm for finding metric matrices performs minimization along a system of directions, the orthogonality of which determines the convergence rate of the learning process. The degree of learning vectors’ orthogonality can be increased both by choosing a QNM and by using additional orthogonalization methods. It has been shown theoretically that the orthogonality degree of learning vectors in the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is higher than in the Davidon–Fletcher–Powell (DFP) method, which determines the advantage of the BFGS method. In our paper, we discuss some orthogonalization techniques. One of them is to include iterations with orthogonalization or an exact one-dimensional descent. As a result, it is theoretically possible to detect the cumulative effect of reducing the optimization space on quadratic functions. Another way to increase the orthogonality degree of learning vectors at the initial stages of the QNM is a special choice of initial metric matrices. Our computational experiments on problems with a high degree of conditionality have confirmed the stated theoretical assumptions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d1a6267efb1747cb7eb0d9964e4bb33c18b17735" target='_blank'>
              Machine Learning in Quasi-Newton Methods
              </a>
            </td>
          <td>
            Vladimit N. Krutikov, Elena Tovbis, P. Stanimirović, L. Kazakovtsev, Darjan Karabašević
          </td>
          <td>2024-04-05</td>
          <td>Axioms</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We present an algorithm for constructing efficient surrogate frequency‐domain models of (nonlinear) parametric dynamical systems in a non‐intrusive way. To capture the dependence of the underlying system on frequency and parameters, our proposed approach combines rational approximation and smooth interpolation. In the approximation effort, locally adaptive sparse grids are applied to effectively explore the parameter domain even if the number of parameters is modest or high. Adaptivity is also employed to build rational approximations that efficiently capture the frequency dependence of the problem. These two features enable our method to build surrogate models that achieve a user‐prescribed approximation accuracy, without wasting resources in “oversampling” the frequency and parameter domains. Thanks to its non‐intrusiveness, our proposed method, as opposed to projection‐based techniques for model order reduction, can be applied regardless of the complexity of the underlying physical model. Notably, our algorithm for adaptive sampling can be used even when prior knowledge of the problem structure is not available. To showcase the effectiveness of our approach, we apply it in the study of an aerodynamic bearing. Our method allows us to build surrogate models that adequately identify the bearing's behavior with respect to both design and operational parameters, while still achieving significant speedups.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/071dd64f8fb372d9b51bd3b285c8518554a143a0" target='_blank'>
              Plug‐and‐play adaptive surrogate modeling of parametric nonlinear dynamics in frequency domain
              </a>
            </td>
          <td>
            Phillip Huwiler, Davide Pradovera, Jürg Schiffmann
          </td>
          <td>2024-04-16</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations. In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time. We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures. The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure. We conduct a numerical analysis of this approximation and investigate optimal parameter choices. Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy. The code and all trained networks are provided on GitHub https://github.com/ScSteffen/neuralEntropyClosures and https://github.com/CSMMLab/KiT-RT.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c1c4dec5fd037121e304bf387cacc229dc80951" target='_blank'>
              Structure-preserving neural networks for the regularized entropy-based closure of the Boltzmann moment system
              </a>
            </td>
          <td>
            S. Schotthöfer, M. P. Laiu, Martin Frank, C. Hauck
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="In data assimilation, an ensemble provides a nonintrusive way to evolve a probability density described by a nonlinear prediction model. Although a large ensemble size is required for statistical accuracy, the ensemble size is typically limited to a small number due to the computational cost of running the prediction model, which leads to a sampling error. Several methods, such as localization, exist to mitigate the sampling error, often requiring problem-dependent fine-tuning and design. This work introduces another sampling error mitigation method using a smoothness constraint in the Fourier space. In particular, this work smoothes out the spectrum of the system to increase the stability and accuracy even under a small ensemble size. The efficacy of the new idea is validated through a suite of stringent test problems, including Lorenz 96 and Kuramoto-Sivashinsky turbulence models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d7471bcf248f7c9ce80a82d586a622ef17b4b1e8" target='_blank'>
              Sampling error mitigation through spectrum smoothing in ensemble data assimilation
              </a>
            </td>
          <td>
            Bosu Choi, Yoonsang Lee
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The FitzHugh–Nagumo model has been used empirically to model certain types of neuronal activities. It is also a non-linear dynamical system applicable to chemical kinetics, population dynamics, epidemiology and pattern formation. In the literature, many approaches have been proposed to study its dynamics. In this paper, initially, we have employed cutting-edge tools from discrete dynamics for discretization and fixed points. It has been proven that an exact discrete scheme exists for this paradigm. This project also considers the phase space and integral surfaces of these evolutionary equations. In addition, it carries out a thorough symmetry analysis of this reaction diffusion system to find equivalent systems. Moreover, steady-state solutions are obtained using ansatzes for traveling wave solutions. The existence of infinite traveling wave solutions has also been proven. Yet again, this investigation establishes the potential of symmetry methods to unravel non-linearity. Finally, singular perturbation theory has been employed to obtain analytical approximations and to study stability in different parameter regimes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecb93780bd3dfbba9d6d7759840de5e98c836b6f" target='_blank'>
              Nonstandard Nearly Exact Analysis of the FitzHugh–Nagumo Model
              </a>
            </td>
          <td>
            Sergei D. Odintsov, Marek Berezowski, Mujahid Abbas, Eddy Kwessi
          </td>
          <td>2024-05-09</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="For pricing American options, %after suitable discretization in space and time, a sequence of discrete linear complementarity problems (LCPs) or equivalently Hamilton-Jacobi-Bellman (HJB) equations need to be solved in a sequential time-stepping manner. In each time step, the policy iteration or its penalty variant is often applied due to their fast convergence rates. In this paper, we aim to solve for all time steps simultaneously, by applying the policy iteration to an ``all-at-once form"of the HJB equations, where two different parallel-in-time preconditioners are proposed to accelerate the solution of the linear systems within the policy iteration. Our proposed methods are generally applicable for such all-at-once forms of the HJB equation, arising from option pricing problems with optimal stopping and nontrivial underlying asset models. Numerical examples are presented to show the feasibility and robust convergence behavior of the proposed methodology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb01d5f9e66c8fef9c3cd130da2485d3b5d954ae" target='_blank'>
              Parallel-in-Time Iterative Methods for Pricing American Options
              </a>
            </td>
          <td>
            Xian-Ming Gu, Jun Liu, C. Oosterlee
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>46</td>
        </tr>

        <tr id="This work is on a user-friendly reduced basis method for solving a family of parametric PDEs by preconditioned Krylov subspace methods including the conjugate gradient method, generalized minimum residual method, and bi-conjugate gradient method. The proposed methods use a preconditioned Krylov subspace method for a high-fidelity discretization of one parameter instance to generate orthogonal basis vectors of the reduced basis subspace. Then large-scale discrete parameter-dependent problems are approximately solved in the low-dimensional Krylov subspace. As shown in the theory and experiments, only a small number of Krylov subspace iterations are needed to simultaneously generate approximate solutions of a family of high-fidelity and large-scale systems in the reduced basis subspace. This reduces the computational cost dramatically because (1) to construct the reduced basis vectors, we only solve one large-scale problem in the high-fidelity level; and (2) the family of large-scale problems restricted to the reduced basis subspace have much smaller sizes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5ddf7e05758a09b26c59d056eee71f5ef64f32d2" target='_blank'>
              Reduced Krylov Basis Methods for Parametric Partial Differential Equations
              </a>
            </td>
          <td>
            Yuwen Li, L. Zikatanov, Cheng Zuo
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="A reservoir computer (RC) is a recurrent neural network (RNN) framework that achieves computational efficiency where only readout layer training is required. Additionally, it effectively predicts nonlinear dynamical system tasks and has various applications. RC is effective for forecasting nonautonomous dynamical systems with gradual changes to the external drive amplitude. This study investigates the predictability of nonautonomous dynamical systems with rapid changes to the phase of the external drive. The forced Van der Pol equation was employed for the base model, implementing forecasting tasks with the RC. The study findings suggest that, despite hidden variables, a nonautonomous dynamical system with rapid changes to the phase of the external drive is predictable. Therefore, RC can offer better schedules for individual shift workers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c287992af2ce64be6c79397ffdf3d5bcc326f821" target='_blank'>
              Forecasting the Forced Van der Pol Equation with Frequent Phase Shifts Using a Reservoir Computer
              </a>
            </td>
          <td>
            Sho Kuno, Hiroshi Kori
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper explores the efficacy of diffusion-based generative models as neural operators for partial differential equations (PDEs). Neural operators are neural networks that learn a mapping from the parameter space to the solution space of PDEs from data, and they can also solve the inverse problem of estimating the parameter from the solution. Diffusion models excel in many domains, but their potential as neural operators has not been thoroughly explored. In this work, we show that diffusion-based generative models exhibit many properties favourable for neural operators, and they can effectively generate the solution of a PDE conditionally on the parameter or recover the unobserved parts of the system. We propose to train a single model adaptable to multiple tasks, by alternating between the tasks during training. In our experiments with multiple realistic dynamical systems, diffusion models outperform other neural operators. Furthermore, we demonstrate how the probabilistic diffusion model can elegantly deal with systems which are only partially identifiable, by producing samples corresponding to the different possible solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/93f7dd73bdb8cf078d6f19120987ab3c21100bc5" target='_blank'>
              Diffusion models as probabilistic neural operators for recovering unobserved states of dynamical systems
              </a>
            </td>
          <td>
            Katsiaryna Haitsiukevich, O. Poyraz, Pekka Marttinen, Alexander Ilin
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This paper designs novel nonparametric Bellman mappings in reproducing kernel Hilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings benefit from the rich approximating properties of RKHSs, adopt no assumptions on the statistics of the data owing to their nonparametric nature, require no knowledge on transition probabilities of Markov decision processes, and may operate without any training data. Moreover, they allow for sampling on-the-fly via the design of trajectory samples, re-use past test data via experience replay, effect dimensionality reduction by random Fourier features, and enable computationally lightweight operations to fit into efficient online or time-adaptive learning. The paper offers also a variational framework to design the free parameters of the proposed Bellman mappings, and shows that appropriate choices of those parameters yield several popular Bellman-mapping designs. As an application, the proposed mappings are employed to offer a novel solution to the problem of countering outliers in adaptive filtering. More specifically, with no prior information on the statistics of the outliers and no training data, a policy-iteration algorithm is introduced to select online, per time instance, the ``optimal'' coefficient p in the least-mean-p-power-error method. Numerical tests on synthetic data showcase, in most of the cases, the superior performance of the proposed solution over several RL and non-RL schemes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/07b978cc5ce476017d504ec76c0a67d801d77dd3" target='_blank'>
              Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering
              </a>
            </td>
          <td>
            Yuki Akiyama, Minh Vu, Konstantinos Slavakis
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/412379906fe28c94d56bd4ab954ab0a0909c9bd3" target='_blank'>
              Revealing trends and persistent cycles of non-autonomous systems with autonomous operator-theoretic techniques
              </a>
            </td>
          <td>
            G. Froyland, Dimitrios Giannakis, Edoardo Luna, J. Slawinska
          </td>
          <td>2024-05-20</td>
          <td>Nature Communications</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e9ac2c4cb35db6f74b769c21e2f2938e4cf1b363" target='_blank'>
              Empirical modeling and prediction of neuronal dynamics
              </a>
            </td>
          <td>
            Pau Fisco-Compte, David Aquilué-Llorens, Nestor Roqueiro, Enric Fossas, Antoni Guillamon
          </td>
          <td>2024-04-10</td>
          <td>Biological Cybernetics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose a categorical semantics for machine learning algorithms in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as MSE and Softmax cross-entropy, and different architectures, shedding new light on their similarities and differences. Furthermore, our approach to learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realised in the discrete setting of Boolean and polynomial circuits. We demonstrate the practical significance of our framework with an implementation in Python.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3ef777c28d6f522553e9af049d3b47742cb543bd" target='_blank'>
              Deep Learning with Parametric Lenses
              </a>
            </td>
          <td>
            Geoffrey S. H. Cruttwell, Bruno Gavranović, Neil Ghani, Paul W. Wilson, Fabio Zanasi
          </td>
          <td>2024-03-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Recent studies show that transformer-based architectures emulate gradient descent during a forward pass, contributing to in-context learning capabilities - an ability where the model adapts to new tasks based on a sequence of prompt examples without being explicitly trained or fine tuned to do so. This work investigates the generalization properties of a single step of gradient descent in the context of linear regression with well-specified models. A random design setting is considered and analytical expressions are derived for the statistical properties and bounds of generalization error in a non-asymptotic (finite sample) setting. These expressions are notable for avoiding arbitrary constants, and thus offer robust quantitative information and scaling relationships. These results are contrasted with those from classical least squares regression (for which analogous finite sample bounds are also derived), shedding light on systematic and noise components, as well as optimal step sizes. Additionally, identities involving high-order products of Gaussian random matrices are presented as a byproduct of the analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f4b24b0d9937ff37347f0bd93a295601f16546f2" target='_blank'>
              Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression
              </a>
            </td>
          <td>
            Karthik Duraisamy
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theory. Additionally, the results illustrate a strong connection between the learned proximal metrics and active constraints at the optima, leading to an interpretation in which the learning of proximal metrics can be viewed as a form of active set learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ef9e9b5a25df49d547c7d8883144219f5ec46cd7" target='_blank'>
              Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming
              </a>
            </td>
          <td>
            Ethan King, James Kotary, Ferdinando Fioretto, Ján Drgoňa
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Learning from expert demonstrations to flexibly program an autonomous system with complex behaviors or to predict an agent's behavior is a powerful tool, especially in collaborative control settings. A common method to solve this problem is inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to the optimization of an intrinsic cost function that reflects its intent and informs its control actions. While the framework is expressive, it is also computationally demanding and generally lacks convergence guarantees. We therefore propose a novel, stability-certified IRL approach by reformulating the cost function inference problem to learning control Lyapunov functions (CLF) from demonstrations data. By additionally exploiting closed-form expressions for associated control policies, we are able to efficiently search the space of CLFs by observing the attractor landscape of the induced dynamics. For the construction of the inverse optimal CLFs, we use a Sum of Squares and formulate a convex optimization problem. We present a theoretical analysis of the optimality properties provided by the CLF and evaluate our approach using both simulated and real-world data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/31a87861d2005c9eeac675e6e65ef03477903336" target='_blank'>
              Stable Inverse Reinforcement Learning: Policies from Control Lyapunov Landscapes
              </a>
            </td>
          <td>
            Samuel Tesfazgi, Leonhard Sprandl, Armin Lederer, Sandra Hirche
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Neural network-based approaches have recently shown significant promise in solving partial differential equations (PDEs) in science and engineering, especially in scenarios featuring complex domains or the incorporation of empirical data. One advantage of the neural network method for PDEs lies in its automatic differentiation (AD), which necessitates only the sample points themselves, unlike traditional finite difference (FD) approximations that require nearby local points to compute derivatives. In this paper, we quantitatively demonstrate the advantage of AD in training neural networks. The concept of truncated entropy is introduced to characterize the training property. Specifically, through comprehensive experimental and theoretical analyses conducted on random feature models and two-layer neural networks, we discover that the defined truncated entropy serves as a reliable metric for quantifying the residual loss of random feature models and the training speed of neural networks for both AD and FD methods. Our experimental and theoretical analyses demonstrate that, from a training perspective, AD outperforms FD in solving partial differential equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dab74a78b1102fae26f5c81587f815591116d925" target='_blank'>
              Automatic Differentiation is Essential in Training Neural Networks for Solving Differential Equations
              </a>
            </td>
          <td>
            Chuqi Chen, Yahong Yang, Yang Xiang, Wenrui Hao
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We present a novel method for generating sequential parameter estimates and quantifying epistemic uncertainty in dynamical systems within a data-consistent (DC) framework. The DC framework differs from traditional Bayesian approaches due to the incorporation of the push-forward of an initial density, which performs selective regularization in parameter directions not informed by the data in the resulting updated density. This extends a previous study that included the linear Gaussian theory within the DC framework and introduced the maximal updated density (MUD) estimate as an alternative to both least squares and maximum a posterior (MAP) estimates. In this work, we introduce algorithms for operational settings of MUD estimation in real or near-real time where spatio-temporal datasets arrive in packets to provide updated estimates of parameters and identify potential parameter drift. Computational diagnostics within the DC framework prove critical for evaluating (1) the quality of the DC update and MUD estimate and (2) the detection of parameter value drift. The algorithms are applied to estimate (1) wind drag parameters in a high-fidelity storm surge model, (2) thermal diffusivity field for a heat conductivity problem, and (3) changing infection and incubation rates of an epidemiological model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/deac584d1ccb0e916dc387a12089fd1c83b78509" target='_blank'>
              Sequential Maximal Updated Density Parameter Estimation for Dynamical Systems with Parameter Drift
              </a>
            </td>
          <td>
            C. Del-Castillo-Negrete, Rylan Spence, Troy Butler, Clint Dawson
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Delay Differential Equations (DDEs) are a class of differential equations that can model diverse scientific phenomena. However, identifying the parameters, especially the time delay, that make a DDE's predictions match experimental results can be challenging. We introduce DDE-Find, a data-driven framework for learning a DDE's parameters, time delay, and initial condition function. DDE-Find uses an adjoint-based approach to efficiently compute the gradient of a loss function with respect to the model parameters. We motivate and rigorously prove an expression for the gradients of the loss using the adjoint. DDE-Find builds upon recent developments in learning DDEs from data and delivers the first complete framework for learning DDEs from data. Through a series of numerical experiments, we demonstrate that DDE-Find can learn DDEs from noisy, limited data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cefa8b57028db46c2e35c60bea8a1a30100e7143" target='_blank'>
              DDE-Find: Learning Delay Differential Equations from Noisy, Limited Data
              </a>
            </td>
          <td>
            Robert Stephany
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present a comprehensive framework for deriving rigorous and efficient bounds on the approximation error of deep neural networks in PDE models characterized by branching mechanisms, such as waves, Schr\"odinger equations, and other dispersive models. This framework utilizes the probabilistic setting established by Henry-Labord\`ere and Touzi. We illustrate this approach by providing rigorous bounds on the approximation error for both linear and nonlinear waves in physical dimensions $d=1,2,3$, and analyze their respective computational costs starting from time zero. We investigate two key scenarios: one involving a linear perturbative source term, and another focusing on pure nonlinear internal interactions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9600e194e213ebbf6b0678e0e760c18f8139348b" target='_blank'>
              Bounds on the approximation error for deep neural networks applied to dispersive models: Nonlinear waves
              </a>
            </td>
          <td>
            Claudio Munoz, Nicol'as Valenzuela
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Solving high-dimensional partial differential equations necessitates methods free of exponential scaling in the dimension of the problem. This work introduces a tensor network approach for the Kolmogorov backward equation via approximating directly the Markov operator. We show that the high-dimensional Markov operator can be obtained under a functional hierarchical tensor (FHT) ansatz with a hierarchical sketching algorithm. When the terminal condition admits an FHT ansatz, the proposed operator outputs an FHT ansatz for the PDE solution through an efficient functional tensor network contraction procedure. In addition, the proposed operator-based approach also provides an efficient way to solve the Kolmogorov forward equation when the initial distribution is in an FHT ansatz. We apply the proposed approach successfully to two challenging time-dependent Ginzburg-Landau models with hundreds of variables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1cf099a3b1f5822a347ab3adac01ba73485f33e7" target='_blank'>
              Solving high-dimensional Kolmogorov backward equations with functional hierarchical tensor operators
              </a>
            </td>
          <td>
            Xun Tang, Leah Collis, Lexing Ying
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id=". Scattered data interpolation aims to reconstruct a continuous (smooth) function that approximates the underlying function by ﬁtting (meshless) data points. There are extensive applications of scattered data interpolation in computer graphics, ﬂuid dynamics, inverse kinematics, machine learning, etc. In this paper, we consider a novel generalized Mercel kernel in the reproducing kernel Banach space for scattered data interpolation. The system of interpolation equations is formulated as a multilinear sys-tem with a structural tensor, which is an absolutely and uniformly convergent inﬁnite series of symmetric rank-one tensors. Then we design a fast numerical method for computing the product of the structural tensor and any vector in arbitrary precision. Whereafter, a scalable optimization approach equipped with limited-memory BFGS and Wolfe line-search techniques is customized for solving these multilinear systems. Using the Łojasiewicz inequality, we prove that the proposed scalable optimization approach is a globally convergent algorithm and possesses a linear or sublinear convergence rate. Numerical experiments illustrate that the proposed scalable optimization approach can improve the accuracy of interpolation ﬁtting and computational efﬁciency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/440cce0394a5ce313aae5c9e7f60b66052e7b553" target='_blank'>
              A Scalable Optimization Approach for the Multilinear System Arising from Scattered Data Interpolation
              </a>
            </td>
          <td>
            Yannan Chen, Kaidong Fu, Can Li and Qi Ye
          </td>
          <td>2024-05-01</td>
          <td>CSIAM Transactions on Applied Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Algorithms developed to solve many-body quantum problems, like tensor networks, can turn into powerful quantum-inspired tools to tackle problems in the classical domain. In this work, we focus on matrix product operators, a prominent numerical technique to study many-body quantum systems, especially in one dimension. It has been previously shown that such a tool can be used for classification, learning of deterministic sequence-to-sequence processes and of generic quantum processes. We further develop a matrix product operator algorithm to learn probabilistic sequence-to-sequence processes and apply this algorithm to probabilistic cellular automata. This new approach can accurately learn probabilistic cellular automata processes in different conditions, even when the process is a probabilistic mixture of different chaotic rules. In addition, we find that the ability to learn these dynamics is a function of the bit-wise difference between the rules and whether one is much more likely than the other.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8110beec24b4ccfaab141e29148faf176cdc0802" target='_blank'>
              Tensor-Networks-based Learning of Probabilistic Cellular Automata Dynamics
              </a>
            </td>
          <td>
            Heitor P. Casagrande, Bo Xing, William J. Munro, Chu Guo, Dario Poletti
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d119b5e0192d10508a2178015de9283c30ffdaf6" target='_blank'>
              PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving
              </a>
            </td>
          <td>
            Zhipeng Zhao, Bowen Li, Yi Du, Taimeng Fu, Chen Wang
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent. In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems. To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE. However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question. We study NCA models at the limit where space-time discretization approaches continuity. We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called"seed". To address this, we propose a solution that utilizes uniform noise as the initial condition. We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities. Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns. We demonstrate this new NCA feature in our interactive online demo. Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/76298dc594d0dec3c5ef61fbe6cde4325b9051b7" target='_blank'>
              NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural Cellular Automata
              </a>
            </td>
          <td>
            Ehsan Pajouheshgar, Yitao Xu, Sabine Süsstrunk
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/adde0c7422a95a372ba0f11c94291409a299307c" target='_blank'>
              Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations
              </a>
            </td>
          <td>
            Predrag Pilipovic, Adeline L. Samson, Susanne Ditlevsen
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Understanding how neural systems efficiently process information through distributed representations is a fundamental challenge at the interface of neuroscience and machine learning. Recent approaches analyze the statistical and geometrical attributes of neural representations as population-level mechanistic descriptors of task implementation. In particular, manifold capacity has emerged as a promising framework linking population geometry to the separability of neural manifolds. However, this metric has been limited to linear readouts. Here, we propose a theoretical framework that overcomes this limitation by leveraging contextual input information. We derive an exact formula for the context-dependent capacity that depends on manifold geometry and context correlations, and validate it on synthetic and real data. Our framework's increased expressivity captures representation untanglement in deep networks at early stages of the layer hierarchy, previously inaccessible to analysis. As context-dependent nonlinearity is ubiquitous in neural systems, our data-driven and theoretically grounded approach promises to elucidate context-dependent computation across scales, datasets, and models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eeb127e6d37330d3773dc9829d77a67179be68f0" target='_blank'>
              Nonlinear classification of neural manifolds with contextual information
              </a>
            </td>
          <td>
            Francesca Mignacco, Chi-Ning Chou, SueYeon Chung
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels. In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches. Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible. We apply the method to quantum process tomography and a quantum machine learning problem. Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography. Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5cd381c451dada5cfca98c8d95c1be5abd4d1c7" target='_blank'>
              Regularization of Riemannian optimization: Application to process tomography and quantum machine learning
              </a>
            </td>
          <td>
            Felix Soest, K. Beyer, W. Strunz
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="This paper deals with a class of neural SDEs and studies the limiting behavior of the associated sampled optimal control problems as the sample size grows to infinity. The neural SDEs with N samples can be linked to the N-particle systems with centralized control. We analyze the Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and establish regularity results which are uniform in N. The uniform regularity estimates are obtained by the stochastic maximum principle and the analysis of a backward stochastic Riccati equation. Using these uniform regularity results, we show the convergence of the minima of objective functionals and optimal parameters of the neural SDEs as the sample size N tends to infinity. The limiting objects can be identified with suitable functions defined on the Wasserstein space of Borel probability measures. Furthermore, quantitative algebraic convergence rates are also obtained.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/57f3f65458cfdf0a4a5311bb7d148e79802fadd4" target='_blank'>
              Convergence analysis of controlled particle systems arising in deep learning: from finite to infinite sample size
              </a>
            </td>
          <td>
            Huafu Liao, Alp'ar R. M'esz'aros, Chenchen Mou, Chao Zhou
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Large neural networks trained on large datasets have become the dominant paradigm in machine learning. These systems rely on maximum likelihood point estimates of their parameters, precluding them from expressing model uncertainty. This may result in overconfident predictions and it prevents the use of deep learning models for sequential decision making. This thesis develops scalable methods to equip neural networks with model uncertainty. In particular, we leverage the linearised Laplace approximation to equip pre-trained neural networks with the uncertainty estimates provided by their tangent linear models. This turns the problem of Bayesian inference in neural networks into one of Bayesian inference in conjugate Gaussian-linear models. Alas, the cost of this remains cubic in either the number of network parameters or in the number of observations times output dimensions. By assumption, neither are tractable. We address this intractability by using stochastic gradient descent (SGD) -- the workhorse algorithm of deep learning -- to perform posterior sampling in linear models and their convex duals: Gaussian processes. With this, we turn back to linearised neural networks, finding the linearised Laplace approximation to present a number of incompatibilities with modern deep learning practices -- namely, stochastic optimisation, early stopping and normalisation layers -- when used for hyperparameter learning. We resolve these and construct a sample-based EM algorithm for scalable hyperparameter learning with linearised neural networks. We apply the above methods to perform linearised neural network inference with ResNet-50 (25M parameters) trained on Imagenet (1.2M observations and 1000 output dimensions). Additionally, we apply our methods to estimate uncertainty for 3d tomographic reconstructions obtained with the deep image prior network.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c4d23a3eb3225dbc9a0c7e943353aa657913497b" target='_blank'>
              Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian Processes to Deep Neural Networks
              </a>
            </td>
          <td>
            Javier Antorán
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The efficient approximation of highly oscillatory integrals plays an important role in a wide range of applications. Whilst traditional quadrature becomes prohibitively expensive in the high-frequency regime, Levin methods provide a way to approximate these integrals in many settings at uniform cost. In this work, we present an accelerated version of Levin methods that can be applied to a wide range of physically important oscillatory integrals, by exploiting the banded action of certain differential operators on a Chebyshev polynomial basis. Our proposed version of the Levin method can be computed essentially in just $\mathcal{O}(\nu\log\nu)$ operations, where $\nu$ is the number of quadrature points and the dependence of the cost on a number of additional parameters is made explicit in the manuscript. This presents a significant speed-up over the direct computation of the Levin method in current state-of-the-art. We outline the construction of this accelerated method for a fairly broad class of integrals and support our theoretical description with a number of illustrative numerical examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a90ac90d0bf5174d3dbb27415a3bfa5f21d562f4" target='_blank'>
              An accelerated Levin-Clenshaw-Curtis method for the evaluation of highly oscillatory integrals
              </a>
            </td>
          <td>
            A. Iserles, G. Maierhofer
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Mean-field control (MFC) problems aim to find the optimal policy to control massive populations of interacting agents. These problems are crucial in areas such as economics, physics, and biology. We consider the non-local setting, where the interactions between agents are governed by a suitable kernel. For $N$ agents, the interaction cost has $\mathcal{O}(N^2)$ complexity, which can be prohibitively slow to evaluate and differentiate when $N$ is large. To this end, we propose an efficient primal-dual algorithm that utilizes basis expansions of the kernels. The basis expansions reduce the cost of computing the interactions, while the primal-dual methodology decouples the agents at the expense of solving for a moderate number of dual variables. We also demonstrate that our approach can further be structured in a multi-resolution manner, where we estimate optimal dual variables using a moderate $N$ and solve decoupled trajectory optimization problems for large $N$. We illustrate the effectiveness of our method on an optimal control of 5000 interacting quadrotors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f2ca48cf67d512b3c58916552379527d295c342" target='_blank'>
              Kernel Expansions for High-Dimensional Mean-Field Control with Non-local Interactions
              </a>
            </td>
          <td>
            Alexander Vidal, Samy Wu Fung, Stanley Osher, Luis Tenorio, L. Nurbekyan
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="The Laplace-Beltrami operator (LBO) emerges from studying manifolds equipped with a Riemannian metric. It is often called the Swiss army knife of geometry processing as it allows to capture intrinsic shape information and gives rise to heat diffusion, geodesic distances, and a multitude of shape descriptors. It also plays a central role in geometric deep learning. In this work, we explore Finsler manifolds as a generalization of Riemannian manifolds. We revisit the Finsler heat equation and derive a Finsler heat kernel and a Finsler-Laplace-Beltrami Operator (FLBO): a novel theoretically justified anisotropic Laplace-Beltrami operator (ALBO). In experimental evaluations we demonstrate that the proposed FLBO is a valuable alternative to the traditional Riemannian-based LBO and ALBOs for spatial filtering and shape correspondence estimation. We hope that the proposed Finsler heat kernel and the FLBO will inspire further exploration of Finsler geometry in the computer vision community.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2fad83864235ef64afd2f947e48eb7d0022a8327" target='_blank'>
              Finsler-Laplace-Beltrami Operators with Application to Shape Analysis
              </a>
            </td>
          <td>
            Simon Nikolaus Weber, Thomas Dagès, Maolin Gao, Daniel Cremers
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Active learning optimizes the exploration of large parameter spaces by strategically selecting which experiments or simulations to conduct, thus reducing resource consumption and potentially accelerating scientific discovery. A key component of this approach is a probabilistic surrogate model, typically a Gaussian Process (GP), which approximates an unknown functional relationship between control parameters and a target property. However, conventional GPs often struggle when applied to systems with discontinuities and non-stationarities, prompting the exploration of alternative models. This limitation becomes particularly relevant in physical science problems, which are often characterized by abrupt transitions between different system states and rapid changes in physical property behavior. Fully Bayesian Neural Networks (FBNNs) serve as a promising substitute, treating all neural network weights probabilistically and leveraging advanced Markov Chain Monte Carlo techniques for direct sampling from the posterior distribution. This approach enables FBNNs to provide reliable predictive distributions, crucial for making informed decisions under uncertainty in the active learning setting. Although traditionally considered too computationally expensive for 'big data' applications, many physical sciences problems involve small amounts of data in relatively low-dimensional parameter spaces. Here, we assess the suitability and performance of FBNNs with the No-U-Turn Sampler for active learning tasks in the 'small data' regime, highlighting their potential to enhance predictive accuracy and reliability on test functions relevant to problems in physical sciences.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ebe4e8caad7fd908989b7e37a05fb880b373a0e4" target='_blank'>
              Active Learning with Fully Bayesian Neural Networks for Discontinuous and Nonstationary Data
              </a>
            </td>
          <td>
            Maxim Ziatdinov
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Using symbolic regression to discover physical laws from observed data is an emerging field. In previous work, we combined genetic algorithm (GA) and machine learning to present a data-driven method for discovering a wave equation. Although it managed to utilize the data to discover the two-dimensional (x,z) acoustic constant-density wave equation u_tt=v^2(u_xx+u_zz) (subscripts of the wavefield, u, are second derivatives in time and space) in a homogeneous medium, it did not provide the complete equation form, where the velocity term is represented by a coefficient rather than directly given by v^2. In this work, we redesign the framework, encoding both velocity information and candidate functional terms simultaneously. Thus, we use GA to simultaneously evolve the candidate functional and coefficient terms in the library. Also, we consider here the physics rationality and interpretability in the randomly generated potential wave equations, by ensuring that both-hand sides of the equation maintain balance in their physical units. We demonstrate this redesigned framework using the acoustic wave equation as an example, showing its ability to produce physically reasonable expressions of wave equations from noisy and sparsely observed data in both homogeneous and inhomogeneous media. Also, we demonstrate that our method can effectively discover wave equations from a more realistic observation scenario.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/50e879c5c0a3fe3bed1e0fee100d86b8ec2435d5" target='_blank'>
              Discovery of physically interpretable wave equations
              </a>
            </td>
          <td>
            Shijun Cheng, T. Alkhalifah
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>40</td>
        </tr>

        <tr id="A technical note aiming to offer deeper intuition for the LayerNorm function common in deep neural networks. LayerNorm is defined relative to a distinguished 'neural' basis, but it does more than just normalize the corresponding vector elements. Rather, it implements a composition -- of linear projection, nonlinear scaling, and then affine transformation -- on input activation vectors. We develop both a new mathematical expression and geometric intuition, to make the net effect more transparent. We emphasize that, when LayerNorm acts on an N-dimensional vector space, all outcomes of LayerNorm lie within the intersection of an (N-1)-dimensional hyperplane and the interior of an N-dimensional hyperellipsoid. This intersection is the interior of an (N-1)-dimensional hyperellipsoid, and typical inputs are mapped near its surface. We find the direction and length of the principal axes of this (N-1)-dimensional hyperellipsoid via the eigen-decomposition of a simply constructed matrix.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9b5d98880d73c9895cb3e33f18e26a11e64d0be2" target='_blank'>
              Geometry and Dynamics of LayerNorm
              </a>
            </td>
          <td>
            P. Riechers
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>11</td>
        </tr>

        <tr id="Data assimilation algorithms integrate prior information from numerical model simulations with observed data. Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology. Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy. In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces. It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties. One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters. A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved. Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge. A deeper understanding of these interactions is crucial for practical applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4a500974f4c0c869f60d3828de42111041646802" target='_blank'>
              Adaptive tempering schedules with approximative intermediate measures for filtering problems
              </a>
            </td>
          <td>
            Iris Rammelmuller, Gottfried Hastermann, Jana de Wiljes
          </td>
          <td>2024-05-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a909ef40e38eda1773cb3a2f56486e02a75e03f0" target='_blank'>
              Existence of reservoir with finite-dimensional output for universal reservoir computing
              </a>
            </td>
          <td>
            Shuhei Sugiura, Ryo Ariizumi, Toru Asai, Shun-Ichi Azuma
          </td>
          <td>2024-04-11</td>
          <td>Scientific Reports</td>
          <td>1</td>
          <td>4</td>
        </tr>

        <tr id="Recent connections in the adaptive control literature to continuous-time analogs of Nesterov's accelerated gradient method have led to the development of new real-time adaptation laws based on accelerated gradient methods. However, previous results assume that the system's uncertainties are linear-in-the-parameters (LIP). To compensate for non-LIP uncertainties, our preliminary results developed a neural network (NN)-based accelerated gradient adaptive controller to achieve trajectory tracking for nonlinear systems; however, the development and analysis only considered single-hidden-layer NNs. In this article, a generalized deep NN (DNN) architecture with an arbitrary number of hidden layers is considered, and a new DNN-based accelerated gradient adaptation scheme is developed to generate estimates of all the DNN weights in real-time. A nonsmooth Lyapunov-based analysis is used to guarantee the developed accelerated gradient-based DNN adaptation design achieves global asymptotic tracking error convergence for general nonlinear control affine systems subject to unknown (non-LIP) drift dynamics and exogenous disturbances. A comprehensive set of simulation studies are conducted on a two-state nonlinear system, a robotic manipulator, and a complex 20-D nonlinear system to demonstrate the improved performance of the developed method. Our simulation studies demonstrate enhanced tracking and function approximation performance from both DNN architectures and accelerated gradient adaptation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6577dd55a18708bb5032a658a9405e8d5028549" target='_blank'>
              Accelerated Gradient Approach For Deep Neural Network-Based Adaptive Control of Unknown Nonlinear Systems.
              </a>
            </td>
          <td>
            Duc M. Le, O. Patil, Cristian F. Nino, Warren E. Dixon
          </td>
          <td>2024-05-14</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="The Sherrington-Kirkpatrick (SK) model is a prototype of a complex non-convex energy landscape. Dynamical processes evolving on such landscapes and locally aiming to reach minima are generally poorly understood. Here, we study quenches, i.e. dynamics that locally aim to decrease energy. We analyse the energy at convergence for two distinct algorithmic classes, single-spin flip and synchronous dynamics, focusing on greedy and reluctant strategies. We provide precise numerical analysis of the finite size effects and conclude that, perhaps counter-intuitively, the reluctant algorithm is compatible with converging to the ground state energy density, while the greedy strategy is not. Inspired by the single-spin reluctant and greedy algorithms, we investigate two synchronous time algorithms, the sync-greedy and sync-reluctant algorithms. These synchronous processes can be analysed using dynamical mean field theory (DMFT), and a new backtracking version of DMFT. Notably, this is the first time the backtracking DMFT is applied to study dynamical convergence properties in fully connected disordered models. The analysis suggests that the sync-greedy algorithm can also achieve energies compatible with the ground state, and that it undergoes a dynamical phase transition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b898decdde6c602e2c508000e1edce329a6bff9" target='_blank'>
              Quenches in the Sherrington-Kirkpatrick model
              </a>
            </td>
          <td>
            Vittorio Erba, Freya Behrens, Florent Krzakala, L. Zdeborov'a
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>56</td>
        </tr>

        <tr id="In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucidating hidden structures in complex physical systems while offering practical applications in various domains requiring accurate short-term forecasting capabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/263a32e783722b09eefe90e0bbdf88a61c93c0c4" target='_blank'>
              A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours
              </a>
            </td>
          <td>
            Aarush Sinha
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Solving inverse problems, which means obtaining model parameters from observed data, using conventional computational fluid dynamics solvers is prohibitively expensive. Here we employ machine learning algorithms to overcome the challenge. As an example, we consider a moderately turbulent fluid flow, excited by a stationary force and described by a two-dimensional Navier-Stokes equation with linear bottom friction. Given sparse and probably noisy data for the velocity and the general form of the model, we reconstruct the dense velocity and pressure fields in the observation domain, infer the driving force, and determine the unknown fluid viscosity and friction coefficient. Our approach involves training a physics-informed neural network by minimizing the loss function, which penalizes deviations from the provided data and violations of the Navier-Stokes equation. The suggested technique extracts additional information from experimental and numerical observations, potentially enhancing the capabilities of particle image/tracking velocimetry.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/862293e827d5dc2041921d5d06dca4b845e5ce01" target='_blank'>
              Inferring parameters and reconstruction of two-dimensional turbulent flows with physics-informed neural networks
              </a>
            </td>
          <td>
            Vladimir Parfenyev, M. Blumenau, Ilya Nikitin
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="-Recent strides in model predictive control (MPC)underscore a dependence on numerical advancements to efficientlyand accurately solve large-scale problems. Given the substantialnumber of variables characterizing typical whole-body optimalcontrol (OC) problems -often numbering in the thousands-exploiting the sparse structure of the numerical problem becomescrucial to meet computational demands, typically in the range ofa few milliseconds. A fundamental building block for computingNewton or Sequential Quadratic Programming (SQP) steps indirect optimal control methods involves addressing the linearquadratic regulator (LQR) problem. This paper concentrateson equality-constrained problems featuring implicit systemdynamics and dual regularization, a characteristic found inadvanced interior-point or augmented Lagrangian solvers. Here,we introduce a parallel algorithm designed for solving an LQRproblem with dual regularization. Leveraging a rewriting of theLQR recursion through block elimination, we first enhanced theefficiency of the serial algorithm, then subsequently generalized itto handle parametric problems. This extension enables us to splitdecision variables and solve multiple subproblems concurrently.Our algorithm is implemented in our nonlinear numerical optimalcontrol library ALIGATOR. It showcases improved performanceover previous serial formulations and we validate its efficacy bydeploying it in the model predictive control of a real quadrupedrobot. This paper follows up from our prior work on augmentedLagrangian methods for numerical optimal control with implicitdynamics and constraints.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/64de033273a0e425c5677a998bf42887622e9bd5" target='_blank'>
              Parallel and Proximal Linear-Quadratic Methods for Real-Time Constrained Model-Predictive Control
              </a>
            </td>
          <td>
            Wilson Jallet, Ewen Dantec, Etienne Arlaud, Justin Carpentier, Nicolas Mansard
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Introduction Brain-inspired computing has become an emerging field, where a growing number of works focus on developing algorithms that bring machine learning closer to human brains at the functional level. As one of the promising directions, Hyperdimensional Computing (HDC) is centered around the idea of having holographic and high-dimensional representation as the neural activities in our brains. Such representation is the fundamental enabler for the efficiency and robustness of HDC. However, existing HDC-based algorithms suffer from limitations within the encoder. To some extent, they all rely on manually selected encoders, meaning that the resulting representation is never adapted to the tasks at hand. Methods In this paper, we propose FLASH, a novel hyperdimensional learning method that incorporates an adaptive and learnable encoder design, aiming at better overall learning performance while maintaining good properties of HDC representation. Current HDC encoders leverage Random Fourier Features (RFF) for kernel correspondence and enable locality-preserving encoding. We propose to learn the encoder matrix distribution via gradient descent and effectively adapt the kernel for a more suitable HDC encoding. Results Our experiments on various regression datasets show that tuning the HDC encoder can significantly boost the accuracy, surpassing the current HDC-based algorithm and providing faster inference than other baselines, including RFF-based kernel ridge regression. Discussion The results indicate the importance of an adaptive encoder and customized high-dimensional representation in HDC.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/63dc07266d88cd0269b658127f9b9a69b33e00cc" target='_blank'>
              Hyperdimensional computing with holographic and adaptive encoder
              </a>
            </td>
          <td>
            Alejandro Hernández-Cano, Yang Ni, Zhuowen Zou, Ali Zakeri, Mohsen Imani
          </td>
          <td>2024-04-09</td>
          <td>Frontiers in Artificial Intelligence</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We present a subspace method based on neural networks (SNN) for solving the partial differential equation with high accuracy. The basic idea of our method is to use some functions based on neural networks as base functions to span a subspace, then find an approximate solution in this subspace. We design two special algorithms in the strong form of partial differential equation. One algorithm enforces the equation and initial boundary conditions to hold on some collocation points, and another algorithm enforces $L^2$-norm of the residual of the equation and initial boundary conditions to be $0$. Our method can achieve high accuracy with low cost of training. Moreover, our method is free of parameters that need to be artificially adjusted. Numerical examples show that the cost of training these base functions of subspace is low, and only one hundred to two thousand epochs are needed for most tests. The error of our method can even fall below the level of $10^{-10}$ for some tests. The performance of our method significantly surpasses the performance of PINN and DGM in terms of the accuracy and computational cost.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bd756a3c78496f109b832163bd37e70bc7e96e3c" target='_blank'>
              Subspace method based on neural networks for solving the partial differential equation
              </a>
            </td>
          <td>
            Zhaodong Xu, Zhiqiang Sheng
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Replicating chaotic characteristics of non-linear dynamics by machine learning (ML) has recently drawn wide attentions. In this work, we propose that a ML model, trained to predict the state one-step-ahead from several latest historic states, can accurately replicate the bifurcation diagram and the Lyapunov exponents of discrete dynamic systems. The characteristics for different values of the hyper-parameters are captured universally by a single ML model, while the previous works considered training the ML model independently by fixing the hyper-parameters to be specific values. Our benchmarks on the one- and two-dimensional Logistic maps show that variational quantum circuit can reproduce the long-term characteristics with higher accuracy than the long short-term memory (a well-recognized classical ML model). Our work reveals an essential difference between the ML for the chaotic characteristics and that for standard tasks, from the perspective of the relation between performance and model complexity. Our results suggest that quantum circuit model exhibits potential advantages on mitigating over-fitting, achieving higher accuracy and stability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3398552790736a431bd98092b77b141dded167f" target='_blank'>
              Universal replication of chaotic characteristics by classical and quantum machine learning
              </a>
            </td>
          <td>
            Shengxing Bai, Shi-Ju Ran
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Willems' fundamental lemma enables a trajectory-based characterization of linear systems through data-based Hankel matrices. However, in the presence of measurement noise, we ask: Is this noisy Hankel-based model expressive enough to re-identify itself? In other words, we study the output prediction accuracy from recursively applying the same persistently exciting input sequence to the model. We find an asymptotic connection to this self-consistency question in terms of the amount of data. More importantly, we also connect this question to the depth (number of rows) of the Hankel model, showing the simple act of reconfiguring a finite dataset significantly improves accuracy. We apply these insights to find a parsimonious depth for LQR problems over the trajectory space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a5a8c3dd68d42cacef8f099fb744c61a43353a85" target='_blank'>
              Deep Hankel matrices with random elements
              </a>
            </td>
          <td>
            Nathan P. Lawrence, Philip D. Loewen, Shuyuan Wang, M. Forbes, R. B. Gopaluni
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="We investigate matrix-weighted bounds for the sublinear non-kernel operators considered by F. Bernicot, D. Frey, and S. Petermichl. We extend their result to sublinear operators acting upon vector-valued functions. First, we dominate these operators by bilinear convex body sparse forms, adapting a recent general principle due to T. Hyt\"onen. Then we use this domination to derive matrix-weighted bounds, adapting arguments of F. Nazarov, S. Petermichl, S. Treil, and A. Volberg. Our requirements on the weight are formulated in terms of two-exponent matrix Muckenhoupt conditions, which surprisingly exhibit a rich structure that is absent in the scalar case. Consequently, we deduce that our matrix-weighted bounds improve the ones that were recently obtained by A. Laukkarinen. The methods we use are flexible, which allows us to complement our results with a limited range extrapolation theorem for matrix weights, extending the results of P. Auscher and J. M. Martell, as well as M. Bownik and D. Cruz-Uribe.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00dd12588aaef6318714867d35cfd92893d5ab4b" target='_blank'>
              Matrix-weighted estimates beyond Calder\'on-Zygmund theory
              </a>
            </td>
          <td>
            S. Kakaroumpas, Thu Hien Nguyen, Dimitris Vardakis
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="A probabilistic framework to study the dependence structure induced by deterministic discrete-time state-space systems between input and output processes is introduced. General sufficient conditions are formulated under which output processes exist and are unique once an input process has been fixed, a property that in the deterministic state-space literature is known as the echo state property. When those conditions are satisfied, the given state-space system becomes a generative model for probabilistic dependences between two sequence spaces. Moreover, those conditions guarantee that the output depends continuously on the input when using the Wasserstein metric. The output processes whose existence is proved are shown to be causal in a specific sense and to generalize those studied in purely deterministic situations. The results in this paper constitute a significant stochastic generalization of sufficient conditions for the deterministic echo state property to hold, in the sense that the stochastic echo state property can be satisfied under contractivity conditions that are strictly weaker than those in deterministic situations. This means that state-space systems can induce a purely probabilistic dependence structure between input and output sequence spaces even when there is no functional relation between those two spaces.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/24727419ab41c9f1dce3858ea785e9f7bb14f18a" target='_blank'>
              State-Space Systems as Dynamic Generative Models
              </a>
            </td>
          <td>
            Juan-Pablo Ortega, Florian Rossmannek
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Stochastic optimization problems are generally known to be ill-conditioned to the form of the underlying uncertainty. A framework is introduced for optimal control problems with partial differential equations as constraints that is robust to inaccuracies in the precise form of the problem uncertainty. The framework is based on problem relaxation and involves optimizing a bivariate,"Rockafellian"objective functional that features both a standard control variable and an additional perturbation variable that handles the distributional ambiguity. In the presence of distributional corruption, the Rockafellian objective functionals are shown in the appropriate settings to $\Gamma$-converge to uncorrupted objective functionals in the limit of vanishing corruption. Numerical examples illustrate the framework's utility for outlier detection and removal and for variance reduction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e1d43d13360e7b9116ecacec237554b666b2f6fe" target='_blank'>
              Rockafellian Relaxation for PDE-Constrained Optimization with Distributional Uncertainty
              </a>
            </td>
          <td>
            Harbir Antil, Sean P. Carney, Hugo D'iaz, J. Royset
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="To solve problems in domains such as filtering, optimization, and posterior sampling, interacting-particle methods have recently received much attention. These parallelizable and often gradient-free algorithms use an ensemble of particles that evolve in time, based on a combination of well-chosen dynamics and interaction between the particles. For computationally expensive dynamics -- for example, dynamics that solve inverse problems with an expensive forward model -- the cost of attaining a high accuracy quickly becomes prohibitive. We exploit a hierarchy of approximations to this forward model and apply multilevel Monte Carlo (MLMC) techniques, improving the asymptotic cost-to-error relation. More specifically, we use MLMC at each time step to estimate the interaction term within a single, globally-coupled ensemble. This technique was proposed by Hoel et al. in the context of the ensemble Kalman filter; the goal of the present paper is to study its applicability to a general framework of interacting-particle methods. After extending the algorithm and its analysis to a broad set of methods with fixed numbers of time steps, we motivate the application of the method to the class of algorithms with an infinite time horizon, which includes popular methods such as ensemble Kalman algorithms for optimization and sampling. Numerical tests confirm the improved asymptotic scaling of the multilevel approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83f0694bf03f146ba6f88c4cdf323e51cc8069f4" target='_blank'>
              Single-ensemble multilevel Monte Carlo for discrete interacting-particle methods
              </a>
            </td>
          <td>
            Arne Bouillon, Toon Ingelaere, Giovanni Samaey
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Subspace predictive control (SPC) is a widely recognized data-driven methodology known for its reliability and convenience. However, effectively applying SPC to complex industrial process systems remains a challenging endeavor. To address this, this paper introduces a nonlinear subspace predictive control approach based on locally weighted projection regression (NSPC-LWPR). By projecting the input space into localized regions, constructing precise local models, and aggregating them through weighted summation, this approach handles the nonlinearity effectively. Additionally, it dynamically adjusts the control strategy based on online process data and model parameters, while eliminating the need for offline process data storage, greatly enhancing the adaptability and efficiency of the approach. The parameter determination criteria and theoretical analysis encompassing feasibility and stability assessments provide a robust foundation for the proposed approach. To illustrate its efficacy and feasibility, the proposed approach is applied to a continuous stirred tank heater (CSTH) benchmark system. Comparative results highlight its superiority over SPC and adaptive subspace predictive control (ASPC) methods, evident in enhanced tracking precision and predictive accuracy. Overall, the proposed NSPC-LWPR approach presents a promising solution for nonlinear control challenges in industrial process systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/329cbed5c987f49ccb1cb43cdf140a767de7584b" target='_blank'>
              A Nonlinear Subspace Predictive Control Approach Based on Locally Weighted Projection Regression
              </a>
            </td>
          <td>
            Xinwei Wu, Xuebo Yang
          </td>
          <td>2024-04-26</td>
          <td>Electronics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Entropy is a central concept in physics, but can be challenging to calculate even for systems that are easily simulated. This is exacerbated out of equilibrium, where generally little is known about the distribution characterizing simulated configurations. However, modern machine learning algorithms can estimate the probability density characterizing an ensemble of images, given nothing more than sample images assumed to be drawn from this distribution. We show that by mapping system configurations to images, such approaches can be adapted to the efficient estimation of the density, and therefore the entropy, from simulated or experimental data. We then use this idea to obtain entropic limit cycles in a kinetic Ising model driven by an oscillating magnetic field. Despite being a global probe, we demonstrate that this allows us to identify and characterize stochastic dynamics at parameters near the dynamical phase transition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc5c2639eefdb691a6b558afc70620258eb7d1be" target='_blank'>
              Nonequilibrium entropy from density estimation
              </a>
            </td>
          <td>
            Samuel D. Gelman, Guy Cohen
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay. In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity. Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships. To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains. The convergence of the loss function is proven. Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a52977fee60d7b3615721c368a708f015928933" target='_blank'>
              Double-activation neural network for solving parabolic equations with time delay
              </a>
            </td>
          <td>
            Qiumei Huang, Qiao Zhu
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. These results also highlight a key challenge for interpretability $-$ or for comparing the representations of models and brains $-$ disentangling extraneous biases from the computationally important aspects of a system's internal representations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3942757c6760c2ebb55f3c5254b81f110a68e3b" target='_blank'>
              Learned feature representations are biased by complexity, learning order, position, and more
              </a>
            </td>
          <td>
            Andrew Kyle Lampinen, Stephanie C. Y. Chan, Katherine Hermann
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="Computing distances on Riemannian manifolds is a challenging problem with numerous applications, from physics, through statistics, to machine learning. In this paper, we introduce the metric-constrained Eikonal solver to obtain continuous, differentiable representations of distance functions on manifolds. The differentiable nature of these representations allows for the direct computation of globally length-minimising paths on the manifold. We showcase the use of metric-constrained Eikonal solvers for a range of manifolds and demonstrate the applications. First, we demonstrate that metric-constrained Eikonal solvers can be used to obtain the Fr\'echet mean on a manifold, employing the definition of a Gaussian mixture model, which has an analytical solution to verify the numerical results. Second, we demonstrate how the obtained distance function can be used to conduct unsupervised clustering on the manifold -- a task for which existing approaches are computationally prohibitive. This work opens opportunities for distance computations on manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5fb91b477e037f512274e4ee3a3298566bed1fc0" target='_blank'>
              Computing distances and means on manifolds with a metric-constrained Eikonal approach
              </a>
            </td>
          <td>
            Daniel Kelshaw, Luca Magri
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we consider a trigonometric polynomial reconstruction of continuous periodic functions from their noisy values at equidistant nodes of the unit circle by a regularized least squares method. We indicate that the constructed trigonometric polynomial can be determined in explicit due to the exactness of trapezoidal rule. Then a concrete error bound is derived based on the estimation of the Lebesgue constant. In particular, we analyze three regularization parameter choice strategies: Morozov's discrepancy principal, L-curve and generalized cross-validation. Finally, numerical examples are given to perform that well chosen parameters by above strategy can improve approximation quality.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afa8cb90fc2a0d9546bbce99f5ee1ebac8d9d76c" target='_blank'>
              Parameter choice strategies for regularized least squares approximation of noisy continuous functions on the unit circle
              </a>
            </td>
          <td>
            Congpei An, Mou Cai
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Simulation of conditioned diffusion processes is an essential tool in inference for stochastic processes, data imputation, generative modelling, and geometric statistics. Whilst simulating diffusion bridge processes is already difficult on Euclidean spaces, when considering diffusion processes on Riemannian manifolds the geometry brings in further complications. In even higher generality, advancing from Riemannian to sub-Riemannian geometries introduces hypoellipticity, and the possibility of finding appropriate explicit approximations for the score of the diffusion process is removed. We handle these challenges and construct a method for bridge simulation on sub-Riemannian manifolds by demonstrating how recent progress in machine learning can be modified to allow for training of score approximators on sub-Riemannian manifolds. Since gradients dependent on the horizontal distribution, we generalise the usual notion of denoising loss to work with non-holonomic frames using a stochastic Taylor expansion, and we demonstrate the resulting scheme both explicitly on the Heisenberg group and more generally using adapted coordinates. We perform numerical experiments exemplifying samples from the bridge process on the Heisenberg group and the concentration of this process for small time.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0803d6af6d30c48b58c31bf11f6d806924a785d9" target='_blank'>
              Score matching for sub-Riemannian bridge sampling
              </a>
            </td>
          <td>
            E. Grong, Karen Habermann, Stefan Sommer
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="We characterize the geometry and topology of the set of all weight vectors for which a linear neural network computes the same linear transformation $W$. This set of weight vectors is called the fiber of $W$ (under the matrix multiplication map), and it is embedded in the Euclidean weight space of all possible weight vectors. The fiber is an algebraic variety that is not necessarily a manifold. We describe a natural way to stratify the fiber--that is, to partition the algebraic variety into a finite set of manifolds of varying dimensions called strata. We call this set of strata the rank stratification. We derive the dimensions of these strata and the relationships by which they adjoin each other. Although the strata are disjoint, their closures are not. Our strata satisfy the frontier condition: if a stratum intersects the closure of another stratum, then the former stratum is a subset of the closure of the latter stratum. Each stratum is a manifold of class $C^\infty$ embedded in weight space, so it has a well-defined tangent space and normal space at every point (weight vector). We show how to determine the subspaces tangent to and normal to a specified stratum at a specified point on the stratum, and we construct elegant bases for those subspaces. To help achieve these goals, we first derive what we call a Fundamental Theorem of Linear Neural Networks, analogous to what Strang calls the Fundamental Theorem of Linear Algebra. We show how to decompose each layer of a linear neural network into a set of subspaces that show how information flows through the neural network. Each stratum of the fiber represents a different pattern by which information flows (or fails to flow) through the neural network. The topology of a stratum depends solely on this decomposition. So does its geometry, up to a linear transformation in weight space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0c79fa624819124f0db63b50f108931bc4c3f483" target='_blank'>
              The Geometry of the Set of Equivalent Linear Neural Networks
              </a>
            </td>
          <td>
            J. Shewchuk, Sagnik Bhattacharya
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="Robust integration of physical knowledge and data is key to improve computational simulations, such as Earth system models. Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification. Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive. Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models. In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework. These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis. Likewise, one can frame data assimilation as observation-conditioned state calibration. In this work, we propose SLAMS: Score-based Latent Assimilation in Multimodal Setting. Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally. Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings. To our knowledge, our work is the first to apply deep generative framework for multimodal data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models. Our code is available at: https://github.com/yongquan-qu/SLAMS">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc3e0b8ce89e1b4701b2bee561c38e13abec072e" target='_blank'>
              Deep Generative Data Assimilation in Multimodal Setting
              </a>
            </td>
          <td>
            Yongquan Qu, Juan Nathaniel, Shuolin Li, Pierre Gentine
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="We study the limiting dynamics of a large class of noisy gradient descent systems in the overparameterized regime. In this regime the set of global minimizers of the loss is large, and when initialized in a neighbourhood of this zero-loss set a noisy gradient descent algorithm slowly evolves along this set. In some cases this slow evolution has been related to better generalisation properties. We characterize this evolution for the broad class of noisy gradient descent systems in the limit of small step size. Our results show that the structure of the noise affects not just the form of the limiting process, but also the time scale at which the evolution takes place. We apply the theory to Dropout, label noise and classical SGD (minibatching) noise, and show that these evolve on different two time scales. Classical SGD even yields a trivial evolution on both time scales, implying that additional noise is required for regularization. The results are inspired by the training of neural networks, but the theorems apply to noisy gradient descent of any loss that has a non-trivial zero-loss set.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/534932034316cdaaacddd5a8407cfd33520a086f" target='_blank'>
              Singular-limit analysis of gradient descent with noise injection
              </a>
            </td>
          <td>
            Anna Shalova, Andr'e Schlichting, M. Peletier
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="The construction of bifurcation diagrams is an essential component of understanding nonlinear dynamical systems. The task can be challenging when one knows the equations of the dynamical system and becomes much more difficult if only the underlying data associated with the system are available. In this work, we present a transformer-based method to directly estimate the bifurcation diagram using only noisy data associated with an arbitrary dynamical system. By splitting a bifurcation diagram into segments at bifurcation points, the transformer is trained to simultaneously predict how many segments are present and to minimize the loss with respect to the predicted position, shape, and asymptotic stability of each predicted segment. The trained model is shown, both quantitatively and qualitatively, to reliably estimate the structure of the bifurcation diagram for arbitrarily generated one- and two-dimensional systems experiencing a codimension-one bifurcation with as few as 30 trajectories. We show that the method is robust to noise in both the state variable and the system parameter.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06e38917deafc4e934ebae972b0c466999eebe0f" target='_blank'>
              Inferring bifurcation diagrams with transformers.
              </a>
            </td>
          <td>
            Lyra Zhornyak, M. A. Hsieh, Eric Forgoston
          </td>
          <td>2024-05-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="
 Model Predictive Control (MPC) is a prominent control paradigm providing accurate state prediction and subsequent control actions for intricate dynamical systems with applications ranging from autonomous driving to star tracking. However, there is an apparent discrepancy between the model’s mathematical description and its behavior in real-world conditions, affecting its performance in real-time. In this work, we propose a novel neuromorphic spiking neural network for continuous adaptive non-linear MPC. By using real-time learning, our design significantly reduces dynamic error and augments model accuracy, while simultaneously addressing unforeseen situations. We evaluated our framework using real-world scenarios in autonomous driving, implemented in a physics-driven simulation. We tested our design with various vehicles (from a Tesla Model 3 to an Ambulance) experiencing malfunctioning and swift steering scenarios. We demonstrate significant improvements in dynamic error rate compared with traditional MPC implementation with up to 89.87% median prediction error reduction with 5 spiking neurons and up to 96.95% with 5000 neurons. Our results may pave the way for novel applications in real-time control and stimulate further studies in the adaptive control realm with spiking neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7f8f1659532465ab69d1e5ebec65f1856bc20b23" target='_blank'>
              Continuous adaptive nonlinear model predictive control using spiking neural networks and real-time learning
              </a>
            </td>
          <td>
            Raz Halaly, Elishai Ezra Tsur
          </td>
          <td>2024-04-23</td>
          <td>Neuromorphic Computing and Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations and adversarial attacks, as well as the stability and safety of systems with neural network controllers. Therefore, estimation of tight bounds on the Lipschitz constant of neural networks is a well-studied topic. However, typical approaches involve solving a large matrix verification problem, the computational cost of which grows significantly for deeper networks. In this letter, we provide a compositional approach to estimate Lipschitz constants for deep feedforward neural networks by obtaining an exact decomposition of the large matrix verification problem into smaller sub-problems. We further obtain a closed-form solution that applies to most common neural network activation functions, which will enable rapid robustness and stability certificates for neural networks deployed in online control settings. Finally, we demonstrate through numerical experiments that our approach provides a steep reduction in computation time while yielding Lipschitz bounds that are very close to those achieved by state-of-the-art approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/813a16e601a432ebd25dd2b3686aa87a5c4fca39" target='_blank'>
              Compositional Estimation of Lipschitz Constants for Deep Neural Networks
              </a>
            </td>
          <td>
            Yuezhu Xu, S. Sivaranjani
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space. More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/811bf918f6e0b636d02b1e042c162f22f48eca6c" target='_blank'>
              Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint
              </a>
            </td>
          <td>
            Vincent Perrier
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers. Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios. In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice. Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes. Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability. Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc30bf2b4d9b8101764d3c2e422b58734d6fff3d" target='_blank'>
              Approximability of the Containment Problem for Zonotopes and Ellipsotopes
              </a>
            </td>
          <td>
            Adrian Kulmburg, Lukas Schafer, Matthias Althoff
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2d72fbac501c75d7e3eae527a5d016165360cba5" target='_blank'>
              Tutorial: a beginner’s guide to building a representative model of dynamical systems using the adjoint method
              </a>
            </td>
          <td>
            Leon Lettermann, Alejandro Jurado, Timo Betz, F. Wörgötter, S. Herzog
          </td>
          <td>2024-04-15</td>
          <td>Communications Physics</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="The stable numerical integration of shocks in compressible flow simulations relies on the reduction or elimination of Gibbs phenomena (unstable, spurious oscillations). A popular method to virtually eliminate Gibbs oscillations caused by numerical discretization in under-resolved simulations is to use a flux limiter. A wide range of flux limiters has been studied in the literature, with recent interest in their optimization via machine learning methods trained on high-resolution datasets. The common use of flux limiters in numerical codes as plug-and-play blackbox components makes them key targets for design improvement. Moreover, while aleatoric (inherent randomness) and epistemic (lack of knowledge) uncertainty is commonplace in fluid dynamical systems, these effects are generally ignored in the design of flux limiters. Even for deterministic dynamical models, numerical uncertainty is introduced via coarse-graining required by insufficient computational power to solve all scales of motion. Here, we introduce a conceptually distinct type of flux limiter that is designed to handle the effects of randomness in the model and uncertainty in model parameters. This new, {\it probabilistic flux limiter}, learned with high-resolution data, consists of a set of flux limiting functions with associated probabilities, which define the frequencies of selection for their use. Using the example of Burgers' equation, we show that a machine learned, probabilistic flux limiter may be used in a shock capturing code to more accurately capture shock profiles. In particular, we show that our probabilistic flux limiter outperforms standard limiters, and can be successively improved upon (up to a point) by expanding the set of probabilistically chosen flux limiting functions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04d9f0a92b077f68a565391c1b9980829fdf7bdd" target='_blank'>
              Probabilistic Flux Limiters
              </a>
            </td>
          <td>
            Nga Nguyen-Fotiadis, Robert Chiodi, Michael McKerns, Daniel Livescu, Andrew Sornborger
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="As one kind important phase field equations, Cahn-Hilliard equations contain spatial high order derivatives, strong nonlinearities, and even singularities. When using the physics informed neural network (PINN) to simulate the long time evolution, it is necessary to decompose the time domain to capture the transition of solutions in different time. Moreover, the baseline PINN can't maintain the mass conservation property for the equations. We propose a mass-preserving spatio-temporal adaptive PINN. This method adaptively dividing the time domain according to the rate of energy decrease, and solves the Cahn-Hilliard equation in each time step using an independent neural network. To improve the prediction accuracy, spatial adaptive sampling is employed in the subdomain to select points with large residual value and add them to the training samples. Additionally, a mass constraint is added to the loss function to compensate the mass degradation problem of the PINN method in solving the Cahn-Hilliard equations. The mass-preserving spatio-temporal adaptive PINN is employed to solve a series of numerical examples. These include the Cahn-Hilliard equations with different bulk potentials, the three dimensional Cahn-Hilliard equation with singularities, and the set of Cahn-Hilliard equations. The numerical results demonstrate the effectiveness of the proposed algorithm.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3067a09d2763478dfa3a700261e63c35c97eb92" target='_blank'>
              Mass-preserving Spatio-temporal adaptive PINN for Cahn-Hilliard equations with strong nonlinearity and singularity
              </a>
            </td>
          <td>
            Qiumei Huang, Jiaxuan Ma, Xu Zhen
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper considers the observer design problem for discrete-time nonlinear dynamical systems with sampled measurement data. Earlier, the recently proposed Iteratively Preconditioned Gradient-Descent (IPG) observer, a Newton-type observer, has been empirically shown to have improved robustness against measurement noise than the prominent nonlinear observers, a property that other Newton-type observers lack. However, no theoretical guarantees on the convergence of the IPG observer were provided. This paper presents a rigorous convergence analysis of the IPG observer for a class of nonlinear systems in deterministic settings, proving its local linear convergence to the actual trajectory. Our assumptions are standard in the existing literature of Newton-type observers, and the analysis further confirms the relation of the IPG observer with the Newton observer, which was only hypothesized earlier.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c99a1ecf08ec5e38db2283a9efc91ed04a1153e" target='_blank'>
              On Convergence of the Iteratively Preconditioned Gradient-Descent (IPG) Observer
              </a>
            </td>
          <td>
            Kushal Chakrabarti, Nikhil Chopra
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="A Tikhonov regularized inertial primal\mbox{-}dual dynamical system with time scaling and vanishing damping is proposed for solving a linearly constrained convex optimization problem in Hilbert spaces. The system under consideration consists of two coupled second order differential equations and its convergence properties depend upon the decaying speed of the product of the time scaling parameter and the Tikhonov regularization parameter (named the rescaled regularization parameter) to zero. When the rescaled regularization parameter converges rapidly to zero, the system enjoys fast convergence rates of the primal-dual gap, the feasibility violation, the objective residual, and the gradient norm of the objective function along the trajectory, and the weak convergence of the trajectory to a primal-dual solution of the linearly constrained convex optimization problem. When the rescaled regularization parameter converges slowly to zero, the generated primal trajectory converges strongly to the minimal norm solution of the problem under suitable conditions. Finally, numerical experiments are performed to illustrate the theoretical findings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/493bbc24e7a362645df3137d454d329dd81822d2" target='_blank'>
              Fast convergence rates and trajectory convergence of a Tikhonov regularized inertial primal\mbox{-}dual dynamical system with time scaling and vanishing damping
              </a>
            </td>
          <td>
            Ting-Ting Zhu, Rong Hu, Yaoli Fang
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Weight learning forms a basis for the machine learning and numerous algorithms have been adopted up to date. Most of the algorithms were either developed in the stochastic framework or aimed at minimization of loss or regret functions. Asymptotic convergence of weight learning, vital for good output prediction, was seldom guaranteed for online applications. Since linear regression is the most fundamental component in machine learning, we focus on this model in this paper. Aiming at online applications, a deterministic analysis method is developed based on LaSalle's invariance principle. Convergence conditions are derived for both the first-order and the second-order learning algorithms, without resorting to any stochastic argument. Moreover, the deterministic approach makes it easy to analyze the noise influence. Specifically, adaptive hyperparameters are derived in this framework and their tuning rules disclosed for the compensation of measurement noise. Comparison with four most popular algorithms validates that this approach has a higher learning capability and is quite promising in enhancing the weight learning performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/115c5d1f8b9152a29e6504eba35f9bd236d7e15a" target='_blank'>
              Deterministic Gradient-Descent Learning of Linear Regressions: Adaptive Algorithms, Convergence Analysis and Noise Compensation.
              </a>
            </td>
          <td>
            Kang-Zhi Liu, Chao Gan
          </td>
          <td>2024-05-10</td>
          <td>IEEE transactions on pattern analysis and machine intelligence</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization. The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions. Further local linear (and potentially superlinear) convergence is shown under standard additional conditions. The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems. These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters. The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements. We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines. We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/695ed3de01e391e3338f9c30f41c35390429161d" target='_blank'>
              An efficient active-set method with applications to sparse approximations and risk minimization
              </a>
            </td>
          <td>
            Spyridon Pougkakiotis, J. Gondzio, Dionysis Kalogerias
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Approximation ability is one of the most important topics in the field of neural networks (NNs). Feedforward NNs, activated by rectified linear units and some of their specific smoothed versions, provide universal approximators to convex as well as continuous functions. However, most of these networks are investigated empirically, or their characteristics are analyzed based on specific operation rules. Moreover, an adequate level of interpretability of the networks is missing as well. In this work, we propose a class of new network architecture, built with reusable neural modules (functional blocks), to supply differentiable and interpretable approximators for convex and continuous target functions. Specifically, first, we introduce a concrete model construction mechanism with particular blocks based on differentiable programming and the composition essence of the max operator, extending the scope of existing activation functions. Moreover, explicit block diagrams are provided for a clear understanding of the external architecture and the internal processing mechanism. Subsequently, the approximation behavior of the proposed network to convex functions and continuous functions is rigorously proved as well, by virtue of mathematical induction. Finally, plenty of numerical experiments are conducted on a wide variety of problems, which exhibit the effectiveness and the superiority of the proposed model over some existing ones.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb22e431201f0753e058726fdf058e8f07404eed" target='_blank'>
              Universal Approximation Abilities of a Modular Differentiable Neural Network.
              </a>
            </td>
          <td>
            Jian Wang, Shujun Wu, Huaqing Zhang, Bin Yuan, Caili Dai, Nikhil R. Pal
          </td>
          <td>2024-04-03</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Dimensionality reduction often serves as the first step toward a minimalist understanding of physical systems as well as the accelerated simulations of them. In particular, neural network-based nonlinear dimensionality reduction methods, such as autoencoders, have shown promising outcomes in uncovering collective variables (CVs). However, the physical meaning of these CVs remains largely elusive. In this work, we constructed a framework that (1) determines the optimal number of CVs needed to capture the essential molecular motions using an ensemble of hierarchical autoencoders and (2) provides topology-based interpretations to the autoencoder-learned CVs with Morse-Smale complex and sublevelset persistent homology. This approach was exemplified using a series of n-alkanes and can be regarded as a general, explainable nonlinear dimensionality reduction method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b589f55faeefb8ff1346474b39d4c4818b853722" target='_blank'>
              Interpretation of autoencoder-learned collective variables using Morse-Smale complex and sublevelset persistent homology: An application on molecular trajectories.
              </a>
            </td>
          <td>
            Shao-Chun Lee, Y. Z
          </td>
          <td>2024-04-09</td>
          <td>The Journal of chemical physics</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Stochastic physics is a central pillar of modern research in many fields, but is rarely presented to undergrad students in a hands-on experiment. Here, we demonstrate how a human-scale, simple, and affordable experimental setup can be used to fill this gap, and to illustrate many advanced concepts in a step-by-step approach. Based on a metal wire (such as a guitar string), our setup facilitates the observation of fluctuating dynamics in the time domain, the frequency spectrum, and in the rotating phase space. The latter allows introducing time-dependent cross-correlations between the sine and cosine quadratures of the stochastic motion, which feature deterministic order even in the absence of any deterministic forces.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c28b7759ee23c42d37c504a441259e0b7515c694" target='_blank'>
              The Stochastic Guitar
              </a>
            </td>
          <td>
            Andreas Eggenberger, Alexander Eichler
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this work, we propose a martingale based neural network, SOC-MartNet, for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) equations where no explicit expression is needed for the Hamiltonian $\inf_{u \in U} H(t,x,u, z,p)$, and stochastic optimal control problems with controls on both drift and volatility. We reformulate the HJB equations into a stochastic neural network learning process, i.e., training a control network and a value network such that the associated Hamiltonian process is minimized and the cost process becomes a martingale.To enforce the martingale property for the cost process, we employ an adversarial network and construct a loss function based on the projection property of conditional expectations. Then, the control/value networks and the adversarial network are trained adversarially, such that the cost process is driven towards a martingale and the minimum principle is satisfied for the control.Numerical results show that the proposed SOC-MartNet is effective and efficient for solving HJB-type equations and SOCP with a dimension up to $500$ in a small number of training epochs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fc413708d5fa348d77335120621208e12c75878e" target='_blank'>
              SOC-MartNet: A Martingale Neural Network for the Hamilton-Jacobi-Bellman Equation without Explicit inf H in Stochastic Optimal Controls
              </a>
            </td>
          <td>
            Wei Cai, Shuixin Fang, Tao Zhou
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Soft actuators, distinguished by their complex nonlinear behavior, are difficult to model analytically and cumbersome to prototype. Finite element (FE) models allow for more efficient behavioral prediction, but often require onerous setup, especially for large systems. We present a physics-informed neural network model formed by combining a low fidelity analytical model and input-convex neural networks to learn an underlying energy potential for the actuator from experimental and finite element simulation data. In doing this, the neural network can provide sufficiently accurate predictions about systems made up of multiple units, essentially scaling the model from a single unit to an assembly of many. To test this concept, we compare predictions of the deformation of a 5-actuator system from an FE model and from the physics-informed neural network. The neural network, which provides a prediction similar in accuracy to the FE equivalent, can more easily be adjusted to execute systems of greater quantities of units without drastic increases in computational consumption. In this way, we can scale our predictive understanding with adequate accuracy without compounding resources.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97898a8ef39e378f477e3b2603114c706798ea97" target='_blank'>
              Physics-Informed Neural Network for Scalable Soft Multi-Actuator Systems
              </a>
            </td>
          <td>
            Carly Mendenhall, Jonathan Hardan, Trysta D. Chiang, Laura H. Blumenschein, A. B. Tepole
          </td>
          <td>2024-04-14</td>
          <td>2024 IEEE 7th International Conference on Soft Robotics (RoboSoft)</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ee4b3249380d73a27acd2244bb01a97c229d9bc" target='_blank'>
              The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks
              </a>
            </td>
          <td>
            Lucius Bushnaq, Stefan Heimersheim, Nicholas Goldowsky-Dill, Dan Braun, Jake Mendel, Kaarel Hanni, Avery Griffin, Jorn Stohler, Magdalena Wache, Marius Hobbhahn
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="We consider a robust dynamic event-driven control (EDC) problem of nonlinear systems having both unmatched perturbations and unknown styles of constraints. Specifically, the constraints imposed on the nonlinear systems' input could be symmetric or asymmetric. Initially, to tackle such constraints, we construct a novel nonquadratic cost function for the constrained auxiliary system. Then, we propose a dynamic event-triggering mechanism relied on the time-based variable and the system states simultaneously for cutting down the computational load. Meanwhile, we show that the robust dynamic EDC of original nonlinear-constrained systems could be acquired by solving the event-driven optimal control problem of the constrained auxiliary system. After that, we develop the corresponding event-driven Hamilton-Jacobi-Bellman equation, and then solve it through a unique critic neural network (CNN) in the reinforcement learning framework. To relax the persistence of excitation condition in tuning CNN's weights, we incorporate experience replay into the gradient descent method. With the aid of Lyapunov's approach, we prove that the closed-loop auxiliary system and the weight estimation error are uniformly ultimately bounded stable. Finally, two examples, including a nonlinear plant and the pendulum system, are utilized to validate the theoretical claims.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/96bf2c389a5101fc7c633266bc2217a647f303d2" target='_blank'>
              Reinforcement Learning for Robust Dynamic Event-Driven Constrained Control.
              </a>
            </td>
          <td>
            Xiong Yang, Ding Wang
          </td>
          <td>2024-05-03</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Increasing effort is put into the development of methods for learning mechanistic models from data. This task entails not only the accurate estimation of parameters, but also a suitable model structure. Recent work on the discovery of dynamical systems formulates this problem as a linear equation system. Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data. We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures. Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models. We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty. We give an outlook on how this challenge can be overcome.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69f3306c5d346d91ce086b55f87d085d98c721dd" target='_blank'>
              Towards Learning Stochastic Population Models by Gradient Descent
              </a>
            </td>
          <td>
            J. N. Kreikemeyer, Philipp Andelfinger, A. Uhrmacher
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8238b73c146d1a68f496f60a928271d8400e362e" target='_blank'>
              Information-Theoretic Generalization Bounds for Deep Neural Networks
              </a>
            </td>
          <td>
            Haiyun He, Christina Lee Yu, Ziv Goldfeld
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>18</td>
        </tr>

        <tr id="Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports"weak linear connectivity"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim"strong linear connectivity"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2bc1a2330df6d2ca3b891402b9f46e21d2b8b01c" target='_blank'>
              Simultaneous linear connectivity of neural networks modulo permutation
              </a>
            </td>
          <td>
            Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick, G. Dziugaite
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>23</td>
        </tr>

        <tr id="In the simulation of differential-algebraic equations (DAEs), it is essential to employ numerical schemes that take into account the inherent structure and maintain explicit or hidden algebraic constraints without altering them. This paper focuses on operator-splitting techniques for coupled systems and aims at preserving the structure in the port-Hamiltonian framework. The study explores two decomposition strategies: one considering the underlying coupled subsystem structure and the other addressing energy-associated properties such as conservation and dissipation. We show that for coupled index-$1$ DAEs with and without private index-2 variables, the splitting schemes on top of a dimension-reducing decomposition achieve the same convergence rate as in the case of ordinary differential equations. Additionally, we discuss an energy-associated decomposition for index-1 pH-DAEs and introduce generalized Cayley transforms to uphold energy conservation. The effectiveness of both strategies is evaluated using port-Hamiltonian benchmark examples from electric circuits.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be526fbf5508663f37cdbaec1f28b6a054ceb0ab" target='_blank'>
              Splitting Techniques for DAEs with port-Hamiltonian Applications
              </a>
            </td>
          <td>
            Andreas Bartel, M. Diab, Andreas Frommer, Michael Günther, N. Marheineke
          </td>
          <td>2024-04-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems. GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks. This work studies a GGN method for optimizing a two-layer neural network with explicit regularization. In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem. This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance. We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters. Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network. The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7fa9754d09446870ffbb8c78af1c076d43eba2d7" target='_blank'>
              Regularized Gauss-Newton for Optimizing Overparameterized Neural Networks
              </a>
            </td>
          <td>
            Adeyemi Damilare Adeoye, Philipp Christian Petersen, Alberto Bemporad
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Over the last decade, data-driven methods have surged in popularity, emerging as valuable tools for control theory. As such, neural network approximations of control feedback laws, system dynamics, and even Lyapunov functions have attracted growing attention. With the ascent of learning based control, the need for accurate, fast, and easy-to-use benchmarks has increased. In this work, we present the first learning-based environment for boundary control of PDEs. In our benchmark, we introduce three foundational PDE problems - a 1D transport PDE, a 1D reaction-diffusion PDE, and a 2D Navier-Stokes PDE - whose solvers are bundled in an user-friendly reinforcement learning gym. With this gym, we then present the first set of model-free, reinforcement learning algorithms for solving this series of benchmark problems, achieving stability, although at a higher cost compared to model-based PDE backstepping. With the set of benchmark environments and detailed examples, this work significantly lowers the barrier to entry for learning-based PDE control - a topic largely unexplored by the data-driven control community. The entire benchmark is available on Github along with detailed documentation and the presented reinforcement learning models are open sourced.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc596b60ea20e2fdbd4665420536af246f4e65eb" target='_blank'>
              PDE Control Gym: A Benchmark for Data-Driven Boundary Control of Partial Differential Equations
              </a>
            </td>
          <td>
            Luke Bhan, Yuexin Bian, Miroslav Krstic, Yuanyuan Shi
          </td>
          <td>2024-05-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control benchmarks. The underlying coarse action space discretizations often yield favourable exploration characteristics while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and energy efficiency, but action costs can be detrimental to exploration during early training. In this work, we aim to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution, taking advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that yield surprisingly strong performance on continuous control tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ea24172e7917c94dcc8d5f5794b0ae23a2cf5e6" target='_blank'>
              Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution
              </a>
            </td>
          <td>
            Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela Rus
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="This contribution proposes a variational symplectic integrator aimed at linear systems issued from the least action principle. An internal quadratic finite-element interpolation of the state is performed at each time step. Then, the action is approximated by Simpson’s quadrature formula. The implemented scheme is implicit, symplectic, and conditionally stable. It is applied to the time integration of systems with quadratic Lagrangians. The example of the linearized double pendulum is treated. Our method is compared with Newmark’s variational integrator. The exact solution of the linearized double pendulum example is used for benchmarking. Simulation results illustrate the precision and convergence of the proposed integrator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3bfd2889feb3b75a1b46c060ea41498bcf0cc1e" target='_blank'>
              Simpson’s Variational Integrator for Systems with Quadratic Lagrangians
              </a>
            </td>
          <td>
            Juan Antonio Rojas-Quintero, François Dubois, José Guadalupe Cabrera-Díaz
          </td>
          <td>2024-04-11</td>
          <td>Axioms</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This study introduces a computational approach leveraging Physics-Informed Neural Networks (PINNs) for the efficient computation of arterial blood flows, particularly focusing on solving the incompressible Navier-Stokes equations by using the domain decomposition technique. Unlike conventional computational fluid dynamics methods, PINNs offer advantages by eliminating the need for discretized meshes and enabling the direct solution of partial differential equations (PDEs). In this paper, we propose the weighted Extended Physics-Informed Neural Networks (WXPINNs) and weighted Conservative Physics-Informed Neural Networks (WCPINNs), tailored for detailed hemodynamic simulations based on generalized space-time domain decomposition techniques. The inclusion of multiple neural networks enhances the representation capacity of the weighted PINN methods. Furthermore, the weighted PINNs can be efficiently trained in parallel computing frameworks by employing separate neural networks for each sub-domain. We show that PINNs simulation results circumvent backflow instabilities, underscoring a notable advantage of employing PINNs over traditional numerical methods to solve such complex blood flow models. They naturally address such challenges within their formulations. The presented numerical results demonstrate that the proposed weighted PINNs outperform traditional PINNs settings, where sub-PINNs are applied to each subdomain separately. This study contributes to the integration of deep learning methodologies with fluid mechanics, paving the way for accurate and efficient high-fidelity simulations in biomedical applications, particularly in modeling arterial blood flow.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/12cdb7499810597d8960adf74360b4aee67e77d0" target='_blank'>
              Enhancing Arterial Blood Flow Simulations through Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Shivam Bhargava, Nagaiah Chamakuri
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2a5b8158db29854109275cb5c3fbcf47c080c1c" target='_blank'>
              Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting
              </a>
            </td>
          <td>
            Christian Klotergens, Vijaya Krishna Yalavarthi, Maximilian Stubbemann, Lars Schmidt-Thieme
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory for its occurring mechanism in deep learning remains yet to be established. In this study, we revisited the phenomenon of double descent and discussed the conditions of its occurrence. This paper introduces the concept of class-activation matrices and a methodology for estimating the effective complexity of functions, on which we unveil that over-parameterized models exhibit more distinct and simpler class patterns in hidden activations compared to under-parameterized ones. We further looked into the interpolation of noisy labelled data among clean representations and demonstrated overfitting w.r.t. expressive capacity. By comprehensively analysing hypotheses and presenting corresponding empirical evidence that either validates or contradicts these hypotheses, we aim to provide fresh insights into the phenomenon of double descent and benign over-parameterization and facilitate future explorations. By comprehensively studying different hypotheses and the corresponding empirical evidence either supports or challenges these hypotheses, our goal is to offer new insights into the phenomena of double descent and benign over-parameterization, thereby enabling further explorations in the field. The source code is available at https://github.com/Yufei-Gu-451/sparse-generalization.git.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e78c3949c265e680727c71aceb69ac0685b16e9" target='_blank'>
              Class-wise Activation Unravelling the Engima of Deep Double Descent
              </a>
            </td>
          <td>
            Yufei Gu
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We introduce and test methods for the calibration of the diffusion term in Stochastic Partial Differential Equations (SPDEs) describing fluids. We take two approaches, one uses ideas from the singular value decomposition and the Biot-Savart law. The other backpropagates through an ensemble forecast, with respect to diffusion parameters, to minimise a probabilistic ensemble forecasting metric. We describe the approaches in the specific context of solutions to SPDEs describing the evolution of fluid particles, sometimes called inviscid vortex methods. The methods are tested in an idealised setting in which the reference data is a known realisation of the parameterised SPDE, and also using a forecast verification metric known as the Continuous Rank Probability Score (CRPS).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/444b3afa4a3893a01b0afe46863898beef23cd5f" target='_blank'>
              Stochastic fluids with transport noise: Approximating diffusion from data using SVD and ensemble forecast back-propagation
              </a>
            </td>
          <td>
            James Woodfield
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The largest eigenvalue of the Hessian, or sharpness, of neural networks is a key quantity to understand their optimization dynamics. In this paper, we study the sharpness of deep linear networks for overdetermined univariate regression. Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one. Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth. We then study the properties of the minimizer found by gradient flow, which is the limit of gradient descent with vanishing learning rate. We show an implicit regularization towards flat minima: the sharpness of the minimizer is no more than a constant times the lower bound. The constant depends on the condition number of the data covariance matrix, but not on width or depth. This result is proven both for a small-scale initialization and a residual initialization. Results of independent interest are shown in both cases. For small-scale initialization, we show that the learned weight matrices are approximately rank-one and that their singular vectors align. For residual initialization, convergence of the gradient flow for a Gaussian initialization of the residual network is proven. Numerical experiments illustrate our results and connect them to gradient descent with non-vanishing learning rate.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2abd335f39acda4ff35276ac159643b761b4c1c7" target='_blank'>
              Deep linear networks for regression are implicitly regularized towards flat minima
              </a>
            </td>
          <td>
            Pierre Marion, L'enaic Chizat
          </td>
          <td>2024-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Investigating solutions of nonlinear equation systems is challenging in a general framework, especially if the equations contain uncertainties about parameters modeled by probability densities. Such random equations, understood as stationary (non-dynamical) equations with parameters as random variables, have a long history and a broad range of applications. In this work, we study nonlinear random equations by combining them with mixture model parameter random variables in order to investigate the combinatorial complexity of such equations and how this can be utilized practically. We derive a general likelihood function and posterior density of approximate best fit solutions while avoiding significant restrictions about the type of nonlinearity or mixture models, and demonstrate their numerically efficient application for the applied researcher. In the results section we are specifically focusing on example simulations of approximate likelihood/posterior solutions for random linear equation systems, nonlinear systems of random conic section equations, as well as applications to portfolio optimization, stochastic control and random matrix theory in order to show the wide applicability of the presented methodology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2312b88e43b170ea531c9719a6dd0c4b2e4ab1c7" target='_blank'>
              Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework
              </a>
            </td>
          <td>
            Wolfgang Hoegele
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024'],
    y: [35],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>