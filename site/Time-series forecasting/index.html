<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../Symbolic%20regression/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Time-series forecasting
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Symbolic%20regression/" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Neural%20ODEs/" class="md-tabs__link">
        
  
    
  
  Neural ODEs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Physics-based%20GNNs/" class="md-tabs__link">
        
  
    
  
  Physics-based GNNs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Latent%20space%20simulators/" class="md-tabs__link">
        
  
    
  
  Latent space simulators

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Parametrizing%20using%20ML/" class="md-tabs__link">
        
  
    
  
  Parametrizing using ML

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../PINNs/" class="md-tabs__link">
        
  
    
  
  PINNs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Koopman%20operator/" class="md-tabs__link">
        
  
    
  
  Koopman operator

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Symbolic%20regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20ODEs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural ODEs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Physics-based%20GNNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physics-based GNNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Latent%20space%20simulators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent space simulators
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parametrizing%20using%20ML/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizing using ML
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../PINNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PINNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Koopman%20operator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Koopman operator
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Time-series forecasting</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-05-21 05:05:12 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Time-series forecasting</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Time-series forecasting</a><br>
      <a href="#recommended_articles">3. Recommended articles on Time-series forecasting</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Time-series forecasting</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Time-series forecasting</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9" target='_blank'>
                A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection
                </a>
              </td>
          <td>
            Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, C. Alippi, G. I. Webb, Irwin King, Shirui Pan
          </td>
          <td>2023-07-07</td>
          <td>arXiv.org, ArXiv</td>
          <td>41</td>
          <td>49</td>

            <td><a href='../recommendations/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/455bfc515eb279cc09023faa1f78c6efb61224ba" target='_blank'>
                Graph-Guided Network for Irregularly Sampled Multivariate Time Series
                </a>
              </td>
          <td>
            , M. Zeman, Theodoros Tsiligkaridis, M. Zitnik
          </td>
          <td>2021-10-11</td>
          <td>ArXiv, International Conference on Learning Representations</td>
          <td>54</td>
          <td>45</td>

            <td><a href='../recommendations/455bfc515eb279cc09023faa1f78c6efb61224ba' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Spatiotemporal graph neural networks have shown to be effective in time series forecasting applications, achieving better performance than standard univariate predictors in several settings. These architectures take advantage of a graph structure and relational inductive biases to learn a single (global) inductive model to predict any number of the input time series, each associated with a graph node. Despite the gain achieved in computational and data efficiency w.r.t. fitting a set of local models, relying on a single global model can be a limitation whenever some of the time series are generated by a different spatiotemporal stochastic process. The main objective of this paper is to understand the interplay between globality and locality in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures. We ascribe to trainable node embeddings the role of amortizing the learning of specialized components. Moreover, embeddings allow for 1) effectively combining the advantages of shared message-passing layers with node-specific parameters and 2) efficiently transferring the learned model to new node sets. Supported by strong empirical evidence, we provide insights and guidelines for specializing graph-based models to the dynamics of each time series and show how this aspect plays a crucial role in obtaining accurate predictions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2a83369383aff37224170c1ae3d3870d5d9e419" target='_blank'>
                Taming Local Effects in Graph-based Spatiotemporal Forecasting
                </a>
              </td>
          <td>
            Andrea Cini, Ivan Marisca, Daniele Zambon, C. Alippi
          </td>
          <td>2023-02-08</td>
          <td>ArXiv, Neural Information Processing Systems</td>
          <td>11</td>
          <td>49</td>

            <td><a href='../recommendations/e2a83369383aff37224170c1ae3d3870d5d9e419' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning problem, we are able to achieve state-of-the-art performance while controlling the sparsity of the learned graph and the computational scalability. We empirically assess the effectiveness of the proposed method on synthetic and real-world benchmarks, showing that the proposed solution can be used as a stand-alone graph identification procedure as well as a graph learning component of an end-to-end forecasting architecture.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d01d21137a5af9f04e4b16a55a0f732cb8a540b" target='_blank'>
                Sparse Graph Learning from Spatiotemporal Time Series
                </a>
              </td>
          <td>
            Andrea Cini, Daniele Zambon, C. Alippi
          </td>
          <td>2022-05-26</td>
          <td>Journal of machine learning research, J. Mach. Learn. Res.</td>
          <td>7</td>
          <td>49</td>

            <td><a href='../recommendations/0d01d21137a5af9f04e4b16a55a0f732cb8a540b' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Graph-based deep learning methods have become popular tools to process collections of correlated time series. Differently from traditional multivariate forecasting methods, neural graph-based predictors take advantage of pairwise relationships by conditioning forecasts on a (possibly dynamic) graph spanning the time series collection. The conditioning can take the form of an architectural inductive bias on the neural forecasting architecture, resulting in a family of deep learning models called spatiotemporal graph neural networks. Such relational inductive biases enable the training of global forecasting models on large time-series collections, while at the same time localizing predictions w.r.t. each element in the set (i.e., graph nodes) by accounting for local correlations among them (i.e., graph edges). Indeed, recent theoretical and practical advances in graph neural networks and deep learning for time series forecasting make the adoption of such processing frameworks appealing and timely. However, most of the studies in the literature focus on proposing variations of existing neural architectures by taking advantage of modern deep learning practices, while foundational and methodological aspects have not been subject to systematic investigation. To fill the gap, this paper aims to introduce a comprehensive methodological framework that formalizes the forecasting problem and provides design principles for graph-based predictive models and methods to assess their performance. At the same time, together with an overview of the field, we provide design guidelines, recommendations, and best practices, as well as an in-depth discussion of open challenges and future research directions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ccea298edb788edf821aef58f0952c3e8debc25a" target='_blank'>
                Graph Deep Learning for Time Series Forecasting
                </a>
              </td>
          <td>
            Andrea Cini, Ivan Marisca, Daniele Zambon, C. Alippi
          </td>
          <td>2023-10-24</td>
          <td>arXiv.org, ArXiv</td>
          <td>2</td>
          <td>49</td>

            <td><a href='../recommendations/ccea298edb788edf821aef58f0952c3e8debc25a' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/123acfbccca0460171b6b06a4012dbb991cde55b" target='_blank'>
                Large Language Models Are Zero-Shot Time Series Forecasters
                </a>
              </td>
          <td>
            Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson
          </td>
          <td>2023-10-11</td>
          <td>ArXiv, Neural Information Processing Systems</td>
          <td>67</td>
          <td>13</td>

            <td><a href='../recommendations/123acfbccca0460171b6b06a4012dbb991cde55b' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1df04f33a8ef313cc2067147dbb79c3ca7c5c99f" target='_blank'>
                Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces
                </a>
              </td>
          <td>
            Chloe X. Wang, Oleksii Tsepa, Jun Ma, Bo Wang
          </td>
          <td>2024-02-01</td>
          <td>arXiv.org, ArXiv</td>
          <td>18</td>
          <td>7</td>

            <td><a href='../recommendations/1df04f33a8ef313cc2067147dbb79c3ca7c5c99f' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f45f85fa1beaa795c24c4ff86f1f2deece72252f" target='_blank'>
                A decoder-only foundation model for time-series forecasting
                </a>
              </td>
          <td>
            Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou
          </td>
          <td>2023-10-14</td>
          <td>arXiv.org, ArXiv</td>
          <td>17</td>
          <td>13</td>

            <td><a href='../recommendations/f45f85fa1beaa795c24c4ff86f1f2deece72252f' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrates superior performance compared to task-specific models and repurposed natural language-based LLMs. UNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities when evaluated on new data domains and tasks. The source code and datasets are available at https://github.com/mims-harvard/UniTS.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bcbcc2e1af8bcf6b07edf866be95116a8ed0bf91" target='_blank'>
                UniTS: Building a Unified Time Series Model
                </a>
              </td>
          <td>
            Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, M. Zitnik
          </td>
          <td>2024-02-29</td>
          <td>arXiv.org, ArXiv</td>
          <td>3</td>
          <td>45</td>

            <td><a href='../recommendations/bcbcc2e1af8bcf6b07edf866be95116a8ed0bf91' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, model weights, and data will be released.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4a111f7a3b56d0468f13104999844885157ef17d" target='_blank'>
                Unified Training of Universal Time Series Forecasting Transformers
                </a>
              </td>
          <td>
            Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo
          </td>
          <td>2024-02-04</td>
          <td>arXiv.org, ArXiv</td>
          <td>6</td>
          <td>21</td>

            <td><a href='../recommendations/4a111f7a3b56d0468f13104999844885157ef17d' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277" target='_blank'>
                Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
                </a>
              </td>
          <td>
            Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, X. Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen
          </td>
          <td>2023-10-03</td>
          <td>arXiv.org, ArXiv</td>
          <td>61</td>
          <td>8</td>

            <td><a href='../recommendations/16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Large pre-trained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pre-training data. Consequently, there has been a recent surge in utilizing pre-trained large language models (LLMs) with token adaptations for TS forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large (~billion parameters) and do not consider cross-channel correlations. To address this, we present Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing fast and tiny general pre-trained models (<1M parameters), exclusively trained on public TS datasets, with effective transfer learning capabilities for forecasting. To tackle the complexity of pre-training on multiple datasets with varied temporal resolutions, we introduce several novel enhancements such as adaptive patching, dataset augmentation via downsampling, and resolution prefix tuning. Moreover, we employ a multi-level modeling strategy to effectively model channel correlations and infuse exogenous signals during fine-tuning, a crucial capability lacking in existing benchmarks. TTM shows significant accuracy gains (12-38\%) over popular benchmarks in few/zero-shot forecasting. It also drastically reduces the compute needs as compared to LLM-TS methods, with a 14X cut in learnable parameters, 106X less total parameters, and substantial reductions in fine-tuning (65X) and inference time (54X). In fact, TTM's zero-shot often surpasses the few-shot results in many popular benchmarks, highlighting the efficacy of our approach. Models and source code are available at https://huggingface.co/ibm/TTM">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2e1f1b8e6c1b7f4f166e15b7c674945856a51b6" target='_blank'>
                Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series
                </a>
              </td>
          <td>
            Vijay Ekambaram, Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M. Gifford, Jayant Kalagnanam
          </td>
          <td>2024-01-08</td>
          <td>arXiv.org, ArXiv</td>
          <td>2</td>
          <td>2</td>

            <td><a href='../recommendations/e2e1f1b8e6c1b7f4f166e15b7c674945856a51b6' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Pre-training on time series poses a unique challenge due to the potential mismatch between pre-training and target domains, such as shifts in temporal dynamics, fast-evolving trends, and long-range and short-cyclic effects, which can lead to poor downstream performance. While domain adaptation methods can mitigate these shifts, most methods need examples directly from the target domain, making them suboptimal for pre-training. To address this challenge, methods need to accommodate target domains with different temporal dynamics and be capable of doing so without seeing any target examples during pre-training. Relative to other modalities, in time series, we expect that time-based and frequency-based representations of the same example are located close together in the time-frequency space. To this end, we posit that time-frequency consistency (TF-C) -- embedding a time-based neighborhood of an example close to its frequency-based neighborhood -- is desirable for pre-training. Motivated by TF-C, we define a decomposable pre-training model, where the self-supervised signal is provided by the distance between time and frequency components, each individually trained by contrastive estimation. We evaluate the new method on eight datasets, including electrodiagnostic testing, human activity recognition, mechanical fault detection, and physical status monitoring. Experiments against eight state-of-the-art methods show that TF-C outperforms baselines by 15.4% (F1 score) on average in one-to-one settings (e.g., fine-tuning an EEG-pretrained model on EMG data) and by 8.4% (precision) in challenging one-to-many settings (e.g., fine-tuning an EEG-pretrained model for either hand-gesture recognition or mechanical fault prediction), reflecting the breadth of scenarios that arise in real-world applications. Code and datasets: https://github.com/mims-harvard/TFC-pretraining.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/648d90b713997a771e2c49f02cd771e8b7b10b37" target='_blank'>
                Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency
                </a>
              </td>
          <td>
            , Ziyuan Zhao, Theodoros Tsiligkaridis, M. Zitnik
          </td>
          <td>2022-06-17</td>
          <td>ArXiv, Neural Information Processing Systems</td>
          <td>128</td>
          <td>45</td>

            <td><a href='../recommendations/648d90b713997a771e2c49f02cd771e8b7b10b37' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="Unsupervised domain adaptation (UDA) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present Raincoat, the first model for both closed-set and universal domain adaptation on complex time series. Raincoat addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, Raincoat improves transferability by identifying label shifts in target domains. Our experiments with 5 datasets and 13 state-of-the-art UDA methods demonstrate that Raincoat can improve transfer learning performance by up to 16.33% and can handle both closed-set and universal domain adaptation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5bd2c0acaf58c25f71617db2396188c74d29bf14" target='_blank'>
                Domain Adaptation for Time Series Under Feature and Label Shifts
                </a>
              </td>
          <td>
            Huan He, Owen Queen, Teddy Koker, Consuelo Cuevas, Theodoros Tsiligkaridis, M. Zitnik
          </td>
          <td>2023-02-06</td>
          <td>DBLP, ArXiv</td>
          <td>18</td>
          <td>45</td>

            <td><a href='../recommendations/5bd2c0acaf58c25f71617db2396188c74d29bf14' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We present the ﬁrst whiteness hypothesis test for graphs, i.e., a whiteness test for multivariate time series associated with the nodes of a dynamic graph; as such, the test represents an important model assessment tool for graph deep learning, e.g., in forecasting setups. The statistical test aims at detecting existing serial dependencies among close-in-time observations, as well as spatial dependencies among neighboring observations given the underlying graph. The proposed AZ-test can be intended as a spatio-temporal extension of traditional tests designed for system identiﬁcation to graph signals. The AZ-test is versatile, allowing the underlying graph to be dynamic, changing in topology and set of nodes over time, and weighted, thus accounting for connections of different strength, as it is the case in many application scenarios like sensor and transportation networks. The asymptotic distribution of the designed test can be derived under the null hypothesis without assuming identically distributed data. We show the effectiveness of the test on both synthetic and real-world problems, and illustrate how it can be employed to assess the quality of spatio-temporal forecasting models by analyzing the prediction residuals appended to the graph stream.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3c94ccc094dcf546e8e31c9a42506302e837524" target='_blank'>
                AZ-whiteness test: a test for signal uncorrelation on spatio-temporal graphs
                </a>
              </td>
          <td>
            Daniele Zambon, C. Alippi
          </td>
          <td>None</td>
          <td>DBLP</td>
          <td>6</td>
          <td>49</td>

            <td><a href='../recommendations/c3c94ccc094dcf546e8e31c9a42506302e837524' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="State-space models constitute an effective modeling tool to describe multivariate time series and operate by maintaining an updated representation of the system state from which predictions are made. Within this framework, relational inductive biases, e.g., associated with functional dependencies existing among signals, are not explicitly exploited leaving unattended great opportunities for effective modeling approaches. The manuscript aims, for the first time, at filling this gap by matching state-space modeling and spatio-temporal data where the relational information, say the functional graph capturing latent dependencies, is learned directly from data and is allowed to change over time. Within a probabilistic formulation that accounts for the uncertainty in the data-generating process, an encoder-decoder architecture is proposed to learn the state-space model end-to-end on a downstream task. The proposed methodological framework generalizes several state-of-the-art methods and demonstrates to be effective in extracting meaningful relational information while achieving optimal forecasting performance in controlled environments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/279cd637b7e38bba1dd8915b5ce68cbcacecbe68" target='_blank'>
                Graph state-space models
                </a>
              </td>
          <td>
            Daniele Zambon, Andrea Cini, L. Livi, C. Alippi
          </td>
          <td>2023-01-04</td>
          <td>arXiv.org, ArXiv</td>
          <td>2</td>
          <td>49</td>

            <td><a href='../recommendations/279cd637b7e38bba1dd8915b5ce68cbcacecbe68' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Time-series forecasting'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Time-series forecasting</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/522828823ff726598675971036fff01b57320578" target='_blank'>
              A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model
              </a>
            </td>
          <td>
            Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, F. Tsung
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal convolutional model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature representation and to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive Convolution Block and leverage self-supervised learning to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and anomaly detection, showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at https://github.com/emadeldeen24/TSLANet.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe0d1bea9a852b35261ef7b9975429d2e61a2485" target='_blank'>
              TSLANet: Rethinking Transformers for Time Series Representation Learning
              </a>
            </td>
          <td>
            Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="
 Spatio-Temporal Graph Neural Network (STGNN) has been used as a common workhorse for traffic forecasting. However, most of them require prohibitive quadratic computational complexity to capture long-range spatio-temporal dependencies, thus hindering their applications to long historical sequences on large-scale road networks in the real-world. To this end, in this paper, we propose BigST, a linear complexity spatio-temporal graph neural network, to efficiently exploit long-range spatio-temporal dependencies for large-scale traffic forecasting. Specifically, we first propose a scalable long sequence feature extractor to encode node-wise long-range inputs (
 e.g.
 , thousands of time-steps in the past week) into low-dimensional representations encompassing rich temporal dynamics. The resulting representations can be pre-computed and hence significantly reduce the computational overhead for prediction. Then, we build a linearized global spatial convolution network to adaptively distill time-varying graph structures, which enables fast runtime message passing along spatial dimensions in linear complexity. We empirically evaluate our model on two large-scale real-world traffic datasets. Extensive experiments demonstrate that BigST can scale to road networks with up to one hundred thousand nodes, while significantly improving prediction accuracy and efficiency compared to state-of-the-art traffic forecasting models.
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10267df58122e8183d1ed151c574e3ccf33764c8" target='_blank'>
              BigST: Linear Complexity Spatio-Temporal Graph Neural Network for Traffic Forecasting on Large-Scale Road Networks
              </a>
            </td>
          <td>
            Jindong Han, Weijiao Zhang, Hao Liu, Tao Tao, Naiqiang Tan, Hui Xiong
          </td>
          <td>2024-01-01</td>
          <td>Proc. VLDB Endow.</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Real-world time series often exhibit complex interdependencies that cannot be captured in isolation. Global models that model past data from multiple related time series globally while producing series-specific forecasts locally are now common. However, their forecasts for each individual series remain isolated, failing to account for the current state of its neighbouring series. Multivariate models like multivariate attention and graph neural networks can explicitly incorporate inter-series information, thus addressing the shortcomings of global models. However, these techniques exhibit quadratic complexity per timestep, limiting scalability. This paper introduces the Context Neural Network, an efficient linear complexity approach for augmenting time series models with relevant contextual insights from neighbouring time series without significant computational overhead. The proposed method enriches predictive models by providing the target series with real-time information from its neighbours, addressing the limitations of global models, yet remaining computationally tractable for large datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cd3a5091d16b3c568041c3d40f0a8e254a91f553" target='_blank'>
              Context Neural Networks: A Scalable Multivariate Model for Time Series Forecasting
              </a>
            </td>
          <td>
            Abishek Sriramulu, Christoph Bergmeir, Slawek Smyl
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Multivariate time series forecasting tasks are usually conducted in a channel-dependent (CD) way since it can incorporate more variable-relevant information. However, it may also involve a lot of irrelevant variables, and this even leads to worse performance than the channel-independent (CI) strategy. This paper combines the strengths of both strategies and proposes the Deep Graph Clustering Transformer (DGCformer) for multivariate time series forecasting. Specifically, it first groups these relevant variables by a graph convolutional network integrated with an autoencoder, and a former-latter masked self-attention mechanism is then considered with the CD strategy being applied to each group of variables while the CI one for different groups. Extensive experimental results on eight datasets demonstrate the superiority of our method against state-of-the-art models, and our code will be publicly available upon acceptance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a0aa5ee7596c85b2364e9aeff57f639653e17cec" target='_blank'>
              DGCformer: Deep Graph Clustering Transformer for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Qinshuo Liu, Yanwen Fang, Pengtao Jiang, Guodong Li
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adaptable Channel Clustering Module (CCM). CCM dynamically groups channels characterized by intrinsic similarities and leverages cluster identity instead of channel identity, combining the best of CD and CI worlds. Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable zero-shot forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/39fbce781d28b36ad41bd897e0a0ca71a285351a" target='_blank'>
              From Similarity to Superiority: Channel Clustering for Time Series Forecasting
              </a>
            </td>
          <td>
            Jialin Chen, J. E. Lenssen, Aosong Feng, Weihua Hu, Matthias Fey, L. Tassiulas, J. Leskovec, Rex Ying
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Time series forecasting is an essential area of machine learning with a wide range of real-world applications. Most of the previous forecasting models aim to capture dynamic characteristics from uni-modal numerical historical data. Although extra knowledge can boost the time series forecasting performance, it is hard to collect such information. In addition, how to fuse the multimodal information is non-trivial. In this paper, we first propose a general principle of collecting the corresponding textual information from different data sources with the help of modern large language models (LLM). Then, we propose a prompt-based LLM framework to utilize both the numerical data and the textual information simultaneously, named GPT4MTS. In practice, we propose a GDELT-based multimodal time series dataset for news impact forecasting, which provides a concise and well-structured version of time series dataset with textual information for further research in communication. Through extensive experiments, we demonstrate the effectiveness of our proposed method on forecasting tasks with extra-textual information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/34eedbb011e45d80045cadebaf1d01b2ddec22a1" target='_blank'>
              GPT4MTS: Prompt-based Large Language Model for Multimodal Time-series Forecasting
              </a>
            </td>
          <td>
            Furong Jia, Kevin Wang, Yixiang Zheng, Defu Cao, Yan Liu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>3</td>
          <td>8</td>
        </tr>

        <tr id="We introduce LatentTimePFN (LaT-PFN), a foundational Time Series model with a strong embedding space that enables zero-shot forecasting. To achieve this, we perform in-context learning in latent space utilizing a novel integration of the Prior-data Fitted Networks (PFN) and Joint Embedding Predictive Architecture (JEPA) frameworks. We leverage the JEPA framework to create a prediction-optimized latent representation of the underlying stochastic process that generates time series and combines it with contextual learning, using a PFN. Furthermore, we improve on preceding works by utilizing related time series as a context and introducing an abstract time axis. This drastically reduces training time and increases the versatility of the model by allowing any time granularity and forecast horizon. We show that this results in superior zero-shot predictions compared to established baselines. We also demonstrate our latent space produces informative embeddings of both individual time steps and fixed-length summaries of entire series. Finally, we observe the emergence of multi-step patch embeddings without explicit training, suggesting the model actively learns discrete tokens that encode local structures in the data, analogous to vision transformers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9591b27d4cc59490b8dea5b42e6529eef836a6e0" target='_blank'>
              LaT-PFN: A Joint Embedding Predictive Architecture for In-context Time-series Forecasting
              </a>
            </td>
          <td>
            Stijn Verdenius, Andrea Zerio, Roy L.M. Wang
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Accurate forecasting of long-term time series has important applications for decision making and planning. However, it remains challenging to capture the long-term dependencies in time series data. To better extract long-term dependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a method that utilizes a shallow dilated convolution architecture to capture the period and trend characteristics of long time series. We design different convolution blocks with exponentially growing dilations and varying kernel sizes to sample time series data at different scales. Furthermore, we utilize traditional autoregressive model to capture the linear relationships within the data. To validate the effectiveness of the proposed approach, we conduct experiments on eight challenging long-term time series forecasting benchmark datasets. The experimental results show that our approach outperforms the prior state-of-the-art approaches and shows significant inference speed improvements compared to several strong baseline methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a74519642e2502c329cf147a1dcf7144547c7b3" target='_blank'>
              Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting
              </a>
            </td>
          <td>
            Feifei Li, Suhan Guo, Feng Han, Jian Zhao, F. Shen
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5ba65dfca3f1f588cc1ddd85e4fbbffde895b71d" target='_blank'>
              Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask
              </a>
            </td>
          <td>
            Zineb Senane, Lele Cao, V. Buchner, Yusuke Tashiro, Lei You, Pawel Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Time series forecasting is an important problem and plays a key role in a variety of applications including weather forecasting, stock market, and scientific simulations. Although transformers have proven to be effective in capturing dependency, its quadratic complexity of attention mechanism prevents its further adoption in long-range time series forecasting, thus limiting them attend to short-range range. Recent progress on state space models (SSMs) have shown impressive performance on modeling long range dependency due to their subquadratic complexity. Mamba, as a representative SSM, enjoys linear time complexity and has achieved strong scalability on tasks that requires scaling to long sequences, such as language, audio, and genomics. In this paper, we propose to leverage a hybrid framework Mambaformer that internally combines Mamba for long-range dependency, and Transformer for short range dependency, for long-short range forecasting. To the best of our knowledge, this is the first paper to combine Mamba and Transformer architecture in time series data. We investigate possible hybrid architectures to combine Mamba layer and attention layer for long-short range time series forecasting. The comparative study shows that the Mambaformer family can outperform Mamba and Transformer in long-short range time series forecasting problem. The code is available at https://github.com/XiongxiaoXu/Mambaformerin-Time-Series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/05235e906d3a0e8e421eca19187779550b310650" target='_blank'>
              Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting
              </a>
            </td>
          <td>
            Xiongxiao Xu, Yueqing Liang, Baixiang Huang, Zhiling Lan, Kai Shu
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>4</td>
        </tr>

        <tr id="In the ever-evolving landscape of data analysis, the need to efficiently and accurately interpret multimodal time series data has become paramount. Traditional methods often fall short in addressing the complex dependencies and dynamics inherent in such data, limiting their effectiveness in real-world applications. This work introduces a comprehensive approach that leverages Graph Attention Networks (GATs), Variational Graph Autoencoders (VGAEs), transfer learning with pretrained transformers, and Bayesian state-space models to overcome these limitations. GATs are selected for their ability to dynamically focus on relevant modalities through attention mechanisms, thereby capturing the intricate relationships between different data modalities. This method significantly enhances the model's ability to integrate multimodal information, leading to notable improvements in classification, prediction, and anomaly detection tasks. VGAEs are utilized to learn latent representations within a graph-based framework, promoting unsupervised learning while unveiling the underlying data structure. The resultant embeddings are pivotal for downstream tasks like clustering and visualization, encapsulating the interactions within multimodal time series data effectively. Furthermore, this work incorporates transfer learning with pretrained transformers to harness extensive knowledge from large datasets, adapting it to multimodal time series analysis. This strategy excels in capturing long-range dependencies, thereby augmenting generalization and performance in data-scarce scenarios. Bayesian state-space models are employed to elucidate the temporal dynamics and uncertainties of time series data, offering a robust framework for probabilistic inference and enhancing the interpretability and reliability of forecasting and anomaly detection. The efficacy of the proposed model is rigorously evaluated using diverse datasets, including the Yahoo! Stock Dataset, Forest Cover Dataset, and an empirical collection of 100k time series data samples. The results demonstrate a significant leap in performance metrics, including a 9.5% increase in precision, 8.5% boost in accuracy, 8.3% rise in recall, 10.4% reduction in delay, 9.4% enhancement in AUC, and a 5.9% improvement in specificity, alongside superior pre-emption capabilities compared to existing methods. This work not only addresses the pressing need for advanced multimodal time series analysis techniques but also sets a new benchmark for efficiency and accuracy. The integration of GATs, VGAEs, transfer learning with pretrained transformers, and Bayesian state-space models presents a formidable approach that significantly advances the field, offering profound impacts on a wide array of applications. ">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/84eca9657239094f2ed73fed20bfc27e8edd4bde" target='_blank'>
              Design of an Iterative Method for Enhanced Multimodal Time Series Analysis Using Graph Attention Networks, Variational Graph Autoencoders, and Transfer Learning
              </a>
            </td>
          <td>
            Vijaya Kamble, Dr. Sanjay Bhargava
          </td>
          <td>2024-04-13</td>
          <td>Journal of Electrical Systems</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have emerged as a powerful tool for effectively mining and learning from graph-structured data, with applications spanning numerous domains. However, most research focuses on static graphs, neglecting the dynamic nature of real-world networks where topologies and attributes evolve over time. By integrating sequence modeling modules into traditional GNN architectures, dynamic GNNs aim to bridge this gap, capturing the inherent temporal dependencies of dynamic graphs for a more authentic depiction of complex networks. This paper provides a comprehensive review of the fundamental concepts, key techniques, and state-of-the-art dynamic GNN models. We present the mainstream dynamic GNN models in detail and categorize models based on how temporal information is incorporated. We also discuss large-scale dynamic GNNs and pre-training techniques. Although dynamic GNNs have shown superior performance, challenges remain in scalability, handling heterogeneous information, and lack of diverse graph datasets. The paper also discusses possible future directions, such as adaptive and memory-enhanced models, inductive learning, and theoretical analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/34d58238ca6b9b12f9bfb4a0f665736634165f80" target='_blank'>
              A survey of dynamic graph neural networks
              </a>
            </td>
          <td>
            Yanping Zheng, Lu Yi, Zhewei Wei
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Deep learning methods have been exerting their strengths in long-term time series forecasting. However, they often struggle to strike a balance between expressive power and computational efficiency. Here, we propose the Coarsened Perceptron Network (CP-Net), a novel architecture that efficiently enhances the predictive capability of MLPs while maintains a linear computational complexity. It utilizes a coarsening strategy as the backbone that leverages two-stage convolution-based sampling blocks. Based purely on convolution, they provide the functionality of extracting short-term semantic and contextual patterns, which is relatively deficient in the global point-wise projection of the MLP layer. With the architectural simplicity and low runtime, our experiments on seven time series forecasting benchmarks demonstrate that CP-Net achieves an improvement of 4.1% compared to the SOTA method. The model further shows effective utilization of the exposed information with a consistent improvement as the look-back window expands.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7ac0da9f23f3720a05c9fbb4c2796688cf62de2" target='_blank'>
              Boosting MLPs with a Coarsening Strategy for Long-Term Time Series Forecasting
              </a>
            </td>
          <td>
            Nannan Bian, Minhong Zhu, Li Chen, Weiran Cai
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Long-term time series forecasting (LTSF) provides longer insights into future trends and patterns. Over the past few years, deep learning models especially Transformers have achieved advanced performance in LTSF tasks. However, LTSF faces inherent challenges such as long-term dependencies capturing and sparse semantic characteristics. Recently, a new state space model (SSM) named Mamba is proposed. With the selective capability on input data and the hardware-aware parallel computing algorithm, Mamba has shown great potential in balancing predicting performance and computational efficiency compared to Transformers. To enhance Mamba's ability to preserve historical information in a longer range, we design a novel Mamba+ block by adding a forget gate inside Mamba to selectively combine the new features with the historical features in a complementary manner. Furthermore, we apply Mamba+ both forward and backward and propose Bi-Mamba+, aiming to promote the model's ability to capture interactions among time series elements. Additionally, multivariate time series data in different scenarios may exhibit varying emphasis on intra- or inter-series dependencies. Therefore, we propose a series-relation-aware decider that controls the utilization of channel-independent or channel-mixing tokenization strategy for specific datasets. Extensive experiments on 8 real-world datasets show that our model achieves more accurate predictions compared with state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf4d1d11c0ee0c4335e8ae6f0e433e7ddd20be87" target='_blank'>
              Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting
              </a>
            </td>
          <td>
            Aobo Liang, Xingguo Jiang, Yan Sun, , 
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than 1k parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is available at: https://github.com/lss-1138/SparseTSF.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/896c83e6dbd42621cb5c084bfc477802ed198b14" target='_blank'>
              SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters
              </a>
            </td>
          <td>
            Shengsheng Lin, Weiwei Lin, Wentai Wu, Haojun Chen, Junjie Yang
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Spatio-temporal forecasting is crucial in real-world dynamic systems, predicting future changes using historical data from diverse locations. Existing methods often prioritize the development of intricate neural networks to capture the complex dependencies of the data, yet their accuracy fails to show sustained improvement. Besides, these methods also overlook node heterogeneity, hindering customized prediction modules from handling diverse regional nodes effectively. In this paper, our goal is not to propose a new model but to present a novel low-rank adaptation framework as an off-the-shelf plugin for existing spatial-temporal prediction models, termed ST-LoRA, which alleviates the aforementioned problems through node-level adjustments. Specifically, we first tailor a node adaptive low-rank layer comprising multiple trainable low-rank matrices. Additionally, we devise a multi-layer residual fusion stacking module, injecting the low-rank adapters into predictor modules of various models. Across six real-world traffic datasets and six different types of spatio-temporal prediction models, our approach minimally increases the parameters and training time of the original models by less than 4%, still achieving consistent and sustained performance enhancement.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5de65a413140ec11b05f8d5d6ff1d8f5f2f9ddf3" target='_blank'>
              Low-rank Adaptation for Spatio-Temporal Forecasting
              </a>
            </td>
          <td>
            Weilin Ruan, Wei Chen, Xilin Dang, Jianxiang Zhou, Weichuang Li, Xu Liu, Yuxuan Liang
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/647a42d78e5bd405014a8c878e37fb7b2da8eaa6" target='_blank'>
              Graph Machine Learning in the Era of Large Language Models (LLMs)
              </a>
            </td>
          <td>
            Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, Qing Li
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations. To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced. We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/95463223b2faba0ccf928714ba4373a07dee1860" target='_blank'>
              Temporal Graph ODEs for Irregularly-Sampled Time Series
              </a>
            </td>
          <td>
            Alessio Gravina, Daniele Zambon, Davide Bacciu, C. Alippi
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at \url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37bcad09a018c66e63f14b5b105e5ad34b441644" target='_blank'>
              A Survey of Large Language Models for Graphs
              </a>
            </td>
          <td>
            Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In the context of increasing demands for long-term multi-energy load forecasting in real-world applications, this paper introduces Patchformer, a novel model that integrates patch embedding with encoder-decoder Transformer-based architectures. To address the limitation in existing Transformer-based models, which struggle with intricate temporal patterns in long-term forecasting, Patchformer employs patch embedding, which predicts multivariate time-series data by separating it into multiple univariate data and segmenting each of them into multiple patches. This method effectively enhances the model's ability to capture local and global semantic dependencies. The numerical analysis shows that the Patchformer obtains overall better prediction accuracy in both multivariate and univariate long-term forecasting on the novel Multi-Energy dataset and other benchmark datasets. In addition, the positive effect of the interdependence among energy-related products on the performance of long-term time-series forecasting across Patchformer and other compared models is discovered, and the superiority of the Patchformer against other models is also demonstrated, which presents a significant advancement in handling the interdependence and complexities of long-term multi-energy forecasting. Lastly, Patchformer is illustrated as the only model that follows the positive correlation between model performance and the length of the past sequence, which states its ability to capture long-range past local semantic information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca175146dfa9293e72c5d6fbb902be8565960d9e" target='_blank'>
              Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A Patch and Transformer-Based Approach
              </a>
            </td>
          <td>
            Qiuyi Hong, Fanlin Meng, Felipe Maldonado
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time series clustering is fundamental in data analysis for discovering temporal patterns. Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series. Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions. In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost. In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series. Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a30bb81d366b403af18415870f68ba38fbef201" target='_blank'>
              Concrete Dense Network for Long-Sequence Time Series Clustering
              </a>
            </td>
          <td>
            Redemptor Jr Laceda Taloma, Patrizio Pisani, Danilo Comminiello
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Deep neural networks, including transformers and convolutional neural networks, have significantly improved multivariate time series classification (MTSC). However, these methods often rely on supervised learning, which does not fully account for the sparsity and locality of patterns in time series data (e.g., diseases-related anomalous points in ECG). To address this challenge, we formally reformulate MTSC as a weakly supervised problem, introducing a novel multiple-instance learning (MIL) framework for better localization of patterns of interest and modeling time dependencies within time series. Our novel approach, TimeMIL, formulates the temporal correlation and ordering within a time-aware MIL pooling, leveraging a tokenized transformer with a specialized learnable wavelet positional token. The proposed method surpassed 26 recent state-of-the-art methods, underscoring the effectiveness of the weakly supervised TimeMIL in MTSC.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e081cca01598a5a6f97cbaea71928294f2d6dfdf" target='_blank'>
              TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning
              </a>
            </td>
          <td>
            Xiwen Chen, Peijie Qiu, Wenhui Zhu, Huayu Li, Hao Wang, Aristeidis Sotiras, Yalin Wang, A. Razi
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="Multivariate time series forecasting plays a crucial role in various fields such as finance, traffic management, energy, and healthcare. Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements. Several methods utilize mechanisms like attention or mixer to address this by capturing channel correlations, but they either introduce excessive complexity or rely too heavily on the correlation to achieve satisfactory results under distribution drifts, particularly with a large number of channels. Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Dispatch (STAD) module. Unlike traditional approaches that manage channel interactions through distributed structures, e.g., attention, STAD employs a centralized strategy. It aggregates all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions effectively. SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity. The broad applicability of the STAD module across different forecasting models is also demonstrated empirically. For further research and development, we have made our code publicly available at https://github.com/Secilia-Cxy/SOFTS.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/94a310a7328b9155f0d6936dd18622f3769220d4" target='_blank'>
              SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion
              </a>
            </td>
          <td>
            Lu Han, Xu-Yang Chen, Han-Jia Ye, De-Chuan Zhan
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="The probability prediction of multivariate time series is a notoriously challenging but practical task. This research proposes to condense high-dimensional multivariate time series forecasting into a problem of latent space time series generation, to improve the expressiveness of each timestamp and make forecasting more manageable. To solve the problem that the existing work is hard to extend to high-dimensional multivariate time series, we present a latent multivariate time series diffusion framework called Latent Diffusion Transformer (LDT), which consists of a symmetric statistics-aware autoencoder and a diffusion-based conditional generator, to implement this idea. Through careful design, the time series autoencoder can compress multivariate timestamp patterns into a concise latent representation by considering dynamic statistics. Then, the diffusion-based conditional generator is able to efficiently generate realistic multivariate timestamp values on a continuous latent space under a novel self-conditioning guidance which is modeled in a non-autoregressive way. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular high-dimensional multivariate time series datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/900e91d93294a0d4f1213d57272e8801c3abdb06" target='_blank'>
              Latent Diffusion Transformer for Probabilistic Time Series Forecasting
              </a>
            </td>
          <td>
            Shibo Feng, Chunyan Miao, Zhong Zhang, Peilin Zhao
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b76e923018986a5c2a12b16573e78bf50685585d" target='_blank'>
              CVTN: Cross Variable and Temporal Integration for Time Series Forecasting
              </a>
            </td>
          <td>
            Han Zhou, Yuntian Chen
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In the post-deep learning era, the Transformer architecture has demonstrated its powerful performance across pre-trained big models and various downstream tasks. However, the enormous computational demands of this architecture have deterred many researchers. To further reduce the complexity of attention models, numerous efforts have been made to design more efficient methods. Among them, the State Space Model (SSM), as a possible replacement for the self-attention based Transformer model, has drawn more and more attention in recent years. In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM. Specifically, we first give a detailed description of principles to help the readers quickly capture the key ideas of SSM. After that, we dive into the reviews of existing SSMs and their various applications, including natural language processing, computer vision, graph, multi-modal and multi-media, point cloud/event stream, time series data, and other domains. In addition, we give statistical comparisons and analysis of these models and hope it helps the readers to understand the effectiveness of different structures on various tasks. Then, we propose possible research points in this direction to better promote the development of the theoretical model and application of SSM. More related works will be continuously updated on the following GitHub: https://github.com/Event-AHU/Mamba_State_Space_Model_Paper_List.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/02d3afd9c306ce0a0d39e211d60468732c591f8f" target='_blank'>
              State Space Model for New-Generation Network Alternative to Transformers: A Survey
              </a>
            </td>
          <td>
            Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bowei Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, Jin Tang
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>11</td>
          <td>33</td>
        </tr>

        <tr id="Multivariate time series (MTS) prediction has been widely adopted in various scenarios. Recently, some methods have employed patching to enhance local semantics and improve model performance. However, length-fixed patch are prone to losing temporal boundary information, such as complete peaks and periods. Moreover, existing methods mainly focus on modeling long-term dependencies across patches, while paying little attention to other dimensions (e.g., short-term dependencies within patches and complex interactions among cross-variavle patches). To address these challenges, we propose a pure MLP-based HDMixer, aiming to acquire patches with richer semantic information and efficiently modeling hierarchical interactions. Specifically, we design a Length-Extendable Patcher (LEP) tailored to MTS, which enriches the boundary information of patches and alleviates semantic incoherence in series. Subsequently, we devise a Hierarchical Dependency Explorer (HDE) based on pure MLPs. This explorer effectively models short-term dependencies within patches, long-term dependencies across patches, and complex interactions among variables. Extensive experiments on 9 real-world datasets demonstrate the superiority of our approach. The code is available at https://github.com/hqh0728/HDMixer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3bc5903647aef09c31a9a0866d59989e3f874eb" target='_blank'>
              HDMixer: Hierarchical Dependency with Extendable Patch for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Qihe Huang, Lei Shen, Ruixin Zhang, Jiahuan Cheng, Shouhong Ding, Zhengyang Zhou, Yang Wang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>2</td>
          <td>10</td>
        </tr>

        <tr id="Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large pre-trained or foundational models across different modalities and tasks. However, its application to time series data, particularly within foundational models, remains underexplored. This paper examines the impact of LoRA on contemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos. We demonstrate LoRA's fine-tuning potential for forecasting the vital signs of sepsis patients in intensive care units (ICUs), emphasizing the models' adaptability to previously unseen, out-of-domain modalities. Integrating LoRA aims to enhance forecasting performance while reducing inefficiencies associated with fine-tuning large models on limited domain-specific data. Our experiments show that LoRA fine-tuning of time series foundational models significantly improves forecasting, achieving results comparable to state-of-the-art models trained from scratch on similar modalities. We conduct comprehensive ablation studies to demonstrate the trade-offs between the number of tunable parameters and forecasting performance and assess the impact of varying LoRA matrix ranks on model performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c961890f89252c79293d0d363c4db8f46565e93" target='_blank'>
              Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting
              </a>
            </td>
          <td>
            Divij Gupta, Anubhav Bhatti, Surajsinh Parmar, Chen Dan, Yuwei Liu, Bingjie Shen, San Lee
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Autoregressive Recurrent Neural Networks are widely employed in time-series forecasting tasks, demonstrating effectiveness in univariate and certain multivariate scenarios. However, their inherent structure does not readily accommodate the integration of future, time-dependent covariates. A proposed solution, outlined by Salinas et al 2019, suggests forecasting both covariates and the target variable in a multivariate framework. In this study, we conducted comprehensive tests on publicly available time-series datasets, artificially introducing highly correlated covariates to future time-step values. Our evaluation aimed to assess the performance of an LSTM network when considering these covariates and compare it against a univariate baseline. As part of this study we introduce a novel approach using seasonal time segments in combination with an RNN architecture, which is both simple and extremely effective over long forecast horizons with comparable performance to many state of the art architectures. Our findings from the results of more than 120 models reveal that under certain conditions jointly training covariates with target variables can improve overall performance of the model, but often there exists a significant performance disparity between multivariate and univariate predictions. Surprisingly, even when provided with covariates informing the network about future target values, multivariate predictions exhibited inferior performance. In essence, compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates. These results suggest that LSTM architectures may not be suitable for forecasting tasks where predicting covariates would typically be expected to enhance model accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2b3cbf2271ce6843a66142f90fa8ab0d01d00c74" target='_blank'>
              Evaluating the effectiveness of predicting covariates in LSTM Networks for Time Series Forecasting
              </a>
            </td>
          <td>
            Gareth Davies
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at https://github.com/XDZhelheim/HimNet.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c09d2846ff81c5dbdf4f8662bd2d9bb4dd61b396" target='_blank'>
              Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting
              </a>
            </td>
          <td>
            , Renhe Jiang, Haotian Gao, Hangchen Liu, , Qingsong Wen, 
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time series data are prevalent in the real world, particularly playing a crucial role in key domains such as meteorology, electricity, and finance. Comprising observations at historical time points, these data, when subjected to in-depth analysis and modeling, enable researchers to predict future trends and patterns, providing support for decision making. In current research, especially in the analysis of long time series, effectively extracting and integrating long-term dependencies with short-term features remains a significant challenge. Long-term dependencies refer to the correlation between data points spaced far apart in a time series, while short-term features focus on more recent changes. Understanding and combining these two features correctly are crucial for constructing accurate and reliable predictive models. To efficiently extract and integrate long-term dependencies and short-term features in long time series, this paper proposes a pyramid attention structure model based on multi-scale feature extraction, referred to as the MSFformer model. Initially, a coarser-scale construction module is designed to obtain coarse-grained information. A pyramid data structure is constructed through feature convolution, with the bottom layer representing the original data and each subsequent layer containing feature information extracted across different time step lengths. As a result, nodes higher up in the pyramid integrate information from more time points, such as every Monday or the beginning of each month, while nodes lower down retain their individual information. Additionally, a Skip-PAM is introduced, where a node only calculates attention with its neighboring nodes, parent node, and child nodes, effectively reducing the model’s time complexity to some extent. Notably, the child nodes refer to nodes selected from the next layer by skipping specific time steps. In this study, we not only propose an innovative time series prediction model but also validate the effectiveness of these methods through a series of comprehensive experiments. To comprehensively evaluate the performance of the designed model, we conducted comparative experiments with baseline models, ablation experiments, and hyperparameter studies. The experimental results demonstrate that the MSFformer model improves by 35.87% and 42.6% on the MAE and MSE indicators, respectively, compared to traditional Transformer models. These results highlight the outstanding performance of our proposed deep learning model in handling complex time series data, particularly in capturing long-term dependencies and integrating short-term features.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/03692dc7ba3b838c6654a6f7ffefad7ac751d086" target='_blank'>
              Time Series Prediction Based on Multi-Scale Feature Extraction
              </a>
            </td>
          <td>
            Ruixue Zhang, Yongtao Hao
          </td>
          <td>2024-03-25</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Traffic flow prediction is an essential task in constructing smart cities and is a typical Multivariate Time Series (MTS) Problem. Recent research has abandoned Gated Recurrent Units (GRU) and utilized dilated convolutions or temporal slicing for feature extraction, and they have the following drawbacks: (1) Dilated convolutions fail to capture the features of adjacent time steps, resulting in the loss of crucial transitional data. (2) The connections within the same temporal slice are strong, while the connections between different temporal slices are too loose. In light of these limitations, we emphasize the importance of analyzing a complete time series repeatedly and the crucial role of GRU in MTS. Therefore, we propose SGRU: Structured Gated Recurrent Units, which involve structured GRU layers and non-linear units, along with multiple layers of time embedding to enhance the model’s fitting performance. We evaluate our approach on four publicly available California traffic datasets: PeMS03, PeMS04, PeMS07, and PeMS08 for regression prediction. Experimental results demonstrate that our model outperforms baseline models with average improvements of 11.7%, 18.6%, 18.5%, and 12.0% respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c6167174e8905cddba8d4f72d255acb8d45f8c5" target='_blank'>
              SGRU: A High-Performance Structured Gated Recurrent Unit for Traffic Flow Prediction
              </a>
            </td>
          <td>
            Wenfeng Zhang, Xin Li, Anqi Li, Xiaoting Huang, Ti Wang, Honglei Gao
          </td>
          <td>2023-12-17</td>
          <td>2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3bb7012edfaac357311eca07515d193b4cf26bb" target='_blank'>
              Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective
              </a>
            </td>
          <td>
            Binwu Wang, Pengkun Wang, Yudong Zhang, Xu Wang, Zhengyang Zhou, Lei Bai, Yang Wang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Time series (TS) forecasting has been an unprecedentedly popular problem in recent years, with ubiquitous applications in both scientific and business fields. Various approaches have been introduced to time series analysis, including both statistical approaches and deep neural networks. Although neural network approaches have illustrated stronger ability of representation than statistical methods, they struggle to provide sufficient interpretablility, and can be too complicated to optimize. In this paper, we present WEITS, a frequency-aware deep learning framework that is highly interpretable and computationally efficient. Through multi-level wavelet decomposition, WEITS novelly infuses frequency analysis into a highly deep learning framework. Combined with a forward-backward residual architecture, it enjoys both high representation capability and statistical interpretability. Extensive experiments on real-world datasets have demonstrated competitive performance of our model, along with its additional advantage of high computation efficiency. Furthermore, WEITS provides a general framework that can always seamlessly integrate with state-of-the-art approaches for time series forecast.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7893853a894dca62385494b56ed19a847f2c1b26" target='_blank'>
              WEITS: A Wavelet-enhanced residual framework for interpretable time series forecasting
              </a>
            </td>
          <td>
            Ziyou Guo, , 
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The intricate nature of time series data analysis benefits greatly from the distinct advantages offered by time and frequency domain representations. While the time domain is superior in representing local dependencies, particularly in non-periodic series, the frequency domain excels in capturing global dependencies, making it ideal for series with evident periodic patterns. To capitalize on both of these strengths, we propose ATFNet, an innovative framework that combines a time domain module and a frequency domain module to concurrently capture local and global dependencies in time series data. Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel mechanism for dynamically adjusting the weights between the two modules based on the periodicity of the input time series. In the frequency domain module, we enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT, designed to address the challenge of discrete frequency misalignment. Additionally, our Complex-valued Spectrum Attention mechanism offers a novel approach to discern the intricate relationships between different frequency combinations. Extensive experiments across multiple real-world datasets demonstrate that our ATFNet framework outperforms current state-of-the-art methods in long-term time series forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab7d47657bc68e33ecac574f87f5668936871998" target='_blank'>
              ATFNet: Adaptive Time-Frequency Ensembled Network for Long-term Time Series Forecasting
              </a>
            </td>
          <td>
            Hengyu Ye, Jiadong Chen, Shijin Gong, Fuxin Jiang, Tieying Zhang, Jianjun Chen, Xiaofeng Gao
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Traffic flow forecasting is a classical spatio-temporal data mining problem with many real-world applications. Recently, various methods based on Graph Neural Networks (GNN) have been proposed for the problem and achieved impressive prediction performance. However, we argue that the majority of existing methods disregarding the importance of certain nodes (referred to as pivotal nodes) that naturally exhibit extensive connections with multiple other nodes. Predicting on pivotal nodes poses a challenge due to their complex spatio-temporal dependencies compared to other nodes. In this paper, we propose a novel GNN-based method called Spatio-Temporal Pivotal Graph Neural Networks (STPGNN) to address the above limitation. We introduce a pivotal node identification module for identifying pivotal nodes. We propose a novel pivotal graph convolution module, enabling precise capture of spatio-temporal dependencies centered around pivotal nodes. Moreover, we propose a parallel framework capable of extracting spatio-temporal traffic features on both pivotal and non-pivotal nodes. Experiments on seven real-world traffic datasets verify our proposed method's effectiveness and efficiency compared to state-of-the-art baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e30f3e3ccb2c647a2caeaa362ac8d4b5ef67727f" target='_blank'>
              Spatio-Temporal Pivotal Graph Neural Networks for Traffic Flow Forecasting
              </a>
            </td>
          <td>
            Weiyang Kong, Ziyu Guo, Yubao Liu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In recent years, significant efforts have been made to refine the design of Graph Neural Network (GNN) layers, aiming to overcome diverse challenges, such as limited expressive power and oversmoothing. Despite their widespread adoption, the incorporation of off-the-shelf normalization layers like BatchNorm or InstanceNorm within a GNN architecture may not effectively capture the unique characteristics of graph-structured data, potentially reducing the expressive power of the overall architecture. Moreover, existing graph-specific normalization layers often struggle to offer substantial and consistent benefits. In this paper, we propose GRANOLA, a novel graph-adaptive normalization layer. Unlike existing normalization layers, GRANOLA normalizes node features by adapting to the specific characteristics of the graph, particularly by generating expressive representations of its neighborhood structure, obtained by leveraging the propagation of Random Node Features (RNF) in the graph. We present theoretical results that support our design choices. Our extensive empirical evaluation of various graph benchmarks underscores the superior performance of GRANOLA over existing normalization techniques. Furthermore, GRANOLA emerges as the top-performing method among all baselines within the same time complexity of Message Passing Neural Networks (MPNNs).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a5732c7ac2fe1ec3951541e14aa70ec8067f95ae" target='_blank'>
              GRANOLA: Adaptive Normalization for Graph Neural Networks
              </a>
            </td>
          <td>
            Moshe Eliasof, Beatrice Bevilacqua, C. Schonlieb, Haggai Maron
          </td>
          <td>2024-04-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Multi-modality spatio-temporal (MoST) data extends spatio-temporal (ST) data by incorporating multiple modalities, which is prevalent in monitoring systems, encompassing diverse traffic demands and air quality assessments. Despite significant strides in ST modeling in recent years, there remains a need to emphasize harnessing the potential of information from different modalities. Robust MoST forecasting is more challenging because it possesses (i) high-dimensional and complex internal structures and (ii) dynamic heterogeneity caused by temporal, spatial, and modality variations. In this study, we propose a novel MoST learning framework via Self-Supervised Learning, namely MoSSL, which aims to uncover latent patterns from temporal, spatial, and modality perspectives while quantifying dynamic heterogeneity. Experiment results on two real-world MoST datasets verify the superiority of our approach compared with the state-of-the-art baselines. Model implementation is available at https://github.com/beginner-sketch/MoSSL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c10d27669d165b26a1256f70ca1349c6cb5f00ea" target='_blank'>
              Multi-Modality Spatio-Temporal Forecasting via Self-Supervised Learning
              </a>
            </td>
          <td>
            Jiewen Deng, Renhe Jiang, Jiaqi Zhang, Xuan Song
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Stock price forecasting is a fundamental yet challenging task in quantitative investment. Various researchers have developed a combination of neural network models (e.g., RNNs, GNNs, Transformers) for capturing complex indicator, temporal and stock correlations of the stock data.While complex architectures are highly expressive, they are often difficult to optimize and the performances are often compromised by the limited stock data. In this paper, we propose a simple MLP-based architecture named StockMixer which is easy to optimize and enjoys strong predictive performance. StockMixer performs indicator mixing, followed by time mixing, and finally stock mixing. Unlike the standard MLP-based mixing, we devise the time mixing to exchange multi-scale time patch information and realize the stock mixing by exploiting stock-to-market and market-to-stock influences explicitly. Extensive experiments on real stock benchmarks demonstrate our proposed StockMixer outperforms various state-of-the-art forecasting methods with a notable margin while reducing memory usage and runtime cost.Code is available at https://github.com/SJTU-Quant/StockMixer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fc6f8d04e7f6a50665a4ad12f320a4853088b355" target='_blank'>
              StockMixer: A Simple Yet Strong MLP-Based Architecture for Stock Price Forecasting
              </a>
            </td>
          <td>
            Jinyong Fan, Yanyan Shen
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the"shallow"aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the"deep"aspect. After that, an interaction and fusion (IF) module is used to further analyze the components. Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d6aea8d96d1655d58d512517481c0cbe090a3417" target='_blank'>
              D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting
              </a>
            </td>
          <td>
            Xiaobing Yuan, Ling Chen
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This study presents a new deep learning framework, combining Spatio-Temporal Graph Convolutional Network (STGCN) with a Large Language Model (LLM), for bike demand forecasting. Addressing challenges in transforming discrete datasets and integrating unstructured language data, the framework leverages LLMs to extract insights from Points of Interest (POI) text data. The proposed STGCN-L model demonstrates competitive performance compared to existing models, showcasing its potential in predicting bike demand. Experiments using Philadelphia datasets highlight the effectiveness of the hybrid model, emphasizing the need for further exploration and enhancements, such as incorporating additional features like weather data for improved accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3f28bf7cb05af5293f933e134c00275d294ddb4" target='_blank'>
              Spatio-Temporal Graph Convolutional Network Combined Large Language Model: A Deep Learning Framework for Bike Demand Forecasting
              </a>
            </td>
          <td>
            Peisen Li, Yizhe Pang, Junyu Ren
          </td>
          <td>2024-03-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks. However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation. The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision. However, the time-consuming process of extracting graph tokens limits its performance. In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens. Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering. Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer. Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation. Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks. The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1b5ce2d412b1c8b15b20d862fdb94d25c76fffb8" target='_blank'>
              ChebMixer: Efficient Graph Representation Learning with MLP Mixer
              </a>
            </td>
          <td>
            Xiaoyan Kui, Haonan Yan, Qinsong Li, Liming Chen, Beiji Zou
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Time series forecasting is an important tool in many businesses. It can range from efficiently allocating resources for web traffic, predicting patient needs for staffing requirements, to forecasting a company's product sales. A particular use case, known as "cold start" forecasting, involves making predictions for time series that have little or no historical data, like a new product just entering the retail market. The key assumption of cold start forecasting is that products with similar characteristics should have similar time series trajectories. In such scenarios, traditional forecasting models that heavily rely on past observations may face challenges, necessitating the development of innovative approaches that can effectively make predictions in the absence of a substantial historical dataset.
In this paper, Temporal fusion transformer neural network architecture was applied for solving cold start time series forecasting task. Modeling of the method was based on the use of a dataset contained in an open repository. After the preprocessing procedures, the dataset has about 370 time series, each of which has different length of series and has one categorical feature. Categorical feature have only 4 types of different values. For model training was performed to search for optimal hyperparameters across such parameters as: number of attention heads, learning rate, dropout percentage and hidden size.Model performed pretty well on this task. For model comparison were chosen metrics: MAE, RMSE, SMAPE. As can be seen from comparison with such popular models as DeepAR and LSTM, the proposed approach demonstrated the smallest forecasting error. Only one downside is that it can have more problems with anomalies in time series than DeepAR. But at the same time still provide interpretability of results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b9742db5b424cd098d8b8cc0817eb3801ae20389" target='_blank'>
              TIME SERIES FORECASTING MODEL FOR SOLVING COLD START PROBLEM VIA TEMPORAL FUSION TRANSFORMER
              </a>
            </td>
          <td>
            Kyrylo Yemets
          </td>
          <td>2024-03-28</td>
          <td>Computer systems and information technologies</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded unparalleled results in predicting temporal and spatial dynamics. However, modeling extensive global information remains a formidable challenge; CNNs are limited by their narrow receptive fields, and ViTs struggle with the intensive computational demands of their attention mechanisms. The emergence of recent Mamba-based architectures has been met with enthusiasm for their exceptional long-sequence modeling capabilities, surpassing established vision models in efficiency and accuracy, which motivates us to develop an innovative architecture tailored for spatiotemporal forecasting. In this paper, we propose the VMRNN cell, a new recurrent unit that integrates the strengths of Vision Mamba blocks with LSTM. We construct a network centered on VMRNN cells to tackle spatiotemporal prediction tasks effectively. Our extensive evaluations show that our proposed approach secures competitive results on a variety of tasks while maintaining a smaller model size. Our code is available at https://github.com/yyyujintang/VMRNN-PyTorch.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7381d645f6ad239aaef2d42899515f21ece650d3" target='_blank'>
              VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting
              </a>
            </td>
          <td>
            Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>3</td>
          <td>15</td>
        </tr>

        <tr id="Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains unknown whether non-trivial forecasting implies that language models can reason about time series. To address this gap, we generate a first-of-its-kind evaluation framework for time series reasoning, including formal tasks and a corresponding dataset of multi-scale time series paired with text captions across ten domains. Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning - given an input time series, can the language model identify the scenario that most likely created it? (2) Question Answering - can a language model answer factual questions about time series? (3) Context-Aided Forecasting - does highly relevant textual context improve a language model's time series forecasts? We find that otherwise highly-capable language models demonstrate surprisingly limited time series reasoning: they score marginally above random on etiological and question answering tasks (up to 30 percentage points worse than humans) and show modest success in using context to improve forecasting. These weakness showcase that time series reasoning is an impactful, yet deeply underdeveloped direction for language model research. We also make our datasets and code public at to support further research in this direction at https://github.com/behavioral-data/TSandLanguage">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8991a260faad1a80f4f1f73e9f1fc6a63b247b0f" target='_blank'>
              Language Models Still Struggle to Zero-shot Reason about Time Series
              </a>
            </td>
          <td>
            Mike A. Merrill, Mingtian Tan, Vinayak Gupta, Tom Hartvigsen, Tim Althoff
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Sequential recommendation is a crucial task in understanding users' evolving interests and predicting their future behaviors. While existing approaches on sequence or graph modeling to learn interaction sequences of users have shown promising performance, how to effectively exploit temporal information and deal with the uncertainty noise in evolving user behaviors is still quite challenging. To this end, in this paper, we propose a Temporal Graph Contrastive Learning method for Sequential Recommendation (TGCL4SR) which leverages not only local interaction sequences but also global temporal graphs to comprehend item correlations and analyze user behaviors from a temporal perspective. Specifically, we first devise a Temporal Item Transition Graph (TITG) to fully leverage global interactions to understand item correlations, and augment this graph by dual transformations based on neighbor sampling and time disturbance. Accordingly, we design a Temporal item Transition graph Convolutional network (TiTConv) to capture temporal item transition patterns in TITG. Then, a novel Temporal Graph Contrastive Learning (TGCL) mechanism is designed to enhance the uniformity of representations between augmented graphs from identical sequences. For local interaction sequences, we design a temporal sequence encoder to incorporate time interval embeddings into the architecture of Transformer. At the training stage, we take maximum mean discrepancy and TGCL losses as auxiliary objectives. Extensive experiments on several real-world datasets show the effectiveness of TGCL4SR against state-of-the-art baselines of sequential recommendation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d7c6cabbd51b9dabee34aa442cb3e2ba6b76958f" target='_blank'>
              Temporal Graph Contrastive Learning for Sequential Recommendation
              </a>
            </td>
          <td>
            Shengzhe Zhang, Liyi Chen, Chao Wang, Shuangli Li, Hui Xiong
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques that have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand the complexity of the model and offer a more accurate solution for recovering time-varying graph signals. Building upon GegenConv, we design the Gegenbauer-based time graph neural network (GegenGNN) architecture, which adopts an encoder-decoder structure. Likewise, our approach also uses a dedicated loss function that incorporates a mean squared error (MSE) component alongside Sobolev smoothness regularization. This combination enables GegenGNN to capture both the fidelity to ground truth and the underlying smoothness properties of the signals, enhancing the reconstruction performance. We conduct extensive experiments on real datasets to evaluate the effectiveness of our proposed approach. The experimental results demonstrate that GegenGNN outperforms state-of-the-art methods, showcasing its superior capability in recovering time-varying graph signals.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/209f0474527ec347adec6d7f48c92264df31a1de" target='_blank'>
              Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
              </a>
            </td>
          <td>
            Jhon A. Castro-Correa, Jhony H. Giraldo, Mohsen Badiey, Fragkiskos D. Malliaros
          </td>
          <td>2024-03-28</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Traffic forecasting is crucial for intelligent transportation systems. It has experienced significant advancements thanks to the power of deep learning in capturing latent patterns of traffic data. However, recent deep-learning architectures require intricate model designs and lack an intuitive understanding of the mapping from input data to predicted results. Achieving both accuracy and responsibility in traffic prediction models remains a challenge due to the complexity of traffic data and the inherent opacity of deep learning models. To tackle these challenges, we propose a Responsible and Reliable Traffic flow forecasting model with Large Language Models (R2T-LLM), which leverages large language models (LLMs) to generate responsible traffic predictions. By transferring multi-modal traffic data into natural language descriptions, R2T-LLM captures complex spatial-temporal patterns and external factors from comprehensive traffic data. The LLM framework is fine-tuned using language-based instructions to align with spatial-temporal traffic flow data. Empirically, R2T-LLM shows competitive accuracy compared with deep learning baselines, while providing an intuitive and reliable explanation for predictions. We discuss the spatial-temporal and input dependencies for conditional future flow forecasting, showcasing R2T-LLM's potential for diverse city prediction tasks. This paper contributes to advancing accountable traffic prediction models and lays a foundation for future exploration of LLM applications in transportation. To the best of our knowledge, this is the first study to use LLM for accountable and reliable prediction of traffic flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/461fd3214d3e0a7e89cb5295f636881e5aaebc79" target='_blank'>
              Towards Responsible and Reliable Traffic Flow Prediction with Large Language Models
              </a>
            </td>
          <td>
            Xusen Guo, Qiming Zhang, Junyue Jiang, Mingxing Peng, Hao Yang, Meixin Zhu
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be2ef67b07ba4cf63e435a4154e83d94ece03c6e" target='_blank'>
              Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model
              </a>
            </td>
          <td>
            Jiqun Chu, Zuoquan Lin
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have revolutionized the field of machine learning on non-Euclidean data such as graphs and networks. GNNs effectively implement node representation learning through neighborhood aggregation and achieve impressive results in many graph-related tasks. However, most neighborhood aggregation approaches are summation-based, which can be problematic as they may not be sufficiently expressive to encode informative graph structures. Furthermore, though the graph pooling module is also of vital importance for graph learning, especially for the task of graph classification, research on graph down-sampling mechanisms is rather limited. To address the above challenges, we propose a concatenation-based graph convolution mechanism that injectively updates node representations to maximize the discriminative power in distinguishing non-isomorphic subgraphs. In addition, we design a novel graph pooling module, called WL-SortPool, to learn important subgraph patterns in a deep-learning manner. WL-SortPool layer-wise sorts node representations (i.e. continuous WL colors) to separately learn the relative importance of subtrees with different depths for the purpose of classification, thus better characterizing the complex graph topology and rich information encoded in the graph. We propose a novel Subgraph Pattern GNN (SPGNN) architecture that incorporates these enhancements. We test the proposed SPGNN architecture on many graph classification benchmarks. Experimental results show that our method can achieve highly competitive results with state-of-the-art graph kernels and other GNN approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b8f9381035e592427840e45002302051d590fa2f" target='_blank'>
              SPGNN: Recognizing Salient Subgraph Patterns via Enhanced Graph Convolution and Pooling
              </a>
            </td>
          <td>
            Zehao Dong, Muhan Zhang, Yixin Chen
          </td>
          <td>2024-04-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ade46150fbb93b4e473f2fafbe39dfbb3346ee94" target='_blank'>
              A Survey on Diffusion Models for Time Series and Spatio-Temporal Data
              </a>
            </td>
          <td>
            Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Cheng-Ming Liu, Bin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Traffic prediction is the core issue of Intelligent Transportation Systems. Recently, researchers have tended to use complex structures, such as transformer-based structures, for tasks such as traffic prediction. Notably, traffic data is simpler to process compared to text and images, which raises questions about the necessity of these structures. Additionally, when handling traffic data, researchers tend to manually design the model structure based on the data features, which makes the structure of traffic prediction redundant and the model generalizability limited. To address the above, we introduce the ‘ModWaveMLP’—A multilayer perceptron (MLP) based model designed according to mode decomposition and wavelet noise reduction information learning concepts. The model is based on simple MLP structure, which achieves the separation and prediction of different traffic modes and does not depend on additional features introduced such as the topology of the traffic network. By performing experiments on real-world datasets METR-LA and PEMS-BAY, our model achieves SOTA, outperforms GNN and transformer-based models, and outperforms those that introduce additional feature data with better generalizability, and we further demonstrate the effectiveness of the various parts of the model through ablation experiments. This offers new insights to subsequent researchers involved in traffic model design. The code is available at: https://github.com/Kqingzheng/ModWaveMLP.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dfa0e7e38dd3ceffe21d54d5beacc173a30bbe7b" target='_blank'>
              ModWaveMLP: MLP-Based Mode Decomposition and Wavelet Denoising Model to Defeat Complex Structures in Traffic Forecasting
              </a>
            </td>
          <td>
            Ke Sun, Pei Liu, Pengfei Li, Zhifang Liao
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Quantitative stock selection is one of the most challenging FinTech tasks due to the non-stationary dynamics and complex market dependencies. Existing studies rely on channel mixing methods, exacerbating the issue of distribution shift in financial time series. Additionally, complex model structures they build make it difficult to handle very long sequences. Furthermore, most of them are based on predefined stock relationships thus making it difficult to capture the dynamic and highly volatile stock markets. To address the above issues, in this paper, we propose Channel-Independent based Spatio-Temporal Hypergraph Pre-trained Attention Networks (CI-STHPAN), a two-stage framework for stock selection, involving Transformer and HGAT based stock time series self-supervised pre-training and stock-ranking based downstream task fine-tuning. We calculate the similarity of stock time series of different channel in dynamic intervals based on Dynamic Time Warping (DTW), and further construct channel-independent stock dynamic hypergraph based on the similarity. Experiments with NASDAQ and NYSE markets data over five years show that our framework outperforms SOTA approaches in terms of investment return ratio (IRR) and Sharpe ratio (SR). Additionally, we find that even without introducing graph information, self-supervised learning based on the vanilla Transformer Encoder also surpasses SOTA results. Notable improvements are gained on the NYSE market. It is mainly attributed to the improvement of fine-tuning approach on Information Coefficient (IC) and Information Ratio based IC (ICIR), indicating that the fine-tuning method enhances the accuracy and stability of the model prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/88e6bae24ae33dc1b381213dfd105c1a21b31e6d" target='_blank'>
              CI-STHPAN: Pre-trained Attention Network for Stock Selection with Channel-Independent Spatio-Temporal Hypergraph
              </a>
            </td>
          <td>
            Hongjie Xia, Huijie Ao, Long Li, Yu Liu, Sen Liu, Guangnan Ye, Hongfeng Chai
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data augmentation serves as a popular regularization technique to combat overfitting challenges in neural networks. While automatic augmentation has demonstrated success in image classification tasks, its application to time-series problems, particularly in long-term forecasting, has received comparatively less attention. To address this gap, we introduce a time-series automatic augmentation approach named TSAA, which is both efficient and easy to implement. The solution involves tackling the associated bilevel optimization problem through a two-step process: initially training a non-augmented model for a limited number of epochs, followed by an iterative split procedure. During this iterative process, we alternate between identifying a robust augmentation policy through Bayesian optimization and refining the model while discarding suboptimal runs. Extensive evaluations on challenging univariate and multivariate forecasting benchmark problems demonstrate that TSAA consistently outperforms several robust baselines, suggesting its potential integration into prediction pipelines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/233d8d32faecb7a7e5544edaf437fe3f968d8cae" target='_blank'>
              Data Augmentation Policy Search for Long-Term Forecasting
              </a>
            </td>
          <td>
            Liran Nochumsohn, Omri Azencot
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="The modeling of environmental ecosystems plays a pivotal role in the sustainable management of our planet. Accurate prediction of key environmental variables over space and time can aid in informed policy and decision-making, thus improving people's livelihood. Recently, deep learning-based methods have shown promise in modeling the spatial-temporal relationships for predicting environmental variables. However, these approaches often fall short in handling incomplete features and distribution shifts, which are commonly observed in environmental data due to the substantial cost of data collection and malfunctions in measuring instruments. To address these issues, we propose LITE -- a multimodal large language model for environmental ecosystems modeling. Specifically, LITE unifies different environmental variables by transforming them into natural language descriptions and line graph images. Then, LITE utilizes unified encoders to capture spatial-temporal dynamics and correlations in different modalities. During this step, the incomplete features are imputed by a sparse Mixture-of-Experts framework, and the distribution shift is handled by incorporating multi-granularity information from past observations. Finally, guided by domain instructions, a language model is employed to fuse the multimodal representations for the prediction. Our experiments demonstrate that LITE significantly enhances performance in environmental spatial-temporal prediction across different domains compared to the best baseline, with a 41.25% reduction in prediction error. This justifies its effectiveness. Our data and code are available at https://github.com/hrlics/LITE.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/64feb7d1bbaee85fece0933b7276e0ac5af8b051" target='_blank'>
              LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models
              </a>
            </td>
          <td>
            Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The widespread deployment of wireless and mobile devices results in a proliferation of spatio-temporal data that is used in applications, e.g., traffic prediction, human mobility mining, and air quality prediction, where spatio-temporal prediction is often essential to enable safety, predictability, or reliability. Many recent proposals that target deep learning for spatio-temporal prediction suffer from so-called catastrophic forgetting, where previously learned knowledge is entirely forgotten when new data arrives. Such proposals may experience deteriorating prediction performance when applied in settings where data streams into the system. To enable spatio-temporal prediction on streaming data, we propose a unified replay-based continuous learning framework. The framework includes a replay buffer of previously learned samples that are fused with training data using a spatio-temporal mixup mechanism in order to preserve historical knowledge effectively, thus avoiding catastrophic forgetting. To enable holistic representation preservation, the framework also integrates a general spatio-temporal autoencoder with a carefully designed spatio-temporal simple siamese (STSimSiam) network that aims to ensure prediction accuracy and avoid holistic feature loss by means of mutual information maximization. The framework further encompasses five spatio-temporal data augmentation methods to enhance the performance of STSimSiam. Extensive experiments on real data offer insight into the effectiveness of the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68f8c6ca1b6cee5d593c4e143b4b1680bd189a96" target='_blank'>
              A Unified Replay-based Continuous Learning Framework for Spatio-Temporal Prediction on Streaming Data
              </a>
            </td>
          <td>
            Hao Miao, Yan Zhao, Chenjuan Guo, Bin Yang, Kai Zheng, Feiteng Huang, Jiandong Xie, Christian S. Jensen
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>26</td>
        </tr>

        <tr id="Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2a5b8158db29854109275cb5c3fbcf47c080c1c" target='_blank'>
              Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting
              </a>
            </td>
          <td>
            Christian Klotergens, Vijaya Krishna Yalavarthi, Maximilian Stubbemann, Lars Schmidt-Thieme
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Multimodal graph neural architecture search (MGNAS) has shown great success for automatically designing the optimal multimodal graph neural network (MGNN) architecture by leveraging multimodal representation, crossmodal information and graph structure in one unified framework. However, existing MGNAS fails to handle distribution shifts that naturally exist in multimodal graph data, since the searched architectures inevitably capture spurious statistical correlations under distribution shifts. To solve this problem, we propose a novel Out-of-distribution Generalized Multimodal Graph Neural Architecture Search (OMG-NAS) method which optimizes the MGNN architecture with respect to its performance on decorrelated OOD data. Specifically, we propose a multimodal graph representation decorrelation strategy, which encourages the searched MGNN model to output representations that eliminate spurious correlations through iteratively optimizing the feature weights and controller. In addition, we propose a global sample weight estimator that facilitates the sharing of optimal sample weights learned from existing architectures. This design promotes the effective estimation of the sample weights for candidate MGNN architectures to generate decorrelated multimodal graph representations, concentrating more on the truly predictive relations between invariant features and ground-truth labels. Extensive experiments on real-world multimodal graph datasets demonstrate the superiority of our proposed method over SOTA baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b18bb8858a6e437b007cadea507a09946061383" target='_blank'>
              Multimodal Graph Neural Architecture Search under Distribution Shifts
              </a>
            </td>
          <td>
            Jie Cai, Xin Wang, Haoyang Li, Ziwei Zhang, Wenwu Zhu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/TimeXplusplus}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41331316e9b6cf30546793e0963a8a78d8691c86" target='_blank'>
              TimeX++: Learning Time-Series Explanations with Information Bottleneck
              </a>
            </td>
          <td>
            Zichuan Liu, Tianchun Wang, Jimeng Shi, Xu Zheng, Zhuomin Chen, Lei Song, Wenqian Dong, J. Obeysekera, Farhad Shirani, Dongsheng Luo
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>36</td>
        </tr>

        <tr id="Transformer-based methods have made significant progress in time series forecasting (TSF). They primarily handle two types of tokens, i.e., temporal tokens that contain all variables of the same timestamp, and variable tokens that contain all input time points for a specific variable. Transformer-based methods rely on positional encoding (PE) to mark tokens' positions, facilitating the model to perceive the correlation between tokens. However, in TSF, research on PE remains insufficient. To address this gap, we conduct experiments and uncover intriguing properties of existing PEs in TSF: (i) The positional information injected by PEs diminishes as the network depth increases; (ii) Enhancing positional information in deep networks is advantageous for improving the model's performance; (iii) PE based on the similarity between tokens can improve the model's performance. Motivated by these findings, we introduce two new PEs: Temporal Position Encoding (T-PE) for temporal tokens and Variable Positional Encoding (V-PE) for variable tokens. Both T-PE and V-PE incorporate geometric PE based on tokens' positions and semantic PE based on the similarity between tokens but using different calculations. To leverage both the PEs, we design a Transformer-based dual-branch framework named T2B-PE. It first calculates temporal tokens' correlation and variable tokens' correlation respectively and then fuses the dual-branch features through the gated unit. Extensive experiments demonstrate the superior robustness and effectiveness of T2B-PE. The code is available at: \href{https://github.com/jlu-phyComputer/T2B-PE}{https://github.com/jlu-phyComputer/T2B-PE}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0806b6e1e06267088c3607c3a068caa23152964e" target='_blank'>
              Intriguing Properties of Positional Encoding in Time Series Forecasting
              </a>
            </td>
          <td>
            Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng, Fuchun Sun, Hui Xiong
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve. We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory. In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution. Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction. In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance. Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs. The ablation studies underscore the necessity of graph reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8c86788d32b9147a7a7c44cab4bda1d8a01ad995" target='_blank'>
              Temporal Generalization Estimation in Evolving Graphs
              </a>
            </td>
          <td>
            Bin Lu, Tingyan Ma, Xiaoying Gan, Xinbing Wang, Yunqiang Zhu, Cheng Zhou, Shiyu Liang
          </td>
          <td>2024-04-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc142aa425bff39a9385154d34485dc13ce3cf95" target='_blank'>
              Vision Mamba: A Comprehensive Survey and Taxonomy
              </a>
            </td>
          <td>
            Xiao Liu, Chenxu Zhang, Lei Zhang
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="The success of graph neural networks (GNNs) has spurred numerous new works leveraging GNNs for modeling multivariate time series anomaly detection. Despite their achieved performance improvements, most of them only consider static graph to describe the spatial-temporal dependencies between time series. Moreover, existing works neglect the time and scale-changing structures of time series. In this work, we propose MDGAD, a novel multi-scale dynamic graph structure learning approach for time series anomaly detection. We design a multi-scale graph structure learning module that captures the complex correlations among time series, constructing an evolving graph at each scale. Meanwhile, an anomaly detector is used to combine bilateral prediction errors to detect abnormal data. Experiments conducted on two time series datasets demonstrate the effectiveness of MDGAD.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a65f4f1c44a5b95cebf0bb51dd10176ca04c712" target='_blank'>
              Multi-Scale Dynamic Graph Learning for Time Series Anomaly Detection (Student Abstract)
              </a>
            </td>
          <td>
            Yixuan Jin, Yutao Wei, Zhangtao Cheng, Wenxin Tai, Chunjing Xiao, Ting Zhong
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Large Language Models (LLMs) offer the potential for automatic time series analysis and reporting, which is a critical task across many domains, spanning healthcare, finance, climate, energy, and many more. In this paper, we propose a framework for rigorously evaluating the capabilities of LLMs on time series understanding, encompassing both univariate and multivariate forms. We introduce a comprehensive taxonomy of time series features, a critical framework that delineates various characteristics inherent in time series data. Leveraging this taxonomy, we have systematically designed and synthesized a diverse dataset of time series, embodying the different outlined features. This dataset acts as a solid foundation for assessing the proficiency of LLMs in comprehending time series. Our experiments shed light on the strengths and limitations of state-of-the-art LLMs in time series understanding, revealing which features these models readily comprehend effectively and where they falter. In addition, we uncover the sensitivity of LLMs to factors including the formatting of the data, the position of points queried within a series and the overall time series length.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/80b06bb6b5ab0e6e6de9eecf8d5829dec2f6df57" target='_blank'>
              Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark
              </a>
            </td>
          <td>
            Elizabeth Fons, Rachneet Kaur, Soham Palande, Zhen Zeng, Svitlana Vyetrenko, T. Balch
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Time series data, representing sequential observations recorded over time, plays a pivotal role in numerous domains. Extracting valuable insights from time series is crucial for informed decision-making, pattern recognition, and predictive modeling. Traditional forecasting methods, such as moving averages and autoregressive models, have been effective for stationary time series. However, the increasing complexity of time series data across various applications has highlighted the limitations of these methods in capturing seasonal and trend patterns. This paper focuses on the CNN-LSTM model, which combines the strengths of both Convolutional Neural Networks and Long Short-Term Memory (LSTM) networks. The Conv-LSTM architectures leverages CNNs' feature extraction capabilities and LSTM’s ability to capture temporal dependencies. The study aims to evaluate the CNN-LSTM’s different architectures performance in forecasting non-stationary time series. It incorporates a decomposition approach to assess the architecture’s ability to capture trend and seasonal components and utilize them as features for making predictions. The results shed light on the effectiveness of the architectures of CNN combined with LSTM in capturing the different patterns of the data, by exhibiting the lowest MSE and MAE for the trend, seasonal, and residual components.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89de953a90e888a2d2e19e4537aa462064dae735" target='_blank'>
              CNN-LSTM architectures for non-stationary time series: decomposition approach
              </a>
            </td>
          <td>
            Ibtissam Amalou, Naoual Mouhni, Abdelmounim Abdali
          </td>
          <td>2024-04-24</td>
          <td>2024 International Conference on Global Aeronautical Engineering and Satellite Technology (GAST)</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Financial prediction from Monetary Policy Conference (MPC) calls is a new yet challenging task, which targets at predicting the price movement and volatility for specific financial assets by analyzing multimodal information including text, video, and audio. Although the existing work has achieved great success using cross-modal transformer blocks, it overlooks the potential external financial knowledge, the varying contributions of different modalities to financial prediction, as well as the innate relations among different financial assets. To tackle these limitations, we propose a novel Modal-Adaptive kNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER. Specifically, MANAGER resorts to FinDKG to obtain the external related knowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit BERT (HuBERT) to extract the video and audio features, respectively. Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph that fully characterizes the semantic relations among text, external knowledge, video and audio, to adaptively utilize the information in different modalities, with ChatGLM2 as the backbone. Extensive experiments on a publicly available dataset Monopoly verify the superiority of our model over cutting-edge methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e77ad3c89173c3c6e622c4935025442bcd5907f" target='_blank'>
              Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from Monetary Policy Conference Calls with LLM
              </a>
            </td>
          <td>
            Kun Ouyang, Yi Liu, Shicheng Li, Ruihan Bao, Keiko Harimoto, Xu Sun
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance. Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets. In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification. This network includes two Dense Blocks. Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length. We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF. To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively. Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a54010847821400c02ebdd0b5d8c14cfd8c9447" target='_blank'>
              AdaFSNet: Time Series Classification Based on Convolutional Network with a Adaptive and Effective Kernel Size Configuration
              </a>
            </td>
          <td>
            Haoxiao Wang, Bo Peng, Jianhua Zhang, Xu Cheng
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Sound classification plays a crucial role in enhancing the interpretation, analysis, and use of acoustic data, leading to a wide range of practical applications, of which environmental sound analysis is one of the most important. In this paper, we explore the representation of audio data as graphs in the context of sound classification. We propose a methodology that leverages pre-trained audio models to extract deep features from audio files, which are then employed as node information to build graphs. Subsequently, we train various graph neural networks (GNNs), specifically graph convolutional networks (GCNs), GraphSAGE, and graph attention networks (GATs), to solve multi-class audio classification problems. Our findings underscore the effectiveness of employing graphs to represent audio data. Moreover, they highlight the competitive performance of GNNs in sound classification endeavors, with the GAT model emerging as the top performer, achieving a mean accuracy of 83% in classifying environmental sounds and 91% in identifying the land cover of a site based on its audio recording. In conclusion, this study provides novel insights into the potential of graph representation learning techniques for analyzing audio data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0611aecf6ab7a34d45e1fbe4294e4d941c507a6a" target='_blank'>
              Graph-Based Audio Classification Using Pre-Trained Models and Graph Neural Networks
              </a>
            </td>
          <td>
            A. E. Castro-Ospina, Miguel Angel Solarte-Sanchez, L. Vega-Escobar, C. Isaza, J. D. Martínez-Vargas
          </td>
          <td>2024-03-26</td>
          <td>Sensors (Basel, Switzerland)</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Inductive representation learning on temporal heterogeneous graphs is crucial for scalable deep learning on heterogeneous information networks (HINs) which are time-varying, such as citation networks. However, most existing approaches are not inductive and thus cannot handle new nodes or edges. Moreover, previous temporal graph embedding methods are often trained with the temporal link prediction task to simulate the link formation process of temporal graphs, while ignoring the evolution of high-order topological structures on temporal graphs. To fill these gaps, we propose a Continuous-Time Representation Learning (CTRL) model on temporal HINs. To preserve heterogeneous node features and temporal structures, CTRL integrates three parts in a single layer, they are 1) a \emph{heterogeneous attention} unit that measures the semantic correlation between nodes, 2) a \emph{edge-based Hawkes process} to capture temporal influence between heterogeneous nodes, and 3) \emph{dynamic centrality} that indicates the dynamic importance of a node. We train the CTRL model with a future event (a subgraph) prediction task to capture the evolution of the high-order network structure. Extensive experiments have been conducted on three benchmark datasets. The results demonstrate that our model significantly boosts performance and outperforms various state-of-the-art approaches. Ablation studies are conducted to demonstrate the effectiveness of the model design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fc09c11d233a74ab1b9b06d760b4034c34c35ef" target='_blank'>
              CTRL: Continuous-Time Representation Learning on Temporal Heterogeneous Information Network
              </a>
            </td>
          <td>
            Chenglin Li, Yuanzhen Xie, Chenyun Yu, Lei Cheng, , Zang Li, Di Niu
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management. This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency. By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7bd16ee4f6547dca8598354be5ddac15369ffb9a" target='_blank'>
              TKAN: Temporal Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Remi Genet, Hugo Inzirillo
          </td>
          <td>2024-05-12</td>
          <td>SSRN Electronic Journal</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Regional traffic forecasting is a critical challenge in urban mobility, with applications to various fields such as the Internet of Everything. In recent years, spatio-temporal graph neural networks have achieved state-of-the-art results in the context of numerous traffic forecasting challenges. This work aims at expanding upon the conventional spatio-temporal graph neural network architectures in a manner that may facilitate the inclusion of information regarding the examined regions, as well as the populations that traverse them, in order to establish a more efficient prediction model. The end-product of this scientific endeavour is a novel spatio-temporal graph neural network architecture that is referred to as WEST (WEighted STacked) GCN-LSTM. Furthermore, the inclusion of the aforementioned information is conducted via the use of two novel dedicated algorithms that are referred to as the Shared Borders Policy and the Adjustable Hops Policy. Through information fusion and distillation, the proposed solution manages to significantly outperform its competitors in the frame of an experimental evaluation that consists of 19 forecasting models, across several datasets. Finally, an additional ablation study determined that each of the components of the proposed solution contributes towards enhancing its overall performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db09b6e3ce2cd426ae576b0e25873e787e758705" target='_blank'>
              WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks for Regional Traffic Forecasting
              </a>
            </td>
          <td>
            T. Theodoropoulos, Angelos-Christos Maroudis, Antonios Makris, Konstantinos Tserpes
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="This work proposes a time series prediction method based on the kernel view of linear reservoirs. In particular, the time series motifs of the reservoir kernel are used as representational basis on which general readouts are constructed. We provide a geometric interpretation of our approach shedding light on how our approach is related to the core reservoir models and in what way the two approaches differ. Empirical experiments then compare predictive performances of our suggested model with those of recent state-of-art transformer based models, as well as the established recurrent network model - LSTM. The experiments are performed on both univariate and multivariate time series and with a variety of prediction horizons. Rather surprisingly we show that even when linear readout is employed, our method has the capacity to outperform transformer models on univariate time series and attain competitive results on multivariate benchmark datasets. We conclude that simple models with easily controllable capacity but capturing enough memory and subsequence structure can outperform potentially over-complicated deep learning models. This does not mean that reservoir motif based models are preferable to other more complex alternatives - rather, when introducing a new complex time series model one should employ as a sanity check simple, but potentially powerful alternatives/baselines such as reservoir models or the models introduced here.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f05d23f2bc78e7af9f51cabbcd6f945a6ec9a7c2" target='_blank'>
              Predictive Modeling in the Reservoir Kernel Motif Space
              </a>
            </td>
          <td>
            Peter Tino, Robert Simon Fong, R. Leonarduzzi
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Time series forecasting is a critical aspect of data analysis, with applications ranging from finance and economics to weather prediction and industrial processes.  This review paper explores the evolution of time series forecasting techniques, analyzing the progression from classical methods to modern approaches. It synthesizes key advancements, discusses challenges, future directions and provides insights into emerging trends. Traditional forecasting methods often struggle with capturing the complex patterns and dynamics present in real-world time series data. This study explores the efficacy of cutting-edge models, such as long short-term memory (LSTM) networks, and recurrent neural networks (RNNs), in capturing intricate temporal dependencies. It also aims to guide researchers and practitioners in selecting appropriate methods for diverse time series forecasting applications. We categorize existing approaches, discuss their strengths and limitations, and highlight emerging trends in the field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a8297af58036f319a7c937b2c59bd98a51d9251" target='_blank'>
              Forecasting the Future: A Comprehensive Review of Time Series Prediction Techniques
              </a>
            </td>
          <td>
            Milind Kolambe, Sandhya Arora
          </td>
          <td>2024-04-04</td>
          <td>Journal of Electrical Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The development of intelligent education has led to the emergence of knowledge tracing as a fundamental task in the learning process. Traditionally, the knowledge state of each student has been determined by assessing their performance in previous learning activities. In recent years, Deep Learning approaches have shown promising results in capturing complex representations of human learning activities. However, the interpretability of these models is often compromised due to the end-to-end training strategy they employ. To address this challenge, we draw inspiration from advancements in graph neural networks and propose a novel model called GELT (Graph Embeddings based Lite-Transformer). The purpose of this model is to uncover and understand the relationships between skills and questions. Additionally, we introduce an energy-saving attention mechanism for predicting knowledge states that is both simple and effective. This approach maintains high prediction accuracy while significantly reducing computational costs compared to conventional attention mechanisms. Extensive experimental results demonstrate the superior performance of our proposed model compared to other state-of-the-art baselines on three publicly available real-world datasets for knowledge tracking.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04a26e176654a177f11bb57eb7d9146299a48a47" target='_blank'>
              GELT: A graph embeddings based lite-transformer for knowledge tracing
              </a>
            </td>
          <td>
            Zhijie Liang, Ruixia Wu, Zhao Liang, Juan Yang, Ling Wang, Jianyu Su
          </td>
          <td>2024-05-07</td>
          <td>PLOS ONE</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4b879f069d023e03bf537309a99bdaeb39916ea5" target='_blank'>
              Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training
              </a>
            </td>
          <td>
            Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Forecasting tasks using large datasets gathering thousands of heterogeneous time series is a crucial statistical problem in numerous sectors. The main challenge is to model a rich variety of time series, leverage any available external signals and provide sharp predictions with statistical guarantees. In this work, we propose a new forecasting model that combines discrete state space hidden Markov models with recent neural network architectures and training procedures inspired by vector quantized variational autoencoders. We introduce a variational discrete posterior distribution of the latent states given the observations and a two-stage training procedure to alternatively train the parameters of the latent states and of the emission distributions. By learning a collection of emission laws and temporarily activating them depending on the hidden process dynamics, the proposed method allows to explore large datasets and leverage available external signals. We assess the performance of the proposed method using several datasets and show that it outperforms other state-of-the-art solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/67b1ca32e081c1237b1c3f428bbd8adbf83d9c58" target='_blank'>
              Variational quantization for state space models
              </a>
            </td>
          <td>
            Étienne David, Jean Bellot, Sylvain Le Corff Lpsm, Su
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Information diffusion prediction plays a crucial role in understanding the propagation of information in social networks, encompassing both macroscopic and microscopic prediction tasks. Macroscopic prediction estimates the overall impact of information diffusion, while microscopic prediction focuses on identifying the next user to be influenced. While prior research often concentrates on one of these aspects, a few tackle both concurrently. These two tasks provide complementary insights into the diffusion process at different levels, revealing common traits and unique attributes. The exploration of leveraging common features across these tasks to enhance information prediction remains an underexplored avenue. In this paper, we propose an intuitive and effective model that addresses both macroscopic and microscopic prediction tasks. Our approach considers the interactions and dynamics among cascades at the macro level and incorporates the social homophily of users in social networks at the micro level. Additionally, we introduce adversarial training and orthogonality constraints to ensure the integrity of shared features. Experimental results on four datasets demonstrate that our model significantly outperforms state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a1e23cc5b8d054d01a30a67dc7c33cd0fd49df5b" target='_blank'>
              Enhancing Multi-Scale Diffusion Prediction via Sequential Hypergraphs and Adversarial Learning
              </a>
            </td>
          <td>
            Pengfei Jiao, Hongqian Chen, Qing Bao, Wang Zhang, Huamin Wu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to fine-tune can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a graph learning problem. TransferGraph constructs a graph using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph's effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual fine-tuning results compared to the state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/544b82074bd539bbb55fc4123850eb991db9dc33" target='_blank'>
              Model Selection with Model Zoo via Graph Learning
              </a>
            </td>
          <td>
            Ziyu Li, Hilco van der Wilk, Danning Zhan, Megha Khosla, A. Bozzon, Rihan Hai
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="Accurate weather forecasting is crucial in various sectors, impacting decision-making processes and societal events. Data-driven approaches based on machine learning models have recently emerged as a promising alternative to numerical weather prediction models given their potential to capture physics of different scales from historical data and the significantly lower computational cost during the prediction stage. Renowned for its state-of-the-art performance across diverse domains, the Transformer model has also gained popularity in machine learning weather prediction. Yet applying Transformer architectures to weather forecasting, particularly on a global scale is computationally challenging due to the quadratic complexity of attention and the quadratic increase in spatial points as resolution increases. In this work, we propose a factorized-attention-based model tailored for spherical geometries to mitigate this issue. More specifically, it utilizes multi-dimensional factorized kernels that convolve over different axes where the computational complexity of the kernel is only quadratic to the axial resolution instead of overall resolution. The deterministic forecasting accuracy of the proposed model on $1.5^\circ$ and 0-7 days' lead time is on par with state-of-the-art purely data-driven machine learning weather prediction models. We also showcase the proposed model holds great potential to push forward the Pareto front of accuracy-efficiency for Transformer weather models, where it can achieve better accuracy with less computational cost compared to Transformer based models with standard attention.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e385b9f0a4b229626db67c16db256964da40e022" target='_blank'>
              CaFA: Global Weather Forecasting with Factorized Attention on Sphere
              </a>
            </td>
          <td>
            Zijie Li, Anthony Zhou, Saurabh Patil, A. Farimani
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for downstream tasks from existing contrastive baselines. Incorporating external information improves the scores by an additional 20\%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f05cefd8f2127ab726105a418899b9dea331fe36" target='_blank'>
              Universal representations for financial transactional data: embracing local, global, and external contexts
              </a>
            </td>
          <td>
            Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan A Kireev, Andrey Savchenko, Alexey Zaytsev
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Time series forecasts of different temporal granularity are widely used in real-world applications, e.g., sales prediction in days and weeks for making different inventory plans. However, these tasks are usually solved separately without ensuring coherence, which is crucial for aligning downstream decisions. Previous works mainly focus on ensuring coherence with some straightforward methods, e.g., aggregation from the forecasts of fine granularity to the coarse ones, and allocation from the coarse granularity to the fine ones. These methods merely take the temporal hierarchical structure to maintain coherence without improving the forecasting accuracy. In this paper, we propose a novel granularity message-passing mechanism (GMP) that leverages temporal hierarchy information to improve forecasting performance and also utilizes an adaptive reconciliation (AR) strategy to maintain coherence without performance loss. Furthermore, we introduce an optimization module to achieve task-based targets while adhering to more real-world constraints. Experiments on real-world datasets demonstrate that our framework (GMP-AR) achieves superior performances on temporal hierarchical forecasting tasks compared to state-of-the-art methods. In addition, our framework has been successfully applied to a real-world task of payment traffic management in Alipay by integrating with the task-based optimization module.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eda45cd6529a34d19acc4f469fc5c2a02a59939e" target='_blank'>
              GMP-AR: Granularity Message Passing and Adaptive Reconciliation for Temporal Hierarchy Forecasting
              </a>
            </td>
          <td>
            Fan Zhou, Chen Pan, Lintao Ma, Yu Liu, Siqiao Xue, James Zhang, Jun Zhou, Hongyuan Mei, Weitao Lin, Zi Zhuang, Wenxin Ning, Yun Hu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="In recent intelligent transportation applications, metro flow forecasting has received much attention from researchers. Most prior arts endeavor to explore spatial or temporal dependencies while ignoring the key characteristic patterns underlying historical flows, e.g., trend and periodicity. Although the multiple granularity distillations or spatial dependency correlation can promote the flow estimation. However, the potential noise and spatial dynamics are under-explored. To this end, we propose a novel Disentanglement-Guided Spatial-Temporal Graph Neural Network or DGST to address the above concerns. It contains a Disentanglement Pre-training procedure for characteristic pattern disentanglement learning, a Characteristic Pattern Prediction for different future characteristic explorations, and a Spatial-Temporal Correlation for spatial-temporal dynamic learning. Experiments on a real-world dataset demonstrate the superiority of our DGST.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97c3d40f5e7ca9257f82b1474dff0a5fcf894536" target='_blank'>
              Disentanglement-Guided Spatial-Temporal Graph Neural Network for Metro Flow Forecasting (Student Abstract)
              </a>
            </td>
          <td>
            Jinyu Hong, Ping Kuang, Qiang Gao, Fan Zhou
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data is essential to performing time series analysis utilizing machine learning approaches, whether for classic models or today's large language models. A good time-series dataset is advantageous for the model's accuracy, robustness, and convergence, as well as task outcomes and costs. The emergence of data-centric AI represents a shift in the landscape from model refinement to prioritizing data quality. Even though time-series data processing methods frequently come up in a wide range of research fields, it hasn't been well investigated as a specific topic. To fill the gap, in this paper, we systematically review different data-centric methods in time series analysis, covering a wide range of research topics. Based on the time-series data characteristics at sample, feature, and period, we propose a taxonomy for the reviewed data selection methods. In addition to discussing and summarizing their characteristics, benefits, and drawbacks targeting time-series data, we also introduce the challenges and opportunities by proposing recommendations, open problems, and possible research topics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c516acf9c06cad81176d354767b897ba0bb4823e" target='_blank'>
              Review of Data-centric Time Series Analysis from Sample, Feature, and Period
              </a>
            </td>
          <td>
            Chenxi Sun, Hongyan Li, Yaliang Li, linda Qiao
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="The widespread application of Electronic Health Records (EHR) data in the medical field has led to early successes in disease risk prediction using deep learning methods. These methods typically require extensive data for training due to their large parameter sets. However, existing works do not exploit the full potential of EHR data. A significant challenge arises from the infrequent occurrence of many medical codes within EHR data, limiting their clinical applicability. Current research often lacks in critical areas: 1) incorporating disease domain knowledge; 2) heterogeneously learning disease representations with rich meanings; 3) capturing the temporal dynamics of disease progression. To overcome these limitations, we introduce a novel heterogeneous graph learning model designed to assimilate disease domain knowledge and elucidate the intricate relationships between drugs and diseases. This model innovatively incorporates temporal data into visit-level embeddings and leverages a time-aware transformer alongside an adaptive attention mechanism to produce patient representations. When evaluated on two healthcare datasets, our approach demonstrated notable enhancements in both prediction accuracy and interpretability over existing methodologies, signifying a substantial advancement towards personalized and proactive healthcare management.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a043b1ca161eee02fd96ed91d51b9c38d1846058" target='_blank'>
              Time-aware Heterogeneous Graph Transformer with Adaptive Attention Merging for Health Event Prediction
              </a>
            </td>
          <td>
            Shibo Li, Hengliang Cheng, Weihua Li
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Multivariate Time Series (MTS) anomaly detection focuses on pinpointing samples that diverge from standard operational patterns, which is crucial for ensuring the safety and security of industrial applications. The primary challenge in this domain is to develop representations capable of discerning anomalies effectively. The prevalent methods for anomaly detection in the literature are predominantly reconstruction-based and predictive in nature. However, they typically concentrate on a single-dimensional instance level, thereby not fully harnessing the complex associations inherent in industrial MTS. To address this issue, we propose a novel self-supervised hierarchical contrastive consistency learning method for detecting anomalies in MTS, named HCL-MTSAD. It innovatively leverages data consistency at multiple levels inherent in industrial MTS, systematically capturing consistent associations across four latent levels-measurement, sample, channel, and process. By developing a multi-layer contrastive loss, HCL-MTSAD can extensively mine data consistency and spatio-temporal association, resulting in more informative representations. Subsequently, an anomaly discrimination module, grounded in self-supervised hierarchical contrastive learning, is designed to detect timestamp-level anomalies by calculating multi-scale data consistency. Extensive experiments conducted on six diverse MTS datasets retrieved from real cyber-physical systems and server machines, in comparison with 20 baselines, indicate that HCL-MTSAD's anomaly detection capability outperforms the state-of-the-art benchmark models by an average of 1.8\% in terms of F1 score.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/648f18a2b925ad4c118bbaa542f3dd4f085ca1f1" target='_blank'>
              HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate Detection of Industrial Multivariate Time Series Anomalies
              </a>
            </td>
          <td>
            Haili Sun, Yan-Ming Huang, Lansheng Han, Cai Fu, Chunjie Zhou
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Long sequence prediction has broad and significant application value in fields such as finance, wind power, and weather. However, the complex long-term dependencies of long sequence data and the potential domain shift problems limit the effectiveness of traditional models in practical scenarios. To this end, we propose an Adaptive Meta-Learning Probabilistic Inference Framework (AMPIF) based on sequence decomposition, which can effectively enhance the long sequence prediction ability of various basic models. Specifically, first, we decouple complex sequences into seasonal and trend components through a frequency domain decomposition module. Then, we design an adaptive meta-learning task construction strategy, which divides the seasonal and trend components into different tasks through a clustering-matching approach. Finally, we design a dual-stream amortized network (ST-DAN) to capture shared information between seasonal-trend tasks and use the support set to generate task-specific parameters for rapid generalization learning on the query set. We conducted extensive experiments on six datasets, including wind power and finance scenarios, and the results show that our method significantly outperforms baseline methods in prediction accuracy, interpretability, and algorithm stability and can effectively enhance the long sequence prediction capabilities of base models. The source code is publicly available at https://github.com/Zhu-JP/AMPIF.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3df192f97d2a111abc95b9e3f3765038dd62583c" target='_blank'>
              Adaptive Meta-Learning Probabilistic Inference Framework for Long Sequence Prediction
              </a>
            </td>
          <td>
            Jianping Zhu, Xin Guo, Yang Chen, Yao Yang, Wenbo Li, Bo Jin, Fei Wu
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Crypto markets present significant challenges in financial time series forecasting with their high volatility and unpredictable nature. In this study, Optuna Based Optimized Transformer (OBOT) was proposed for time series forecasting for Bitcoin, the pioneer of cryptocurrency markets. To compare the proposed OBOT, Autoregressive Integrated Moving Average (ARIMA), Gradient Boosting Trees, Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), Temporal Convolutional Network (TCN) with optimized hyperparameters were used. In particular, after the success of Transformer models in natural language processing, studies have been conducted on their potential for time series problems. The models were evaluated using Optuna for hyperparameter optimization and their performance was compared with Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. The generalized performance of the models was tested by dividing the data set into different time steps (6–12) and training and test sets at different rates (0.5-0.5, 0.7-0.3, 0.8-0.2). The results show that the proposed OBOT approach stands out for Bitcoin with an RMSE value of 0.0079 and a MAE value of 0.0122. These findings reveal that the proposed OBOT approach have significant potential in crypto market forecasting and should be examined in more detail in future studies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e8b18e0c7c0423c8df20320c41d13e04b994bd1a" target='_blank'>
              Optuna Based Optimized Transformer Model Approach in Bitcoin Time Series Analysis
              </a>
            </td>
          <td>
            Berkay Yildirim, M. Taskiran
          </td>
          <td>2024-03-27</td>
          <td>2024 26th International Conference on Digital Signal Processing and its Applications (DSPA)</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging transfer learning. While benchmarks validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to transfer learning success. Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on zero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data. We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9fa1a78352db25fd589f78ba6ea4ceda0fd0758a" target='_blank'>
              The impact of data set similarity and diversity on transfer learning success in time series forecasting
              </a>
            </td>
          <td>
            Claudia Ehrig, Catherine Cleophas, G. Forestier
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Time-series and text data are prevalent in healthcare and frequently co-exist, yet they are typically modeled in isolation. Even studies that jointly model time-series and text, do so by converting time-series to images or graphs. We hypothesize that explicitly modeling time-series jointly with text can improve tasks such as summarization and question answering for time-series data, which have received little attention so far. To address this gap, we introduce JoLT to jointly learn desired representations from pre-trained time-series and text models. JoLT utilizes a Querying Transformer (Q-Former) to align the time-series and text representations. Our experiments on a large real-world electrocardiography dataset for medical time-series summarization show that JoLT outperforms state-of-the-art image captioning approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b42ecaf9bdf55709b69c9cc9ccd55ed5aea29bf" target='_blank'>
              JoLT: Jointly Learned Representations of Language and Time-Series for Clinical Time-Series Interpretation (Student Abstract)
              </a>
            </td>
          <td>
            Yifu Cai, Arvind Srinivasan, Mononito Goswami, Arjun Choudhry, Artur Dubrawski
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>2</td>
          <td>7</td>
        </tr>

        <tr id="Sea ice at the North Pole is vital to global climate dynamics. However, accurately forecasting sea ice poses a significant challenge due to the intricate interaction among multiple variables. Leveraging the capability to integrate multiple inputs and powerful performances seamlessly, many studies have turned to neural networks for sea ice forecasting. This paper introduces a novel deep architecture named Unicorn, designed to forecast weekly sea ice. Our model integrates multiple time series images within its architecture to enhance its forecasting performance. Moreover, we incorporate a bottleneck layer within the U-Net architecture, serving as neural ordinary differential equations with convolution operations, to capture the spatiotemporal dynamics of latent variables. Through real data analysis with datasets spanning from 1998 to 2021, our proposed model demonstrates significant improvements over state-of-the-art models in the sea ice concentration forecasting task. It achieves an average MAE improvement of 12% compared to benchmark models. Additionally, our method outperforms existing approaches in sea ice extent forecasting, achieving a classification performance improvement of approximately 18%. These experimental results show the superiority of our proposed model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/acead6f74c4c0256a98a24346c024e52f726e59a" target='_blank'>
              Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations
              </a>
            </td>
          <td>
            Jae-Min Park, Sungchul Hong, Yoonseo Cho, Jong-June Jeon
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Recent state-of-the-art forecasting methods are trained on collections of time series. These methods, often referred to as global models, can capture common patterns in different time series to improve their generalization performance. However, they require large amounts of data that might not be readily available. Besides this, global models sometimes fail to capture relevant patterns unique to a particular time series. In these cases, data augmentation can be useful to increase the sample size of time series datasets. The main contribution of this work is a novel method for generating univariate time series synthetic samples. Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations. In this context, we frame the problem of training a forecasting model as an imbalanced learning task. Oversampling strategies are popular approaches used to deal with the imbalance problem in machine learning. We use these techniques to create synthetic time series observations and improve the accuracy of forecasting models. We carried out experiments using 7 different databases that contain a total of 5502 univariate time series. We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2aa19a1142e9b6fe481ef797d0f383efac768b7c" target='_blank'>
              Time Series Data Augmentation as an Imbalanced Learning Problem
              </a>
            </td>
          <td>
            Vítor Cerqueira, Nuno Moniz, Ricardo In'acio, Carlos Soares
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have achieved state-of-the-art performance in graph representation learning. Message passing neural networks, which learn representations through recursively aggregating information from each node and its neighbors, are among the most commonly-used GNNs. However, a wealth of structural information of individual nodes and full graphs is often ignored in such process, which restricts the expressive power of GNNs. Various graph data augmentation methods that enable the message passing with richer structure knowledge have been introduced as one main way to tackle this issue, but they are often focused on individual structure features and difficult to scale up with more structure features. In this work we propose a novel approach, namely collective structure knowledge-augmented graph neural network (CoS-GNN), in which a new message passing method is introduced to allow GNNs to harness a diverse set of node- and graph-level structure features, together with original node features/attributes, in augmented graphs. In doing so, our approach largely improves the structural knowledge modeling of GNNs in both node and graph levels, resulting in substantially improved graph representations. This is justified by extensive empirical results where CoS-GNN outperforms state-of-the-art models in various graph-level learning tasks, including graph classification, anomaly detection, and out-of-distribution generalization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18193bee8659d58e297f3935dda0f25ca0ad8245" target='_blank'>
              Harnessing Collective Structure Knowledge in Data Augmentation for Graph Neural Networks
              </a>
            </td>
          <td>
            , Guansong Pang, 
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Machine learning models have made significant progress in load forecasting, but their forecast accuracy is limited in cases where historical load data is scarce. Inspired by the outstanding performance of large language models (LLMs) in computer vision and natural language processing, this paper aims to discuss the potential of large time series models in load forecasting with scarce historical data. Specifically, the large time series model is constructed as a time series generative pre-trained transformer (TimeGPT), which is trained on massive and diverse time series datasets consisting of 100 billion data points (e.g., finance, transportation, banking, web traffic, weather, energy, healthcare, etc.). Then, the scarce historical load data is used to fine-tune the TimeGPT, which helps it to adapt to the data distribution and characteristics associated with load forecasting. Simulation results show that TimeGPT outperforms the benchmarks (e.g., popular machine learning models and statistical models) for load forecasting on several real datasets with scarce training samples, particularly for short look-ahead times. However, it cannot be guaranteed that TimeGPT is always superior to benchmarks for load forecasting with scarce data, since the performance of TimeGPT may be affected by the distribution differences between the load data and the training data. In practical applications, we can divide the historical data into a training set and a validation set, and then use the validation set loss to decide whether TimeGPT is the best choice for a specific dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/380b0cca3c6d155f4149f7c445a5b359bf449205" target='_blank'>
              TimeGPT in Load Forecasting: A Large Time Series Model Perspective
              </a>
            </td>
          <td>
            Wenlong Liao, F. Porté-Agel, Jiannong Fang, Christian Rehtanz, Shouxiang Wang, Dechang Yang, Zhe Yang
          </td>
          <td>2024-04-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>63</td>
        </tr>

        <tr id="We present an imagification approach for multivariate time-series data tailored to constrained NN-based forecasting model training environments. Our imagification process consists of two key steps: Re-stacking and time embedding. In the Re-stacking stage, time-series data are arranged based on high correlation, forming the first image channel using a sliding window technique. The time embedding stage adds two additional image channels by incorporating real-time information. We evaluate our method by comparing it with three benchmark imagification techniques using a simple CNN-based model. Additionally, we conduct a comparison with LSTM, a conventional time-series forecasting model. Experimental results demonstrate that our proposed approach achieves three times faster model training termination while maintaining forecasting accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f3b38957454b22f08121ce24897affa05bdeffe6" target='_blank'>
              Multivariate Time-Series Imagification with Time Embedding in Constrained Environments (Student Abstract)
              </a>
            </td>
          <td>
            Seungwoo Kang, Ohyun Jo
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="There are many time series forecasting methods, but there are few research methods for long-term multivariate time series forecasting, which are mainly dominated by a series of forecasting models developed on the basis of a transformer. The aim of this study is to perform forecasting for multivariate time series data and to improve the forecasting accuracy of the model. In the recent past, it has appeared that the prediction effect of linear models surpasses that of the family of self-attention mechanism models, which encourages us to look for new methods to solve the problem of long-term multivariate time series forecasting. In order to overcome the problem that the temporal order of information is easily broken in the self-attention family and that it is difficult to capture information on long-distance data using recurrent neural network models, we propose a matrix attention mechanism, which is able to weight each previous data point equally without breaking the temporal order of the data, so that the overall data information can be fully utilized. We used the matrix attention mechanism as the basic module to construct the frequency domain block and time domain block. Since complex and variable seasonal component features are difficult to capture in the time domain, mapping them to the frequency domain reduces the complexity of the seasonal components themselves and facilitates data feature extraction. Therefore, we use the frequency domain block to extract the seasonal information with high randomness and poor regularity to help the model capture the local dynamics. The time domain block is used to extract the smooth floating trend component information to help the model capture long-term change patterns. This also improves the overall prediction performance of the model. It is experimentally demonstrated that our model achieves the best prediction results on three public datasets and one private dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/81bdaf55e392e38529a4084bf20e27468a913fa9" target='_blank'>
              Long-Term Forecasting Using MAMTF: A Matrix Attention Model Based on the Time and Frequency Domains
              </a>
            </td>
          <td>
            Kaixin Guo, Xin Yu
          </td>
          <td>2024-03-29</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e35989ddf9bca3dfab54e4eff50e839ba9f356f" target='_blank'>
              Time Machine GPT
              </a>
            </td>
          <td>
            Felix Drinkall, Eghbal Rahimikia, J. Pierrehumbert, Stefan Zohren
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>45</td>
        </tr>

        <tr id="Traffic prediction is pivotal for rational transportation supply scheduling and allocation. Existing researches into short-term traffic prediction, however, face challenges in adequately addressing exceptional circumstances and integrating non-numerical contextual information like weather into models. While, Large language models offer a promising solution due to their inherent world knowledge. However, directly using them for traffic prediction presents drawbacks such as high cost, lack of determinism, and limited mathematical capability. To mitigate these issues, this study proposes a novel approach. Instead of directly employing large models for prediction, it utilizes them to process textual information and obtain embeddings. These embeddings are then combined with historical traffic data and inputted into traditional spatiotemporal forecasting models. The study investigates two types of special scenarios: regional-level and node-level. For regional-level scenarios, textual information is represented as a node connected to the entire network. For node-level scenarios, embeddings from the large model represent additional nodes connected only to corresponding nodes. This approach shows a significant improvement in prediction accuracy according to our experiment of New York Bike dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/977b4a327e0becf13893db66aa10bec5e5e7df59" target='_blank'>
              Enhancing Traffic Prediction with Textual Data Using Large Language Models
              </a>
            </td>
          <td>
            Xiannan Huang
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recently, many works have proposed various financial large language models (FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on financial corpora. However, existing FinLLMs exhibit unsatisfactory performance in understanding financial text when numeric variables are involved in questions. In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance. We first construct a financial corpus from financial textbooks which is essential for improving numeric capability of LLMs during fine-tuning. After that, we train two individual low-rank adaptation (LoRA) modules by fine-tuning on our constructed financial corpus. One module is for adapting general-purpose LLMs to financial domain, and the other module is for enhancing the ability of NumLLM to understand financial text with numeric variables. Lastly, we merge the two LoRA modules into the foundation model to obtain NumLLM for inference. Experiments on financial question-answering benchmark show that NumLLM can boost the performance of the foundation model and can achieve the best overall performance compared to all baselines, on both numeric and non-numeric questions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8beb65880b49dd060f5dd4252a26399e074162b9" target='_blank'>
              NumLLM: Numeric-Sensitive Large Language Model for Chinese Finance
              </a>
            </td>
          <td>
            Huan-Yi Su, Ke Wu, Yu-Hao Huang, Wu-Jun Li
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLAD the first latent space graph generative model with competitive performance. Our source code is published at: \url{https://github.com/v18nguye/GLAD}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/02f65f5f1a6c4b31fed98018629d9ba734b7c5b3" target='_blank'>
              Discrete Latent Graph Generative Modeling with Diffusion Bridges
              </a>
            </td>
          <td>
            Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system. Existing methods aim to reconstruct STTD using low-dimensional models. However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations. Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation. To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables. To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes. We further enable modeling in irregular spaces such as sensor graphs using spectral embedding. Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics. It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns. We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales. Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach extends to different data domains, output resolutions, and network topologies. Comprehensive model analyses provide further insight into the inductive bias of STTD. We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/23d51413d2346feb2800af411f23446029d61123" target='_blank'>
              Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner
              </a>
            </td>
          <td>
            Tong Nie, Guoyang Qin, Wei Ma, Jiangming Sun
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3f2aab368eecadb4f3e9caa45ea0625fd6698e4" target='_blank'>
              Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph
              </a>
            </td>
          <td>
            Zhiwei Wang, Jun Huang, Longhua Ma, Chengyu Wu, Hongyu Ma
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering (Q$\&$A), and stock movement prediction (binary classification), with a notable gap in the application of LLMs for financial risk prediction. Addressing this gap, in this paper, we introduce \textbf{RiskLabs}, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates. Our approach involves a multi-stage process: initially extracting and analyzing ECC data using LLMs, followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes. Using multimodal fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction. Empirical experiment results demonstrate RiskLab's effectiveness in forecasting both volatility and variance in financial markets. Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of LLMs in this context. Our findings not only contribute to the AI in finance application but also open new avenues for applying LLMs in financial risk assessment.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79ad2001981acc7f24f70cdd8307821b70289fb9" target='_blank'>
              RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data
              </a>
            </td>
          <td>
            Yupeng Cao, Zhi Chen, Qingyun Pei, Fabrizio Dimino, Lorenzo Ausiello, Prashant Kumar, K.P. Subbalakshmi, Papa Momar Ndiaye
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="In recent years, the global e-commerce landscape has witnessed rapid growth, with sales reaching a new peak in the past year and expected to rise further in the coming years. Amid this e-commerce boom, accurately predicting user purchase behavior has become crucial for commercial success. We introduce a novel framework integrating three innovative approaches to enhance the prediction model’s effectiveness. First, we integrate an event-based timestamp encoding within a time-series attention model, effectively capturing the dynamic and temporal aspects of user behavior. This aspect is often neglected in traditional user purchase prediction methods, leading to suboptimal accuracy. Second, we incorporate Graph Neural Networks (GNNs) to analyze user behavior. By modeling users and their actions as nodes and edges within a graph structure, we capture complex relationships and patterns in user behavior more effectively than current models, offering a nuanced and comprehensive analysis. Lastly, our framework transcends traditional learning strategies by implementing advanced meta-learning techniques. This enables the model to autonomously adjust learning parameters, including the learning rate, in response to new and evolving data environments, thereby significantly enhancing its adaptability and learning efficiency. Through extensive experiments on diverse real-world e-commerce datasets, our model demonstrates superior performance, particularly in accuracy and adaptability in large-scale data scenarios. This study not only overcomes the existing challenges in analyzing e-commerce user behavior but also sets a foundation for future exploration in this dynamic field. We believe our contributions provide significant insights and tools for e-commerce platforms to better understand and cater to their users, ultimately driving sales and improving user experiences.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1bf00e7cd0944e21ac20139e7a642fc6c6a9a7bf" target='_blank'>
              Advancing e-commerce user purchase prediction: Integration of time-series attention with event-based timestamp encoding and Graph Neural Network-Enhanced user profiling
              </a>
            </td>
          <td>
            Shuang Zhou, Norlaile Salleh Hudin
          </td>
          <td>2024-04-18</td>
          <td>PLOS ONE</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="While graph neural networks (GNNs) have become the de-facto standard for graph-based node classification, they impose a strong assumption on the availability of sufficient labeled samples. This assumption restricts the classification performance of prevailing GNNs on many real-world applications suffering from low-data regimes. Specifically, features extracted from scarce labeled nodes could not provide sufficient supervision for the unlabeled samples, leading to severe over-fitting. In this work, we point out that leveraging subgraphs to capture long-range dependencies can augment the representation of a node with homophily properties, thus alleviating the low-data regime. However, prior works leveraging subgraphs fail to capture the long-range dependencies among nodes. To this end, we present a novel self-supervised learning framework, called multi-view subgraph neural networks (Muse), for handling long-range dependencies. In particular, we propose an information theory-based identification mechanism to identify two types of subgraphs from the views of input space and latent space, respectively. The former is to capture the local structure of the graph, while the latter captures the long-range dependencies among nodes. By fusing these two views of subgraphs, the learned representations can preserve the topological properties of the graph at large, including the local structure and long-range dependencies, thus maximizing their expressiveness for downstream node classification tasks. Experimental results show that Muse outperforms the alternative methods on node classification tasks with limited labeled data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/408dce5da2dcb865df16bcef4b931c0e393484e7" target='_blank'>
              Multi-View Subgraph Neural Networks: Self-Supervised Learning with Scarce Labeled Data
              </a>
            </td>
          <td>
            Zhenzhong Wang, Qingyuan Zeng, Wanyu Lin, Min Jiang, Kay Chen Tan
          </td>
          <td>2024-04-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b9c7c36d07828403234e8483ba36fdc9814f2c00" target='_blank'>
              Towards Incremental Learning in Large Language Models: A Critical Review
              </a>
            </td>
          <td>
            M. Jovanovic, Peter Voss
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="The field of Knowledge Tracing aims to understand how students learn and master knowledge over time by analyzing their historical behaviour data. To achieve this goal, many researchers have proposed Knowledge Tracing models that use data from Intelligent Tutoring Systems to predict students' subsequent actions. However, with the development of Intelligent Tutoring Systems, large-scale datasets containing long-sequence data began to emerge. Recent deep learning based Knowledge Tracing models face obstacles such as low efficiency, low accuracy, and low interpretability when dealing with large-scale datasets containing long-sequence data. To address these issues and promote the sustainable development of Intelligent Tutoring Systems, we propose a LSTM BERT-based Knowledge Tracing model for long sequence data processing, namely LBKT, which uses a BERT-based architecture with a Rasch model-based embeddings block to deal with different difficulty levels information and an LSTM block to process the sequential characteristic in students' actions. LBKT achieves the best performance on most benchmark datasets on the metrics of ACC and AUC. Additionally, an ablation study is conducted to analyse the impact of each component of LBKT's overall performance. Moreover, we used t-SNE as the visualisation tool to demonstrate the model's embedding strategy. The results indicate that LBKT is faster, more interpretable, and has a lower memory cost than the traditional deep learning based Knowledge Tracing methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3653c9d55ca1c91f37bf41f6861e80d493485ac0" target='_blank'>
              Integrating LSTM and BERT for Long-Sequence Data Analysis in Intelligent Tutoring Systems
              </a>
            </td>
          <td>
            Zhaoxing Li, Jujie Yang, Jindi Wang, Lei Shi, Sebastian Stein
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Dynamic graphs play a crucial role in various real-world applications, such as link prediction and node classification on social media and e-commerce platforms. Temporal Graph Neural Networks (T-GNNs) have emerged as a leading approach for handling dynamic graphs, using temporal message passing to compute temporal node embeddings. However, training existing T-GNNs on large-scale dynamic graphs is prohibitively expensive due to the ill-suited batching scheme and significant data access overhead. In this paper, we introduce ETC, a generic framework designed specifically for efficient T-GNN training at scale. ETC incorporates a novel data batching scheme that enables large training batches improving model computation efficiency, while preserving model effectiveness by restricting information loss in each training batch. To reduce data access overhead, ETC employs a three-step data access policy that leverages the data access pattern in T-GNN training, significantly reducing redundant data access volume. Additionally, ETC utilizes an inter-batch pipeline mechanism, decoupling data access from model computation and further reducing data access costs. Extensive experimental results demonstrate the effectiveness of ETC, showcasing its ability to achieve significant training speedups compared to state-of-the-art training frameworks for T-GNNs on real-world dynamic graphs with millions of interactions. ETC provides a training speedup ranging from 1.6X to 62.4X, highlighting its potential for efficient training on large-scale dynamic graphs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1233b6787cb382ffec0d89657f7e1ffe8eed67f3" target='_blank'>
              ETC: Efficient Training of Temporal Graph Neural Networks over Large-scale Dynamic Graphs
              </a>
            </td>
          <td>
            Shihong Gao, Yiming Li, Yanyan Shen, Yingxia Shao, Lei Chen
          </td>
          <td>2024-01-01</td>
          <td>Proc. VLDB Endow.</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Predictive Process Monitoring focuses on predicting future states of ongoing process executions, such as forecasting the remaining time. Recent developments in Object-Centric Process Mining have enriched event data with objects and their explicit relations between events. To leverage this enriched data, we propose the Heterogeneous Object Event Graph encoding (HOEG), which integrates events and objects into a graph structure with diverse node types. It does so without aggregating object features, thus creating a more nuanced and informative representation. We then adopt a heterogeneous Graph Neural Network architecture, which incorporates these diverse object features in prediction tasks. We evaluate the performance and scalability of HOEG in predicting remaining time, benchmarking it against two established graph-based encodings and two baseline models. Our evaluation uses three Object-Centric Event Logs (OCELs), including one from a real-life process at a major Dutch financial institution. The results indicate that HOEG competes well with existing models and surpasses them when OCELs contain informative object attributes and event-object interactions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5d10f4f8e5030c6c5d0758be957de1024be32586" target='_blank'>
              HOEG: A New Approach for Object-Centric Predictive Process Monitoring
              </a>
            </td>
          <td>
            Tim K. Smit, H. Reijers, Xixi Lu
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Accurate prediction of water quality and quantity is crucial for sustainable development and human well-being. However, existing data-driven methods often suffer from spatial biases in model performance due to heterogeneous data, limited observations, and noisy sensor data. To overcome these challenges, we propose Fair-Graph, a novel graph-based recurrent neural network that leverages interrelated knowledge from multiple rivers to predict water flow and temperature within large-scale stream networks. Additionally, we introduce node-specific graph masks for information aggregation and adaptation to enhance prediction over heterogeneous river segments. To reduce performance disparities across river segments, we introduce a centralized coordination strategy that adjusts training priorities for segments. We evaluate the prediction of water temperature within the Delaware River Basin, and the prediction of streamflow using simulated data from U.S. National Water Model in the Houston River network. The results showcase improvements in predictive performance and highlight the proposed model's ability to maintain spatial fairness over different river segments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/10948624b3665e7ffce725049c58003ff3d6caba" target='_blank'>
              Fair Graph Learning Using Constraint-Aware Priority Adjustment and Graph Masking in River Networks
              </a>
            </td>
          <td>
            Erhu He, Yiqun Xie, Alexander Sun, Jacob Zwart, Jie Yang, Zhenong Jin, Yang Wang, Hassan Karimi, Xiaowei Jia
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Semi-supervised time-series classification could effectively alleviate the issue of lacking labeled data. However, existing approaches usually ignore model interpretability, making it difficult for humans to understand the principles behind the predictions of a model. Shapelets are a set of discriminative subsequences that show high interpretability in time series classification tasks. Shapelet learning-based methods have demonstrated promising classification performance. Unfortunately, without enough labeled data, the shapelets learned by existing methods are often poorly discriminative, and even dissimilar to any subsequence of the original time series. To address this issue, we propose the Diffusion Language-Shapelets model (DiffShape) for semi-supervised time series classification. In DiffShape, a self-supervised diffusion learning mechanism is designed, which uses real subsequences as a condition. This helps to increase the similarity between the learned shapelets and real subsequences by using a large amount of unlabeled data. Furthermore, we introduce a contrastive language-shapelets learning strategy that improves the discriminability of the learned shapelets by incorporating the natural language descriptions of the time series. Experiments have been conducted on the UCR time series archive, and the results reveal that the proposed DiffShape method achieves state-of-the-art performance and exhibits superior interpretability over baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7755152a055770da299724fa2ca4576caa148de8" target='_blank'>
              Diffusion Language-Shapelets for Semi-supervised Time-Series Classification
              </a>
            </td>
          <td>
            Zhen Liu, Wenbin Pei, Disen Lan, Qianli Ma
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of each module. In conclusion, we demonstrated that, even with variations within TGN, it is possible to achieve good performance in the anomaly detection task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/551ad62d178a49651b9c1e4517b61e4078295484" target='_blank'>
              Temporal Graph Networks for Graph Anomaly Detection in Financial Networks
              </a>
            </td>
          <td>
            Yejin Kim, Youngbin Lee, Minyoung Choe, Sungju Oh, Yongjae Lee
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Efficiently processing time series data streams in real-time on resource-constrained devices offers significant advantages in terms of enhanced computational energy efficiency and reduced time-related risks. We introduce an innovative streaming time series classification network that utilizes attentive power iteration, enabling real-time processing on resource-constrained devices. Our model continuously updates a compact representation of the entire time series, enhancing classification accuracy while conserving energy and processing time. Notably, it excels in streaming scenarios without requiring complete time series access, enabling swift decisions. Experimental results show that our approach excels in classification accuracy and energy efficiency, with over 70% less consumption and threefold faster task completion than benchmarks. This work advances real-time responsiveness, energy conservation, and operational effectiveness for constrained devices, contributing to optimizing various applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c71ba9ae0a734d8120db657c9f5804e22df447cc" target='_blank'>
              Energy Efficient Streaming Time Series Classification with Attentive Power Iteration
              </a>
            </td>
          <td>
            Hao Huang, Tapan Shah, Scott Evans, Shinjae Yoo
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Spatiotemporal predictive learning is a paradigm that empowers models to learn spatial and temporal patterns by predicting future frames from past frames in an unsupervised manner. This method typically uses recurrent units to capture long-term dependencies, but these units often come with high computational costs and limited performance in real-world scenes. This paper presents an innovative Wavelet-based SpatioTemporal (WaST) framework, which extracts and adaptively controls both low and high-frequency components at image and feature levels via 3D discrete wavelet transform for faster processing while maintaining high-quality predictions. We propose a Time-Frequency Aware Translator uniquely crafted to efficiently learn short- and long-range spatiotemporal information by individually modeling spatial frequency and temporal variations. Meanwhile, we design a wavelet-domain High-Frequency Focal Loss that effectively supervises high-frequency variations. Extensive experiments across various real-world scenarios, such as driving scene prediction, traffic flow prediction, human motion capture, and weather forecasting, demonstrate that our proposed WaST achieves state-of-the-art performance over various spatiotemporal prediction methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b4d2527c78839dcb903bbf1b178df2c1fbcbc30d" target='_blank'>
              Wavelet-Driven Spatiotemporal Predictive Learning: Bridging Frequency and Time Variations
              </a>
            </td>
          <td>
            Xuesong Nie, Yunfeng Yan, Siyuan Li, Cheng Tan, Xi Chen, Haoyuan Jin, Zhihang Zhu, Stan Z. Li, Donglian Qi
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Most deep learning-based time series clustering models concentrate on data representation in a separate process from clustering. This leads to that clustering loss cannot guide feature extraction. Moreover, most methods solely analyze data from the temporal domain, disregarding the potential within the frequency domain.

To address these challenges, we introduce a novel end-to-end Cross-Domain Contrastive learning model for time series Clustering (CDCC). Firstly, it integrates the clustering process and feature extraction using contrastive constraints at both cluster-level and instance-level. Secondly, the data is encoded simultaneously in both temporal and frequency domains, leveraging contrastive learning to enhance within-domain representation. Thirdly, cross-domain constraints are proposed to align the latent representations and category distribution across domains. With the above strategies, CDCC not only achieves end-to-end output but also effectively integrates frequency domains. Extensive experiments and visualization analysis are conducted on 40 time series datasets from UCR, demonstrating the superior performance of the proposed model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f97766ea14aa26b89a7981eb7f63567c413e3c82" target='_blank'>
              Cross-Domain Contrastive Learning for Time Series Clustering
              </a>
            </td>
          <td>
            Furong Peng, Jiachen Luo, Xuan Lu, Sheng Wang, Feijiang Li
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Graph Neural Networks (GNNs) have received widespread attention and applications due to their excellent performance in graph representation learning. Most existing GNNs can only aggregate 1-hop neighbors in a GNN layer, so they usually stack multiple GNN layers to obtain more information from larger neighborhoods. However, many studies have shown that model performance experiences a significant degradation with the increase of GNN layers. In this paper, we first introduce the concept of distinguishability of class to indirectly evaluate the learned node representations, and verify the positive correlation between distinguishability of class and model performance. Then, we propose a Graph Neural Network guided by Distinguishability of class (Disc-GNN) to monitor the representation learning, so as to learn better node representations and improve model performance. Specifically, we first perform inter-layer filtering and initial compensation based on Local Distinguishability of Class (LDC) in each layer, so that the learned node representations have the ability to distinguish different classes. Furthermore, we add a regularization term based on Global Distinguishability of Class (GDC) to achieve global optimization of model performance. Extensive experiments on six real-world datasets have shown that the competitive performance of Disc-GNN to the state-of-the-art methods on node classification and node clustering tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/428685ee522ba82bbed74ecf743ff892c5b1da83" target='_blank'>
              Improving Distinguishability of Class for Graph Neural Networks
              </a>
            </td>
          <td>
            Dongxiao He, Shuwei Liu, Meng Ge, Zhizhi Yu, Guangquan Xu, Zhiyong Feng
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Accurate prediction of meteorological elements, such as temperature and relative humidity, is important to human livelihood, early warning of extreme weather, and urban governance. Recently, neural network-based methods have shown impressive performance in this field. However, most of them are overcomplicated and impenetrable. In this paper, we propose a straightforward and interpretable differential framework, where the key lies in explicitly estimating the evolutionary trends. Specifically, three types of trends are exploited. (1) The proximity trend simply uses the most recent changes. It works well for approximately linear evolution. (2) The sequential trend explores the global information, aiming to capture the nonlinear dynamics. Here, we develop an attention-based trend unit to help memorize long-term features. (3) The flow trend is motivated by the nature of evolution, i.e., the heat or substance flows from one region to another. Here, we design a flow-aware attention unit. It can reflect the interactions via performing spatial attention over flow maps. Finally, we develop a trend fusion module to adaptively fuse the above three trends. Extensive experiments on two datasets demonstrate the effectiveness of our method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/712ea3f0d79b54e915d46b5e2bdb6221c7a2b436" target='_blank'>
              iTrendRNN: An Interpretable Trend-Aware RNN for Meteorological Spatiotemporal Prediction
              </a>
            </td>
          <td>
            Xu Huang, Chuyao Luo, Bowen Zhang, Huiwei Lin, Xutao Li, Yunming Ye
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e747258b5bcfff29c7073725c1bf9abb40202d1e" target='_blank'>
              Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy
              </a>
            </td>
          <td>
            Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ef2f4b51e25747c179f3ae06f995311be4401a65" target='_blank'>
              Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks
              </a>
            </td>
          <td>
            Huifeng Yin, M. Xu, Jing Pei, Lei Deng
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The existing federated learning (FL) methods for spatio-temporal forecasting fail to capture the inherent spatio-temporal heterogeneity, which calls for personalized FL (PFL) methods to model the spatio-temporally variant patterns. While contrastive learning approach is promising in addressing spatio-temporal heterogeneity, the existing methods are noneffective in determining negative pairs and can hardly apply to PFL paradigm. To tackle this limitation, we propose a novel PFL method, named Federated dUal sEmantic aLignment-based contraStive learning (FUELS), which can adaptively align positive and negative pairs based on semantic similarity, thereby injecting precise spatio-temporal heterogeneity into the latent representation space by auxiliary contrastive tasks. From temporal perspective, a hard negative filtering module is introduced to dynamically align heterogeneous temporal representations for the supplemented intra-client contrastive task. From spatial perspective, we design lightweight-but-efficient prototypes as client-level semantic representations, based on which the server evaluates spatial similarity and yields client-customized global prototypes for the supplemented inter-client contrastive task. Extensive experiments demonstrate that FUELS outperforms state-of-the-art methods, with communication cost decreasing by around 94%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0480085b8a8d1c6a1a98c1e90cd93480e243e104" target='_blank'>
              Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual Semantic Alignment-Based Contrastive Approach
              </a>
            </td>
          <td>
            Qing Liu, Sheng Sun, Yuxuan Liang, Jingjing Xue, Min Liu
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="
 Anomaly detection in time series data (e.g., sensor data) is becoming a fundamental research problem that has various applications. Due to the complex inter-sensor relationships, it is challenging to detect anomalous events such as system faults and attacks hidden the high-dimensional time series. Recent advancements in deep learning approaches such as Graph Neural Networks (GNN) have greatly improved anomaly detection performance in time series data. However, existing methods usually use separate components to capture spatial and temporal dependence relationship and ignore the heterogeneities in spatial-temporal data which motivates us to capture them together. In this paper, we propose a novel approach STGDNN (short for Spatial-Temporal Graph Deviation Neural Networks) that learns the spatial-temporal dependence relationship together in structure learning of graph neural networks. In addition, we use the learned spatial-temporal graph structure and attention weights to explain the detected anomalies. The experiments on three real world datasets show our superiority in detection accuracy, anomaly diagnosis, and model interpretation compared with state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d4b06e1b1b3171295e5ae68e6bca204455b80fc4" target='_blank'>
              Spatial-Temporal Graph Deviation Neural Networks for Anomaly Detection in Time Series
              </a>
            </td>
          <td>
            Ziyu Guo, Weiyang Kong, Yubao Liu
          </td>
          <td>2024-04-01</td>
          <td>Journal of Physics: Conference Series</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism$\unicode{x2013}$a powerful tool originally designed for natural language processing$\unicode{x2013}$have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f07fccb9dc24559d81ae9afbd2e634f065252683" target='_blank'>
              Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning
              </a>
            </td>
          <td>
            Junfeng Chen, Kailiang Wu
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Propagation models in social networks are critical, with extensive applications across various fields and downstream tasks. However, existing propagation models are often oversimplified, scenario-specific, and lack real-world user social attributes. These limitations detaching from real-world analysis lead to inaccurate representations of the propagation process in social networks. To address these issues, we propose a User Features Attention-based DAG-Aware Variational Autoencoder (DAVA) for propagation graph generation. First, nearly 1 million pieces of user attributes data are collected. Then DAVA can integrate the analysis of propagation graph topology and corresponding user attributes as prior knowledge. By leveraging a lightweight attention-based framework and a sliding window mechanism based on BFS permutations weighted by user influence, DAVA significantly enhances the ability to generate realistic, large-scale propagation data, yielding graph scales ten times greater than those produced by existing SOTA methods. Every module of DAVA has flexibility and extension that allows for easy substitution to suit other generation tasks. Additionally, we provide a comprehensive evaluation of DAVA, one focus is the effectiveness of generated data in improving the performance of downstream tasks. During the generation process, we discover the Credibility Erosion Effect by modifying the generation rules, revealing a social phenomenon in social network propagation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/de80d4056d7958c1c11dd1a17779487dad12ca21" target='_blank'>
              DAG-Aware Variational Autoencoder for Social Propagation Graph Generation
              </a>
            </td>
          <td>
            Dongpeng Hou, Chao Gao, Xuelong Li, Zhen Wang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Anomaly detection in multivariate time series (MVTS) is a significant research domain across several industries, including cybersecurity and industrial systems. There have been several deep learning-based methods proposed within this research domain. The development of high capacity frameworks has received the majority of attention in recent years due to the availability of large open-source datasets and improvement in computer processing power. However, there has not been much attention in investigating the importance of how the data is presented to these techniques during training. Curriculum learning (CL), a technique based on ordered learning, was proposed for machine learning. In CL, the model first learns from easy data and is progressively trained with increasingly difficult data. In this paper, we propose data-based CL for MVTS anomaly detection. We further introduce the CL concept to the learner (model), in which we first train a simple model and then utilize a complex model in the final training round. To the best of our knowledge, we are the first to investigate these approaches in MVTS anomaly detection. We evaluate the proposed designs on the SWaT dataset using the F1 score and the results show an improvement in performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/75e50dd6cc0dcf06d34011f5e2c10e00805a2624" target='_blank'>
              Curriculum Learning-Based Multivariate Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Devilliers Caleb Dube, Mehmet Akar
          </td>
          <td>2024-04-24</td>
          <td>2024 35th Conference of Open Innovations Association (FRUCT)</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The widespread use of Artificial Intelligence (AI) has highlighted the importance of understanding AI model behavior. This understanding is crucial for practical decision-making, assessing model reliability, and ensuring trustworthiness. Interpreting time series forecasting models faces unique challenges compared to image and text data. These challenges arise from the temporal dependencies between time steps and the evolving importance of input features over time. My thesis focuses on addressing these challenges by aiming for more precise explanations of feature interactions, uncovering spatiotemporal patterns, and demonstrating the practical applicability of these interpretability techniques using real-world datasets and state-of-the-art deep learning models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bd7607de4ddc02ec4808146ed33b363af66b16c5" target='_blank'>
              Temporal Dependencies and Spatio-Temporal Patterns of Time Series Models
              </a>
            </td>
          <td>
            Md. Khairul Islam
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Confidence calibration in Graph Neural Networks (GNNs) aims to align a model's predicted confidence with its actual accuracy. Recent studies have indicated that GNNs exhibit an under-confidence bias, which contrasts the over-confidence bias commonly observed in deep neural networks. However, our deeper investigation into this topic reveals that not all GNNs exhibit this behavior. Upon closer examination of message passing in GNNs, we found a clear link between message aggregation and confidence levels. Specifically, GNNs with extensive message aggregation, often seen in deep architectures or when leveraging large amounts of labeled data, tend to exhibit overconfidence. This overconfidence can be attributed to factors like over-learning and over-smoothing. Conversely, GNNs with fewer layers, known for their balanced message passing and superior node representation, may exhibit under-confidence. To counter these confidence biases, we introduce the Adaptive Unified Label Smoothing (AU-LS) technique. Our experiments show that AU-LS outperforms existing methods, addressing both over and under-confidence in various GNN scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/403d45dca2d4b71fff8df9e32185354f197827dc" target='_blank'>
              Moderate Message Passing Improves Calibration: A Universal Way to Mitigate Confidence Bias in Graph Neural Networks
              </a>
            </td>
          <td>
            Min Wang, Hao Yang, Jincai Huang, Qing Cheng
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Graph Neural Networks (GNNs) have emerged as a powerful tool for node representation learning within graph structures. However, designing a robust GNN architecture for node classification remains a challenge. This study introduces an efficient and straightforward Residual Attention Augmentation GNN (RAA-GNN) model, which incorporates an attention mechanism with skip connections to discerningly weigh node features and overcome the over-smoothing problem of GNNs. Additionally, a novel MixUp data augmentation method was developed to improve model training. The proposed approach was rigorously evaluated on various node classification benchmarks, encompassing both social and citation networks. The proposed method outperformed state-of-the-art techniques by achieving up to 1% accuracy improvement. Furthermore, when applied to the novel Twitch social network dataset, the proposed model yielded remarkably promising results. These findings provide valuable insights for researchers and practitioners working with graph-structured data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/74a10191b3c1bcd72e3ca3b96ea246615ef4da40" target='_blank'>
              Residual Attention Augmentation Graph Neural Network for Improved Node Classification
              </a>
            </td>
          <td>
            Muhammad Affan Abbas, Waqar Ali, F. Smarandache, Sultan S. Alshamrani, Muhammad Ahsan Raza, Abdullah Alshehri, Mubashir Ali
          </td>
          <td>2024-04-02</td>
          <td>Engineering, Technology &amp; Applied Science Research</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/081eb8781725e560f597b01c624fe65618c3c0f8" target='_blank'>
              Kolmogorov-Arnold Networks (KANs) for Time Series Analysis
              </a>
            </td>
          <td>
            Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of GPT models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized simulation to assess the impact of training examples on the training dynamics of GPT models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in GPT models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized simulation of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both fine-tuning and instruction-tuning scenarios, spanning tasks in natural language understanding and generation. We will make our code and data publicly available.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/02ffeb8cb7a1d1336e0e135c7c1000610ae7b942" target='_blank'>
              On Training Data Influence of GPT Models
              </a>
            </td>
          <td>
            Qingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun, Keze Wang, Hua Wu
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Unsupervised (a.k.a. Self-supervised) representation learning (URL) has emerged as a new paradigm for time series analysis, because it has the ability to learn generalizable time series representation beneficial for many downstream tasks without using labels that are usually difficult to obtain. Considering that existing approaches have limitations in the design of the representation encoder and the learning objective, we have proposed Contrastive Shapelet Learning (CSL), the first URL method that learns the general-purpose shapelet-based representation through unsupervised contrastive learning, and shown its superior performance in several analysis tasks, such as time series classification, clustering, and anomaly detection. In this paper, we develop TimeCSL, an end-to-end system that makes full use of the general and interpretable shapelets learned by CSL to achieve explorable time series analysis in a unified pipeline. We introduce the system components and demonstrate how users interact with TimeCSL to solve different analysis tasks in the unified pipeline, and gain insight into their time series by exploring the learned shapelets and representation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/17ed2e7b21e83bbdb1c6aaea0db5ae049f97d2b2" target='_blank'>
              TimeCSL: Unsupervised Contrastive Learning of General Shapelets for Explorable Time Series Analysis
              </a>
            </td>
          <td>
            Zhiyu Liang, Cheng Liang, Zheng Liang, Hongzhi Wang, Bo Zheng
          </td>
          <td>2024-04-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Graph representation learning has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding. Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network's multiple layers. Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2b17cac3213d434798240671b97b77930bb69f4e" target='_blank'>
              MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding
              </a>
            </td>
          <td>
            Marco Bongiovanni, Luca Gallo, Roberto Grasso, Alfredo Pulvirenti
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time-series analysis plays a pivotal role across a range of critical applications, from finance to healthcare, which involves various tasks, such as forecasting and classification. To handle the inherent complexities of time-series data, such as high dimensionality and noise, traditional supervised learning methods first annotate extensive labels for time-series data in each task, which is very costly and impractical in real-world applications. In contrast, pre-trained foundation models offer a promising alternative by leveraging unlabeled data to capture general time series patterns, which can then be fine-tuned for specific tasks. However, existing approaches to pre-training such models typically suffer from high-bias and low-generality issues due to the use of predefined and rigid augmentation operations and domain-specific data training. To overcome these limitations, this paper introduces UniCL, a universal and scalable contrastive learning framework designed for pretraining time-series foundation models across cross-domain datasets. Specifically, we propose a unified and trainable time-series augmentation operation to generate pattern-preserved, diverse, and low-bias time-series data by leveraging spectral information. Besides, we introduce a scalable augmentation algorithm capable of handling datasets with varying lengths, facilitating cross-domain pretraining. Extensive experiments on two benchmark datasets across eleven domains validate the effectiveness of UniCL, demonstrating its high generalization on time-series analysis across various fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/88044910494a3e00fb43c3dad08e6e42077cbd0a" target='_blank'>
              UniCL: A Universal Contrastive Learning Framework for Large Time Series Models
              </a>
            </td>
          <td>
            Jiawei Li, , Haoyang Li, 
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Weather forecasting is a crucial task for meteorologic research, with direct social and economic impacts. Recently, data-driven weather forecasting models based on deep learning have shown great potential, achieving superior performance compared with traditional numerical weather prediction methods. However, these models often require massive training data and computational resources. In this paper, we propose EWMoE, an effective model for accurate global weather forecasting, which requires significantly less training data and computational resources. Our model incorporates three key components to enhance prediction accuracy: meteorology-specific embedding, a core Mixture-of-Experts (MoE) layer, and two specific loss functions. We conduct our evaluation on the ERA5 dataset using only two years of training data. Extensive experiments demonstrate that EWMoE outperforms current models such as FourCastNet and ClimaX at all forecast time, achieving competitive performance compared with the state-of-the-art Pangu-Weather model in evaluation metrics such as Anomaly Correlation Coefficient (ACC) and Root Mean Square Error (RMSE). Additionally, ablation studies indicate that applying the MoE architecture to weather forecasting offers significant advantages in improving accuracy and resource efficiency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d5d49e426ee21cc00979a2ee831f7cf9b587fb82" target='_blank'>
              EWMoE: An effective model for global weather forecasting with mixture-of-experts
              </a>
            </td>
          <td>
            Lihao Gan, Xin Man, Chenghong Zhang, 
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="
 Time series forecasting is a well-known deep learning application field in which previous data are used to predict the future behavior of the series. Recently, several deep learning approaches have been proposed in which several nonlinear functions are applied to the input to obtain the output. In this paper, we introduce a novel method to improve the performance of deep learning models in time series forecasting. This method divides the model into hierarchies or levels from simpler to more complex ones. Simpler levels handle smoothed versions of the input, whereas the most complex level processes the original time series. This method follows the human learning process where general/simpler tasks are performed first, and afterward, more precise/harder ones are accomplished. Our proposed methodology has been applied to the LSTM architecture, showing remarkable performance in various time series. In addition, a comparison is reported including a standard LSTM and novel methods such as DeepAR, Temporal Fusion Transformer, NBEATS and Echo State Network.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c673000f42f9cc9dd6fe34c45cf4da8bba3213ea" target='_blank'>
              From simple to complex: a sequential method for enhancing time series forecasting with deep learning
              </a>
            </td>
          <td>
            M. Jiménez-Navarro, M. Martínez-Ballesteros, F. Martínez-Álvarez, A. Troncoso, G. Asencio-Cortés
          </td>
          <td>2024-05-14</td>
          <td>Logic Journal of the IGPL</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Anomaly detection for multivariate time series is an essential task in the modern industrial field. Although several methods have been developed for anomaly detection, they usually fail to effectively exploit the metrical-temporal correlation and the other dependencies among multiple variables. To address this problem, we propose a stacked attention autoencoder for anomaly detection in multivariate time series (SA2E-AD); it focuses on fully utilizing the metrical and temporal relationships among multivariate time series. We design a multiattention block, alternately containing the temporal attention and metrical attention components in a hierarchical structure to better reconstruct normal time series, which is helpful in distinguishing the anomalies from the normal time series. Meanwhile, a two-stage training strategy is designed to further separate the anomalies from the normal data. Experiments on three publicly available datasets show that SA2E-AD outperforms the advanced baseline methods in detection performance and demonstrate the effectiveness of each part of the process in our method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c350e9bc3943205b9716f241e6f948e31f900855" target='_blank'>
              SA2E-AD: A Stacked Attention Autoencoder for Anomaly Detection in Multivariate Time Series
              </a>
            </td>
          <td>
            Mengyao Li, Zhiyong Li, Zhibang Yang, Xu Zhou, Yifan Li, Ziyan Wu, Lingzhao Kong, Ke Nai
          </td>
          <td>2024-03-26</td>
          <td>ACM Transactions on Knowledge Discovery from Data</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="In time series analysis, traditional bootstrapping methods often fall short due to their assumption of data independence, a condition rarely met in time-dependent data. This paper introduces tsbootstrap, a python package designed specifically to address this challenge. It offers a comprehensive suite of bootstrapping techniques, including Block, Residual, and advanced methods like Markov and Sieve Bootstraps, each tailored to respect the temporal dependencies in time series data. This framework not only enhances the accuracy of uncertainty estimation in time series analysis but also integrates seamlessly with the existing python data science ecosystem, making it an invaluable asset for researchers and practitioners in various fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e4669519f5e89e51653b4ea3d2fe700545e9a0b7" target='_blank'>
              tsbootstrap: Enhancing Time Series Analysis with Advanced Bootstrapping Techniques
              </a>
            </td>
          <td>
            Sankalp Gilda, Benedikt Heidrich, Franz Kiraly
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping existing measurements (temperature, vibration) to bearing loads. Since temperature and vibration signals exhibit vastly different dynamics, we propose Heterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models these signal types and their interactions for effective load prediction. Our results demonstrate that HTGNN outperforms Convolutional Neural Networks (CNNs), which struggle to capture both spatial and heterogeneous signal characteristics. These findings highlight the importance of capturing the complex spatial interactions between temperature, vibration, and load.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73d5adcb64b709cc6daaed22fef893cf9455cf91" target='_blank'>
              Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks
              </a>
            </td>
          <td>
            Mengjie Zhao, C. Taal, Stephan Baggerohr, Olga Fink
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>16</td>
        </tr>

        <tr id="Heterogeneous Graph Neural Networks (HGNNs) play a vital role in advancing the field of graph representation learning by addressing the complexities arising from diverse data types and interconnected relationships in real-world scenarios. However, traditional HGNNs face challenges when applied to large-scale graphs due to the necessity of training or inferring on the entire graph. As the size of the heterogeneous graphs increases, the time and memory overhead required by these models escalates rapidly, even reaching unacceptable levels. To address this issue, in this paper, we present a novel framework named (SubInfer), which conducts training and inferring on subgraphs instead of the entire graphs, hence efficiently handling large-scale heterogeneous graphs. The proposed framework comprises three main steps: 1) partitioning the heterogeneous graph from multiple perspectives to preserve various semantic information, 2) completing the subgraphs to improve the convergence speed of subgraph training and the performance of subgraph inference, and 3) training and inferring the HGNN model on distributed clusters to further reduce the time overhead. The framework is applicable to the vast majority of HGNN models. Experiments on five benchmark datasets demonstrate that SubInfer effectively optimizes the training and inference phase, delivering comparable performance to traditional HGNN models while significantly reducing time and memory overhead.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6adbc5bdd0cb04ce46ec6494365f8bdca2091dd8" target='_blank'>
              An Efficient Subgraph-Inferring Framework for Large-Scale Heterogeneous Graphs
              </a>
            </td>
          <td>
            Wei Zhou, Hong Huang, Ruize Shi, Kehan Yin, Hai Jin
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Time series data, characterized by its sequential and temporal nature, plays a crucial role in various domains such as finance, healthcare, and industrial processes. Identifying anomalies within time series data is a critical task with applications ranging from fault detection to fraud prevention. Traditional anomaly detection techniques often struggle to capture complex temporal patterns and dependencies in time series data. This study presents a novel time series anomaly detection method using long short-term memory (LSTM) neural networks. LSTMs are a type of recurrent neural networks (RNNs) designed to model long-term dependencies, making them suitable for capturing the complex temporal relationships present in time series data. Here we use a financial dataset featuring opening and closing time, we predict the volume of money and the current price of that money at a specific time. The results demonstrate the efficacy of the deep learning-based approach in detecting anomalies within time series data, outperforming traditional methods in terms of sensitivity and adaptability to complex temporal patterns. The proposed methodology presents a promising solution for real-world applications where early detection of anomalies is crucial for proactive decision-making and system integrity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ddda3c562ccd12d582aaad21a57de21296574f40" target='_blank'>
              Uncovering time series anomaly using deep learning technique
              </a>
            </td>
          <td>
            P. Chiranjeevi, Yadavalli Ramya, Chinthala Balaji, Bathini Shashank, Abbdi Sainath Reddy
          </td>
          <td>2024-04-30</td>
          <td>World Journal of Advanced Research and Reviews</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA"on autoPilot", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an unsupervised validation loss to effectively assess the alignment between augmentation type and anomaly type. Case studies show TSAP's ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA self-supervised models, on diverse TSAD tasks exhibiting different anomaly types.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1609cde5a319eacb23d587ed0eebfa9c1362eca3" target='_blank'>
              End-To-End Self-tuning Self-supervised Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Boje Deforce, Meng-Chieh Lee, Bart Baesens, E. S. Asensio, Jaemin Yoo, L. Akoglu
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>43</td>
        </tr>

        <tr id="Traffic flow prediction is a significant application of deep learning in spatio-temporal forecasting analysis. Existing research faces challenges that hinder prediction accuracy. One challenge is the inadequate capturing of spatial dependencies in traffic flow due to fixed pre-defined graph structures. Moreover, manually designed prior graphs still have limitations in extracting spatial features. Another challenge is the instability in short-term prediction performance when pre-defined graphs are completely abandoned in favor of parameter training. Additionally, ordinary RNN sequence convolution methods also struggle to capture long-term sequential patterns in large historical traffic data, leading to gradient vanishing or exploding issues. To address these challenges, we proposes a graph convolutional network for traffic flow prediction. We combine an improved prior graph with an adaptive graph to form a dual-branch spatio-temporal neural network. In the first branch, we introduce a time graph based on rough data inference to complement the predefined static graph. In the second branch, we construct an adaptive learning framework that dynamically learns the adjacency matrix and captures global road information. By utilizing enhanced multi-scale gated convolution, we extract spatio-temporal dependencies. Our method surpasses most baseline approaches according to extensive experiments conducted on public datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e5ab78ba0ca9866d71d63702fed9ffb3f8d12c9" target='_blank'>
              Enhancing traffic flow prediction: Dual-branch graph convolutional network with rough data inference and adaptive spatial dependencies
              </a>
            </td>
          <td>
            Ning Zhou, Zhihao Ren
          </td>
          <td>2024-03-27</td>
          <td>Journal of Intelligent &amp; Fuzzy Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The growing availability and importance of time series data across various domains, including environmental science, epidemiology, and economics, has led to an increasing need for time-series causal discovery methods that can identify the intricate relationships in the non-stationary, non-linear, and often noisy real world data. However, the majority of current time series causal discovery methods assume stationarity and linear relations in data, making them infeasible for the task. Further, the recent deep learning-based methods rely on the traditional causal structure learning approaches making them computationally expensive. In this paper, we propose a Time-Series Causal Neural Network (TS-CausalNN) - a deep learning technique to discover contemporaneous and lagged causal relations simultaneously. Our proposed architecture comprises (i) convolutional blocks comprising parallel custom causal layers, (ii) acyclicity constraint, and (iii) optimization techniques using the augmented Lagrangian approach. In addition to the simple parallel design, an advantage of the proposed model is that it naturally handles the non-stationarity and non-linearity of the data. Through experiments on multiple synthetic and real world datasets, we demonstrate the empirical proficiency of our proposed approach as compared to several state-of-the-art methods. The inferred graphs for the real world dataset are in good agreement with the domain understanding.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8072f6e96371a2006e81a6cc3b2f05e744a24241" target='_blank'>
              TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data
              </a>
            </td>
          <td>
            Omar Faruque, Sahara Ali, Xue Zheng, Jianwu Wang
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In autonomous driving, accurately interpreting the movements of other road users and leveraging this knowledge to forecast future trajectories is crucial. This is typically achieved through the integration of map data and tracked trajectories of various agents. Numerous methodologies combine this information into a singular embedding for each agent, which is then utilized to predict future behavior. However, these approaches have a notable drawback in that they may lose exact location information during the encoding process. The encoding still includes general map information. However, the generation of valid and consistent trajectories is not guaranteed. This can cause the predicted trajectories to stray from the actual lanes. This paper introduces a new refinement module designed to project the predicted trajectories back onto the actual map, rectifying these discrepancies and leading towards more consistent predictions. This versatile module can be readily incorporated into a wide range of architectures. Additionally, we propose a novel scene encoder that handles all relations between agents and their environment in a single unified heterogeneous graph attention network. By analyzing the attention values on the different edges in this graph, we can gain unique insights into the neural network's inner workings leading towards a more explainable prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d19229a511713c0390e6616a0adea61b17c2af53" target='_blank'>
              Towards Consistent and Explainable Motion Prediction using Heterogeneous Graph Attention
              </a>
            </td>
          <td>
            Tobias Demmler, Andreas Tamke, Thao Dang, Karsten Haug, Lars Mikelsons
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Origin-destination (OD) crowd flow, if more accurately inferred at a fine-grained level, has the potential to enhance the efficacy of various urban applications. While in practice for mining OD crowd flow with effect, the problem of spatially interpolating OD crowd flow occurs since the ineluctable missing values. This problem is further complicated by the inherent scarcity and noise nature of OD crowd flow data. In this paper, we propose an uncertainty-aware interpolative and explainable framework, namely UApex, for realizing reliable and trustworthy OD crowd flow interpolation. Specifically, we first design a Variational Multi-modal Recurrent Graph Auto-Encoder (VMR-GAE) for uncertainty-aware OD crowd flow interpolation. A key idea here is to formulate the problem as semi-supervised learning on directed graphs. Next, to mitigate the data scarcity, we incorporate a distribution alignment mechanism that can introduce supplementary modals into variational inference. Then, a dedicated decoder with a Poisson prior is proposed for OD crowd flow interpolation. Moreover, to make VMR-GAE more trustworthy, we develop an efficient and uncertainty-aware explainer that can provide explanations from the spatiotemporal topology perspective via the Shapley value. Extensive experiments on two real-world datasets validate that VMR-GAE outperforms the state-of-the-art baselines. Also, an exploratory empirical study shows that the proposed explainer can generate meaningful spatiotemporal explanations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/16e9a10ac7a83d80cb6538d9fbefece51b0a6657" target='_blank'>
              Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder
              </a>
            </td>
          <td>
            Qiang Zhou, Xinjiang Lu, Jingjing Gu, Zhe Zheng, Bo Jin, Jingbo Zhou
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Merge trees are a valuable tool in scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes. To address this challenge, we introduce the merge tree neural networks (MTNN), a learned neural network model designed for merge tree comparison. The MTNN enables rapid and high-quality similarity computation. We first demonstrate how graph neural networks (GNNs), which emerged as an effective encoder for graphs, can be trained to produce embeddings of merge trees in vector spaces that enable efficient similarity comparison. Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism. We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets. Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency. In particular, we speed up the prior state-of-the-art by more than 100x on the benchmark datasets while maintaining an error rate below 0.1%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/73212f3cd5db72e4613c5d2751de60660bc6ae0f" target='_blank'>
              Rapid and Precise Topological Comparison with Merge Tree Neural Networks
              </a>
            </td>
          <td>
            Yu Qin, Brittany Terese Fasy, C. Wenk, Brian Summa
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference. In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting. Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner and avoiding over-fitting the proximity information of graphs. To solve these problems and improve the model performance of VGAEs for inductive graph representation learning, in this work, we propose the Self-Label Augmented VGAE model. To leverage the label information for training, our model takes node labels as one-hot encoded inputs and then performs label reconstruction in model training. To overcome the scarcity problem of node labels for semi-supervised settings, we further propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels generated by our model with a node-wise masking approach to enhance the label information. Experiments on benchmark inductive learning graph datasets verify that our proposed model archives promising results on node classification with particular superiority under semi-supervised learning settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f436b1063687a96b355a04725f14d84350a97dc8" target='_blank'>
              Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification
              </a>
            </td>
          <td>
            Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Responding to the escalating interest in long-term forecasting within the industry, we introduce EasyTS, a comprehensive toolkit engineered to streamline data collection, analysis, and model creation procedures. EasyTS acts as a unified solution, driving progress in long-term time series forecasting. The platform provides effortless access to various time series datasets, including a newly open-sourced multi-scenario dataset in the electricity domain. Integrated visualization and analysis tools help unveil inherent data features and relationships. EasyTS facilitates a user-friendly model validation approach with versatile evaluation criteria. This toolkit allows researchers to compare their models proficiently against renowned benchmarks. With our ongoing commitment to expanding our dataset collection and enhancing toolkit functionalities, we aspire to contribute significantly to the time series forecasting domain. Code is available at this repository: https://github.com/EdgeBigBang/EasyTS.git.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8e2deaab09cc1585df38b7847f66cf149f88af0f" target='_blank'>
              EasyTS: The Express Lane to Long Time Series Forecasting
              </a>
            </td>
          <td>
            Tiancheng Zhang, Shaoyuan Huang, Cheng Zhang, Xiaofei Wang, Wenyu Wang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Deep learning algorithms have revolutionized various fields by achieving remarkable results in time series analysis. Among the different architectures, recurrent neural networks (RNNs) have played a significant role in sequential data processing. This study presents a comprehensive comparison of prominent RNN variants: long short-term memory (LSTM), Bidirectional LSTM (BiLSTM), gated recurrent unit (GRU), bidirectional GRU (BiGRU), and RNN, to analyze their respective strengths and weaknesses of national stock exchange India (NSEI). The Python application developed for this research aims to evaluate and determine the most effective algorithm among the variants. To conduct the evaluation, data from the public domain covering the period from 1/1/2004 to 30/06/2023 is collected. The dataset considers significant events such as demonetization, market crashes, the COVID-19 pandemic, downturns in the automobile sector, and rises in unemployment. Stocks from various sectors including banking, automobile, oil and gas, metal, and Pharma are selected for analysis. Finally, the results reveal that algorithm performance varies across different stocks. Specifically, in certain cases, BiLSTM outperforms, while in others, both BiGRU and LSTM are surpassed. Notably, the overall performance of simple RNN is consistently the lowest across all stocks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/227823f82b05ce3feec76f4b8f54d86a987b3f61" target='_blank'>
              Unveiling deep learning powers: LSTM, BiLSTM, GRU, BiGRU, RNN comparison
              </a>
            </td>
          <td>
            Z. M. Shaikh, S. Ramadass
          </td>
          <td>2024-07-01</td>
          <td>Indonesian Journal of Electrical Engineering and Computer Science</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The SARS-CoV-2 global pandemic prompted governments, institutions, and researchers to investigate its impact, developing strategies based on general indicators to make the most precise predictions possible. Approaches based on epidemiological models were used but the outcomes demonstrated forecasting with uncertainty due to insufficient or missing data. Besides the lack of data, machine-learning models including random forest, support vector regression, LSTM, Auto-encoders, and traditional time-series models such as Prophet and ARIMA were employed in the task, achieving remarkable results with limited effectiveness. Some of these methodologies have precision constraints in dealing with multi-variable inputs, which are important for problems like pandemics that require short and long-term forecasting. Given the under-supply in this scenario, we propose a novel approach for time-series prediction based on stacking auto-encoder structures using three variations of the same model for the training step and weight adjustment to evaluate its forecasting performance. We conducted comparison experiments with previously published data on COVID-19 cases, deaths, temperature, humidity, and air quality index (AQI) in São Paulo City, Brazil. Additionally, we used the percentage of COVID-19 cases from the top ten affected countries worldwide until May 4th, 2020. The results show 80.7% and 10.3% decrease in RMSE to entire and test data over the distribution of 50 trial-trained models, respectively, compared to the first experiment comparison. Also, model type#3 achieved 4th better overall ranking performance, overcoming the NBEATS, Prophet, and Glounts time-series models in the second experiment comparison. This model shows promising forecast capacity and versatility across different input dataset lengths, making it a prominent forecasting model for time-series tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b604bf9d8691c6ba1b77975d5c07fa6ea1c6ff6a" target='_blank'>
              A New Auto-Regressive Multi-Variable Modified Auto-Encoder for Multivariate Time-Series Prediction: A Case Study with Application to COVID-19 Pandemics
              </a>
            </td>
          <td>
            Emerson Vilar de Oliveira, Dunfrey Pires Aragão, Luiz M. G. Gonçalves
          </td>
          <td>2024-04-01</td>
          <td>International Journal of Environmental Research and Public Health</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Over the past few years, as large language models have ushered in an era of intelligence emergence, there has been an intensified focus on scaling networks. Currently, many network architectures are designed manually, often resulting in sub-optimal configurations. Although Neural Architecture Search (NAS) methods have been proposed to automate this process, they suffer from low search efficiency. This study introduces Differentiable Model Scaling (DMS), increasing the efficiency for searching optimal width and depth in networks. DMS can model both width and depth in a direct and fully differentiable way, making it easy to optimize. We have evaluated our DMS across diverse tasks, ranging from vision tasks to NLP tasks and various network architectures, including CNNs and Transformers. Results consistently indicate that our DMS can find improved structures and outperforms state-of-the-art NAS methods. Specifically, for image classification on ImageNet, our DMS improves the top-1 accuracy of EfficientNet-B0 and Deit-Tiny by 1.4% and 0.6%, respectively, and outperforms the state-of-the-art zero-shot NAS method, ZiCo, by 1.3% while requiring only 0.4 GPU days for searching. For object detection on COCO, DMS improves the mAP of Yolo-v8-n by 2.0%. For language modeling, our pruned Llama-7B outperforms the prior method with lower perplexity and higher zero-shot classification accuracy. We will release our code in the future.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7b019fe7c86b0eedde9ea29dc539ff80104d22ac" target='_blank'>
              Differentiable Model Scaling using Differentiable Topk
              </a>
            </td>
          <td>
            Kai Liu, Ruohui Wang, Jianfei Gao, Kai Chen
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Self-interpretable graph learning methods provide insights to unveil the black-box nature of GNNs by providing predictions with built-in explanations. However, current works suffer from performance degradation compared to GNNs trained without built-in explanations. We argue the main reason is that they fail to generate explanations satisfying both sufficiency and necessity, and the biased explanations further hurt GNNs' performance. In this work, we propose a novel framework for generating SUfficient aNd NecessarY explanations (SUNNY-GNN for short) that benefit GNNs' predictions. The key idea is to conduct augmentations by structurally perturbing given explanations and employ a contrastive loss to guide the learning of explanations toward sufficiency and necessity directions. SUNNY-GNN introduces two coefficients to generate hard and reliable contrastive samples. We further extend SUNNY-GNN to heterogeneous graphs. Empirical results on various GNNs and real-world graphs show that SUNNY-GNN yields accurate predictions and faithful explanations, outperforming the state-of-the-art methods by improving 3.5% prediction accuracy and 13.1% explainability fidelity on average. Our code and data are available at https://github.com/SJTU-Quant/SUNNY-GNN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3056fb32a8c65ce905e79035d6293d332d25e1d4" target='_blank'>
              Self-Interpretable Graph Learning with Sufficient and Necessary Explanations
              </a>
            </td>
          <td>
            Jiale Deng, Yanyan Shen
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="With hundreds of SARS-CoV-2 lineages circulating in the global population, there is an urgent need for forecasting lineage frequencies and thus identifying rapidly expanding lineages. To address this need, we constructed a framework for SARS-CoV-2 lineage frequency forecasting (CovTransformer), based on the transformer architecture. We designed our framework to navigate challenges such as a limited amount of data with high levels of noise and bias. We first trained and tested the model using data from the UK and the US, and then tested the generalization ability of the model on data collected across the globe. Remarkably, the model makes predictions two months into the future with high levels of accuracy in 31 countries. Finally, we show that our model performed substantially better than the current gold-standard, i.e. a regression-based model implemented in Nextstrain. Overall, our work demonstrates transformer models represent a promising approach for lineage forecasting and pandemic monitoring.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/768bbe5dc48a9d0262f5cff2104d776acbf0c371" target='_blank'>
              CovTransformer: A transformer model for SARS-CoV-2 lineage frequency forecasting
              </a>
            </td>
          <td>
            Y. Feng, E. E. Goldberg, M. Kupperman, X. Zhang, Y. Lin, R. Ke
          </td>
          <td>2024-04-01</td>
          <td>None</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The investigation of time series data forecasting is a critical topic within the realms of economics and business. The autoregressive integrated moving average (ARIMA) model has been prevalently utilized, notwithstanding its limitations, which include the necessity for a substantial quantity of data points and the presumption of data linearity. However, with recent developments, the long short-term memory (LSTM) network has emerged as a promising alternative, potentially overcoming these limitations. The objective of this study is to determine an effective approach for managing time series data characterized by volatility and missing values. Evaluation was conducted using RMSE for accuracy assessment, and the execution time measured using the Python Timeit library. The findings indicates that in a dataset comprising 60 data points, the LSTM model (RMSE 0.037618) surpasses the ARIMA model (RMSE 0.062667) in terms of accuracy. However, this trend reverses in a larger dataset of 228 data points, where the ARIMA model demonstrates superior accuracy (RMSE 0.006949) compared to the LSTM model (RMSE 0.036025). In scenarios with missing data, the LSTM model consistently outperforms the ARIMA model, although the accuracy of both models diminishes with an increase in the number of missing values. The ARIMA model significantly outpaces the LSTM model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04e91ee60f5701a9bdbb5f9a2e084251c493d694" target='_blank'>
              Comparative analysis of ARIMA and LSTM for predicting fluctuating time series data
              </a>
            </td>
          <td>
            Deddy Gunawan Taslim, I. M. Murwantara
          </td>
          <td>2024-06-01</td>
          <td>Bulletin of Electrical Engineering and Informatics</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Graph neural networks (GNNs) has demonstrated its capabilities in the field of scene graph generation (SGG) by updating node representations from neighboring nodes. Actually it can be viewed as a form of low-pass filter in the spatial domain, which smooths node feature representation and retains commonalities among nodes. However, spatial GNNs does not work well in the case of heterophilic SGG in which fine-grained predicates are always connected to a large number of coarse-grained predicates. Blind smoothing undermines the discriminative information of the fine-grained predicates, resulting in failure to predict them accurately. To address the heterophily, our key idea is to design tailored filters by wavelet transform from the spectral domain. First, we prove rigorously that when the heterophily on the scene graph increases, the spectral energy gradually shifts towards the high-frequency part. Inspired by this observation, we subsequently propose the Kumaraswamy Wavelet Graph Neural Network (KWGNN). KWGNN leverages complementary multi-group Kumaraswamy wavelets to cover all frequency bands. Finally, KWGNN adaptively generates band-pass filters and then integrates the filtering results to better accommodate varying levels of smoothness on the graph. Comprehensive experiments on the Visual Genome and Open Images datasets show that our method achieves state-of-the-art performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c34d39dce5b8611900f747cc27990ea8ac36ef4c" target='_blank'>
              Kumaraswamy Wavelet for Heterophilic Scene Graph Generation
              </a>
            </td>
          <td>
            Lianggangxu Chen, Youqi Song, Shaohui Lin, Changbo Wang, Gaoqi He
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Deep neural networks (DNNs) have achieved significant advancements in click-through rate (CTR) prediction by demonstrating strong generalization on training data. However, in real-world scenarios, the assumption of independent and identically distributed (i.i.d.) conditions, which is fundamental to this problem, is often violated due to temporal distribution shifts. This violation can lead to suboptimal model performance when optimizing empirical risk without access to future data, resulting in overfitting on the training data and convergence to a single sharp minimum. To address this challenge, we propose a novel model updating framework called Slow and Fast Trajectory Learning (SFTL) network. SFTL aims to mitigate the discrepancy between past and future domains while quickly adapting to recent changes in small temporal drifts. This mechanism entails two interactions among three complementary learners: (i) the Working Learner, which updates model parameters using modern optimizers (e.g., Adam, Adagrad) and serves as the primary learner in the recommendation system, (ii) the Slow Learner, which is updated in each temporal domain by directly assigning the model weights of the working learner, and (iii) the Fast Learner, which is updated in each iteration by assigning exponentially moving average weights of the working learner. Additionally, we propose a novel rank-based trajectory loss to facilitate interaction between the working learner and trajectory learner, aiming to adapt to temporal drift and enhance performance in the current domain compared to the past. We provide theoretical understanding and conduct extensive experiments on real-world CTR prediction datasets to validate the effectiveness and efficiency of SFTL in terms of both convergence speed and model performance. The results demonstrate the superiority of SFTL over existing approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/30170172a98512badd6b1e6ad2ccb3e7d97e116e" target='_blank'>
              Generalize for Future: Slow and Fast Trajectory Learning for CTR Prediction
              </a>
            </td>
          <td>
            Jian Zhu, Congcong Liu, Xue Jiang, Changping Peng, Zhangang Lin, Jingping Shao
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Cross-domain Graph Meta-learning (CGML) has shown its promise, where meta-knowledge is extracted from few-shot graph data in multiple relevant but distinct domains. However, several recent efforts assume target data available, which commonly does not established in practice. In this paper, we devise a novel Cross-domain Data Augmentation for Graph Meta-Learning (CDA-GML), which incorporates the superiorities of CGML and Data Augmentation， has addressed intractable shortcomings of label sparsity, domain shift, and the absence of target data simultaneously. Specifically, our method simulates instance-level and task-level domain shift to alleviate the cross-domain generalization issue in conventional graph meta-learning. Experiments show that our method outperforms the existing state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1bfb3d769227e204ce10d3fed9b8c04127015c69" target='_blank'>
              When Sparse Graph Representation Learning Falls into Domain Shift: Data Augmentation for Cross-Domain Graph Meta-Learning (Student Abstract)
              </a>
            </td>
          <td>
            Simin Niu, Xun Liang, Sensen Zhang, Shichao Song, Xuan Zhang, Xiaoping Zhou
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8bd3e0c1b6a68a1068da83003335ac01f1af8dcf" target='_blank'>
              KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding
              </a>
            </td>
          <td>
            Zhen Chen, Dalin Zhang, Shanshan Feng, Kaixuan Chen, Lisi Chen, Peng Han, Shuo Shang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="An essential component of autonomous transportation system management and decision-making is precise and real-time traffic flow forecast. Predicting future traffic conditionsis a difficult undertaking because of the intricate spatio-temporal relationships involved. Existing techniques often employ separate modules to model spatio-temporal features independently, thereby neglecting the temporally and spatially heterogeneous features among nodes. Simultaneously, many existing methods overlook the long-term relationships included in traffic data, subsequently impacting prediction accuracy. We introduce a novel method to traffic flow forecasting based on the combination of the feature-augmented down-sampling dynamic graph convolutional network and multi-head attention mechanism. Our method presents a feature augmentation mechanism to integrate traffic data features at different scales. The subsampled convolutional network enhances information interaction in spatio-temporal data, and the dynamic graph convolutional network utilizes the generated graph structure to better simulate the dynamic relationships between nodes, enhancing the model’s capacity for capturing spatial heterogeneity. Through the feature-enhanced subsampled dynamic graph convolutional network, the model can simultaneously capture spatio-temporal dependencies, and coupled with the process of multi-head temporal attention, it achieves long-term traffic flow forecasting. The findings demonstrate that the ADDGCN model demonstrates superior prediction capabilities on two real datasets (PEMS04 and PEMS08). Notably, for the PEMS04 dataset, compared to the best baseline, the performance of ADDGCN is improved by 2.46% in MAE and 2.90% in RMSE; for the PEMS08 dataset, compared to the best baseline, the ADDGCN performance is improved by 1.50% in RMSE, 3.46% in MAE, and 0.21% in MAPE, indicating our method’s superior performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f430032f1d25a638bb5b96fdec83887a86883c22" target='_blank'>
              ADDGCN: A Novel Approach with Down-Sampling Dynamic Graph Convolution and Multi-Head Attention for Traffic Flow Forecasting
              </a>
            </td>
          <td>
            Zuhua Li, Siwei Wei, Haibo Wang, Chunzhi Wang
          </td>
          <td>2024-05-13</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the long short-term memory (LSTM) network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. Simulation results confirm the superior performance of our approach compared to existing alternatives.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f7913dafeccf6d06cd4bb26ae53b6223af6b3f40" target='_blank'>
              Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation
              </a>
            </td>
          <td>
            Minghui Chen, Zichao Meng, Yanping Liu, Longbo Luo, Ye Guo, Kang Wang
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The utilization of wavelet-based techniques in graph neural networks (GNNs) has gained considerable attention, particularly in the context of node classification. Although existing wavelet-based approaches have shown promise, they are constrained by their reliance on pre-defined wavelet filters, rendering them incapable of effectively adapting to signals that reside on graphs based on tasks at hand. Recent research endeavors address this issue through the introduction of a wavelet lifting transform. However, this technique necessitates the use of bipartite graphs, causing a transformation of the original graph structure into a bipartite configuration. This alteration of graph topology results in the generation of undesirable wavelet filters, thereby undermining the effectiveness of the method. In response to these challenges, we propose a novel simple and effective adaptive graph wavelet neural network (SEA-GWNN) class that employs the lifting scheme on arbitrary graph structures while upholding the original graph topology by leveraging multi-hop computation trees. A noteworthy aspect of the approach is the focus on local substructures represented as acyclic trees, wherein the lifting strategy is applied in a localized manner. This locally defined lifting scheme effectively combines high-pass and low-pass frequency information to enhance node representations. Furthermore, to reduce computing costs, we propose to decouple the higher- order lifting operators and induce them from the lower-order structures. Finally, we benchmark our model on several real- world datasets spanning four distinct categories, including citation networks, webpages, the film industry, and large-scale graphs and the experimental results showcase the efficacy of the proposed SEA-GWNN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69171fd2fdc94b2b8c7b49d0a36a11edb62afa56" target='_blank'>
              SEA-GWNN: Simple and Effective Adaptive Graph Wavelet Neural Network
              </a>
            </td>
          <td>
            Swakshar Deb, Sejuti Rahman, Shafin Rahman
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Although recent advances in machine learning have shown its success to learn from independent and identically distributed (IID) data, it is vulnerable to out-of-distribution (OOD) data in an open world. Domain generalization (DG) deals with such an issue and it aims to learn a model from multiple source domains that can be generalized to unseen target domains. Existing studies on DG have largely focused on stationary settings with homogeneous source domains. However, in many applications, domains may evolve along a specific direction (e.g., time, space). Without accounting for such non-stationary patterns, models trained with existing methods may fail to generalize on OOD data. In this paper, we study domain generalization in non-stationary environment. We first examine the impact of environmental non-stationarity on model performance and establish the theoretical upper bounds for the model error at target domains. Then, we propose a novel algorithm based on adaptive invariant representation learning, which leverages the non-stationary pattern to train a model that attains good performance on target domains. Experiments on both synthetic and real data validate the proposed algorithm.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/96015ed0284d4bbaf35ff8803d4035759a66d8a9" target='_blank'>
              Non-stationary Domain Generalization: Theory and Algorithm
              </a>
            </td>
          <td>
            Thai-Hoang Pham, Xueru Zhang, Ping Zhang
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="The field of meteorological forecasting has undergone a significant transformation with the integration of large models, especially those employing deep learning techniques. This paper reviews the advancements and applications of these models in weather prediction, emphasizing their role in transforming traditional forecasting methods. Models like FourCastNet, Pangu-Weather, GraphCast, ClimaX, and FengWu have made notable contributions by providing accurate, high-resolution forecasts, surpassing the capabilities of traditional Numerical Weather Prediction (NWP) models. These models utilize advanced neural network architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformers, to process diverse meteorological data, enhancing predictive accuracy across various time scales and spatial resolutions. The paper addresses challenges in this domain, including data acquisition and computational demands, and explores future opportunities for model optimization and hardware advancements. It underscores the integration of artificial intelligence with conventional meteorological techniques, promising improved weather prediction accuracy and a significant contribution to addressing climate-related challenges. This synergy positions large models as pivotal in the evolving landscape of meteorological forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15ccc72b3ab3ccf1c9dce22cf5d1a297b750939b" target='_blank'>
              Forecasting the Future with Future Technologies: Advancements in Large Meteorological Models
              </a>
            </td>
          <td>
            Hailong Shu, Yue Wang, Weiwei Song, Huichuang Guo, Zhen Song
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics. We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation. Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction. Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8592296b8f228ba2cb496768cbf50a295b5ddeef" target='_blank'>
              CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series
              </a>
            </td>
          <td>
            Tien-Yu Chang, Hao Dai, Vincent S. Tseng
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Fluid dynamics problems are characterized by being multidimensional and nonlinear, causing the experiments and numerical simulations being complex, time-consuming and monetarily expensive. In this sense, there is a need to find new ways to obtain data in a more economical manner. Thus, in this work we study the application of time series forecasting to fluid dynamics problems, where the aim is to predict the flow dynamics using only past information. We focus our study on models based on deep learning that do not require a high amount of data for training, as this is the problem we are trying to address. Specifically in this work we have tested three autoregressive models where two of them are fully based on deep learning and the other one is a hybrid model that combines modal decomposition with deep learning. We ask these models to generate $200$ time-ahead predictions of two datasets coming from a numerical simulation and experimental measurements, where the latter is characterized by being turbulent. We show how the hybrid model generates more reliable predictions in the experimental case, as it is physics-informed in the sense that the modal decomposition extracts the physics in a way that allows us to predict it.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7b99cbcae24e51709d38ebd55a9ffa77001107f" target='_blank'>
              Exploring the efficacy of a hybrid approach with modal decomposition over fully deep learning models for flow dynamics forecasting
              </a>
            </td>
          <td>
            Rodrigo Abad'ia-Heredia, A. Corrochano, Manuel López-Martín, S. L. Clainche
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3ee2e639751cbe75683537aa0d6841bd9a7341f" target='_blank'>
              From Generalization Analysis to Optimization Designs for State Space Models
              </a>
            </td>
          <td>
            Fusheng Liu, Qianxiao Li
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics. This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics. NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly. The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go. NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data. Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2063b00f6c1d0ea80dc4fa01e9b3b3bddc847ea9" target='_blank'>
              NeuroKoopman Dynamic Causal Discovery
              </a>
            </td>
          <td>
            Rahmat Adesunkanmi, Balaji Sesha Srikanth Pokuri, Ratnesh Kumar
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f94c8d22f66a1306595c5aa5dfe94da89dbf65b3" target='_blank'>
              Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure
              </a>
            </td>
          <td>
            Tobias Ladner, Michael Eichelbeck, Matthias Althoff
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Dynamic graphs are ubiquitous in the real world, yet there is a lack of suitable theoretical frameworks to effectively extend existing static graph models into the temporal domain. Additionally, for link prediction tasks on discrete dynamic graphs, the requirement of substantial GPU memory to store embeddings of all nodes hinders the scalability of existing models. In this paper, we introduce an Input {\bf S}napshots {\bf F}usion based {\bf Dy}namic {\bf G}raph Neural Network (SFDyG). By eliminating the partitioning of snapshots within the input window, we obtain a multi-graph (more than one edge between two nodes). Subsequently, by introducing a graph denoising problem with the assumption of temporal decayed smoothing, we integrate Hawkes process theory into Graph Neural Networks to model the generated multi-graph. Furthermore, based on the multi-graph, we propose a scalable three-step mini-batch training method and demonstrate its equivalence to full-batch training counterpart. Our experiments, conducted on eight distinct dynamic graph datasets for future link prediction tasks, revealed that SFDyG generally surpasses related methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/535a2dc896a7ac80e7bb685b913f82d5e909d0e5" target='_blank'>
              Input Snapshots Fusion for Scalable Discrete Dynamic Graph Nerual Networks
              </a>
            </td>
          <td>
            QingGuo Qi, Hongyang Chen, Minhao Cheng, Han Liu
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68c5cf91fca6e0f4c4be7d30396abc3cfb157425" target='_blank'>
              On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices
              </a>
            </td>
          <td>
            A. Brusaferri, A. Ballarino, Luigi Grossi, F. Laurini
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="In this article, the topic of time series modelling is discussed. It highlights the criticality of analysing and forecasting time series data across various sectors, identifying five primary application areas: denoising, forecasting, nonlinear transient modelling, anomaly detection, and degradation modelling. It further outlines the mathematical frameworks employed in a time series modelling task, categorizing them into statistical, linear algebra, and machine- or deep-learning-based approaches, with each category serving distinct dimensions and complexities of time series problems. Additionally, the article reviews the extensive literature on time series modelling, covering statistical processes, state space representations, and machine and deep learning applications in various fields. The unique contribution of this work lies in its presentation of a Python-based toolkit for time series modelling (PyDTS) that integrates popular methodologies and offers practical examples and benchmarking across diverse datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11f172f463badd08d6233e51c108cd5a1f485637" target='_blank'>
              PyDTS: A Python Toolkit for Deep Learning Time Series Modelling
              </a>
            </td>
          <td>
            P. Schirmer, I. Mporas
          </td>
          <td>2024-03-31</td>
          <td>Entropy</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3142ff658b29bb6120d0dc6588d86aa1d326273" target='_blank'>
              TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
              </a>
            </td>
          <td>
            Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, Bin Yang
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>26</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6bf601536277aa0753655295c23265bb6ac9a45e" target='_blank'>
              Granger causal representation learning for groups of time series
              </a>
            </td>
          <td>
            Ruichu Cai, Yunjin Wu, Xiaokai Huang, Wei Chen, Tom Z. J. Fu, Zhifeng Hao
          </td>
          <td>2024-04-01</td>
          <td>Science China Information Sciences</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In the era of data‐driven transportation development, traffic forecasting is crucial. Established studies either ignore the inherent spatial structure of the traffic network or ignore the global spatial correlation and may not capture the spatial relationships adequately. In this work, a Dynamic Spatial‐Temporal Network (DSTN) based on Joint Latent Space Representation (JLSR) is proposed for traffic forecasting. Specifically, in the spatial dimension, a JLSR network is developed by integrating graph convolution and spatial attention operations to model complex spatial dependencies. Since it can adaptively fuse the representation information of local topological space and global dynamic space, a more comprehensive spatial dependency can be captured. In the temporal dimension, a Stacked Bidirectional Unidirectional Gated Recurrent Unit (SBUGRU) network is developed, which captures long‐term temporal dependencies through both forward and backward computations and superimposed recurrent layers. On these bases, DSTN is developed in an encoder‐decoder framework and periodicity is flexibly modeled by embedding branches. The performance of DSTN is validated on two types of real‐world traffic flow datasets, and it improves over baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ebaed92da591b662802e7ff030301db413a2db3" target='_blank'>
              Dynamic spatial‐temporal network for traffic forecasting based on joint latent space representation
              </a>
            </td>
          <td>
            , Liang Ma, Pei Lai, Jin Guo
          </td>
          <td>2024-05-14</td>
          <td>IET Intelligent Transport Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics. We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its state-of-the-art performance compared to existing models. GrINd offers a promising approach for forecasting physical systems from sparse, scattered observational data, extending the applicability of deep learning methods to real-world scenarios with limited data availability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3d697e1e7480043b4d016b56a9e2fe2c97f066b" target='_blank'>
              GrINd: Grid Interpolation Network for Scattered Observations
              </a>
            </td>
          <td>
            Andrzej Dulny, Paul Heinisch, Andreas Hotho, Anna Krause
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Time series prediction is a fundamental problem in scientific exploration and artificial intelligence (AI) technologies have substantially bolstered its efficiency and accuracy. A well-established paradigm in AI-driven time series prediction is injecting physical knowledge into neural networks through signal decomposition methods, and sustaining progress in numerous scenarios has been reported. However, we uncover non-negligible evidence that challenges the effectiveness of signal decomposition in AI-based time series prediction. We confirm that improper dataset processing with subtle future label leakage is unfortunately widely adopted, possibly yielding abnormally superior but misleading results. By processing data in a strictly causal way without any future information, the effectiveness of additional decomposed signals diminishes. Our work probably identifies an ingrained and universal error in time series modeling, and the de facto progress in relevant areas is expected to be revisited and calibrated to prevent future scientific detours and minimize practical losses.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/53cbaf8d71d1c09ab562766c8295756c1d2810f3" target='_blank'>
              Revisiting the Efficacy of Signal Decomposition in AI-based Time Series Prediction
              </a>
            </td>
          <td>
            Kexin Jiang, Chuhan Wu, Yaoran Chen
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/093f9b0cd66a2356f18ecac15cf4209edae1ca4c" target='_blank'>
              Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks
              </a>
            </td>
          <td>
            Anwoy Chatterjee, Eshaan Tanwar, Subhabrata Dutta, Tanmoy Chakraborty
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="This paper tackles the challenge of time series forecasting in the presence of missing data. Traditional methods often struggle with such data, which leads to inaccurate predictions. We propose a novel framework that combines the strengths of Generative Adversarial Networks (GANs) and Bayesian inference. The framework utilizes a Conditional GAN (C-GAN) to realistically impute missing values in the time series data. Subsequently, Bayesian inference is employed to quantify the uncertainty associated with the forecasts due to the missing data. This combined approach improves the robustness and reliability of forecasting compared to traditional methods. The effectiveness of our proposed method is evaluated on a real-world dataset of air pollution data from Mexico City. The results demonstrate the framework’s capability to handle missing data and achieve improved forecasting accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/03b16464808f3732f9ffbf139c36099fea416c45" target='_blank'>
              Time Series Forecasting with Missing Data Using Generative Adversarial Networks and Bayesian Inference
              </a>
            </td>
          <td>
            Xiaoou Li
          </td>
          <td>2024-04-15</td>
          <td>Inf.</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Attaining the equilibrium geometry of a catalyst-adsorbate system is key to fundamentally assessing its effective properties, such as adsorption energy. While machine learning methods with advanced representation or supervision strategies have been applied to boost and guide the relaxation processes of catalysis systems, existing methods that produce linearly aggregated geometry predictions are susceptible to edge representations ambiguity, and are therefore vulnerable to graph variations. In this paper, we present a novel graph neural network (GNN) supervision and prediction strategy DR-Label. Our approach mitigates the multiplicity of solutions in edge representation and encourages model predictions that are independent of graph structural variations. DR-Label first Deconstructs finer-grained equilibrium state information to the model by projecting the node-level supervision signal to each edge. Reversely, the model Reconstructs a more robust equilibrium state prediction by converting edge-level predictions to node-level via a sphere-fitting algorithm. When applied to three fundamentally different models, DR-Label consistently enhanced performance. Leveraging the graph structure invariance of the DR-Label strategy, we further propose DRFormer, which applied explicit intermediate positional update and achieves a new state-of-the-art performance on the Open Catalyst 2020 (OC20) dataset and the Cu-based single-atom alloys CO adsorption (SAA) dataset. We expect our work to highlight vital principles for advancing geometric GNN models for catalysis systems and beyond. Our code is available at https://github.com/bowenwang77/DR-Label">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7a6ea01655322f614d7ac84d7a94ab11c171c315" target='_blank'>
              DR-Label: Label Deconstruction and Reconstruction of GNN Models for Catalysis Systems
              </a>
            </td>
          <td>
            Bo-Lan Wang, Chen Liang, Jiaze Wang, Jiezhong Qiu, Furui Liu, Shaogang Hao, Dong Li, Guangyong Chen, Xiaolong Zou, P. Heng
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="With recent studies related to Neural Networks being used on different forecasting and time series investigations, this study aims to expand these contexts to ferry passenger traffic. The primary objective of the study is to investigate and evaluate an LSTM-based Neural Networks' capability to forecast ferry passengers of two ports in the Philippines. The proposed model's fitting and evaluation of the passenger flow forecasting of the two ports is based on monthly passenger traffic from 2016 to 2022 data that was acquired from the Philippine Ports Authority (PPA). This work uses Mean Absolute Percentage Error (MAPE) as its primary metric to evaluate the model's forecasting capability. The proposed LSTM-based Neural Networks model achieved 72% forecasting accuracy to the Batangas port ferry passenger data and 74% forecasting accuracy to the Mindoro port ferry passenger data. Using Keras and Scikit-learn Python libraries, this work concludes a reasonable forecasting performance of the presented LSTM model. Aside from these notable findings, this study also recommends further investigation and studies on employing other statistical, machine learning, and deep learning methods on forecasting ferry passenger flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/24729cb958f39afc26b4d58abf4ade3e2a8da58a" target='_blank'>
              Forecasting Ferry Passenger Flow Using Long-Short Term Memory Neural Networks
              </a>
            </td>
          <td>
            Daniel Fesalbon
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's potential in clarifying TGNN decision-making, increasing their transparency and trustworthiness in practice.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fbf8f46c1657b5646a9247a657260b4ccdd239aa" target='_blank'>
              GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs
              </a>
            </td>
          <td>
            Zhan Qu, Daniel Gomm, Michael Färber
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="High-dimensional longitudinal time series data is prevalent across various real-world applications. Many such applications can be modeled as regression problems with high-dimensional time series covariates. Deep learning has been a popular and powerful tool for fitting these regression models. Yet, the development of interpretable and reproducible deep-learning models is challenging and remains underexplored. This study introduces a novel method, Deep Learning Inference using Knockoffs for Time series data (DeepLINK-T), focusing on the selection of significant time series variables in regression while controlling the false discovery rate (FDR) at a predetermined level. DeepLINK-T combines deep learning with knockoff inference to control FDR in feature selection for time series models, accommodating a wide variety of feature distributions. It addresses dependencies across time and features by leveraging a time-varying latent factor structure in time series covariates. Three key ingredients for DeepLINK-T are 1) a Long Short-Term Memory (LSTM) autoencoder for generating time series knockoff variables, 2) an LSTM prediction network using both original and knockoff variables, and 3) the application of the knockoffs framework for variable selection with FDR control. Extensive simulation studies have been conducted to evaluate DeepLINK-T's performance, showing its capability to control FDR effectively while demonstrating superior feature selection power for high-dimensional longitudinal time series data compared to its non-time series counterpart. DeepLINK-T is further applied to three metagenomic data sets, validating its practical utility and effectiveness, and underscoring its potential in real-world applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f0393270cd2a06fcffcd1b1f1aacd3cf0f323ff9" target='_blank'>
              DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM
              </a>
            </td>
          <td>
            Wenxuan Zuo, Zifan Zhu, Yuxuan Du, Yi-Chun Yeh, J. Fuhrman, Jinchi Lv, Yingying Fan, Fengzhu Sun
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>103</td>
        </tr>

        <tr id="In light of recent breakthroughs in large language models (LLMs) that have revolutionized natural language processing (NLP), there is an urgent need for new benchmarks to keep pace with the fast development of LLMs. In this paper, we propose CFLUE, the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. Specifically, CFLUE provides datasets tailored for both knowledge assessment and application assessment. In knowledge assessment, it consists of 38K+ multiple-choice questions with associated solution explanations. These questions serve dual purposes: answer prediction and question reasoning. In application assessment, CFLUE features 16K+ test instances across distinct groups of NLP tasks such as text classification, machine translation, relation extraction, reading comprehension, and text generation. Upon CFLUE, we conduct a thorough evaluation of representative LLMs. The results reveal that only GPT-4 and GPT-4-turbo achieve an accuracy exceeding 60\% in answer prediction for knowledge assessment, suggesting that there is still substantial room for improvement in current LLMs. In application assessment, although GPT-4 and GPT-4-turbo are the top two performers, their considerable advantage over lightweight LLMs is noticeably diminished. The datasets and scripts associated with CFLUE are openly accessible at https://github.com/aliyun/cflue.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21c22e2a16e3e6a95cc8900d687f3f0d14bd2f64" target='_blank'>
              Benchmarking Large Language Models on CFLUE -- A Chinese Financial Language Understanding Evaluation Dataset
              </a>
            </td>
          <td>
            , Junhui Li, , Lifan Guo
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute distribution and topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels. Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in distribution shift on node features and topological structures simultaneously.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c888160af563da7297af0c1867304ae01ab2e1b" target='_blank'>
              Graphs Generalization under Distribution Shifts
              </a>
            </td>
          <td>
            Qin Tian, Wenjun Wang, Chen Zhao, Minglai Shao, Wang Zhang, Dong Li
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/58555d2c57bbf63e2c0a628bf97e5a9c0e77882a" target='_blank'>
              Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation
              </a>
            </td>
          <td>
            Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>5</td>
        </tr>

        <tr id="Contemporary automatic first break (FB) picking methods typically analyze 1D signals, 2D source gathers, or 3D source-receiver gathers. Utilizing higher-dimensional data, such as 2D or 3D, incorporates global features, improving the stability of local picking. Despite the benefits, high-dimensional data requires structured input and increases computational demands. Addressing this, we propose a novel approach using deep graph learning called DGL-FB, constructing a large graph to efficiently extract information. In this graph, each seismic trace is represented as a node, connected by edges that reflect similarities. To manage the size of the graph, we develop a subgraph sampling technique to streamline model training and inference. Our proposed framework, DGL-FB, leverages deep graph learning for FB picking. It encodes subgraphs into global features using a deep graph encoder. Subsequently, the encoded global features are combined with local node signals and fed into a ResUNet-based 1D segmentation network for FB detection. Field survey evaluations of DGL-FB show superior accuracy and stability compared to a 2D U-Net-based benchmark method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d91eb72fdf045333a615d1c8a1db1e3c5ae61a2" target='_blank'>
              Seismic First Break Picking in a Higher Dimension Using Deep Graph Learning
              </a>
            </td>
          <td>
            Hongtao Wang, Lihong Long, Jiangshe Zhang, Xiaoli Wei, Chunxia Zhang, Zhenbo Guo
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Hyperbolic graph convolutional networks (HGCNs) have demonstrated representational capabilities of modeling hierarchical-structured graphs. However, as in general GCNs, over-smoothing may occur as the number of model layers increases, limiting the representation capabilities of most current HGCN models. In this paper, we propose residual hyperbolic graph convolutional networks (R-HGCNs) to address the over-smoothing problem. We introduce a hyperbolic residual connection function to overcome the over-smoothing problem, and also theoretically prove the effectiveness of the hyperbolic residual function. Moreover, we use product manifolds and HyperDrop to facilitate the R-HGCNs. The distinctive features of the R-HGCNs are as follows: (1) The hyperbolic residual connection preserves the initial node information in each layer and adds a hyperbolic identity mapping to prevent node features from being indistinguishable. (2) Product manifolds in R-HGCNs have been set up with different origin points in different components to facilitate the extraction of feature information from a wider range of perspectives, which enhances the representing capability of R-HGCNs. (3) HyperDrop adds multiplicative Gaussian noise into hyperbolic representations, such that perturbations can be added to alleviate the over-fitting problem without deconstructing the hyperbolic geometry.
Experiment results demonstrate the effectiveness of R-HGCNs under various graph convolution layers and different structures of product manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e80d742332bc9a28ce588bb0337bc545506494fe" target='_blank'>
              Residual Hyperbolic Graph Convolution Networks
              </a>
            </td>
          <td>
            Yangkai Xue, Jindou Dai, Zhipeng Lu, Yuwei Wu, Yunde Jia
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/912e380d103ab58f428ce02d8238424b201e9060" target='_blank'>
              Towards efficient similarity embedded temporal Transformers via extended timeframe analysis
              </a>
            </td>
          <td>
            Kenniy Olorunnimbe, Herna Viktor
          </td>
          <td>2024-04-08</td>
          <td>Complex &amp; Intelligent Systems</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Forecasting methods are important decision support tools in geo-distributed sensor networks. However, challenges such as the multivariate nature of data, the existence of multiple nodes, and the presence of spatio-temporal autocorrelation increase the complexity of the task. Existing forecasting methods are unable to address these challenges in a combined manner, resulting in a suboptimal model accuracy. In this article, we propose, a novel geo-distributed forecasting method that leverages the synergic interaction of graph convolution, attention-based long short-term memory (LSTM), 2-D-convolution, and latent memory states to effectively exploit spatio-temporal autocorrelation in multivariate data generated by multiple nodes, resulting in improved modeling capabilities. Our extensive evaluation, involving real-world datasets on traffic, energy, and pollution domains, showcases the ability of our method to outperform state-of-the-art forecasting methods. An ablation study confirms that all method components provide a positive contribution to the accuracy of the extracted forecasts. The method also provides an interpretable visualization that complements forecasts with additional insights for domain experts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/698c810e3963fd24e0dc8076d427fc60b2e8d325" target='_blank'>
              GAP-LSTM: Graph-Based Autocorrelation Preserving Networks for Geo-Distributed Forecasting.
              </a>
            </td>
          <td>
            Massimiliano Altieri, Roberto Corizzo, Michelangelo Ceci
          </td>
          <td>2024-05-17</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster execution time than state-of-the-art ARM approaches in many scenarios. We believe that this study advances a new way of extracting associations from representations and has the potential to inspire more research in this field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aba08ca833bd25e819591944b599c7d4a037e419" target='_blank'>
              AE SemRL: Learning Semantic Association Rules with Autoencoders
              </a>
            </td>
          <td>
            Erkan Karabulut, Victoria Degeler, Paul Groth
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We demonstrate the application of a recurrent neural network (RNN) to perform multistep and multivariate time-series performance predictions for stirred and static mixers as exemplars of complex multiphase systems. We employ two network architectures in this study, fitted with either long short-term memory and gated recurrent unit cells, which are trained on high-fidelity, three-dimensional, computational fluid dynamics simulations of the mixer performance, in the presence and absence of surfactants, in terms of drop size distributions and interfacial areas as a function of system parameters; these include physicochemical properties, mixer geometry, and operating conditions. Our results demonstrate that while it is possible to train RNNs with a single fully connected layer more efficiently than with an encoder–decoder structure, the latter is shown to be more capable of learning long-term dynamics underlying dispersion metrics. Details of the methodology are presented, which include data preprocessing, RNN model exploration, and methods for model performance visualization; an ensemble-based procedure is also introduced to provide a measure of the model uncertainty. The workflow is designed to be generic and can be deployed to make predictions in other industrial applications with similar time-series data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f097de3e6d9869ed978fbb5d99261ced7a9d1250" target='_blank'>
              Liquid–Liquid Dispersion Performance Prediction and Uncertainty Quantification Using Recurrent Neural Networks
              </a>
            </td>
          <td>
            Fuyue Liang, J. Valdés, Sibo Cheng, L. Kahouadji, Seungwon Shin, J. Chergui, D. Juric, Rossella Arcucci, O. K. Matar
          </td>
          <td>2024-04-22</td>
          <td>Industrial & Engineering Chemistry Research</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="We focus on the online-based active learning (OAL) setting where an agent operates over a stream of observations and trades-off between the costly acquisition of information (labelled observations) and the cost of prediction errors. We propose a novel foundation for OAL tasks based on partial monitoring, a theoretical framework specialized in online learning from partially informative actions. We show that previously studied binary and multi-class OAL tasks are instances of partial monitoring. We expand the real-world potential of OAL by introducing a new class of cost-sensitive OAL tasks. We propose NeuralCBP, the first PM strategy that accounts for predictive uncertainty with deep neural networks. Our extensive empirical evaluation on open source datasets shows that NeuralCBP has favorable performance against state-of-the-art baselines on multiple binary, multi-class and cost-sensitive OAL tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c72fefe1c3a7628f4bfa877b393912c5aaf9a2df" target='_blank'>
              Neural Active Learning Meets the Partial Monitoring Framework
              </a>
            </td>
          <td>
            M. Heuillet, Ola Ahmad, Audrey Durand
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e8042bfbf449f5a199713dab9782b3bc45758ec7" target='_blank'>
              Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models
              </a>
            </td>
          <td>
            Aneesh Komanduri, Chengli Zhao, Feng Chen, Xintao Wu
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="While a number of knowledge graph representation learning (KGRL) methods have been proposed over the past decade, very few theoretical analyses have been conducted on them. In this paper, we present the first PAC-Bayesian generalization bounds for KGRL methods. To analyze a broad class of KGRL models, we propose a generic framework named ReED (Relation-aware Encoder-Decoder), which consists of a relation-aware message passing encoder and a triplet classification decoder. Our ReED framework can express at least 15 different existing KGRL models, including not only graph neural network-based models such as R-GCN and CompGCN but also shallow-architecture models such as RotatE and ANALOGY. Our generalization bounds for the ReED framework provide theoretical grounds for the commonly used tricks in KGRL, e.g., parameter-sharing and weight normalization schemes, and guide desirable design choices for practical KGRL methods. We empirically show that the critical factors in our generalization bounds can explain actual generalization errors on three real-world knowledge graphs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4b897f2832773f6d95ffc55d9285bdf6e6b3bb81" target='_blank'>
              PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning
              </a>
            </td>
          <td>
            Jaejun Lee, Minsung Hwang, J. Whang
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In the presence of impulsive noise, and missing observations, accurate online prediction of time-varying graph signals poses a crucial challenge in numerous application domains. We propose the Adaptive Least Mean $p^{th}$ Power Graph Neural Networks (LMP-GNN), a universal framework combining adaptive filter and graph neural network for online graph signal estimation. LMP-GNN retains the advantage of adaptive filtering in handling noise and missing observations as well as the online update capability. The incorporated graph neural network within the LMP-GNN can train and update filter parameters online instead of predefined filter parameters in previous methods, outputting more accurate prediction results. The adaptive update scheme of the LMP-GNN follows the solution of a $l_p$-norm optimization, rooting to the minimum dispersion criterion, and yields robust estimation results for time-varying graph signals under impulsive noise. A special case of LMP-GNN named the Sign-GNN is also provided and analyzed, Experiment results on two real-world datasets of temperature graph and traffic graph under four different noise distributions prove the effectiveness and robustness of our proposed LMP-GNN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2586c398b847113e4aefdb4073bb29499507af68" target='_blank'>
              Adaptive Least Mean pth Power Graph Neural Networks
              </a>
            </td>
          <td>
            Changran Peng, Yi Yan, E. Kuruoglu
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Time series anomaly detection is a critical machine learning task for numerous applications, such as finance, healthcare, and industrial systems. However, even high-performed models may exhibit potential issues such as biases, leading to unreliable outcomes and misplaced confidence. While model explanation techniques, particularly visual explanations, offer valuable insights to detect such issues by elucidating model attributions of their decision, many limitations still exist -- They are primarily instance-based and not scalable across dataset, and they provide one-directional information from the model to the human side, lacking a mechanism for users to address detected issues. To fulfill these gaps, we introduce HILAD, a novel framework designed to foster a dynamic and bidirectional collaboration between humans and AI for enhancing anomaly detection models in time series. Through our visual interface, HILAD empowers domain experts to detect, interpret, and correct unexpected model behaviors at scale. Our evaluation with two time series datasets and user studies demonstrates the effectiveness of HILAD in fostering a deeper human understanding, immediate corrective actions, and the reliability enhancement of models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/41a094747f24e8296ed5b836b4d9f2bc019b0ae6" target='_blank'>
              A Reliable Framework for Human-in-the-Loop Anomaly Detection in Time Series
              </a>
            </td>
          <td>
            Ziquan Deng, Xiwei Xuan, Kwan-Liu Ma, Zhaodan Kong
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Maintaining high standards in socio-economic research and achieving leadership positions in scientific circles requires a scientist to have a perfect command of mathematical tools developing accurate forecasts. Traditional forecasting methods typically involve fitting data to a pre-established relationship between dependent and independent variables, often making specific assumptions about a stochastic process. In contrast, machine learning presents an alternative approach to statistical analysis and forecasting, emphasising a data-driven methodology that does not assume any predefined statistical relationships in the data. The ARIMA method is recognised as the most effective forecasting method in the social sciences and is widely used for analyzing time series data. At the same time, deep learning methods (neural network models) are now forming a serious alternative to traditional econometric models, including ARIMA, as they can exploit nonlinear patterns in the data, which are often hidden from standard linear models. The aim of this study is to evaluate the performance of different neural network (NN) models compared to ARIMA models for economic forecasting. Five different network architectures are studied: Multilayer Perceptron (MLP), Simple Recurrent Neural Network (Simple RNN), Long Short-Term Memory Network (LSTM), Bidirectional LSTM (BILSTM), Convolutional Neural Networks (CNN). The performance of these models is compared with two benchmark ARIMA models: ARIMA based on AIC criteria (ARIMA-AIC) and ARIMA based on BIC criteria (ARIMA-BIC). The performance of these models is compared with two benchmark ARIMA models: ARIMA based on AIC criteria (ARIMA-AIC) and ARIMA based on BIC criteria (ARIMA-BIC). Prior training the NN models, hyperparameters were fine-tuned to obtain optimal performance for each model. The models’ performance on out-of-sample forecasting (test dataset) was evaluated using two metrics: RMSE (root mean square error) and MAE (mean absolute error). The data used to test the effectiveness of these models were monthly year-on-year consumer price inflation data in Algeria. The results show that the MLP model outperformed other models, including the benchmark, in the short and medium term (6-12 months). At the outperformed same time, the LSTM model outperformed all other models, including the benchmarks, in the long term (18-24 month forecasting horizon). Although Simple RNN models performed well in short-term forecasting, their performance deteriorated with the increase in the forecasting horizon. For the benchmark models, the forecasting results were the worst among all models, even in the short term. As a result, MLP and LSTM models were found to be the most appropriate for forecasting Algerian inflation, and deep learning is a promising alternative to traditional time series forecasting methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8359fee04e6e1c8d9502fb5d458e35219a6c762a" target='_blank'>
              Research Leadership and High Standards in Economic Forecasting: Neural Network Models Compared with Etalon ARIMA Models
              </a>
            </td>
          <td>
            Hacene Tchoketch-Kebir, Abderazak Madouri
          </td>
          <td>2024-04-03</td>
          <td>Business Ethics and Leadership</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="
The forecast uncertainty, particularly for precipitation, serves as a crucial indicator of the reliability of deterministic forecasts. Traditionally, forecast uncertainty is estimated by ensemble forecasting, which is computationally expensive since the forecast model is run multiple times with perturbations. Recently, deep learning methods have been explored to learn the statistical properties of ensemble prediction systems due to their low computational costs. However, accurately and effectively capturing the uncertainty information in precipitation forecasts remains challenging. In this study, we present a novel spatiotemporal transformer network (ST-TransNet) as an alternative approach to estimate uncertainty with ensemble spread and probabilistic forecasts, by learning from historical ensemble forecasts. ST-TransNet features a hierarchical structure for extracting multiscale features and incorporates a spatiotemporal transformer module with window-based attention to capture correlations in both spatial and temporal dimensions. Additionally, window-based attention can not only extract local precipitation patterns but also reduce computational costs. The proposed ST-TransNet is evaluated on the TIGGE ensemble forecast dataset and Global Precipitation Measurement (GPM) precipitation products. Results show that ST-TransNet outperforms both traditional and deep learning methods across various metrics. Case studies further demonstrate its ability to generate reasonable and accurate spread and probability forecasts from a single deterministic precipitation forecast. It demonstrates the capacity and efficiency of neural networks in estimating precipitation forecast uncertainty.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44dbf50610c1dcdeaffcb68a009d9a28fa87a267" target='_blank'>
              ST-TransNet: A Spatiotemporal Transformer Network for Uncertainty Estimation from a Single Deterministic Precipitation Forecast
              </a>
            </td>
          <td>
            Jingnan Wang, Xiaodong Wang, Jiping Guan, Lifeng Zhang, , W. Yu
          </td>
          <td>2024-05-01</td>
          <td>Monthly Weather Review</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Recent endeavors aimed at forecasting future traffic flow states through deep learning encounter various challenges and yield diverse outcomes. A notable obstacle arises from the substantial data requirements of deep learning models, a resource often scarce in traffic flow systems. Despite the abundance of domain knowledge concerning traffic flow dynamics, prevailing deep learning methodologies frequently fail to fully exploit it. To address these issues, we propose an innovative solution that merges Convolutional Neural Networks (CNNs) with Long Short-Term Memory (LSTM) architecture to enhance the prediction of traffic flow dynamics. A key revelation of our research is the feasibility of sampling training data for large traffic systems from simulations conducted on smaller traffic systems. This insight suggests the potential for referencing a macroscopic-level distribution to inform the sampling of microscopic data. Such sampling is facilitated by the observed scale invariance in the normalized energy distribution of the statistical mechanics model, thereby streamlining the data generation process for large-scale traffic systems. Our simulations demonstrate promising agreement between predicted and actual traffic flow dynamics, underscoring the efficacy of our proposed approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44c9cc9aa4545e4cc59779727815273cf0d65b8d" target='_blank'>
              Energy-Guided Data Sampling for Traffic Prediction with Mini Training Datasets
              </a>
            </td>
          <td>
            Zhaohui Yang, Kshitij Jerath
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Multiple clustering has gained significant attention in recent years due to its potential to reveal multiple hidden structures of data from different perspectives. The advent of deep multiple clustering techniques has notably advanced the performance by uncovering complex patterns and relationships within large datasets. However, a major challenge arises as users often do not need all the clusterings that algorithms generate, and figuring out the one needed requires a substantial understanding of each clustering result. Traditionally, aligning a user's brief keyword of interest with the corresponding vision components was challenging, but the emergence of multi-modal and large language models (LLMs) has begun to bridge this gap. In response, given unlabeled target visual data, we propose Multi-MaP, a novel method employing a multi-modal proxy learning process. It leverages CLIP encoders to extract coherent text and image embeddings, with GPT-4 integrating users' interests to formulate effective textual contexts. Moreover, reference word constraint and concept-level constraint are designed to learn the optimal text proxy according to the user's interest. Multi-MaP not only adeptly captures a user's interest via a keyword but also facilitates identifying relevant clusterings. Our extensive experiments show that Multi-MaP consistently outperforms state-of-the-art methods in all benchmark multi-clustering vision tasks. Our code is available at https://github.com/Alexander-Yao/Multi-MaP.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/42e45470f8d3c2f79bb03a62b149ca2439e919b3" target='_blank'>
              Multi-Modal Proxy Learning Towards Personalized Visual Multiple Clustering
              </a>
            </td>
          <td>
            Jiawei Yao, Qi Qian, Juhua Hu
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Real-time prediction plays a vital role in various control systems, such as traffic congestion control and wireless channel resource allocation. In these scenarios, the predictor usually needs to track the evolution of the latent statistical patterns in the modern high-dimensional streaming time series continuously and quickly, which presents new challenges for traditional prediction methods. This paper proposes a novel algorithm based on tensor factorization to predict streaming tensor time series online. The proposed algorithm updates the predictor in a low-complexity online manner to adapt to the time-evolving data. Additionally, an automatically adaptive version of the algorithm is presented to mitigate the negative impact of stale data. Simulation results demonstrate that our proposed methods achieve prediction accuracy similar to that of conventional offline tensor prediction methods, while being much faster than them during long-term online prediction. Therefore, our proposed algorithm provides an effective and efficient solution for the online prediction of streaming tensor time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/244cb9f2dff4ea3410904432bb2fd7dbb460c1d8" target='_blank'>
              Online Prediction for Streaming Tensor Time Series
              </a>
            </td>
          <td>
            Zhenting Luan, Haoning Wang, Liping Zhang, Shansuo Liang, Wei Han
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Accurate water demand forecasting is crucial for optimizing the strategies across multiple water sources. This paper proposes the Ensemble Empirical Mode Decomposition Granger causality test Dynamic Graph Attention Transformer Network (EG-DGATN) for multi-sensor cross-temporal granularity water demand forecasting, which combines the Transformer and Graph Neural Networks. It employs the EEMD–Granger test to delineate the interconnections among sensors and extracts the spatiotemporal features within the causal domain by stacking dynamical graph spatiotemporal attention layers. The experimental results demonstrate that compared to baseline models, the EG-DGATN improves the MAPE metrics by 2.12%, 4.33%, and 6.32% in forecasting intervals of 15 min, 45 min, and 90 min, respectively. The model achieves an R2 score of 0.97, indicating outstanding predictive accuracy and exceptional explanatory power for the target variable. This research highlights significant potential applications in predictive tasks within smart water management systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/108a0029fea8639af9eef4279b4539e6c81f321c" target='_blank'>
              Ensemble Empirical Mode Decomposition Granger Causality Test Dynamic Graph Attention Transformer Network: Integrating Transformer and Graph Neural Network Models for Multi-Sensor Cross-Temporal Granularity Water Demand Forecasting
              </a>
            </td>
          <td>
            Wenhong Wu, Yunkai Kang
          </td>
          <td>2024-04-18</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment. With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training. This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series. We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences. Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes. We design a module to provide feedback on the causality during DRL training. Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process. Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1297f09f9d714bde1c5142ffdbc38f19583bb0f6" target='_blank'>
              CIER: A Novel Experience Replay Approach with Causal Inference in Deep Reinforcement Learning
              </a>
            </td>
          <td>
            Jingwen Wang, Dehui Du, Yida Li, Yiyang Li, Yikang Chen
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Nowadays, Generative Large Language Models (GLLMs) have made a significant impact in the field of Artificial Intelligence (AI). One of the domains extensively explored for these models is their ability as generators of functional source code for software projects. Nevertheless, their potential as assistants to write the code needed to generate and model Machine Learning (ML) or Deep Learning (DL) architectures has not been fully explored to date. For this reason, this work focuses on evaluating the extent to which different tools based on GLLMs, such as ChatGPT or Copilot, are able to correctly define the source code necessary to generate viable predictive models. The use case defined is the forecasting of a time series that reports the indoor temperature of a greenhouse. The results indicate that, while it is possible to achieve good accuracy metrics with simple predictive models generated by GLLMs, the composition of predictive models with complex architectures using GLLMs is still far from improving the accuracy of predictive models generated by human data scientists.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e404d419e4a89381bcb2ce5e45fd5b49495525f0" target='_blank'>
              Developing Time Series Forecasting Models with Generative Large Language Models
              </a>
            </td>
          <td>
            Juan Morales-García, Antonio Llanes, Francisco Arcas-Túnez, Fernando Terroso-Sáenz
          </td>
          <td>2024-05-07</td>
          <td>ACM Transactions on Intelligent Systems and Technology</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db62b0e33eef2a7a9b3bcd10f1c68dd9b007c5f8" target='_blank'>
              Lgt: long-range graph transformer for early rumor detection
              </a>
            </td>
          <td>
            Jinghong Xia, Yuling Li, Kui Yu
          </td>
          <td>2024-05-13</td>
          <td>Social Network Analysis and Mining</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In the realm of natural language processing (NLP), a diverse array of language models has emerged, catering to a wide spectrum of tasks, ranging from speaker recognition and auto-correction to sentiment analysis and stock prediction. The significance of language models in enabling the execution of these NLP tasks cannot be overstated. This study proposes an approach to enhance accuracy by leveraging a hybrid language model, combining the strengths of long short-term memory (LSTM) and gated recurrent unit (GRU). LSTM excels in preserving long-term dependencies in data, while GRU's simpler gating mechanism expedites the training process. The research endeavors to evaluate four variations of this hybrid model: LSTM, GRU, bidirectional long short-term memory (Bi-LSTM), and a combination of LSTM with GRU. These models are subjected to rigorous testing on two distinct datasets: one focused on IBM stock price prediction, and the other on Jigsaw toxic comment classification (sentiment analysis). This work represents a significant stride towards democratizing NLP capabilities, ensuring that even in resource-constrained settings, NLP models can exhibit improved performance. The anticipated implications of these findings span a wide spectrum of real-world applications and hold the potential to stimulate further research in the field of NLP. ">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6302a517001a46835f9205ad7dc742a08cb1ef2c" target='_blank'>
              Evaluating the machine learning models based on natural language processing tasks
              </a>
            </td>
          <td>
            Meeradevi Meeradevi, S. B. J., Swetha B. N.
          </td>
          <td>2024-06-01</td>
          <td>IAES International Journal of Artificial Intelligence (IJ-AI)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This paper investigates the enhancement of financial time series forecasting with the use of neural networks through supervised autoencoders, aiming to improve investment strategy performance. It specifically examines the impact of noise augmentation and triple barrier labeling on risk-adjusted returns, using the Sharpe and Information Ratios. The study focuses on the S&P 500 index, EUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30, 2022. Findings indicate that supervised autoencoders, with balanced noise augmentation and bottleneck size, significantly boost strategy effectiveness. However, excessive noise and large bottleneck sizes can impair performance, highlighting the importance of precise parameter tuning. This paper also presents a derivation of a novel optimization metric that can be used with triple barrier labeling. The results of this study have substantial policy implications, suggesting that financial institutions and regulators could leverage techniques presented to enhance market stability and investor protection, while also encouraging more informed and strategic investment approaches in various financial sectors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ebcf5861c81ab65c9bc839ce007819df55a99f0" target='_blank'>
              Supervised Autoencoder Mlp for Financial Time Series Forecasting
              </a>
            </td>
          <td>
            Bartosz Bieganowski, R. Ślepaczuk
          </td>
          <td>2024-04-02</td>
          <td>SSRN Electronic Journal</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Deep learning, a crucial technique for achieving artificial intelligence (AI), has been successfully applied in many fields. The gradual application of the latest architectures of deep learning in the field of time series forecasting (TSF), such as Transformers, has shown excellent performance and results compared to traditional statistical methods. These applications are widely present in academia and in our daily lives, covering many areas including forecasting electricity consumption in power systems, meteorological rainfall, traffic flow, quantitative trading, risk control in finance, sales operations and price predictions for commercial companies, and pandemic prediction in the medical field. Deep learning-based TSF tasks stand out as one of the most valuable AI scenarios for research, playing an important role in explaining complex real-world phenomena. However, deep learning models still face challenges: they need to deal with the challenge of large-scale data in the information age, achieve longer forecasting ranges, reduce excessively high computational complexity, etc. Therefore, novel methods and more effective solutions are essential. In this paper, we review the latest developments in deep learning for TSF. We begin by introducing the recent development trends in the field of TSF and then propose a new taxonomy from the perspective of deep neural network models, comprehensively covering articles published over the past five years. We also organize commonly used experimental evaluation metrics and datasets. Finally, we point out current issues with the existing solutions and suggest promising future directions in the field of deep learning combined with TSF. This paper is the most comprehensive review related to TSF in recent years and will provide a detailed index for researchers in this field and those who are just starting out.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d93c9854b0ec15d7272319e91ee9c2b294327e49" target='_blank'>
              Deep Time Series Forecasting Models: A Comprehensive Survey
              </a>
            </td>
          <td>
            Xinhe Liu, Wenmin Wang
          </td>
          <td>2024-05-11</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Data-Free Knowledge Distillation (DFKD) is a promising task to train high-performance small models to enhance actual deployment without relying on the original training data. Existing methods commonly avoid relying on private data by utilizing synthetic or sampled data. However, a long-overlooked issue is that the severe distribution shifts between their substitution and original data, which manifests as huge differences in the quality of images and class proportions. The harmful shifts are essentially the confounder that significantly causes performance bottlenecks. To tackle the issue, this paper proposes a novel perspective with causal inference to disentangle the student models from the impact of such shifts. By designing a customized causal graph, we first reveal the causalities among the variables in the DFKD task. Subsequently, we propose a Knowledge Distillation Causal Intervention (KDCI) framework based on the backdoor adjustment to de-confound the confounder. KDCI can be flexibly combined with most existing state-of-the-art baselines. Experiments in combination with six representative DFKD methods demonstrate the effectiveness of our KDCI, which can obviously help existing methods under almost all settings, \textit{e.g.}, improving the baseline by up to 15.54\% accuracy on the CIFAR-100 dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8df2db1f34a1fcb75ef4b9e71bc0b49d618ab582" target='_blank'>
              De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts
              </a>
            </td>
          <td>
            Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Real-life graphs often exhibit intricate dynamics that evolve continuously over time. To effectively represent continuous-time dynamic graphs (CTDGs), various temporal graph neural networks (TGNNs) have been developed to model their dynamics and topological structures in Euclidean space. Despite their notable achievements, the performance of Euclidean-based TGNNs is limited and bounded by the representation capabilities of Euclidean geometry, particularly for complex graphs with hierarchical and power-law structures. This is because Euclidean space does not have enough room (its volume grows polynomially with respect to radius) to learn hierarchical structures that expand exponentially. As a result, this leads to high-distortion embeddings and suboptimal temporal graph representations. To break the limitations and enhance the representation capabilities of TGNNs, in this article, we propose a scalable and effective TGNN with hyperbolic geometries for CTDG representation (called STGNh ). It captures evolving behaviors and stores hierarchical structures simultaneously by integrating a memory-based module and a structure-based module into a unified framework, which can scale to billion-scale graphs. Concretely, a simple hyperbolic update gate (HuG) is designed as the memory-based module to store temporal dynamics efficiently; for the structure-based module, we propose an effective hyperbolic temporal Transformer (HyT) model to capture complex graph structures and generate up-to-date node embeddings. Extensive experimental results on a variety of medium-scale and billion-scale graphs demonstrate the superiority of the proposed STGNh for CTDG representation, as it significantly outperforms baselines in various downstream tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bad11c8c673935ce207afa87a3a1f298b7e64b17" target='_blank'>
              Scalable and Effective Temporal Graph Representation Learning With Hyperbolic Geometry.
              </a>
            </td>
          <td>
            Yuanyuan Xu, Wenjie Zhang, Xiwei Xu, Binghao Li, Ying Zhang
          </td>
          <td>2024-05-10</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In real-world scenarios, time series forecasting often demands timeliness, making research on model backbones a perennially hot topic. To meet these performance demands, we propose a novel backbone from the perspective of information fusion. Introducing the Basic Probability Assignment (BPA) Module and the Time Evidence Fusion Network (TEFN), based on evidence theory, allows us to achieve superior performance. On the other hand, the perspective of multi-source information fusion effectively improves the accuracy of forecasting. Due to the fact that BPA is generated by fuzzy theory, TEFN also has considerable interpretability. In real data experiments, the TEFN partially achieved state-of-the-art, with low errors comparable to PatchTST, and operating efficiency surpass performance models such as Dlinear. Meanwhile, TEFN has high robustness and small error fluctuations in the random hyperparameter selection. TEFN is not a model that achieves the ultimate in single aspect, but a model that balances performance, accuracy, stability, and interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b1764f753b4888953e0547ef0c7733cc8f47c18" target='_blank'>
              Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting
              </a>
            </td>
          <td>
            Tianxiang Zhan, Yuanpeng He, Zhen Li, Yong Deng
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Adapting to concept drift is a challenging task in machine learning, which is usually tackled using incremental learning techniques that periodically re-fit a learning model leveraging newly available data. A primary limitation of these techniques is their reliance on substantial amounts of data for retraining. The necessity of acquiring fresh data introduces temporal delays prior to retraining, potentially rendering the models inaccurate if a sudden concept drift occurs in-between two consecutive retrainings. In communication networks, such issue emerges when performing traffic forecasting following a~failure event: post-failure re-routing may induce a drastic shift in distribution and pattern of traffic data, thus requiring a timely model adaptation. In this work, we address this challenge for the problem of traffic forecasting and propose an approach that exploits adaptive learning algorithms, namely, liquid neural networks, which are capable of self-adaptation to abrupt changes in data patterns without requiring any retraining. Through extensive simulations of failure scenarios, we compare the predictive performance of our proposed approach to that of a reference method based on incremental learning. Experimental results show that our proposed approach outperforms incremental learning-based methods in situations where the shifts in traffic patterns are drastic.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85ff9bdb48cac992c4bad81ccda2d7efab6511c3" target='_blank'>
              Liquid Neural Network-based Adaptive Learning vs. Incremental Learning for Link Load Prediction amid Concept Drift due to Network Failures
              </a>
            </td>
          <td>
            Omran Ayoub, Davide Andreoletti, Aleksandra Knapi'nska, R'o.za Go'scie'n, Piotr Lechowicz, T. Leidi, Silvia Giordano, C. Rottondi, Krzysztof Walkowiak
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c4ad25bb62b2fe867acef9a03b2c6995b9534cf2" target='_blank'>
              Cosine similarity knowledge distillation for surface anomaly detection
              </a>
            </td>
          <td>
            Siyu Sheng, Junfeng Jing, Zhen Wang, Huanhuan Zhang
          </td>
          <td>2024-04-08</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks. However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments. Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks. This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks. The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns. Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network. Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios. We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics. Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bc8f4512a0cb0bae50e20e2bc556cdc583b3c1ca" target='_blank'>
              RACH Traffic Prediction in Massive Machine Type Communications
              </a>
            </td>
          <td>
            Hossein Mehri, Hao Chen, Hani Mehrpouyan
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced interpretability and flexibility over traditional designs. Nevertheless, they still suffer from scalability challenges when it comes to the training cost. Although many methods have been proposed to address the scalability issues, they mostly focus on per-iteration efficiency, without worst-case convergence guarantees. Moreover, those methods typically add components to or modify the original model, thus possibly breaking the interpretability of Unfolded GNNs. In this paper, we propose HERTA: a High-Efficiency and Rigorous Training Algorithm for Unfolded GNNs that accelerates the whole training process, achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA converges to the optimum of the original model, thus preserving the interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we propose a new spectral sparsification method applicable to normalized and regularized graph Laplacians that ensures tighter bounds for our algorithm than existing spectral sparsifiers do. Experiments on real-world datasets verify the superiority of HERTA as well as its adaptability to various loss functions and optimizers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4883aa6647a841e703b7f1a9001cd4108144ea15" target='_blank'>
              HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded Graph Neural Networks
              </a>
            </td>
          <td>
            Yongyi Yang, Jiaming Yang, Wei Hu, Michal Derezi'nski
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The graph neural network (GNN) has shown outstanding performance in processing unstructured data. However, the downstream task performance of GNN strongly depends on the accuracy of data graph structural features and, as a type of deep learning (DL) model, the size of the training dataset is equally crucial to its performance. This paper is based on graph neural networks to predict and complete the target radio environment map (REM) through multiple complete REMs and sparse spectrum monitoring data in the target domain. Due to the complexity of radio wave propagation in space, it is difficult to accurately and explicitly construct the spatial graph structure of the spectral data. In response to the two above issues, we propose a multi-source domain adaptive of GNN for regression (GNN-MDAR) model, which includes two key modules: (1) graph structure alignment modules are used to capture and learn graph structure information shared by cross-domain radio propagation and extract reliable graph structure information for downstream reference signal receiving power (RSRP) prediction task; and (2) a spatial distribution matching module is used to reduce the feature distribution mismatch across spatial grids and improve the model’s ability to remain domain invariant. Based on the measured REMs dataset, the comparative results of simulation experiments show that the GNN-MDAR outperforms the other four benchmark methods in accuracy when there is less RSRP label data in the target domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06a062b58a628eb45efd3df15a6e75a6ce2cb38d" target='_blank'>
              Reconstruction of Radio Environment Map Based on Multi-Source Domain Adaptive of Graph Neural Network for Regression
              </a>
            </td>
          <td>
            Xiaomin Wen, Shengliang Fang, Youchen Fan
          </td>
          <td>2024-04-01</td>
          <td>Sensors (Basel, Switzerland)</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction. Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered. Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units. This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels. Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8efabc8e4b3f33379031ce2814195b2dc18ca822" target='_blank'>
              Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes
              </a>
            </td>
          <td>
            Seokhyun Chung, R. Kontar
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9e09bf6a8a79b266089e4fe4816c0babca7887d3" target='_blank'>
              Detecting abnormal cell behaviors from dry mass time series.
              </a>
            </td>
          <td>
            Romain Bailly, Marielle Malfante, Cédric Allier, Chiara Paviolo, Lamya Ghenim, Kiran Padmanabhan, Sabine Bardin, Jérôme Mars
          </td>
          <td>2024-03-25</td>
          <td>Scientific reports</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Continual learning (CL) has spurred the development of several methods aimed at consolidating previous knowledge across sequential learning. Yet, the evaluations of these methods have primarily focused on the final output, such as changes in the accuracy of predicted classes, overlooking the issue of representational forgetting within the model. In this paper, we propose a novel representation-based evaluation framework for CL models. This approach involves gathering internal representations from throughout the continual learning process and formulating three-dimensional tensors. The tensors are formed by stacking representations, such as layer activations, generated from several inputs and model `snapshots', throughout the learning process. By conducting tensor component analysis (TCA), we aim to uncover meaningful patterns about how the internal representations evolve, expecting to highlight the merits or shortcomings of examined CL strategies. We conduct our analyses across different model architectures and importance-based continual learning strategies, with a curated task selection. While the results of our approach mirror the difference in performance of various CL strategies, we found that our methodology did not directly highlight specialized clusters of neurons, nor provide an immediate understanding the evolution of filters. We believe a scaled down version of our approach will provide insight into the benefits and pitfalls of using TCA to study continual learning dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a73798c2efad4134bbeae0fd17132fd7ecc4430a" target='_blank'>
              Examining Changes in Internal Representations of Continual Learning Models Through Tensor Decomposition
              </a>
            </td>
          <td>
            Nishant Suresh Aswani, Amira Guesmi, M. Hanif, Muhammad Shafique
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="This paper introduces a novel approach to predicting periodic time series using reservoir computing. The model is tailored to deliver precise forecasts of rhythms, a crucial aspect for tasks such as generating musical rhythm. Leveraging reservoir computing, our proposed method is ultimately oriented towards predicting human perception of rhythm. Our network accurately predicts rhythmic signals within the human frequency perception range. The model architecture incorporates primary and intermediate neurons tasked with capturing and transmitting rhythmic information. Two parameter matrices, denoted as c and k, regulate the reservoir's overall dynamics. We propose a loss function to adapt c post-training and introduce a dynamic selection (DS) mechanism that adjusts $k$ to focus on areas with outstanding contributions. Experimental results on a diverse test set showcase accurate predictions, further improved through real-time tuning of the reservoir via c and k. Comparative assessments highlight its superior performance compared to conventional models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5148c8074111a79c16d26ed0dd4d21eb948f1c62" target='_blank'>
              A novel Reservoir Architecture for Periodic Time Series Prediction
              </a>
            </td>
          <td>
            Zhongju Yuan, Geraint Wiggins, Dick Botteldooren
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHormer explicitly capture global structure information and recover the expressive power of spiking embeddings, respectively. In experiments, SGHormer achieves comparable performances to other full-precision GTs with extremely low computational energy consumption. The results show that SGHomer makes a remarkable progress in the field of low-energy GTs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d398e802b5492f4cdc32781804737a708a8e9926" target='_blank'>
              SGHormer: An Energy-Saving Graph Transformer Driven by Spikes
              </a>
            </td>
          <td>
            Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks. Our source code is available at https://github.com/liupei101/MIREL.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6872e4b1af6f7a4b30f1cbe2279d941f081da1d" target='_blank'>
              Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation
              </a>
            </td>
          <td>
            Pei Liu, Luping Ji
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The necessity for interpretability in natural language processing (NLP) has risen alongside the growing prominence of large language models. Among the myriad tasks within NLP, text generation stands out as a primary objective of autoregressive models. The NLP community has begun to take a keen interest in gaining a deeper understanding of text generation, leading to the development of model-agnostic explainable artificial intelligence (xAI) methods tailored to this task. The design and evaluation of explainability methods are non-trivial since they depend on many factors involved in the text generation process, e.g., the autoregressive model and its stochastic nature. This paper outlines 17 challenges categorized into three groups that arise during the development and assessment of attribution-based explainability methods. These challenges encompass issues concerning tokenization, defining explanation similarity, determining token importance and prediction change metrics, the level of human intervention required, and the creation of suitable test datasets. The paper illustrates how these challenges can be intertwined, showcasing new opportunities for the community. These include developing probabilistic word-level explainability methods and engaging humans in the explainability pipeline, from the data design to the final evaluation, to draw robust conclusions on xAI methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b58f56e70c18abb76b0d5ea2e4d5190c2c2b1c7b" target='_blank'>
              Challenges and Opportunities in Text Generation Explainability
              </a>
            </td>
          <td>
            Kenza Amara, R. Sevastjanova, Mennatallah El-Assady
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="A plethora of fair graph neural networks (GNNs) have been proposed to promote algorithmic fairness for high-stake real-life contexts. Meanwhile, explainability is generally proposed to help machine learning practitioners debug models by providing human-understandable explanations. However, seldom work on explainability is made to generate explanations for fairness diagnosis in GNNs. From the explainability perspective, this paper explores the problem of what subgraph patterns cause the biased behavior of GNNs, and what actions could practitioners take to rectify the bias? By answering the two questions, this paper aims to produce compact, diagnostic, and actionable explanations that are responsible for discriminatory behavior. Specifically, we formulate the problem of generating diagnostic and actionable explanations as a multi-objective combinatorial optimization problem. To solve the problem, a dedicated multi-objective evolutionary algorithm is presented to ensure GNNs' explainability and fairness in one go. In particular, an influenced nodes-based gradient approximation is developed to boost the computation efficiency of the evolutionary algorithm. We provide a theoretical analysis to illustrate the effectiveness of the proposed framework. Extensive experiments have been conducted to demonstrate the superiority of the proposed method in terms of classification performance, fairness, and interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cff2fdc06235bf6406129616d6c69185a99a8885" target='_blank'>
              Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks
              </a>
            </td>
          <td>
            Zhenzhong Wang, Qingyuan Zeng, Wanyu Lin, Min Jiang, Kay Chen Tan
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="Along with the popularity of mobile Internet and smart applications, more and more high-dimensional sensor data have appeared, and these high-dimensional sensor data have hidden information about system performance degradation, system failure, etc., and how to mine them to obtain such information is a very difficult problem. This challenge can be solved by anomaly detection techniques, which is an important field of research in data mining, especially in the domains of network security, credit card fraud detection, industrial fault identification, etc. However, there are many difficulties in anomaly detection in multivariate time-series data, including poor accuracy, fast data generation, lack of labeled data, and how to capture information between sensors. To address these issues, we present a mutual information and graph embedding based anomaly detection algorithm in multivariate time series, called MGAD (mutual information and graph embedding based anomaly detection). The MGAD algorithm consists of four steps: (1) Embedding of sensor data, where heterogeneous sensor data become different vectors in the same vector space; (2) Constructing a relationship graph between sensors using their mutual information about each other; (3) Learning the relationship graph between sensors using a graph attention mechanism, to predict the sensor data at the next moment; (4) Compare the predicted values with the real sensor data to detect potential outliers. Our contributions are as follows: (1) we propose an unsupervised outlier detection called MGAD with a high interpretability and accuracy; (2) massive experiments on benchmark datasets have demonstrated the superior performance of the MGAD algorithm, compared with state-of-the-art baselines in terms of ROC, F1, and AP.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3948226763c34d98c5c7bea179357fc3b3e59e84" target='_blank'>
              MGAD: Mutual Information and Graph Embedding Based Anomaly Detection in Multivariate Time Series
              </a>
            </td>
          <td>
            Yuehua Huang, Wenfen Liu, Song Li, Ying Guo, Wen Chen
          </td>
          <td>2024-04-01</td>
          <td>Electronics</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Spatio-temporal time series analysis is a growing area of research that includes different types of tasks, such as forecasting, prediction, clustering, and visualization. In many domains, like epidemiology or economics, time series data are collected to describe the observed phenomenon in particular locations over a predefined time slot and predict future behavior. Regression methods provide a simple mechanism for evaluating empirical functions over scattered data points. In particular, kernel-based regressions are suitable for cases in which the relationship between the data points and the function is not linear. In this work, we propose a kernel-based iterative regression model, which fuses data from several spatial locations for improving the forecasting accuracy of a given time series. In more detail, the proposed method approximates and extends a function based on two or more spatial input modalities coded by a series of multiscale kernels, which are averaged as a convex combination. The proposed spatio-temporal regression resembles ideas that are present in deep learning architectures, such as passing information between scales. Nevertheless, the construction is easy to implement, and it is also suitable for modeling data sets of limited size. Experimental results demonstrate the proposed model for solar energy prediction, forecasting epidemiology infections, and future number of fire events. The method is compared with well-known regression techniques and highlights the benefits of the proposed model in terms of accuracy and flexibility. The reliable outcome of the proposed model and its nonparametric nature yield a robust tool to be integrated as a forecasting component in wide range of decision support systems that analyze time series data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This research was supported by the Israel Science Foundation [Grant 1144/20] and partly supported by the Ministry of Science and Technology, Israel [Grant 5614]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6417440/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0019 ).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00426b5f1177aa24a7cc04a6cbbce7a698aa2793" target='_blank'>
              Spatio-Temporal Time Series Forecasting Using an Iterative Kernel-Based Regression
              </a>
            </td>
          <td>
            Ben Hen, N. Rabin
          </td>
          <td>2024-04-16</td>
          <td>INFORMS Journal on Data Science</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have attracted significant interest recently since they can effectively process and analyze graph-structured data commonly found in real-world applications. However, the predicament that GNNs are difficult to train becomes worse as the layers increase. The essence of this problem is that stacking layers will reduce the stability of forward propagation and gradient back-propagation. And as the increasing scale of models (measured by the number of parameters), how to efficiently and effectively adapt it to particular downstream tasks becomes an intriguing research issue. In this work, motivated by the effect of orthogonality constraints, we propose a simple orthogonal training framework to impose the orthogonality constraints on GNNs, which can help models find a solution vector in a specific low dimensional subspace and stabilize the signaling processes at both the forward and backward directions. Specifically, we propose a novel polar decomposition-based orthogonal initialization (PDOI-R) algorithm, which can identify the low intrinsic dimension within the Stiefel Manifold and stabilize the training process. Extensive experiments demonstrate the effectiveness of the proposed method in multiple downstream tasks, showcasing its generality. The simple method can help existing state-of-the-art models achieve better performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db73d1cb2c06a45537250c717efe90d5bb7d828a" target='_blank'>
              Simple Orthogonal Graph Representation Learning (Student Abstract)
              </a>
            </td>
          <td>
            Taoyong Cui, Yuhan Dong
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In the realm of time series analysis, accurately measuring similarity is crucial for applications such as forecasting, anomaly detection, and clustering. However, existing metrics often fail to capture the complex, multidimensional nature of time series data, limiting their effectiveness and application. This paper introduces the Structured Similarity Index Measure for Time Series (TS3IM), a novel approach inspired by the success of the Structural Similarity Index Measure (SSIM) in image analysis, tailored to address these limitations by assessing structural similarity in time series. TS3IM evaluates multiple dimensions of similarity-trend, variability, and structural integrity-offering a more nuanced and comprehensive measure. This metric represents a significant leap forward, providing a robust tool for analyzing temporal data and offering more accurate and comprehensive sequence analysis and decision support in fields such as monitoring power consumption, analyzing traffic flow, and adversarial recognition. Our extensive experimental results also show that compared with traditional methods that rely heavily on computational correlation, TS3IM is 1.87 times more similar to Dynamic Time Warping (DTW) in evaluation results and improves by more than 50% in adversarial recognition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/77d0b56e8f38a92cc887f23e1a9eed755f428834" target='_blank'>
              TS3IM: Unveiling Structural Similarity in Time Series through Image Similarity Assessment Insights
              </a>
            </td>
          <td>
            Yuhan Liu, Ke Tu
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graphs are ubiquitous in various fields, and deep learning methods have been successful applied in graph classification tasks. However, building large and diverse graph datasets for training can be expensive. While augmentation techniques exist for structured data like images or numerical data, the augmentation of graph data remains challenging. This is primarily due to the complex and non-Euclidean nature of graph data. In this paper, it has been proposed a novel augmentation strategy for graphs that operates in a non-Euclidean space. This approach leverages graphon estimation, which models the generative mechanism of networks sequences. Computational results demonstrate the effectiveness of the proposed augmentation framework in improving the performance of graph classification models. Additionally, using a non-Euclidean distance, specifically the Gromow-Wasserstein distance, results in better approximations of the graphon. This framework also provides a means to validate different graphon estimation approaches, particularly in real-world scenarios where the true graphon is unknown.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7b4e7e1a503232caa2117db5e02ce72c25b7145d" target='_blank'>
              Graph data augmentation with Gromow-Wasserstein Barycenters
              </a>
            </td>
          <td>
            Andrea Ponti
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time series are a common form of data, which are of great importance in multiple fields. Multivariate time series whose relationship of dimension is indeterminacy are particularly common within these. For multivariate time series, we proposed a dual-branch structure model, composed of an attention branch and a convolution branch, respectively. The algorithm proposed in our work is implemented for custom computing optimization and deployed on the Xilinx Ultra 96V2 device. Comparative results with other state-of-the-art time series algorithms on public datasets indicate that the proposed method achieves optimal performance.The power consumption of the system is 6.38 W, which is 47.02 times lower than that of a GPU.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/31919a92ff909c681ff5eb5e791f998585b21b96" target='_blank'>
              A Dual-Branch Structure Network of Custom Computing for Multivariate Time Series
              </a>
            </td>
          <td>
            Jingfeng Yu, Yingqi Feng, Zunkai Huang
          </td>
          <td>2024-04-03</td>
          <td>Electronics</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this paper, we aim to both borrow information from existing units and incorporate the target unit's history data in time series forecasting. We consider a situation when we have time series data from multiple units that share similar patterns when aligned in terms of an internal time. The internal time is defined as an index according to evolving features of interest. When mapped back to the calendar time, these time series can span different time intervals that can include the future calendar time of the targeted unit, over which we can borrow the information from other units in forecasting the targeted unit. We first build a hierarchical state space model for the multiple time series data in terms of the internal time, where the shared components capture the similarities among different units while allowing for unit-specific deviations. A conditional state space model is then constructed to incorporate the information of existing units as the prior information in forecasting the targeted unit. By running the Kalman filtering based on the conditional state space model on the targeted unit, we incorporate both the information from the other units and the history of the targeted unit. The forecasts are then transformed from internal time back into calendar time for ease of interpretation. A simulation study is conducted to evaluate the finite sample performance. Forecasting state-level new COVID-19 cases in United States is used for illustration.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f80cbf4a9e8c19e4234812ee4761aa6172be2f6d" target='_blank'>
              Dynamic hierarchical state space forecasting.
              </a>
            </td>
          <td>
            Ziyue Liu, Wensheng Guo
          </td>
          <td>2024-05-01</td>
          <td>Statistics in medicine</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Passenger flow forecasting is an important task in metro operation management. In order to achieve more accurate metro passenger flow forecasting, this paper proposes a metro passenger flow forecasting model based on time-series evolution interaction graph. First, by introducing two kinds of inter-station interaction graphs, namely connectivity graph and temporal correlation graph, to capture the potential interaction relationship among metro passenger flow stations. Then, by using the time-series evolving graph, the weights of the graph convolutional neural network are dynamically evolved in time series. Finally, taking Suzhou Metro as an example, the short-term passenger flow of the metro is forecasted. The experimental results show that the Root Mean Squared Error (RMSE) of this model is 34.17, the Mean Absolute Error (MAE) is 16.35, the R-Squared is 0.94, the Mean Absolute Percent Error (MAPE) is 0.21. All evaluation metrics are better than the baseline models, thus verifying the effectiveness and applicability of the metro passenger flow forecasting model based on the time series evolution interaction graph.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8bc88490e4475e8363d37a4fc5e2bacd119ae9a0" target='_blank'>
              A Metro Passenger Flow Forecasting Model Based On Time-series Evolving Interaction Graph Network
              </a>
            </td>
          <td>
            Jianming Han, Pei Li, Long Li, Yingdi Li, Hantao Zhao
          </td>
          <td>2024-04-18</td>
          <td>Advances in Computer and Engineering Technology Research</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="For industrial processes, it is significant to carry out the dynamic modeling of data series for quality prediction. However, there are often different sampling rates between the input and output sequences. For the most traditional data series models, they have to carefully select the labeled sample sequence to build the dynamic prediction model, while the massive unlabeled input sequences between labeled samples are directly discarded. Moreover, the interactions of the variables and samples are usually not fully considered for quality prediction at each labeled step. To handle these problems, a hierarchical self-attention network (HSAN) is designed for adaptive dynamic modeling. In HSAN, a dynamic data augmentation is first designed for each labeled step to include the unlabeled input sequences. Then, a self-attention layer of variable level is proposed to learn the variable interactions and short-interval temporal dependencies. After that, a self-attention layer of sample level is further developed to model the long-interval temporal dependencies. Finally, a long short-term memory network (LSTM) network is constructed to model the new sequence that contains abundant interactions for quality prediction. The experiment on an industrial hydrocracking process shows the effectiveness of HSAN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ced7280fb59345b0d396521079a4e8aa7e233867" target='_blank'>
              Hierarchical Self-Attention Network for Industrial Data Series Modeling With Different Sampling Rates Between the Input and Output Sequences.
              </a>
            </td>
          <td>
            Xiaofeng Yuan, Zhenzhen Jia, Zijian Xu, Nuo Xu, Lingjian Ye, Kai Wang, Yalin Wang, Chunhua Yang, Weihua Gui, Feifan Shen
          </td>
          <td>2024-04-24</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="Complex urban systems can be difficult to monitor, diagnose and manage because the complete states of such systems are only partially observable with sensors. State estimation techniques can be used to determine the
underlying dynamic behavior of such complex systems with their highly non-linear processes and external time-variant influences.
States can be estimated by clustering observed sensor readings. However,
clustering performance degrades as the number of sensors and readings (i.e. feature dimension) increases. To address this problem, we propose a framework that learns a feature-centric lower dimensional representation of data for clustering to support analysis of system dynamics. We propose Unsupervised Feature Attention with Compact Representation (UFACR) to rank features contributing to a cluster assignment. These weighted features are then used to learn a reduced-dimension temporal representation of the data with a deep-learning model. The resulting low-dimensional representation can be effectively clustered into states. UFACR is evaluated on real-world and synthetic wastewater treatment plant data sets, and feature ranking outcomes were validated by Wastewater treatment domain experts. Our quantitative and qualitative experimental analyses demonstrate the effectiveness of UFACR for uncovering system dynamics in an automated and unsupervised manner to offer guidance to wastewater engineers to enhance industrial productivity and treatment efficiency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68044a97458ed435c3aaac65b727a6f8bc39ca14" target='_blank'>
              Automated State Estimation for Summarizing the Dynamics of Complex Urban Systems Using Representation Learning
              </a>
            </td>
          <td>
            Maira Alvi, Tim French, Philip Keymer, Rachel Cardell-Oliver
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In order to address the issues of predefined adjacency matrices inadequately representing information in road networks, insufficiently capturing spatial dependencies of traffic networks, and the potential problem of excessive smoothing or neglecting initial node information as the layers of graph convolutional neural networks increase, thus affecting traffic prediction performance, this paper proposes a prediction model based on Adaptive Multi-channel Graph Convolutional Neural Networks (AMGCN). The model utilizes an adaptive adjacency matrix to automatically learn implicit graph structures from data, introduces a mixed skip propagation graph convolutional neural network model, which retains the original node states and selectively acquires outputs of convolutional layers, thus avoiding the loss of node initial states and comprehensively capturing spatial correlations of traffic flow. Finally, the output is fed into Long Short-Term Memory networks to capture temporal correlations. Comparative experiments on two real datasets validate the effectiveness of the proposed model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/51c48d60ad1e294d57bebeb298a6c41e771463c6" target='_blank'>
              Research on traffic flow prediction method based on adaptive multi-channel graph convolutional neural networks
              </a>
            </td>
          <td>
            Zhengzheng Xu, Junhua Gu
          </td>
          <td>2024-04-25</td>
          <td>Advances in Engineering Innovation</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal convolutional graph attention network. Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales. We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6742955563c4576ac324c5d0921c9f3d701d5c75" target='_blank'>
              Enhancing Demand Prediction in Open Systems by Cartogram-aided Deep Learning
              </a>
            </td>
          <td>
            Sangjoon Park, Yongsung Kwon, Hyungjoon Soh, Mi Jin Lee, Seung-Woo Son
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows connecting two adjacent time series points to point in the same direction, while the Edge Length Loss (ELL) penalizes arrows - which effectively represent time gaps in the visualized embedding - based on their length. Both loss terms are differentiable and can be easily incorporated into existing dimensionality reduction techniques. By promoting local directionality of the directed edges, our procedure produces more temporally meaningful and less cluttered visualizations. We demonstrate the effectiveness of our approach on a toy dataset and two real-world datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/53a5d42e0e7b9229a8d8353a5d5b058991c029bc" target='_blank'>
              Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE
              </a>
            </td>
          <td>
            P. G. Policar, Blaz Zupan
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In edge computing environments, limited storage and computational resources pose significant challenges to complex super-resolution network models. To address these challenges, we propose an agile super-resolution network via intelligent path selection (ASRN) that utilizes a policy network for dynamic path selection, thereby optimizing the inference process of super-resolution network models. Its primary objective is to substantially reduce the computational burden while maximally maintaining the super-resolution quality. To achieve this goal, a unique reward function is proposed to guide the policy network towards identifying optimal policies. The proposed ASRN not only streamlines the inference process but also significantly boosts inference speed on edge devices without compromising the quality of super-resolution images. Extensive experiments across multiple datasets confirm ASRN’s remarkable ability to accelerate inference speeds while maintaining minimal performance degradation. Additionally, we explore the broad applicability and practical value of ASRN in various edge computing scenarios, indicating its widespread potential in this rapidly evolving domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba682e5d34a8d92689bfd879eee78995b6e4458c" target='_blank'>
              An Agile Super-Resolution Network via Intelligent Path Selection
              </a>
            </td>
          <td>
            Longfei Jia, Yuguo Hu, Xianlong Tian, Wenwei Luo, Yanning Ye
          </td>
          <td>2024-04-05</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Abstract. In this paper, we anticipate geospatial population distributions to quantify the future number of people living in earthquake-prone and tsunami-prone areas of Lima and Callao, Peru. We capitalize upon existing gridded population time series data sets, which are provided on an open-source basis globally, and implement machine learning models tailored for time series analysis, i.e., based on long short-term memory (LSTM) networks, for prediction of future time steps. Specifically, we harvest WorldPop population data and teach LSTM and convolutional LSTM models equipped with both unidirectional and bidirectional learning mechanisms, which are derived from different feature sets, i.e., driving factors. To gain insights regarding the competitive performance of LSTM-based models in this application context, we also implement multilinear regression and random forest models for comparison. The results clearly underline the value of the LSTM-based models for forecasting gridded population data; the most accurate prediction obtained with an LSTM equipped with a bidirectional learning scheme features a root-mean-squared error of 3.63 people per 100 × 100 m grid cell while maintaining an excellent model fit (R2= 0.995). We deploy this model for anticipation of population along a 3-year interval until the year 2035. Especially in areas of high peak ground acceleration of 207–210 cm s−2, the population is anticipated to experience growth of almost 30 % over the forecasted time span, which simultaneously corresponds to 70 % of the predicted additional inhabitants of Lima. The population in the tsunami inundation area is anticipated to grow by 61 % until 2035, which is substantially more than the average growth of 35 % for the city. Uncovering those relations can help urban planners and policymakers to develop effective risk mitigation strategies.
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69e881abe179276012c78c855bbfbbdc15649ab9" target='_blank'>
              Anticipating a risky future: long short-term memory (LSTM) models for spatiotemporal extrapolation of population data in areas prone to earthquakes and tsunamis in Lima, Peru
              </a>
            </td>
          <td>
            C. Geiss, Jana Maier, Emily So, E. Schoepfer, Sven Harig, Juan Camilo Gomez- Zapata, Yue Zhu
          </td>
          <td>2024-03-28</td>
          <td>Natural Hazards and Earth System Sciences</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Buildings are responsible for around 30% and 42% of the consumed energy at the global and European levels, respectively. Accurate building power consumption estimation is crucial for resource saving. This research investigates the combination of graph convolutional networks (GCNs) and long short-term memory networks (LSTMs) to analyze power building consumption, thereby focusing on predictive modeling. Specifically, by structuring graphs based on Pearson’s correlation and Euclidean distance methods, GCNs are employed to discern intricate spatial dependencies, and LSTM is used for temporal dependencies. The proposed models are applied to data from a multistory, multizone educational building, and they are then compared with baseline machine learning, deep learning, and statistical models. The performance of all models is evaluated using metrics such as the mean absolute error (MAE), mean squared error (MSE), R-squared (R2), and the coefficient of variation of the root mean squared error (CV(RMSE)). Among the proposed computation models, one of the Euclidean-based models consistently achieved the lowest MAE and MSE values, thus indicating superior prediction accuracy. The suggested methods seem promising and highlight the effectiveness of GCNs in improving accuracy and reliability in predicting power consumption. The results could be useful in the planning of building energy policies by engineers, as well as in the evaluation of the energy management of structures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59b785cc75513fa129f46173e0966737c5a28cac" target='_blank'>
              Estimating Spatio-Temporal Building Power Consumption Based on Graph Convolution Network Method
              </a>
            </td>
          <td>
            Georgios Vontzos, Vasileios Laitsos, A. Charakopoulos, D. Bargiotas, T. Karakasidis
          </td>
          <td>2024-05-02</td>
          <td>Dynamics</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="In recent years, numerous fuzzy time series (FTS) forecasting models have been developed to address complex and incomplete problems. However, the accuracy of these models is specific to the problem at hand and varies across datasets. Despite claims of superiority over traditional statistical and single machine learning-based models, achieving improved forecasting accuracy remains a formidable challenge. In FTS models, the lengths of intervals and fuzzy relationship groups are considered crucial factors influencing forecasting accuracy. Hence, this study introduces an FTS forecasting model based on the graph-based clustering technique. The clustering algorithm, utilized during the fuzzification stage, enables the derivation of unequal interval lengths. The proposed model is applied to forecast two numerical datasets: enrollment data from the University of Alabama and the datasets of Gas prices RON95 in Vietnam. Comparisons of forecasting results between the proposed model and others are conducted for enrollment forecasts at the University of Alabama. The findings reveal that the proposed model achieves higher forecasting accuracy across all orders of fuzzy relationships when compared to its counterparts">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9b1494e2c261496d606acdd8268c81a739a48db5" target='_blank'>
              Minimizing of Forecasting Error in Fuzzy Time Series Model Using Graph-Based Clustering Method
              </a>
            </td>
          <td>
            N. T. Hai Yen
          </td>
          <td>2024-04-22</td>
          <td>Engineering and Technology Journal</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Neural Ordinary Differential Equations typically struggle to generalize to new dynamical behaviors created by parameter changes in the underlying system, even when the dynamics are close to previously seen behaviors. The issue gets worse when the changing parameters are unobserved, i.e., their value or influence is not directly measurable when collecting data. We introduce Neural Context Flow (NCF), a framework that encodes said unobserved parameters in a latent context vector as input to a vector field. NCFs leverage differentiability of the vector field with respect to the parameters, along with first-order Taylor expansion to allow any context vector to influence trajectories from other parameters. We validate our method and compare it to established Multi-Task and Meta-Learning alternatives, showing competitive performance in mean squared error for in-domain and out-of-distribution evaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scott problems. This study holds practical implications for foundational models in science and related areas that benefit from conditional neural ODEs. Our code is openly available at https://github.com/ddrous/ncflow.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c738e0450649ed6c04ff7e82d993987c381e35a" target='_blank'>
              Neural Context Flows for Learning Generalizable Dynamical Systems
              </a>
            </td>
          <td>
            Roussel Desmond Nzoyem, David A.W. Barton, Tom Deakin
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The development of machine learning applications has increased significantly in recent years, motivated by the remarkable ability of learning-powered systems to discover and generalize intricate patterns hidden in massive datasets. Modern learning models, while powerful, often exhibit a level of complexity that renders them opaque black boxes, resulting in a notable lack of transparency that hinders our ability to decipher their decision-making processes. Opacity challenges the interpretability and practical application of machine learning, especially in critical domains where understanding the underlying reasons is essential for informed decision-making. Explainable Artificial Intelligence (XAI) rises to meet that challenge, unraveling the complexity of black boxes by providing elucidating explanations. Among the various XAI approaches, feature attribution/importance XAI stands out for its capacity to delineate the significance of input features in the prediction process. However, most existing attribution methods have limitations, such as instability, when divergent explanations may result from similar or even the same instance. In this work, we introduce T-Explainer, a novel local additive attribution explainer based on Taylor expansion endowed with desirable properties, such as local accuracy and consistency, while stable over multiple runs. We demonstrate T-Explainer's effectiveness through benchmark experiments with well-known attribution methods. In addition, T-Explainer is developed as a comprehensive XAI framework comprising quantitative metrics to assess and visualize attribution explanations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2aa77cc7a7e741d338b42d9b5ae20d92169e0a8d" target='_blank'>
              T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients
              </a>
            </td>
          <td>
            Evandro S. Ortigossa, F'abio F. Dias, Brian Barr, Claudio T. Silva, L. G. Nonato
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Machine learning-assisted retrosynthesis prediction models have been gaining widespread adoption, though their performances oftentimes degrade significantly when deployed in real-world applications embracing out-of-distribution (OOD) molecules or reactions. Despite steady progress on standard benchmarks, our understanding of existing retrosynthesis prediction models under the premise of distribution shifts remains stagnant. To this end, we first formally sort out two types of distribution shifts in retrosynthesis prediction and construct two groups of benchmark datasets. Next, through comprehensive experiments, we systematically compare state-of-the-art retrosynthesis prediction models on the two groups of benchmarks, revealing the limitations of previous in-distribution evaluation and re-examining the advantages of each model. More remarkably, we are motivated by the above empirical insights to propose two model-agnostic techniques that can improve the OOD generalization of arbitrary off-the-shelf retrosynthesis prediction algorithms. Our preliminary experiments show their high potential with an average performance improvement of 4.6%, and the established benchmarks serve as a foothold for further retrosynthesis prediction research towards OOD generalization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9947402dae7238243f29496fc00451606eb151c7" target='_blank'>
              RetroOOD: Understanding Out-of-Distribution Generalization in Retrosynthesis Prediction
              </a>
            </td>
          <td>
            Yemin Yu, Luotian Yuan, Ying Wei, Hanyu Gao, Xinhai Ye, Zhihua Wang, Fei Wu
          </td>
          <td>2023-12-18</td>
          <td>DBLP, ArXiv</td>
          <td>1</td>
          <td>4</td>
        </tr>

        <tr id="Earnings call transcripts hold valuable insights that are vital for investors and analysts when making informed decisions. However, extracting these insights from lengthy and complex transcripts can be a challenging task. The traditional manual examination is not only time-consuming but also prone to errors and biases. Deep learning-based representation learning methods have emerged as promising and automated approaches to tackle this problem. Nevertheless, they may encounter significant challenges, such as the unreliability of the representation encoding process and certain domain-specific requirements in the context of finance. To address these issues, we propose a novel transcript representation learning model. Our model leverages the structural information of transcripts to effectively extract key insights, while endowing model with explainability via variational information bottleneck. Extensive experiments on two downstream financial tasks demonstrate the effectiveness of our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87bac1755cf0e6dd5ee461486517f80c7c7713a3" target='_blank'>
              Explainable Earnings Call Representation Learning (Student Abstract)
              </a>
            </td>
          <td>
            Yanlong Huang, Yue Lei, Wenxin Tai, Zhangtao Cheng, Ting Zhong, Kunpeng Zhang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In this study, we apply 1D quantum convolution to address the task of time series forecasting. By encoding multiple points into the quantum circuit to predict subsequent data, each point becomes a feature, transforming the problem into a multidimensional one. Building on theoretical foundations from prior research, which demonstrated that Variational Quantum Circuits (VQCs) can be expressed as multidimensional Fourier series, we explore the capabilities of different architectures and ansatz. This analysis considers the concepts of circuit expressibility and the presence of barren plateaus. Analyzing the problem within the framework of the Fourier series enabled the design of an architecture that incorporates data reuploading, resulting in enhanced performance. Rather than a strict requirement for the number of free parameters to exceed the degrees of freedom of the Fourier series, our findings suggest that even a limited number of parameters can produce Fourier functions of higher degrees. This highlights the remarkable expressive power of quantum circuits. This observation is also significant in reducing training times. The ansatz with greater expressibility and number of non-zero Fourier coefficients consistently delivers favorable results across different scenarios, with performance metrics improving as the number of qubits increases.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4fd001be473b84f27090d74293837d9162138767" target='_blank'>
              Fourier Series Guided Design of Quantum Convolutional Neural Networks for Enhanced Time Series Forecasting
              </a>
            </td>
          <td>
            Sandra Leticia Ju'arez Osorio, Mayra Alejandra Rivera Ruiz, Andres Mendez-Vazquez, Eduardo Rodriguez-Tello
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper introduces a novel deep learning approach for intraday stock price direction prediction, motivated by the need for more accurate models to enable profitable algorithmic trading. The key problems addressed are effectively modelling complex limit order book (LOB) and order flow (OF) microstructure data and improving prediction accuracy over current state-of-the-art models. The proposed deep learning model, TrioFlow Fusion of Convolutional Layers and Gated Recurrent Units (TFF-CL-GRU), takes LOB and OF features as input and consists of convolutional layers splitting into three channels before rejoining into a Gated Recurrent Unit. Key innovations include a tailored input representation incorporating LOB and OF features across recent timestamps, a hierarchical feature-learning architecture leveraging convolutional and recurrent layers, and a model design specifically optimised for LOB and OF data. Experiments utilise a new dataset (MICEX LOB OF) with over 1.5 million LOB and OF records and the existing LOBSTER dataset. Comparative evaluation against the state-of-the-art models demonstrates significant performance improvements with the TFF-CL-GRU approach. Through simulated trading experiments, the model also demonstrates practical applicability, yielding positive returns when used for trade signals. This work contributes a new dataset, performance improvements for microstructure-based price prediction, and insights into effectively applying deep learning to financial time-series data. The results highlight the viability of data-driven deep learning techniques in algorithmic trading systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fda95866cd797c65090982e6d2859dafff5deb4" target='_blank'>
              The Intraday Dynamics Predictor: A TrioFlow Fusion of Convolutional Layers and Gated Recurrent Units for High-Frequency Price Movement Forecasting
              </a>
            </td>
          <td>
            Ilia Zaznov, Julian M. Kunkel, A. Badii, Alfonso Dufour
          </td>
          <td>2024-04-02</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="
 Multivariate Time Series Anomaly Detection (MTS-AD) is crucial for the effective management and maintenance of devices in complex systems such as server clusters, spacecrafts and financial systems etc. However, upgrade or cross-platform deployment of these devices will introduce the issue of cross-domain distribution shift, which leads to the prototypical problem of Domain Adaptation for MTS-AD. Compared with general domain adaptation problems, MTS-AD domain adaptation presents two peculiar challenges: 1) The dimensions of data from the source domain and the target domain are usually different, so alignment without losing any information is necessary. 2) The association between different variates plays a vital role in the MTS-AD task, which is overlooked by traditional domain adaptation approaches. Aiming at addressing the above issues, we propose a
 V
 ariate
 A
 ssociated domai
 N
 a
 D
 aptation method combined with a Gr
 A
 ph Deviation Network (abbreviated as
 VANDA
 ) for MTS-AD, which includes two major contributions. First, we characterize the intra-domain variate associations of the source domain by a graph deviation network (GDN), which can share parameters across domains without dimension alignment. Second, we propose a sliding similarity to measure the inter-domain variate associations and perform joint training by minimizing the optimal transport distance between source and target data for transferring variate associations across domains.
 VANDA
 achieves domain adaptation by transferring both variate associations and GDN parameters from the source domain to the target domain. We construct two pairs of MTS-AD datasets from existing MTS-AD data and combine three domain adaptation strategies with six MTS-AD backbones as the benchmark methods for experimental evaluation and comparison. Extensive experiments demonstrate the effectiveness of our approach, which outperforms the benchmark methods, and significantly improves the AD performance of the target domain by effectively utilizing the source domain knowledge.
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c45846b8324c7475291aec48de39c2a80e03ac3c" target='_blank'>
              Variate Associated Domain Adaptation for Unsupervised Multivariate Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Yifan He, Yatao Bian, Xi Ding, Bingzhe Wu, Jihong Guan, Ji Zhang, Shuigeng Zhou
          </td>
          <td>2024-05-03</td>
          <td>ACM Transactions on Knowledge Discovery from Data</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of 20 time steps, while also demonstrating graceful degradation down to 80% accuracy under 50% noise, as well as excellent scalability to swarm sizes from 10 to 100 agents. These capabilities are promising for real-time decision-making support in defense scenarios by rapidly inferring insights about swarm behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9eab35a45dec89e9cd1b4109dcd79f59c78f95f5" target='_blank'>
              Swarm Characteristics Classification Using Neural Networks
              </a>
            </td>
          <td>
            Donald W. Peltier, Isaac Kaminer, Abram H. Clark, Marko Orescanin
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time series prediction is an effective tool for marine scientific research. The Hierarchical Temporal Memory (HTM) model has advantages over traditional recurrent neural network (RNN)-based models due to its online learning and prediction capabilities. Given that the neuronal structure of HTM is ill-equipped for the complexity of long-term marine time series applications, this study proposes a new, improved HTM model, incorporating Gated Recurrent Units (GRUs) neurons into the temporal memory algorithm to overcome this limitation. The capacities and advantages of the proposed model were tested and evaluated on time series data collected from the Xiaoqushan Seafloor Observatory in the East China Sea. The improved HTM model both outperforms the original one in short-term and long-term predictions and presents results with lower errors and better model stability than the GRU model, which is proficient in long-term predictions. The findings allow for the conclusion that the mechanism of online learning has certain advantages in predicting ocean observation data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c5e14ab8fb580a920cf2170a72e735555c0973ff" target='_blank'>
              Improved Hierarchical Temporal Memory for Online Prediction of Ocean Time Series Data
              </a>
            </td>
          <td>
            Tianao Qin, Ruixin Chen, Rufu Qin, Yang Yu
          </td>
          <td>2024-03-28</td>
          <td>Journal of Marine Science and Engineering</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recently, Graph Neural Networks (GNNs) using aggregating neighborhood collaborative information have shown effectiveness in recommendation. However, GNNs-based models suffer from over-smoothing and data sparsity problems. Due to its self-supervised nature, contrastive learning has gained considerable attention in the field of recommendation, aiming at alleviating highly sparse data. Graph contrastive learning models are widely used to learn the consistency of representations by constructing different graph augmentation views. Most current graph augmentation with random perturbation destroy the original graph structure information, which mislead embeddings learning. In this paper, an effective graph contrastive learning paradigm CollaGCL is proposed, which constructs graph augmentation by using singular value decomposition to preserve crucial structure information. CollaGCL enables perturbed views to effectively capture global collaborative information, mitigating the negative impact of graph structural perturbations. To optimize the contrastive learning task, the extracted meta-knowledge was propagate throughout the original graph to learn reliable embedding representations. The self-information learning between views enhances the semantic information of nodes, thus alleviating the problem of over-smoothing. Experimental results on three real-world datasets demonstrate the significant improvement of CollaGCL over state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d5e23ace595a5275241fd0ecbe41d91e559d5096" target='_blank'>
              Collaborative graph contrastive learning for recommendation
              </a>
            </td>
          <td>
            Tao Wei, Changchun Yang, Yanqi Zheng, Jingxue Zhang
          </td>
          <td>2024-04-03</td>
          <td>Journal of Intelligent &amp; Fuzzy Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Traffic state classification and relevance calculation at intersections are both difficult problems in traffic control. In this paper, we propose an intersection relevance model based on a temporal graph attention network, which can solve the above two problems at the same time. First, the intersection features and interaction time of the intersections are regarded as input quantities together with the initial labels of the traffic data. Then, they are inputted into the temporal graph attention (TGAT) model to obtain the classification accuracy of the target intersections in four states—free, stable, slow moving, and congested—and the obtained neighbouring intersection weights are used as the correlation between the intersections. Finally, it is validated by VISSIM simulation experiments. In terms of classification accuracy, the TGAT model has a higher classification accuracy than the three traditional classification models and can cope well with the uneven distribution of the number of samples. The information gain algorithm from the information entropy theory was used to derive the average delay as the most influential factor on intersection status. The correlation from the TGAT model positively correlates with traffic flow, making it interpretable. Using this correlation to control the division of subareas improves the road network’s operational efficiency more than the traditional correlation model does. This demonstrates the effectiveness of the TGAT model’s correlation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b65d407aea8183d7455f6d29386511a8d7c6e8e2" target='_blank'>
              A Study of Adjacent Intersection Correlation Based on Temporal Graph Attention Network
              </a>
            </td>
          <td>
            Pengcheng Li, Baotian Dong, Sixian Li
          </td>
          <td>2024-04-30</td>
          <td>Entropy</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In semi-supervised learning, methods that rely on confidence learning to generate pseudo-labels have been widely proposed. However, increasing research finds that when faced with noisy and biased data, the model's representation network is more reliable than the classification network. Additionally, label generation methods based on model predictions often show poor adaptability across different datasets, necessitating customization of the classification network. Therefore, we propose a Hierarchical Dynamic Labeling (HDL) algorithm that does not depend on model predictions and utilizes image embeddings to generate sample labels. We also introduce an adaptive method for selecting hyperparameters in HDL, enhancing its versatility. Moreover, HDL can be combined with general image encoders (e.g., CLIP) to serve as a fundamental data processing module. We extract embeddings from datasets with class-balanced and long-tailed distributions using pre-trained semi-supervised models. Subsequently, samples are re-labeled using HDL, and the re-labeled samples are used to further train the semi-supervised models. Experiments demonstrate improved model performance, validating the motivation that representation networks are more reliable than classifiers or predictors. Our approach has the potential to change the paradigm of pseudo-label generation in semi-supervised learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8257800d72568527316c97a6367227a373bad488" target='_blank'>
              Exploring Beyond Logits: Hierarchical Dynamic Labeling Based on Embeddings for Semi-Supervised Classification
              </a>
            </td>
          <td>
            Yanbiao Ma, Licheng Jiao, Fang Liu, Lingling Li, Shuyuan Yang, Xu Liu
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="Network‐wide short‐term passenger flow prediction is critical for the operation and management of metro systems. However, it is challenging due to the inherent non‐stationarity, nonlinearity, and spatial–temporal dependencies within passenger flow. To tackle these challenges, this paper introduces a hybrid model called multi‐scale dynamic propagation spatial–temporal network (MSDPSTN). Specifically, the model employs multivariate empirical mode decomposition to jointly decompose the multivariate passenger flow into multi‐scale intrinsic mode functions. Then, a set of dynamic graphs is developed to reveal the passenger propagation law in metro networks. Based on the representation, a deep learning model is proposed to achieve multistep passenger flow prediction, which employs the dynamic propagation graph attention network with long short‐term memory to extract the spatial–temporal dependencies. Extensive experiments conducted on a real‐world dataset from Chengdu, China, validate the superiority of the proposed model. Compared to state‐of‐the‐art baselines, MSDPSTN reduces the mean absolute error, root mean squared error, and mean absolute percentage error by at least 3.243%, 4.451%, and 4.139%, respectively. Further quantitative analyses confirm the effectiveness of the components in MSDPSTN. This paper contributes to addressing inherent features of passenger flow to enhance prediction performance, offering critical insights for decision‐makers in implementing real‐time operational strategies.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/687d90bf0e6fcdf8e281e706939259c5bb7f76da" target='_blank'>
              A dynamic graph deep learning model with multivariate empirical mode decomposition for network‐wide metro passenger flow prediction
              </a>
            </td>
          <td>
            Hao Huang, Jiannan Mao, Leilei Kang, Weike Lu, Sijia Zhang, Lan Liu
          </td>
          <td>2024-04-29</td>
          <td>Computer-Aided Civil and Infrastructure Engineering</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Time series analysis enables the identification of trends and patterns in data, allowing for the development of forecasting models that predict future values. One effective approach for forecasting seasonal time series data is the Seasonal Autoregressive Integrated Moving Average (SARIMA) method. Bagging Exponential Smoothing with STL Decomposition (BES-STL) is an ensemble machine learning method aimed at enhancing forecasting accuracy. STL Method, which stands for Seasonal-Trend decomposition using Loess, is utilized to decompose time series data into three components, namely trend, seasonal, and remainder components. In the remainder component, the process of bootstrap aggregation (bagging) with Moving Block Bootstrapping (MBB) is used to obtain synthetic data, followed by averaging the value by month from the entire series as the forecast results. A comparative analysis was conducted using the SARIMA, BES-STL, and BES-RSTL models. The optimal model, with the lowest MAPE and RMSE, is then implemented to predict national red chili production. The results indicate that the SARIMA(1,1,1)(0,1,1)12 model has the best performance with a MAPE of 7.06 and a RMSE of 95,473. The top-performing model is utilized to forecast data from January to December 2022. Additionally, the forecasted results are compared to the actual data, resulting in a highly precise MAPE of 5.39.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a96b00856143443a65769d402d7ef1211fac9cae" target='_blank'>
              Comparison of SARIMA, Bagging Exponential Smoothing with STL Decomposition and Robust STL Decomposition for Forecasting Red Chili Production
              </a>
            </td>
          <td>
            Titin Agustina, Anwar Fitrianto, Indahwati
          </td>
          <td>2024-03-25</td>
          <td>International Journal of Scientific Research in Science, Engineering and Technology</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The prediction of typhoon tracks in the Northwest Pacific is key to reducing human casualties and property damage. Traditional numerical forecasting models often require substantial computational resources, are high-cost, and have significant limitations in prediction speed. This research is dedicated to using deep learning methods to address the shortcomings of traditional methods. Our method (AFR-SimVP) is based on a large-kernel convolutional spatio-temporal prediction network combined with multi-feature fusion for forecasting typhoon tracks in the Northwest Pacific. In order to more effectively suppress the effect of noise in the dataset to enhance the generalization ability of the model, we use a multi-branch structure, incorporate an atmospheric reconstruction subtask, and propose a second-order smoothing loss to further improve the prediction ability of the model. More importantly, we innovatively propose a multi-time-step typhoon prediction network (HTAFR-SimVP) that does not use the traditional recurrent neural network family of models at all. Instead, through fine-to-coarse hierarchical temporal feature extraction and dynamic self-distillation, multi-time-step prediction is achieved using only a single regression network. In addition, combined with atmospheric field reconstruction, the network achieves integrated prediction for multiple tasks, which greatly enhances the model’s range of applications. Experiments show that our proposed network achieves optimal performance in the 24 h typhoon track prediction task. Our regression network outperforms previous recurrent network-based typhoon prediction models in the multi-time-step prediction task and also performs well in multiple integration tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c689389865010ab7ef6f25d5096c4761d092855" target='_blank'>
              Hierarchical Predictions of Fine-to-Coarse Time Span and Atmospheric Field Reconstruction for Typhoon Track Prediction
              </a>
            </td>
          <td>
            , Zhendong Zhang, 
          </td>
          <td>2024-05-16</td>
          <td>Atmosphere</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8e9cd0106c1e2a1ce4d125ecfde2873792eaf202" target='_blank'>
              MTG_CD: Multi-scale learnable transformation graph for fault classification and diagnosis in microservices
              </a>
            </td>
          <td>
            Juan Chen, Rui Zhang, Peng Chen, , Zongling Wu, , Xi Li, Ling Xiong
          </td>
          <td>2024-05-15</td>
          <td>J. Cloud Comput.</td>
          <td>0</td>
          <td>4</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2023', '2024'],
    y: [1, 37],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>