<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../PINNs/">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey (VPE)" class="md-header__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey (VPE)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Koopman operator
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Time-series%20forecasting/" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Symbolic%20regression/" class="md-tabs__link">
        
  
    
  
  Symbolic regression

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Neural%20ODEs/" class="md-tabs__link">
        
  
    
  
  Neural ODEs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Physics-based%20GNNs/" class="md-tabs__link">
        
  
    
  
  Physics-based GNNs

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Latent%20space%20simulators/" class="md-tabs__link">
        
  
    
  
  Latent space simulators

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../Parametrizing%20using%20ML/" class="md-tabs__link">
        
  
    
  
  Parametrizing using ML

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../PINNs/" class="md-tabs__link">
        
  
    
  
  PINNs

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Koopman operator

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey (VPE)" class="md-nav__button md-logo" aria-label="Literature Survey (VPE)" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey (VPE)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Time-series%20forecasting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Symbolic%20regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Symbolic regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20ODEs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural ODEs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Physics-based%20GNNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physics-based GNNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Latent%20space%20simulators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent space simulators
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parametrizing%20using%20ML/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizing using ML
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../PINNs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PINNs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Koopman operator
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Koopman operator</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-05-21 05:06:45 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Koopman operator</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Koopman operator</a><br>
      <a href="#recommended_articles">3. Recommended articles on Koopman operator</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Koopman operator</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Koopman operator</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf657b5049c1a5c839369d3948ffb4c0584cd1d2" target='_blank'>
                Hamiltonian Systems and Transformation in Hilbert Space.
                </a>
              </td>
          <td>
            B. O. Koopman
          </td>
          <td>1931-05-01</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>1611</td>
          <td>18</td>

            <td><a href='../recommendations/bf657b5049c1a5c839369d3948ffb4c0584cd1d2' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="A majority of methods from dynamical system analysis, especially those in applied settings, rely on Poincaré's geometric picture that focuses on "dynamics of states." While this picture has fueled our field for a century, it has shown difficulties in handling high-dimensional, ill-described, and uncertain systems, which are more and more common in engineered systems design and analysis of "big data" measurements. This overview article presents an alternative framework for dynamical systems, based on the "dynamics of observables" picture. The central object is the Koopman operator: an infinite-dimensional, linear operator that is nonetheless capable of capturing the full nonlinear dynamics. The first goal of this paper is to make it clear how methods that appeared in different papers and contexts all relate to each other through spectral properties of the Koopman operator. The second goal is to present these methods in a concise manner in an effort to make the framework accessible to researchers who would like to apply them, but also, expand and improve them. Finally, we aim to provide a road map through the literature where each of the topics was described in detail. We describe three main concepts: Koopman mode analysis, Koopman eigenquotients, and continuous indicators of ergodicity. For each concept, we provide a summary of theoretical concepts required to define and study them, numerical methods that have been developed for their analysis, and, when possible, applications that made use of them. The Koopman framework is showing potential for crossing over from academic and theoretical use to industrial practice. Therefore, the paper highlights its strengths in applied and numerical contexts. Additionally, we point out areas where an additional research push is needed before the approach is adopted as an off-the-shelf framework for analysis and design.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c9be1e38f978f43427ea5293b3138e0c4fede71" target='_blank'>
                Applied Koopmanism.
                </a>
              </td>
          <td>
            M. Budišić, Ryan Mohr, I. Mezić
          </td>
          <td>2012-06-14</td>
          <td>Chaos</td>
          <td>713</td>
          <td>49</td>

            <td><a href='../recommendations/2c9be1e38f978f43427ea5293b3138e0c4fede71' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace spanned by specially chosen observable functions. The Koopman operator is an infinite-dimensional linear operator that evolves functions of the state of a dynamical system. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems. Choosing the right nonlinear observable functions to form an invariant subspace where it is possible to obtain linear reduced-order models, especially those that are useful for control, is an open challenge. Here, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis by leveraging a new algorithm to determine relevant terms in a dynamical system by ℓ1-regularized regression of the data in a nonlinear function space; we also show how this algorithm is related to DMD. Finally, we demonstrate the usefulness of nonlinear observable subspaces in the design of Koopman operator optimal control laws for fully nonlinear systems using techniques from linear optimal control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3c279828af3621d2c16ac26e5900b970383f60e" target='_blank'>
                Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control
                </a>
              </td>
          <td>
            S. Brunton, Bingni W. Brunton, J. Proctor, J. Kutz
          </td>
          <td>2015-10-11</td>
          <td>PLoS ONE</td>
          <td>443</td>
          <td>61</td>

            <td><a href='../recommendations/a3c279828af3621d2c16ac26e5900b970383f60e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6adeda1af8abc6bc3c17c0b39f635a845476cd9f" target='_blank'>
                Deep learning for universal linear embeddings of nonlinear dynamics
                </a>
              </td>
          <td>
            Bethany Lusch, J. Kutz, S. Brunton
          </td>
          <td>2017-12-27</td>
          <td>Nature Communications</td>
          <td>950</td>
          <td>61</td>

            <td><a href='../recommendations/6adeda1af8abc6bc3c17c0b39f635a845476cd9f' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We develop a deep autoencoder architecture that can be used to find a coordinate transformation which turns a non-linear partial differential equation (PDE) into a linear PDE. Our architecture is motivated by the linearising transformations provided by the Cole–Hopf transform for Burgers’ equation and the inverse scattering transform for completely integrable PDEs. By leveraging a residual network architecture, a near-identity transformation can be exploited to encode intrinsic coordinates in which the dynamics are linear. The resulting dynamics are given by a Koopman operator matrix K. The decoder allows us to transform back to the original coordinates as well. Multiple time step prediction can be performed by repeated multiplication by the matrix K in the intrinsic coordinates. We demonstrate our method on a number of examples, including the heat equation and Burgers’ equation, as well as the substantially more challenging Kuramoto–Sivashinsky equation, showing that our method provides a robust architecture for discovering linearising transforms for non-linear PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b" target='_blank'>
                Deep learning models for global coordinate transformations that linearise PDEs
                </a>
              </td>
          <td>
            Craig Gin, Bethany Lusch, S. Brunton, J. Kutz
          </td>
          <td>2019-11-07</td>
          <td>European Journal of Applied Mathematics</td>
          <td>30</td>
          <td>61</td>

            <td><a href='../recommendations/0ce6f9c3d9dccdc5f7567646be7a7d4c6415576b' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural byproduct of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11df7f23f72703ceefccc6367a6a18719850c53e" target='_blank'>
                From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction
                </a>
              </td>
          <td>
            Henning Lange, S. Brunton, N. Kutz
          </td>
          <td>2020-04-01</td>
          <td>Journal of machine learning research, J. Mach. Learn. Res.</td>
          <td>52</td>
          <td>61</td>

            <td><a href='../recommendations/11df7f23f72703ceefccc6367a6a18719850c53e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68b6ca45a588d538b36335b23f6969c960cf2e6e" target='_blank'>
                Modern Koopman Theory for Dynamical Systems
                </a>
              </td>
          <td>
            S. Brunton, M. Budišić, E. Kaiser, J. Kutz
          </td>
          <td>2021-02-24</td>
          <td>SIAM Review, SIAM Rev.</td>
          <td>253</td>
          <td>61</td>

            <td><a href='../recommendations/68b6ca45a588d538b36335b23f6969c960cf2e6e' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/893768d957f8a46f0ba5bab11e5f2e2698ef1409" target='_blank'>
                Parsimony as the ultimate regularizer for physics-informed machine learning
                </a>
              </td>
          <td>
            J. Kutz, S. Brunton
          </td>
          <td>2022-01-20</td>
          <td>Nonlinear Dynamics</td>
          <td>20</td>
          <td>61</td>

            <td><a href='../recommendations/893768d957f8a46f0ba5bab11e5f2e2698ef1409' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Koopman operator'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Koopman operator</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators. By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis. While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra. Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution. At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing. This provides a robust decomposition encompassing both discrete and continuous spectral elements. We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures. Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability. We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility. This work paves the way for future research and applications of decompositions with continuous spectra.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7d70b9446ba0535cfb98ba6fff01f71c3423ae3" target='_blank'>
              Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators
              </a>
            </td>
          <td>
            Matthew J. Colbrook, Catherine Drysdale, Andrew Horning
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>16</td>
        </tr>

        <tr id="Koopman operators are infinite-dimensional operators that linearize nonlinear dynamical systems, facilitating the study of their spectral properties and enabling the prediction of the time evolution of observable quantities. Recent methods have aimed to approximate Koopman operators while preserving key structures. However, approximating Koopman operators typically requires a dictionary of observables to capture the system's behavior in a finite-dimensional subspace. The selection of these functions is often heuristic, may result in the loss of spectral information, and can severely complicate structure preservation. This paper introduces Multiplicative Dynamic Mode Decomposition (MultDMD), which enforces the multiplicative structure inherent in the Koopman operator within its finite-dimensional approximation. Leveraging this multiplicative property, we guide the selection of observables and define a constrained optimization problem for the matrix approximation, which can be efficiently solved. MultDMD presents a structured approach to finite-dimensional approximations and can more accurately reflect the spectral properties of the Koopman operator. We elaborate on the theoretical framework of MultDMD, detailing its formulation, optimization strategy, and convergence properties. The efficacy of MultDMD is demonstrated through several examples, including the nonlinear pendulum, the Lorenz system, and fluid dynamics data, where we demonstrate its remarkable robustness to noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0cc59f7258cc0c3e219e7edb0f6cbaf13b67c680" target='_blank'>
              Multiplicative Dynamic Mode Decomposition
              </a>
            </td>
          <td>
            Nicolas Boull'e, Matthew J. Colbrook
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world. Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions. In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes. In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems. The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system. Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/253489dec37ed05e844568d9dae4237b151b936f" target='_blank'>
              Koopman-based Deep Learning for Nonlinear System Estimation
              </a>
            </td>
          <td>
            Zexin Sun, Mingyu Chen, John Baillieul
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Several related works have introduced Koopman-based Machine Learning architectures as a surrogate model for dynamical systems. These architectures aim to learn non-linear measurements (also known as observables) of the system's state that evolve by a linear operator and are, therefore, amenable to model-based linear control techniques. So far, mainly simple systems have been targeted, and Koopman architectures as reduced-order models for more complex dynamics have not been fully explored. Hence, we use a Koopman-inspired architecture called the Linear Recurrent Autoencoder Network (LRAN) for learning reduced-order dynamics in convection flows of a Rayleigh B\'enard Convection (RBC) system at different amounts of turbulence. The data is obtained from direct numerical simulations of the RBC system. A traditional fluid dynamics method, the Kernel Dynamic Mode Decomposition (KDMD), is used to compare the LRAN. For both methods, we performed hyperparameter sweeps to identify optimal settings. We used a Normalized Sum of Square Error measure for the quantitative evaluation of the models, and we also studied the model predictions qualitatively. We obtained more accurate predictions with the LRAN than with KDMD in the most turbulent setting. We conjecture that this is due to the LRAN's flexibility in learning complicated observables from data, thereby serving as a viable surrogate model for the main structure of fluid dynamics in turbulent convection settings. In contrast, KDMD was more effective in lower turbulence settings due to the repetitiveness of the convection flow. The feasibility of Koopman-based surrogate models for turbulent fluid flows opens possibilities for efficient model-based control techniques useful in a variety of industrial settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b14d40f8f539b3f8e07c3779360a96930d9f97db" target='_blank'>
              Koopman-Based Surrogate Modelling of Turbulent Rayleigh-B\'enard Convection
              </a>
            </td>
          <td>
            Thorben Markmann, Michiel Straat, Barbara Hammer
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Linearity of Koopman operators and simplicity of their estimators coupled with model-reduction capabilities has lead to their great popularity in applications for learning dynamical systems. While nonparametric Koopman operator learning in infinite-dimensional reproducing kernel Hilbert spaces is well understood for autonomous systems, its control system analogues are largely unexplored. Addressing systems with control inputs in a principled manner is crucial for fully data-driven learning of controllers, especially since existing approaches commonly resort to representational heuristics or parametric models of limited expressiveness and scalability. We address the aforementioned challenge by proposing a universal framework via control-affine reproducing kernels that enables direct estimation of a single operator even for control systems. The proposed approach, called control-Koopman operator regression (cKOR), is thus completely analogous to Koopman operator regression of the autonomous case. First in the literature, we present a nonparametric framework for learning Koopman operator representations of nonlinear control-affine systems that does not suffer from the curse of control input dimensionality. This allows for reformulating the infinite-dimensional learning problem in a finite-dimensional space based solely on data without apriori loss of precision due to a restriction to a finite span of functions or inputs as in other approaches. For enabling applications to large-scale control systems, we also enhance the scalability of control-Koopman operator estimators by leveraging random projections (sketching). The efficacy of our novel cKOR approach is demonstrated on both forecasting and control tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6527db73e0af15e2dff15cbf1ecfc8adfcdd5716" target='_blank'>
              Nonparametric Control-Koopman Operator Learning: Flexible and Scalable Models for Prediction and Control
              </a>
            </td>
          <td>
            Petar Bevanda, Bas Driessen, Lucian-Cristian Iacob, Roland Toth, Stefan Sosnowski, Sandra Hirche
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="The discovery of linear embedding is the key to the synthesis of linear control techniques for nonlinear systems. In recent years, while Koopman operator theory has become a prominent approach for learning these linear embeddings through data-driven methods, these algorithms often exhibit limitations in generalizability beyond the distribution captured by training data and are not robust to changes in the nominal system dynamics induced by intrinsic or environmental factors. To overcome these limitations, this study presents an adaptive Koopman architecture capable of responding to the changes in system dynamics online. The proposed framework initially employs an autoencoder-based neural network that utilizes input-output information from the nominal system to learn the corresponding Koopman embedding offline. Subsequently, we augment this nominal Koopman architecture with a feed-forward neural network that learns to modify the nominal dynamics in response to any deviation between the predicted and observed lifted states, leading to improved generalization and robustness to a wide range of uncertainties and disturbances compared to contemporary methods. Extensive tracking control simulations, which are undertaken by integrating the proposed scheme within a Model Predictive Control framework, are used to highlight its robustness against measurement noise, disturbances, and parametric variations in system dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/302db61f58f8a2e62340fcfaacbceec2620e551a" target='_blank'>
              Adaptive Koopman Embedding for Robust Control of Complex Dynamical Systems
              </a>
            </td>
          <td>
            Rajpal Singh, Chandan Kumar Sah, J. Keshavan
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Data-driven modelling techniques provide a method for deriving models of dynamical systems directly from complicated data streams. However, tracking and forecasting such data streams poses a significant challenge to most methods, as they assume the underlying process and model does not change over time. In this paper, we apply one such data-driven method, the Koopman autoencoder (KAE), to high-dimensional oscillatory data to generate a low-dimensional latent space and model, where the system's dynamics appear linear. This allows one to accurately track and forecast systems where the underlying model may change over time. States and the model in the reduced order latent space can then be efficiently updated as new data becomes available, using data assimilation techniques such as the ensemble Kalman filter (EnKF), in a technique we call the KAE EnKF. We demonstrate that this approach is able to effectively track and forecast time-varying, nonlinear dynamical systems in synthetic examples. We then apply the KAE EnKF to a video of a physical pendulum, and achieve a significant improvement over current state-of-the-art methods. By generating effective latent space reconstructions, we find that we are able to construct accurate short-term forecasts and efficient adaptations to externally forced changes to the pendulum's frequency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/be5728bd13fd3650c966df73ef5772bf8b905261" target='_blank'>
              Tracking and forecasting oscillatory data streams using Koopman autoencoders and Kalman filtering
              </a>
            </td>
          <td>
            Stephen A Falconer, David J. B. Lloyd, N. Santitissadeekorn
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Global information about dynamical systems can be extracted by analysing associated infinite-dimensional transfer operators, such as Perron-Frobenius and Koopman operators as well as their infinitesimal generators. In practice, these operators typically need to be approximated from data. Popular approximation methods are extended dynamic mode decomposition (EDMD) and generator extended mode decomposition (gEDMD). We propose a unified framework that leverages Monte Carlo sampling to approximate the operator of interest on a finite-dimensional space spanned by a set of basis functions. Our framework contains EDMD and gEDMD as special cases, but can also be used to approximate more general operators. Our key contributions are proofs of the convergence of the approximating operator and its spectrum under non-restrictive conditions. Moreover, we derive explicit convergence rates and account for the presence of noise in the observations. Whilst all these results are broadly applicable, they also refine previous analyses of EDMD and gEDMD. We verify the analytical results with the aid of several numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4fd97793c90e0875a903bea631c29c0d483308d" target='_blank'>
              Data-driven approximation of Koopman operators and generators: Convergence rates and error bounds
              </a>
            </td>
          <td>
            Liam Llamazares-Elias, Samir Llamazares-Elias, Jonas Latz, Stefan Klus
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The Koopman operator has gained significant attention in recent years for its ability to verify evolutionary properties of continuous-time nonlinear systems by lifting state variables into an infinite-dimensional linear vector space. The challenge remains in providing estimations for transitional properties pertaining to the system's vector fields based on discrete-time observations. To retrieve such infinitesimal system transition information, leveraging the structure of Koopman operator learning, current literature focuses on developing techniques free of time derivatives through the use of the Koopman operator logarithm. However, the soundness of these methods has so far been demonstrated only for maintaining effectiveness within a restrictive function space, together with knowledge of the operator spectrum properties. To better adapt to the practical applications in learning and control of unknown systems, we propose a logarithm-free technique for learning the infinitesimal generator without disrupting the Koopman operator learning framework. This approach claims compatibility with other system verification tools using the same set of training data. We provide numerical examples to demonstrate its effectiveness in applications of system identification and stability prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7ed5e47d57a5053b1782ff88ac92f4f59c800817" target='_blank'>
              Koopman-Based Learning of Infinitesimal Generators without Operator Logarithm
              </a>
            </td>
          <td>
            Yiming Meng, Rui Zhou, Melkior Ornik, Jun Liu
          </td>
          <td>2024-03-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="We introduce the optimized dynamic mode decomposition algorithm for constructing an adaptive and computationally efficient reduced order model and forecasting tool for global atmospheric chemistry dynamics. By exploiting a low-dimensional set of global spatio-temporal modes, interpretable characterizations of the underlying spatial and temporal scales can be computed. Forecasting is also achieved with a linear model that uses a linear superposition of the dominant spatio-temporal features. The DMD method is demonstrated on three months of global chemistry dynamics data, showing its significant performance in computational speed and interpretability. We show that the presented decomposition method successfully extracts known major features of atmospheric chemistry, such as summertime surface pollution and biomass burning activities. Moreover, the DMD algorithm allows for rapid reconstruction of the underlying linear model, which can then easily accommodate non-stationary data and changes in the dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eafb6d2897f54e1cb6786f47eb557bb66835ba35" target='_blank'>
              Optimized Dynamic Mode Decomposition for Reconstruction and Forecasting of Atmospheric Chemistry Data
              </a>
            </td>
          <td>
            Meghana Velegar, Christoph Keller, J. Kutz
          </td>
          <td>2024-04-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Dynamic Mode Decomposition (DMD) is a widely used data-driven algorithm for estimating the Koopman Operator.This paper investigates how the estimation process is affected when the data is quantized. Specifically, we examine the fundamental connection between estimates of the operator obtained from unquantized data and those from quantized data. Furthermore, using the law of large numbers, we demonstrate that, under a large data regime, the quantized estimate can be considered a regularized version of the unquantized estimate. This key theoretical finding paves the way to accurately recover the unquantized estimate from quantized data. We also explore the relationship between the two estimates in the finite data regime. The theory is validated through repeated numerical experiments conducted on three different dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca8743a3b7438c0b66cccbd2244dc69b48383c58" target='_blank'>
              On the Effect of Quantization on Dynamic Mode Decomposition
              </a>
            </td>
          <td>
            Dipankar Maity, Debdipta Goswami, Sriram Narayanan
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Deep learning algorithms provide a new paradigm to study high-dimensional dynamical behaviors, such as those in fusion plasma systems. Development of novel model reduction methods, coupled with detection of abnormal modes with plasma physics, opens a unique opportunity for building efficient models to identify plasma instabilities for real-time control. Our Fusion Transfer Learning (FTL) model demonstrates success in reconstructing nonlinear kink mode structures by learning from a limited amount of nonlinear simulation data. The knowledge transfer process leverages a pre-trained neural encoder-decoder network, initially trained on linear simulations, to effectively capture nonlinear dynamics. The low-dimensional embeddings extract the coherent structures of interest, while preserving the inherent dynamics of the complex system. Experimental results highlight FTL's capacity to capture transitional behaviors and dynamical features in plasma dynamics -- a task often challenging for conventional methods. The model developed in this study is generalizable and can be extended broadly through transfer learning to address various magnetohydrodynamics (MHD) modes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c07715bd86e219597bf8c42b1d974b4316c9ebd8" target='_blank'>
              FTL: Transfer Learning Nonlinear Plasma Dynamic Transitions in Low Dimensional Embeddings via Deep Neural Networks
              </a>
            </td>
          <td>
            Zhe Bai, Xishuo Wei, William Tang, L. Oliker, Zhihong Lin, Samuel Williams
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>45</td>
        </tr>

        <tr id="Extended dynamic mode decomposition (EDMD) is a well-established method to generate a data-driven approximation of the Koopman operator for analysis and prediction of nonlinear dynamical systems. Recently, kernel EDMD (kEDMD) has gained popularity due to its ability to resolve the challenging task of choosing a suitable dictionary by defining data-based observables. In this paper, we provide the first pointwise bounds on the approximation error of kEDMD. The main idea consists of two steps. First, we show that the reproducing kernel Hilbert spaces of Wendland functions are invariant under the Koopman operator. Second, exploiting that the learning problem given by regression in the native norm can be recast as an interpolation problem, we prove our novel error bounds by using interpolation estimates. Finally, we validate our findings with numerical experiments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aadfea10252491aaad7828e01bd6789958854e69" target='_blank'>
              L∞-error bounds for approximations of the Koopman operator by kernel extended dynamic mode decomposition
              </a>
            </td>
          <td>
            Frederik Köhne, Friedrich M. Philipp, M. Schaller, Anton Schiela, K. Worthmann
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>23</td>
        </tr>

        <tr id="The Koopman operator has entered and transformed many research areas over the last years. Although the underlying concept$\unicode{x2013}$representing highly nonlinear dynamical systems by infinite-dimensional linear operators$\unicode{x2013}$has been known for a long time, the availability of large data sets and efficient machine learning algorithms for estimating the Koopman operator from data make this framework extremely powerful and popular. Koopman operator theory allows us to gain insights into the characteristic global properties of a system without requiring detailed mathematical models. We will show how these methods can also be used to analyze complex networks and highlight relationships between Koopman operators and graph Laplacians.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71e77a5371257de746caf49b4ba2c4de559e5197" target='_blank'>
              Dynamical systems and complex networks: A Koopman operator perspective
              </a>
            </td>
          <td>
            Stefan Klus, Natavsa Djurdjevac Conrad
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/39a4abed8031137161277512e595c8105316e865" target='_blank'>
              Dominant subspaces of high-fidelity polynomial structured parametric dynamical systems and model reduction
              </a>
            </td>
          <td>
            P. Goyal, I. P. Duff, Peter Benner
          </td>
          <td>2024-05-03</td>
          <td>Advances in Computational Mathematics</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="The modeling of nonlinear dynamics based on Koopman operator theory, which is originally applicable only to autonomous systems with no control, is extended to non-autonomous control system without approximation to input matrix B. Prevailing methods using a least square estimate of the B matrix may result in an erroneous input matrix, misinforming the controller about the structure of the input matrix in a lifted space. Here, a new method for constructing a Koopman model that comprises the exact input matrix B is presented. A set of state variables are introduced so that the control inputs are linearly involved in the dynamics of actuators. With these variables, a lifted linear model with the exact control matrix, called a Control-Coherent Koopman Model, is constructed by superposing control input terms, which are linear in local actuator dynamics, to the Koopman operator of the associated autonomous nonlinear system. The proposed method is applied to multi degree-of-freedom robotic arms and multi-cable manipulation systems. Model Predictive Control is applied to the former. It is demonstrated that the prevailing Dynamic Mode Decomposition with Control (DMDc) using an approximate control matrix B does not provide a satisfactory result, while the Control-Coherent Koopman Model performs well with the correct B matrix.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/584a2a67167a69a0d0d957344641d6468886ca9a" target='_blank'>
              Control-Coherent Koopman Modeling: A Physical Modeling Approach
              </a>
            </td>
          <td>
            H. H. Asada, Jose A. Solano-Castellanos
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we consider the design of data-driven predictive controllers for nonlinear systems from input-output data via linear-in-control input Koopman lifted models. Instead of identifying and simulating a Koopman model to predict future outputs, we design a subspace predictive controller in the Koopman space. This allows us to learn the observables minimizing the multi-step output prediction error of the Koopman subspace predictor, preventing the propagation of prediction errors. To avoid losing feasibility of our predictive control scheme due to prediction errors, we compute a terminal cost and terminal set in the Koopman space and we obtain recursive feasibility guarantees through an interpolated initial state. As a third contribution, we introduce a novel regularization cost yielding input-to-state stability guarantees with respect to the prediction error for the resulting closed-loop system. The performance of the developed Koopman data-driven predictive control methodology is illustrated on a nonlinear benchmark example from the literature.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf5ac27ebe1f37285c1d7a4518dacc14d7c81c12" target='_blank'>
              Koopman Data-Driven Predictive Control with Robust Stability and Recursive Feasibility Guarantees
              </a>
            </td>
          <td>
            T. D. Jong, V. Breschi, Maarten Schoukens, Mircea Lazar
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression. However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally. In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification. To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem. First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints. Instead, we implement soft constraints by modifying the cost function to be minimized. This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model. Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0c03c126b4d641a81099470f03a7d5215a2a6820" target='_blank'>
              Multi-objective SINDy for parameterized model discovery from single transient trajectory data
              </a>
            </td>
          <td>
            Javier A. Lemus, Benjamin Herrmann
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Koopman operator theory is a kind of data-driven modelling approach that accurately captures the nonlinearities of mechatronic systems such as vehicles against physics-based methods. However, the infinite-dimensional Koopman operator is impossible to implement in real-world applications. To approximate the infinite-dimensional Koopman operator through collection dataset rather than manual trial and error, we adopt deep neural networks (DNNs) to extract basis functions by offline training and map the nonlinearities of vehicle planar dynamics into a linear form in the lifted space. Besides, the effects of the dimensions of basis functions on the model accuracy are explored. Further, the extended state observer (ESO) is introduced to online estimate the total disturbance in the lifted space and compensate for the modelling errors and residuals of the learned deep Koopman operator (DK) while also improving its generalization. Then, the proposed model is applied to predict vehicle states within prediction horizons and later formulates the constrained finite-time optimization problem of model predictive control (MPC), i.e., ESO-DKMPC. In terms of the trajectory tracking of autonomous vehicles, the ESO-DKMPC generates the wheel steering angle to govern lateral motions based on the decoupling control structure. The various conditions under the double-lane change scenarios are built on the CarSim/Simulink co-simulation platform, and extensive comparisons are conducted with the linear MPC (LMPC) and nonlinear MPC (NMPC) informed by the physics-based model. The results indicate that the proposed ESO-DKMPC has better tracking performance and moderate efficacy both within linear and nonlinear regions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f35ec41629e3225b938d93d6beae5fc75e89733" target='_blank'>
              Incorporating ESO into Deep Koopman Operator Modeling for Control of Autonomous Vehicles
              </a>
            </td>
          <td>
            Hao Chen, Chengqi Lv
          </td>
          <td>2024-05-16</td>
          <td>IEEE Transactions on Control Systems Technology</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="The recently introduced class of architectures known as Neural Operators has emerged as highly versatile tools applicable to a wide range of tasks in the field of Scientific Machine Learning (SciML), including data representation and forecasting. In this study, we investigate the capabilities of Neural Implicit Flow (NIF), a recently developed mesh-agnostic neural operator, for representing the latent dynamics of canonical systems such as the Kuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon (SG) equations, as well as for extracting dynamically relevant information from them. Finally we assess the applicability of NIF as a dimensionality reduction algorithm and conduct a comparative analysis with another widely recognized family of neural operators, known as Deep Operator Networks (DeepONets).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bd2ea0dbc681d0e289d33f89b612d08a007f2fc6" target='_blank'>
              Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems
              </a>
            </td>
          <td>
            Imran Nasim, Joao Lucas de Sousa Almeida
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Turbulent flows are chaotic and multi-scale dynamical systems, which have large numbers of degrees of freedom. Turbulent flows, however, can be modelled with a smaller number of degrees of freedom when using the appropriate coordinate system, which is the goal of dimensionality reduction via nonlinear autoencoders. Autoencoders are expressive tools, but they are difficult to interpret. The goal of this paper is to propose a method to aid the interpretability of autoencoders. This is the decoder decomposition. First, we propose the decoder decomposition, which is a post-processing method to connect the latent variables to the coherent structures of flows. Second, we apply the decoder decomposition to analyse the latent space of synthetic data of a two-dimensional unsteady wake past a cylinder. We find that the dimension of latent space has a significant impact on the interpretability of autoencoders. We identify the physical and spurious latent variables. Third, we apply the decoder decomposition to the latent space of wind-tunnel experimental data of a three-dimensional turbulent wake past a bluff body. We show that the reconstruction error is a function of both the latent space dimension and the decoder size, which are correlated. Finally, we apply the decoder decomposition to rank and select latent variables based on the coherent structures that they represent. This is useful to filter unwanted or spurious latent variables, or to pinpoint specific coherent structures of interest. The ability to rank and select latent variables will help users design and interpret nonlinear autoencoders.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b0d533f9a79dba8eb27c8b9aeb89ecbe15955407" target='_blank'>
              Decoder Decomposition for the Analysis of the Latent Space of Nonlinear Autoencoders With Wind-Tunnel Experimental Data
              </a>
            </td>
          <td>
            Yaxin Mo, Tullio Traverso, L. Magri
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations. The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis. In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems. Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation. For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders. The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3105448eec3c079d49fb5005676d95696e680c56" target='_blank'>
              Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction
              </a>
            </td>
          <td>
            N. R. Franco, Andrea Manzoni, P. Zunino, J. Hesthaven
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>63</td>
        </tr>

        <tr id="The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles. In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models. To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy. Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning. Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations. Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers. Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions. Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fa0602282d2da4dfe0c28a52a63f971fa7313c78" target='_blank'>
              Machine Learning in Viscoelastic Fluids via Energy-Based Kernel Embedding
              </a>
            </td>
          <td>
            Samuel E. Otto, C. Oishi, Fabio Amaral, S. Brunton, J. Kutz
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>61</td>
        </tr>

        <tr id="We apply random matrix theory to study the impact of measurement uncertainty on dynamic mode decomposition. Specifically, when the measurements follow a normal probability density function, we show how the moments of that density propagate through the dynamic mode decomposition. While we focus on the first and second moments, the analytical expressions we derive are general and can be extended to higher-order moments. Further, the proposed numerical method to propagate uncertainty is agnostic of specific dynamic mode decomposition formulations. Of particular relevance, the estimated second moments provide confidence bounds that may be used as a metric of trustworthiness, that is, how much one can rely on a finite-dimensional linear operator to represent an underlying dynamical system. We perform numerical experiments on two canonical systems and verify the estimated confidence levels by comparing the moments to those obtained from Monte Carlo simulations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/524d339e45ff61f43c2adfef5429f8530d8f8541" target='_blank'>
              Statistical analysis and method to propagate the impact of measurement uncertainty on dynamic mode decomposition
              </a>
            </td>
          <td>
            Pooja Algikar, Pranav Sharma, M. Netto, Lamine Mili, P. Sharma, M. Netto
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Establishing appropriate mathematical models for complex systems in natural phenomena not only helps deepen our understanding of nature but can also be used for state estimation and prediction. However, the extreme complexity of natural phenomena makes it extremely challenging to develop full-order models (FOMs) and apply them to studying many quantities of interest. In contrast, appropriate reduced-order models (ROMs) are favored due to their high computational efficiency and ability to describe the key dynamics and statistical characteristics of natural phenomena. Taking the viscous Burgers equation as an example, this paper constructs a Convolutional Autoencoder-Reservoir Computing-Normalizing Flow algorithm framework, where the Convolutional Autoencoder is used to construct latent space representations, and the Reservoir Computing-Normalizing Flow framework is used to characterize the evolution of latent state variables. In this way, a data-driven stochastic parameter reduced-order model is constructed to describe the complex system and its dynamic behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f199be4cede75b9048f7e55d590a2a44646f487b" target='_blank'>
              Stochastic parameter reduced-order model based on hybrid machine learning approaches
              </a>
            </td>
          <td>
            Cheng Fang, Jinqiao Duan
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Koopman operator framework holds promise for spectral analysis of nonlinear dynamical systems based on linear operators. Eigenvalues and eigenfunctions of the Koopman operator, so-called Koopman eigenvalues and Koopman eigenfunctions, respectively, mirror global properties of the system's flow. In this paper we perform the Koopman analysis of the singularly-perturbed van der Pol system. First, we show the spectral signature depending on singular perturbation: how two Koopman principle eigenvalues are ordered and what distinct shapes emerge in their associated Koopman eigenfunctions. Second, we discuss the singular limit of the Koopman operator, which is derived through the concatenation of Koopman operators for the fast and slow subsystems. From the spectral properties of the Koopman operator for the singularl-perturbed system and the singular limit, we suggest that the Koopman eigenfunctions inherit geometric properties of the singularly-perturbed system. These results are applicable to general planar singularly-perturbed systems with stable limit cycles.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44832568206bd357d00878ddf822a227a13fd679" target='_blank'>
              Koopman Analysis of the Singularly-Perturbed van der Pol Oscillator
              </a>
            </td>
          <td>
            Natsuki Katayama, Yoshihiko Susuki
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="An iterated multistep forecasting scheme based on recurrent neural networks (RNN) is proposed for the time series generated by causal chains with infinite memory. This forecasting strategy contains, as a particular case, the iterative prediction strategies for dynamical systems that are customary in reservoir computing. Readily computable error bounds are obtained as a function of the forecasting horizon, functional and dynamical features of the specific RNN used, and the approximation error committed by it. The framework in the paper circumvents difficult-to-verify embedding hypotheses that appear in previous references in the literature and applies to new situations like the finite-dimensional observations of functional differential equations or the deterministic parts of stochastic processes to which standard embedding techniques do not necessarily apply.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/08c01d5b50fd31d1eec0663fad8a6c82d4655e41" target='_blank'>
              Forecasting causal dynamics with universal reservoirs
              </a>
            </td>
          <td>
            Lyudmila Grigoryeva, James Louw, Juan-Pablo Ortega
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="We present a data-driven pipeline for model building that combines interpretable machine learning, hydrodynamic theories, and microscopic models. The goal is to uncover the underlying processes governing nonlinear dynamics experiments. We exemplify our method with data from microfluidic experiments where crystals of streaming droplets support the propagation of nonlinear waves absent in passive crystals. By combining physics-inspired neural networks, known as neural operators, with symbolic regression tools, we generate the solution, as well as the mathematical form, of a nonlinear dynamical system that accurately models the experimental data. Finally, we interpret this continuum model from fundamental physics principles. Informed by machine learning, we coarse grain a microscopic model of interacting droplets and discover that non-reciprocal hydrodynamic interactions stabilise and promote nonlinear wave propagation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/687a2bdf4046a679f876d2b660bf758b24b136f2" target='_blank'>
              Interpreting neural operators: how nonlinear waves propagate in non-reciprocal solids
              </a>
            </td>
          <td>
            Jonathan Colen, Alexis Poncet, Denis Bartolo, Vincenzo Vitelli
          </td>
          <td>2024-04-19</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>6</td>
        </tr>

        <tr id="The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data. However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets. Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms. To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework. SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization. In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit. In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm. In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9f2e0f138fdb706edb87999a79e0c8ba055c75b7" target='_blank'>
              GN-SINDy: Greedy Sampling Neural Network in Sparse Identification of Nonlinear Partial Differential Equations
              </a>
            </td>
          <td>
            A. Forootani, Peter Benner
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="This paper is concerned with collective variables, or reaction coordinates, that map a discrete-in-time Markov process $X_n$ in $\mathbb{R}^d$ to a (much) smaller dimension $k \ll d$. We define the effective dynamics under a given collective variable map $\xi$ as the best Markovian representation of $X_n$ under $\xi$. The novelty of the paper is that it gives strict criteria for selecting optimal collective variables via the properties of the effective dynamics. In particular, we show that the transition density of the effective dynamics of the optimal collective variable solves a relative entropy minimization problem from certain family of densities to the transition density of $X_n$. We also show that many transfer operator-based data-driven numerical approaches essentially learn quantities of the effective dynamics. Furthermore, we obtain various error estimates for the effective dynamics in approximating dominant timescales / eigenvalues and transition rates of the original process $X_n$ and how optimal collective variables minimize these errors. Our results contribute to the development of theoretical tools for the understanding of complex dynamical systems, e.g. molecular kinetics, on large timescales. These results shed light on the relations among existing data-driven numerical approaches for identifying good collective variables, and they also motivate the development of new methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/000831a8effc8d5eba43ba65cb8b22d1d5706ebb" target='_blank'>
              On finding optimal collective variables for complex systems by minimizing the deviation between effective and full dynamics
              </a>
            </td>
          <td>
            Wei Zhang, Christof Schutte
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Discovering a suitable neural network architecture for modeling complex dynamical systems poses a formidable challenge, often involving extensive trial and error and navigation through a high-dimensional hyper-parameter space. In this paper, we discuss a systematic approach to constructing neural architectures for modeling a subclass of dynamical systems, namely, Linear Time-Invariant (LTI) systems. We use a variant of continuous-time neural networks in which the output of each neuron evolves continuously as a solution of a first-order or second-order Ordinary Differential Equation (ODE). Instead of deriving the network architecture and parameters from data, we propose a gradient-free algorithm to compute sparse architecture and network parameters directly from the given LTI system, leveraging its properties. We bring forth a novel neural architecture paradigm featuring horizontal hidden layers and provide insights into why employing conventional neural architectures with vertical hidden layers may not be favorable. We also provide an upper bound on the numerical errors of our neural networks. Finally, we demonstrate the high accuracy of our constructed networks on three numerical examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db63cb90ec895b72fae7529101e17e991276dd72" target='_blank'>
              Systematic construction of continuous-time neural networks for linear dynamical systems
              </a>
            </td>
          <td>
            Chinmay Datar, Adwait Datar, Felix Dietrich, W. Schilders
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="We utilize extreme learning machines for the prediction of partial differential equations (PDEs). Our method splits the state space into multiple windows that are predicted individually using a single model. Despite requiring only few data points (in some cases, our method can learn from a single full-state snapshot), it still achieves high accuracy and can predict the flow of PDEs over long time horizons. Moreover, we show how additional symmetries can be exploited to increase sample efficiency and to enforce equivariance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a4edc85da3f3309576bf3b09310e6de02a94929" target='_blank'>
              Predicting PDEs Fast and Efficiently with Equivariant Extreme Learning Machines
              </a>
            </td>
          <td>
            Hans Harder, Sebastian Peitz
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Adjoint methods have been the pillar of gradient-based optimization for decades. They enable the accurate computation of a gradient (sensitivity) of a quantity of interest with respect to all system's parameters in one calculation. When the gradient is embedded in an optimization routine, the quantity of interest can be optimized for the system to have the desired behaviour. Adjoint methods require the system's Jacobian, whose computation can be cumbersome, and is problem dependent. We propose a computational strategy to infer the adjoint sensitivities from data (observables), which bypasses the need of the Jacobian of the physical system. The key component of this strategy is an echo state network, which learns the dynamics of nonlinear regimes with varying parameters, and evolves dynamically via a hidden state. Although the framework is general, we focus on thermoacoustics governed by nonlinear and time-delayed systems. First, we show that a parameter-aware Echo State Network (ESN) infers the parameterized dynamics. Second, we derive the adjoint of the ESN to compute the sensitivity of time-averaged cost functionals. Third, we propose the Thermoacoustic Echo State Network (T-ESN), which hard constrains the physical knowledge in the network architecture. Fourth, we apply the framework to a variety of nonlinear thermoacoustic regimes of a prototypical system. We show that the T-ESN accurately infers the correct adjoint sensitivities of the time-averaged acoustic energy with respect to the flame parameters. The results are robust to noisy data, from periodic, through quasiperiodic, to chaotic regimes. A single network predicts the nonlinear bifurcations on unseen scenarios, and so the inferred adjoint sensitivities are employed to suppress an instability via steepest descent. This work opens new possibilities for gradient-based data-driven design optimization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0daae2897c821b15dac81a757826d1009a038ffb" target='_blank'>
              Data-driven computation of adjoint sensitivities without adjoint solvers: An application to thermoacoustics
              </a>
            </td>
          <td>
            D. E. Ozan, Luca Magri
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Identifying differential operators from data is essential for the mathematical modeling of complex physical and biological systems where massive datasets are available. These operators must be stable for accurate predictions for dynamics forecasting problems. In this article, we propose a novel methodology for learning sparse differential operators that are theoretically linearly stable by solving a constrained regression problem. These underlying constraints are obtained following linear stability for dynamical systems. We further extend this approach for learning nonlinear differential operators by determining linear stability constraints for linearized equations around an equilibrium point. The applicability of the proposed method is demonstrated for both linear and nonlinear partial differential equations such as 1-D scalar advection-diffusion equation, 1-D Burgers equation and 2-D advection equation. The results indicated that solutions to constrained regression problems with linear stability constraints provide accurate and linearly stable sparse differential operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4ff687c7535d984211e6b3fc207c4c872443a9a0" target='_blank'>
              Data-driven identification of stable differential operators using constrained regression
              </a>
            </td>
          <td>
            Aviral Prakash, Yongjie Jessica Zhang
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Machine learning methods allow the prediction of nonlinear dynamical systems from data alone. The Koopman operator is one of them, which enables us to employ linear analysis for nonlinear dynamical systems. The linear characteristics of the Koopman operator are hopeful to understand the nonlinear dynamics and perform rapid predictions. The extended dynamic mode decomposition (EDMD) is one of the methods to approximate the Koopman operator as a finite-dimensional matrix. In this work, we propose a method to compress the Koopman matrix using hierarchical clustering. Numerical demonstrations for the cart-pole model and comparisons with the conventional singular value decomposition (SVD) are shown; the results indicate that the hierarchical clustering performs better than the naive SVD compressions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e05c81f08c9108b61ecead6726856816130a266e" target='_blank'>
              Compression of the Koopman matrix for nonlinear physical models via hierarchical clustering
              </a>
            </td>
          <td>
            Tomoya Nishikata, Jun Ohkubo
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Operator learning is a variant of machine learning that is designed to approximate maps between function spaces from data. The Fourier Neural Operator (FNO) is a common model architecture used for operator learning. The FNO combines pointwise linear and nonlinear operations in physical space with pointwise linear operations in Fourier space, leading to a parameterized map acting between function spaces. Although FNOs formally involve convolutions of functions on a continuum, in practice the computations are performed on a discretized grid, allowing efficient implementation via the FFT. In this paper, the aliasing error that results from such a discretization is quantified and algebraic rates of convergence in terms of the grid resolution are obtained as a function of the regularity of the input. Numerical experiments that validate the theory and describe model stability are performed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e80b1772b17debdc45bc24fc3693519e8f9ffe73" target='_blank'>
              Discretization Error of Fourier Neural Operators
              </a>
            </td>
          <td>
            S. Lanthaler, Andrew M. Stuart, Margaret Trautner
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5fc1afa64ae2fa95d6884fb9a6da03533001d37e" target='_blank'>
              Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design
              </a>
            </td>
          <td>
            Jan Heiland, Yongho Kim, Steffen W. R. Werner
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. One main innovation established here is the construction of data-based realizations for any number of parameters. Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets. To achieve this, a new generalized multivariate rational function realization is defined. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations. The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b36130682d6d70c9ceb26aefd1dd20ecbe5695c2" target='_blank'>
              The Loewner framework for parametric systems: Taming the curse of dimensionality
              </a>
            </td>
          <td>
            A. Antoulas, I. V. Gosea, Charles Poussot-Vassal
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d99de93bf2840cc46f345bc79ece190229de9b97" target='_blank'>
              Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations
              </a>
            </td>
          <td>
            Huan Zhang, Yifan Chen, Eric Vanden-Eijnden, Benjamin Peherstorfer
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="In many applications, one is interested in classifying trajectories of Hamiltonian systems as invariant tori, islands, or chaos. The convergence rate of ergodic Birkhoff averages can be used to categorize these regions, but many iterations of the return map are needed to implement this directly. Recently, it has been shown that a weighted Birkhoff average can be used to accelerate the convergence, resulting in a useful method for categorizing trajectories. In this paper, we show how a modified version the reduced rank extrapolation method (named Birkhoff RRE) can also be used to find optimal weights for the weighted average with a single linear least-squares solve.Using these, we classify trajectories with fewer iterations of the map than the standard weighted Birkhoff average. Furthermore, for the islands and invariant circles, a subsequent eigenvalue problem gives the number of islands and the rotation number. Using these numbers, we find Fourier parameterizations of invariant circles and islands. We show examples of Birkhoff RRE on the standard map and on magnetic field line dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d798d334459d29a47b042e54f85674b5b2143b42" target='_blank'>
              Finding Birkhoff Averages via Adaptive Filtering
              </a>
            </td>
          <td>
            Maximilian Ruth, David Bindel
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The conditional mean embedding (CME) encodes Markovian stochastic kernels through their actions on probability distributions embedded within the reproducing kernel Hilbert spaces (RKHS). The CME plays a key role in several well-known machine learning tasks such as reinforcement learning, analysis of dynamical systems, etc. We present an algorithm to learn the CME incrementally from data via an operator-valued stochastic gradient descent. As is well-known, function learning in RKHS suffers from scalability challenges from large data. We utilize a compression mechanism to counter the scalability challenge. The core contribution of this paper is a finite-sample performance guarantee on the last iterate of the online compressed operator learning algorithm with fast-mixing Markovian samples, when the target CME may not be contained in the hypothesis space. We illustrate the efficacy of our algorithm by applying it to the analysis of an example dynamical system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/151de1508e748acb71af40180cea95758cfaa6c9" target='_blank'>
              Compressed Online Learning of Conditional Mean Embedding
              </a>
            </td>
          <td>
            Boya Hou, Sina Sanjari, Alec Koppel, S. Bose
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Estimating governing equations from observed time-series data is crucial for understanding dynamical systems. From the perspective of system comprehension, the demand for accurate estimation and interpretable results has been particularly emphasized. Herein, we propose a novel data-driven method for estimating the governing equations of dynamical systems based on machine learning with high accuracy and interpretability. The proposed method enhances the estimation accuracy for dynamical systems using sparse modeling by incorporating physical constraints derived from Hamiltonian mechanics. Unlike conventional approaches used for estimating governing equations for dynamical systems, we employ a sparse representation of Hamiltonian, allowing for the estimation. Using noisy observational data, the proposed method demonstrates a capability to achieve accurate parameter estimation and extraction of essential nonlinear terms. In addition, it is shown that estimations based on energy conservation principles exhibit superior accuracy in long-term predictions. These results collectively indicate that the proposed method accurately estimates dynamical systems while maintaining interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18c6dbc4e20d80b81497e8c444e6f9a9ee1acf36" target='_blank'>
              Sparse Estimation for Hamiltonian Mechanics
              </a>
            </td>
          <td>
            Yuya Note, Masahito Watanabe, Hiroaki Yoshimura, Takaharu Yaguchi, Toshiaki Omori
          </td>
          <td>2024-03-25</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data. The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design. A rigorous proof of convergence as the distance between observation data points converges to zero is provided. Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques. We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification. The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/62f5043fbad579fdcc9ee3d3f2353a98feac5ae8" target='_blank'>
              Machine learning of continuous and discrete variational ODEs with convergence guarantee and uncertainty quantification
              </a>
            </td>
          <td>
            Christian Offen
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Identifying Ordinary Differential Equations (ODEs) from measurement data requires both fitting the dynamics and assimilating, either implicitly or explicitly, the measurement data. The Sparse Identification of Nonlinear Dynamics (SINDy) method involves a derivative estimation step (and optionally, smoothing) and a sparse regression step on a library of candidate ODE terms. Kalman smoothing is a classical framework for assimilating the measurement data with known noise statistics. Previously, derivatives in SINDy and its python package, pysindy, had been estimated by finite difference, L1 total variation minimization, or local filters like Savitzky-Golay. In contrast, Kalman allows discovering ODEs that best recreate the essential dynamics in simulation, even in cases when it does not perform as well at recovering coefficients, as measured by their F1 score and mean absolute error. We have incorporated Kalman smoothing, along with hyperparameter optimization, into the existing pysindy architecture, allowing for rapid adoption of the method. Numerical experiments on a number of dynamical systems show Kalman smoothing to be the most amenable to parameter selection and best at preserving problem structure in the presence of noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5435325a3ad3a24c95b8947ff93859f828f47937" target='_blank'>
              Learning Nonlinear Dynamics Using Kalman Smoothing
              </a>
            </td>
          <td>
            Jacob Stevens-Haas, Yash Bhangale, Aleksandr Y Aravkin, Nathan Kutz
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We propose a self-supervised approach for learning physics-based subspaces for real-time simulation. Existing learning-based methods construct subspaces by approximating pre-defined simulation data in a purely geometric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space dimensions, and generalizes poorly beyond the training set. To overcome these limitations, we propose a self-supervised approach that directly minimizes the system's mechanical energy during training. We show that our method leads to learned subspaces that reflect physical equilibrium constraints, resolve overfitting issues of previous methods, and offer interpretable latent space parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8af663bff69f1f9808eaf47048759e64491d65cf" target='_blank'>
              Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces
              </a>
            </td>
          <td>
            Jiahong Wang, Yinwei Du, Stelian Coros, Bernhard Thomaszewski
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Recurrent Neural Networks excel at predicting and generating complex high-dimensional temporal patterns. Due to their inherent nonlinear dynamics and memory, they can learn unbounded temporal dependencies from data. In a Machine Learning setting, the network's parameters are adapted during a training phase to match the requirements of a given task/problem increasing its computational capabilities. After the training, the network parameters are kept fixed to exploit the learned computations. The static parameters thereby render the network unadaptive to changing conditions, such as external or internal perturbation. In this manuscript, we demonstrate how keeping parts of the network adaptive even after the training enhances its functionality and robustness. Here, we utilize the conceptor framework and conceptualize an adaptive control loop analyzing the network's behavior continuously and adjusting its time-varying internal representation to follow a desired target. We demonstrate how the added adaptivity of the network supports the computational functionality in three distinct tasks: interpolation of temporal patterns, stabilization against partial network degradation, and robustness against input distortion. Our results highlight the potential of adaptive networks in machine learning beyond training, enabling them to not only learn complex patterns but also dynamically adjust to changing environments, ultimately broadening their applicability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/81d77920a1f3057b33f9ab48db38a16dc2b0f292" target='_blank'>
              Adaptive control of recurrent neural networks using conceptors
              </a>
            </td>
          <td>
            Guillaume Pourcel, Mirko Goldmann, Ingo Fischer, Miguel C. Soriano
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="A new knowledge-based and machine learning hybrid modeling approach, called conditional Gaussian neural stochastic differential equation (CGNSDE), is developed to facilitate modeling complex dynamical systems and implementing analytic formulae of the associated data assimilation (DA). In contrast to the standard neural network predictive models, the CGNSDE is designed to effectively tackle both forward prediction tasks and inverse state estimation problems. The CGNSDE starts by exploiting a systematic causal inference via information theory to build a simple knowledge-based nonlinear model that nevertheless captures as much explainable physics as possible. Then, neural networks are supplemented to the knowledge-based model in a specific way, which not only characterizes the remaining features that are challenging to model with simple forms but also advances the use of analytic formulae to efficiently compute the nonlinear DA solution. These analytic formulae are used as an additional computationally affordable loss to train the neural networks that directly improve the DA accuracy. This DA loss function promotes the CGNSDE to capture the interactions between state variables and thus advances its modeling skills. With the DA loss, the CGNSDE is more capable of estimating extreme events and quantifying the associated uncertainty. Furthermore, crucial physical properties in many complex systems, such as the translate-invariant local dependence of state variables, can significantly simplify the neural network structures and facilitate the CGNSDE to be applied to high-dimensional systems. Numerical experiments based on chaotic systems with intermittency and strong non-Gaussian features indicate that the CGNSDE outperforms knowledge-based regression models, and the DA loss further enhances the modeling skills of the CGNSDE.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a639323a3ab8c39800f9e9f42ae3d95438cb1ec6" target='_blank'>
              CGNSDE: Conditional Gaussian Neural Stochastic Differential Equation for Modeling Complex Systems and Data Assimilation
              </a>
            </td>
          <td>
            Chuanqi Chen, Nan Chen, Jingbo Wu
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5e18a33ab8b574bef024af1692a95b86dffe47ac" target='_blank'>
              Statistical algorithms for low-frequency diffusion data: A PDE approach
              </a>
            </td>
          <td>
            Matteo Giordano, Sven Wang
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In the study of stochastic dynamics, the committor function describes the probability that a process starting from an initial configuration $x$ will reach set $A$ before set $B$. This paper introduces a fast and interpretable method for approximating the committor, called the"fast committor machine"(FCM). The FCM is based on simulated trajectory data, and it uses this data to train a kernel model. The FCM identifies low-dimensional subspaces that optimally describe the $A$ to $B$ transitions, and the subspaces are emphasized in the kernel model. The FCM uses randomized numerical linear algebra to train the model with runtime that scales linearly in the number of data points. This paper applies the FCM to example systems including the alanine dipeptide miniprotein: in these experiments, the FCM is generally more accurate and trains more quickly than a neural network with a similar number of parameters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c6715c8482d3bab4d35021659cc8d135cb847116" target='_blank'>
              The fast committor machine: Interpretable prediction with kernels
              </a>
            </td>
          <td>
            D. Aristoff, , G. Simpson, R. J. Webber
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We develop time-series machine learning (ML) methods for closure modeling of the Unsteady Reynolds Averaged Navier Stokes (URANS) equations applied to stably stratified turbulence (SST). SST is strongly affected by fine balances between forces and becomes more anisotropic in time for decaying cases. Moreover, there is a limited understanding of the physical phenomena described by some of the terms in the URANS equations. Rather than attempting to model each term separately, it is attractive to explore the capability of machine learning to model groups of terms, i.e., to directly model the force balances. We consider decaying SST which are homogeneous and stably stratified by a uniform density gradient, enabling dimensionality reduction. We consider two time-series ML models: Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equation (NODE). Both models perform accurately and are numerically stable in a posteriori tests. Furthermore, we explore the data requirements of the ML models by extracting physically relevant timescales of the complex system. We find that the ratio of the timescales of the minimum information required by the ML models to accurately capture the dynamics of the SST corresponds to the Reynolds number of the flow. The current framework provides the backbone to explore the capability of such models to capture the dynamics of higher-dimensional complex SST flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6b6cca1a9a38a3cab38adaf5f136dbc8ce93c3f9" target='_blank'>
              Machine-Learned Closure of URANS for Stably Stratified Turbulence: Connecting Physical Timescales&Data Hyperparameters of Deep Time-Series Models
              </a>
            </td>
          <td>
            Muralikrishnan Gopalakrishnan Meena, Demetri Liousas, Andrew D. Simin, Aditya Kashi, Wesley Brewer, James J. Riley, S. D. B. Kops
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="We introduce Geometric Neural Operators (GNPs) for accounting for geometric contributions in data-driven deep learning of operators. We show how GNPs can be used (i) to estimate geometric properties, such as the metric and curvatures, (ii) to approximate Partial Differential Equations (PDEs) on manifolds, (iii) learn solution maps for Laplace-Beltrami (LB) operators, and (iv) to solve Bayesian inverse problems for identifying manifold shapes. The methods allow for handling geometries of general shape including point-cloud representations. The developed GNPs provide approaches for incorporating the roles of geometry in data-driven learning of operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44f2c8d5ee8bf6e5107f615a0cdc8afd7cb4c7a4" target='_blank'>
              Geometric Neural Operators (GNPs) for Data-Driven Deep Learning of Non-Euclidean Operators
              </a>
            </td>
          <td>
            Blaine Quackenbush, P. Atzberger
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f116a6da3fbd691c6355c87e9f75c42ef5145170" target='_blank'>
              Zero-Shot Transfer of Neural ODEs
              </a>
            </td>
          <td>
            Tyler Ingebrand, Adam J. Thorpe, U. Topcu
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>47</td>
        </tr>

        <tr id="We study the problem of stabilizing infinite-dimensional systems with input and output quantization. The closed-loop system we consider is subject to packet loss in the sensor-to-controller channels, whose duration is assumed to be averagely bounded. Given a bound on the initial state, we propose design methods for dynamic quantizers with zoom parameters. We show that the closed-loop state staring in a given region exponentially converges to zero if the bounds of quantization errors and packet-loss duration satisfy suitable conditions. Since the norms of the operators representing the system dynamics are used in the proposed quantization schemes, we also present methods for approximately computing the operator norms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/74a8b97a85ed1412fab3f946b36e87644f036b76" target='_blank'>
              Stabilization of infinite-dimensional systems under quantization and packet loss
              </a>
            </td>
          <td>
            Masashi Wakaiki
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/819bc0d5e0ea2efadac1364064e40b76cf3a3a11" target='_blank'>
              Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation
              </a>
            </td>
          <td>
            Jingmin Sun, Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In many real-world applications where the system dynamics has an underlying interdependency among its variables (such as power grid, economics, neuroscience, omics networks, environmental ecosystems, and others), one is often interested in knowing whether the past values of one time series influences the future of another, known as Granger causality, and the associated underlying dynamics. This paper introduces a Koopman-inspired framework that leverages neural networks for data-driven learning of the Koopman bases, termed NeuroKoopman Dynamic Causal Discovery (NKDCD), for reliably inferring the Granger causality along with the underlying nonlinear dynamics. NKDCD employs an autoencoder architecture that lifts the nonlinear dynamics to a higher dimension using data-learned bases, where the lifted time series can be reliably modeled linearly. The lifting function, the linear Granger causality lag matrices, and the projection function (from lifted space to base space) are all represented as multilayer perceptrons and are all learned simultaneously in one go. NKDCD also utilizes sparsity-inducing penalties on the weights of the lag matrices, encouraging the model to select only the needed causal dependencies within the data. Through extensive testing on practically applicable datasets, it is shown that the NKDCD outperforms the existing nonlinear Granger causality discovery approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2063b00f6c1d0ea80dc4fa01e9b3b3bddc847ea9" target='_blank'>
              NeuroKoopman Dynamic Causal Discovery
              </a>
            </td>
          <td>
            Rahmat Adesunkanmi, Balaji Sesha Srikanth Pokuri, Ratnesh Kumar
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems. Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging. Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process. Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators. Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions. We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f005c8b19ca1d9171afc191fc78b81f136ddfb8c" target='_blank'>
              LINOCS: Lookahead Inference of Networked Operators for Continuous Stability
              </a>
            </td>
          <td>
            Noga Mudrik, Eva Yezerets, Yenho Chen, Christopher Rozell, Adam Charles
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The Singular Value Decomposition (SVD) of linear functions facilitates the calculation of their 2-induced norm and row and null spaces, hallmarks of linear control theory. In this work, we present a function representation that, similar to SVD, provides an upper bound on the 2-induced norm of bounded-input bounded-output functions, as well as facilitates the computation of generalizations of the notions of row and null spaces. Borrowing from the notion of"lifting"in Koopman operator theory, we construct a finite-dimensional lifting of inputs that relaxes the unitary property of the right-most matrix in traditional SVD, $V^*$, to be an injective, norm-preserving mapping to a slightly higher-dimensional space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/535509f18e90d04fe23ee79a1110a2c4936a88d3" target='_blank'>
              An SVD-like Decomposition of Bounded-Input Bounded-Output Functions
              </a>
            </td>
          <td>
            Brian Charles Brown, Michael King, Sean Warnick, Enoch Yeung, David Grimsman
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="We develop a fast and scalable numerical approach to solve Wasserstein gradient flows (WGFs), particularly suitable for high-dimensional cases. Our approach is to use general reduced-order models, like deep neural networks, to parameterize the push-forward maps such that they can push a simple reference density to the one solving the given WGF. The new dynamical system is called parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter space equipped with a pullback Wasserstein metric. Our numerical scheme can approximate the solutions of WGFs for general energy functionals effectively, without requiring spatial discretization or nonconvex optimization procedures, thus avoiding some limitations of classical numerical methods and more recent deep-learning-based approaches. A comprehensive analysis of the approximation errors measured by Wasserstein distance is also provided in this work. Numerical experiments show promising computational efficiency and verified accuracy on various WGF examples using our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f4a9e55509ca4e5d72a6f1b3b99f8552bd505b8" target='_blank'>
              Parameterized Wasserstein Gradient Flow
              </a>
            </td>
          <td>
            Yijie Jin, Shu Liu, Hao Wu, Xiaojing Ye, Haomin Zhou
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan. Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter. While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored. We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\'e rotation numbers, and canonical action-angle variables. In the second part, we establish connections between these maps and general chaotic maps in standard form. Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization). Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude. This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude. Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a8d139a939d3e8f40522265c89f6fb72d4cb1c3" target='_blank'>
              Dynamics of McMillan mappings I. McMillan multipoles
              </a>
            </td>
          <td>
            T. Zolkin, Sergei Nagaitsev, Ivan Morozov
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Stable partitioned techniques for simulating unsteady fluid-structure interaction (FSI) are known to be computationally expensive when high added-mass is involved. Multiple coupling strategies have been developed to accelerate these simulations, but often use predictors in the form of simple finite-difference extrapolations. In this work, we propose a non-intrusive data-driven predictor that couples reduced-order models of both the solid and fluid subproblems, providing an initial guess for the nonlinear problem of the next time step calculation. Each reduced order model is composed of a nonlinear encoder-regressor-decoder architecture and is equipped with an adaptive update strategy that adds robustness for extrapolation. In doing so, the proposed methodology leverages physics-based insights from high-fidelity solvers, thus establishing a physics-aware machine learning predictor. Using three strongly coupled FSI examples, this study demonstrates the improved convergence obtained with the new predictor and the overall computational speedup realized compared to classical approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d970acf208dab6cca1199c36511842e22572e1f" target='_blank'>
              Machine-Learning Enhanced Predictors for Accelerated Convergence of Partitioned Fluid-Structure Interaction Simulations
              </a>
            </td>
          <td>
            Azzeddine Tiba, Thibault Dairay, F. Vuyst, Iraj Mortazavi, Juan-Pedro Berro Ramirez
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs). Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases. The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning. The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab062559eb3a22087cb23a00b82a93b0fff80ee0" target='_blank'>
              Convergence Rates of Online Critic Value Function Approximation in Native Spaces
              </a>
            </td>
          <td>
            Shengyuan Niu, Ali Bouland, Haoran Wang, Filippos Fotiadis, Andrew J. Kurdila, Andrea L'Afflitto, S. Paruchuri, K. Vamvoudakis
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>34</td>
        </tr>

        <tr id="Transport phenomena (e.g., fluid flows) are governed by time-dependent partial differential equations (PDEs) describing mass, momentum, and energy conservation, and are ubiquitous in many engineering applications. However, deep learning architectures are fundamentally incompatible with the simulation of these PDEs. This paper clearly articulates and then solves this incompatibility. The local-dependency of generic transport PDEs implies that it only involves local information to predict the physical properties at a location in the next time step. However, the deep learning architecture will inevitably increase the scope of information to make such predictions as the number of layers increases, which can cause sluggish convergence and compromise generalizability. This paper aims to solve this problem by proposing a distributed data scoping method with linear time complexity to strictly limit the scope of information to predict the local properties. The numerical experiments over multiple physics show that our data scoping method significantly accelerates training convergence and improves the generalizability of benchmark models on large-scale engineering simulations. Specifically, over the geometries not included in the training data for heat transferring simulation, it can increase the accuracy of Convolutional Neural Networks (CNNs) by 21.7 \% and that of Fourier Neural Operators (FNOs) by 38.5 \% on average.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5355d8f1cafbe46f9c3a9a1f31193f495099644" target='_blank'>
              Data Scoping: Effectively Learning the Evolution of Generic Transport PDEs
              </a>
            </td>
          <td>
            Jiangce Chen, Wenzhuo Xu, Zeda Xu, Noelia Grande Guti'errez, S. Narra, Christopher McComb
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="The process of training an artificial neural network involves iteratively adapting its parameters so as to minimize the error of the network’s prediction, when confronted with a learning task. This iterative change can be naturally interpreted as a trajectory in network space–a time series of networks–and thus the training algorithm (e.g., gradient descent optimization of a suitable loss function) can be interpreted as a dynamical system in graph space. In order to illustrate this interpretation, here we study the dynamical properties of this process by analyzing through this lens the network trajectories of a shallow neural network, and its evolution through learning a simple classification task. We systematically consider different ranges of the learning rate and explore both the dynamical and orbital stability of the resulting network trajectories, finding hints of regular and chaotic behavior depending on the learning rate regime. Our findings are put in contrast to common wisdom on convergence properties of neural networks and dynamical systems theory. This work also contributes to the cross-fertilization of ideas between dynamical systems theory, network theory and machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/72e81727419ca3da67887cc9cd6a76a2a0394b00" target='_blank'>
              Dynamical stability and chaos in artificial neural network trajectories along training
              </a>
            </td>
          <td>
            Kaloyan Danovski, Miguel C. Soriano, Lucas Lacasa
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Duality between estimation and control is a foundational concept in Control Theory. Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory. Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete. Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors. A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control. The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models. The explanation is used to motivate the difference between the linear and the nonlinear models. Once the difference is understood, duality for HMMs is described based on our recent work. The article also includes a comparison and discussion of the different types of duality considered in literature.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cff50920a2cdf0632a55e8e52a013a32a3926e8a" target='_blank'>
              Arrow of Time in Estimation and Control: Duality Theory Beyond the Linear Gaussian Model
              </a>
            </td>
          <td>
            J. W. Kim, Prashant G. Mehta
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Semi-algebraic priors are ubiquitous in signal processing and machine learning. Prevalent examples include a) linear models where the signal lies in a low-dimensional subspace; b) sparse models where the signal can be represented by only a few coefficients under a suitable basis; and c) a large family of neural network generative models. In this paper, we prove a transversality theorem for semi-algebraic sets in orthogonal or unitary representations of groups: with a suitable dimension bound, a generic translate of any semi-algebraic set is transverse to the orbits of the group action. This, in turn, implies that if a signal lies in a low-dimensional semi-algebraic set, then it can be recovered uniquely from measurements that separate orbits. As an application, we consider the implications of the transversality theorem to the problem of recovering signals that are translated by random group actions from their second moment. As a special case, we discuss cryo-EM: a leading technology to constitute the spatial structure of biological molecules, which serves as our prime motivation. In particular, we derive explicit bounds for recovering a molecular structure from the second moment under a semi-algebraic prior and deduce information-theoretic implications. We also obtain information-theoretic bounds for three additional applications: factoring Gram matrices, multi-reference alignment, and phase retrieval. Finally, we deduce bounds for designing permutation invariant separators in machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff7bfe2c7ca023b1e749fa79e559d3d479d5e655" target='_blank'>
              A transversality theorem for semi-algebraic sets with application to signal recovery from the second moment and cryo-EM
              </a>
            </td>
          <td>
            Tamir Bendory, Nadav Dym, D. Edidin, Arun Suresh
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="This paper is devoted to the estimation of the Lipschitz constant of neural networks using semidefinite programming. For this purpose, we interpret neural networks as time-varying dynamical systems, where the $k$-th layer corresponds to the dynamics at time $k$. A key novelty with respect to prior work is that we use this interpretation to exploit the series interconnection structure of neural networks with a dynamic programming recursion. Nonlinearities, such as activation functions and nonlinear pooling layers, are handled with integral quadratic constraints. If the neural network contains signal processing layers (convolutional or state space model layers), we realize them as 1-D/2-D/N-D systems and exploit this structure as well. We distinguish ourselves from related work on Lipschitz constant estimation by more extensive structure exploitation (scalability) and a generalization to a large class of common neural network architectures. To show the versatility and computational advantages of our method, we apply it to different neural network architectures trained on MNIST and CIFAR-10.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a76d04075753704069e46f5f5e4d7d5b55ae992b" target='_blank'>
              Lipschitz constant estimation for general neural network architectures using control tools
              </a>
            </td>
          <td>
            Patricia Pauli, Dennis Gramlich, Frank Allgower
          </td>
          <td>2024-05-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Formulating dynamical models for physical phenomena is essential for understanding the interplay between the different mechanisms and predicting the evolution of physical states. However, a dynamical model alone is often insufficient to address these fundamental tasks, as it suffers from model errors and uncertainties. One common remedy is to rely on data assimilation, where the state estimate is updated with observations of the true system. Ensemble filters sequentially assimilate observations by updating a set of samples over time. They operate in two steps: a forecast step that propagates each sample through the dynamical model and an analysis step that updates the samples with incoming observations. For accurate and robust predictions of dynamical systems, discrete solutions must preserve their critical invariants. While modern numerical solvers satisfy these invariants, existing invariant-preserving analysis steps are limited to Gaussian settings and are often not compatible with classical regularization techniques of ensemble filters, e.g., inflation and covariance tapering. The present work focuses on preserving linear invariants, such as mass, stoichiometric balance of chemical species, and electrical charges. Using tools from measure transport theory (Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear ensemble filters that automatically preserve desired linear invariants in non-Gaussian filtering problems. By specializing this framework to the Gaussian setting, we recover a constrained formulation of the Kalman filter. Then, we show how to combine existing regularization techniques for the ensemble Kalman filter (Evensen, 1994, J. Geophys. Res.) with the preservation of the linear invariants. Finally, we assess the benefits of preserving linear invariants for the ensemble Kalman filter and nonlinear ensemble filters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d141c58886abcc457131832eb29eed4ce25f26e6" target='_blank'>
              Preserving linear invariants in ensemble filtering methods
              </a>
            </td>
          <td>
            M. Provost, Jan Glaubitz, Youssef Marzouk
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We present a computational method for open-loop minimum-norm control synthesis for fixed-endpoint transfer of bilinear ensemble systems that are indexed by two continuously varying parameters. We suppose that one ensemble parameter scales the homogeneous, linear part of the dynamics, and the second parameter scales the effect of the applied control inputs on the inhomogeneous, bilinear dynamics. This class of dynamical systems is motivated by robust quantum control pulse synthesis, where the ensemble parameters correspond to uncertainty in the free Hamiltonian and inhomogeneity in the control Hamiltonian, respectively. Our computational method is based on polynomial approximation of the ensemble state in parameter space and discretization of the evolution equations in the time domain using a product of matrix exponentials corresponding to zero-order hold controls over the time intervals. The dynamics are successively linearized about control and trajectory iterates to formulate a sequence of quadratic programs for computing perturbations to the control that successively improve the objective until the iteration converges. We use a two-stage computation to first ensure transfer to the desired terminal state, and then minimize the norm of the control function. The method is demonstrated for the canonical uniform transfer problem for the Bloch system that appears in nuclear magnetic resonance, as well as the matter-wave splitting problem for the Raman-Nath system that appears in ultra-cold atom interferometry.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3a9d6cc713542be31c971c346aa4e1d08dd497f3" target='_blank'>
              Convergence of Iterative Quadratic Programming for Robust Fixed-Endpoint Transfer of Bilinear Systems
              </a>
            </td>
          <td>
            Luke S. Baker, A. L. P. D. Lima, Anatoly Zlotnik, Jr-Shin Li
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Observable operator models (OOMs) offer a powerful framework for modelling stochastic processes, surpassing the traditional hidden Markov models (HMMs) in generality and efficiency. However, using OOMs to model infinite-dimensional processes poses significant theoretical challenges. This article explores a rigorous approach to developing an approximation theory for OOMs of infinite-dimensional processes. Building upon foundational work outlined in an unpublished tutorial [Jae98], an inner product structure on the space of future distributions is rigorously established and the continuity of observable operators with respect to the associated 2-norm is proven. The original theorem proven in this thesis describes a fundamental obstacle in making an infinite-dimensional space of future distributions into a Hilbert space. The presented findings lay the groundwork for future research in approximating observable operators of infinite-dimensional processes, while a remedy to the encountered obstacle is suggested.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/87e6a7d4e9af5babec0554b7f23c6a6ebe8ad5cd" target='_blank'>
              Towards an Approximation Theory of Observable Operator Models
              </a>
            </td>
          <td>
            Wojciech Anyszka
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this work we introduce the notion of an angular spectrum for a linear discrete time nonautonomous dynamical system. The angular spectrum comprises all accumulation points of longtime averages formed by maximal principal angles between successive subspaces generated by the dynamical system. The angular spectrum is bounded by angular values which have previously been investigated by the authors. In this contribution we derive explicit formulas for the angular spectrum of some autonomous and specific nonautonomous systems. Based on a reduction principle we set up a numerical method for the general case; we investigate its convergence and apply the method to systems with a homoclinic orbit and a strange attractor. Our main theoretical result is a theorem on the invariance of the angular spectrum under summable perturbations of the given matrices (roughness theorem). It applies to systems with a so-called complete exponential dichotomy (CED), a concept which we introduce in this paper and which imposes more stringent conditions than those underlying the exponential dichotomy spectrum.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4c11794957c48141b4f2177d97d8670cee0ef62" target='_blank'>
              Angular spectra of linear dynamical systems in discrete time
              </a>
            </td>
          <td>
            W. Beyn, Thorsten Huls
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="Representation learning for the electronic structure problem is a major challenge of machine learning in computational condensed matter and materials physics. Within quantum mechanical first principles approaches, Kohn-Sham density functional theory (DFT) is the preeminent tool for understanding electronic structure, and the high-dimensional wavefunctions calculated in this approach serve as the building block for downstream calculations of correlated many-body excitations and related physical observables. Here, we use variational autoencoders (VAE) for the unsupervised learning of high-dimensional DFT wavefunctions and show that these wavefunctions lie in a low-dimensional manifold within the latent space. Our model autonomously determines the optimal representation of the electronic structure, avoiding limitations due to manual feature engineering and selection in prior work. To demonstrate the utility of the latent space representation of the DFT wavefunction, we use it for the supervised training of neural networks (NN) for downstream prediction of the quasiparticle bandstructures within the GW formalism, which includes many-electron correlations beyond DFT. The GW prediction achieves a low error of 0.11 eV for a combined test set of metals and semiconductors drawn from the Computational 2D Materials Database (C2DB), suggesting that latent space representation captures key physical information from the original data. Finally, we explore the interpretability of the VAE representation and show that the successful representation learning and downstream prediction by our model is derived from the smoothness of the VAE latent space, which also enables the generation of wavefunctions on arbitrary points in latent space. Our work provides a novel and general machine-learning framework for investigating electronic structure and many-body physics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9db6d6b2521a0d5948c53b20a8da887829b4ab0d" target='_blank'>
              Unsupervised Learning of Individual Kohn-Sham States: Interpretable Representations and Consequences for Downstream Predictions of Many-Body Effects
              </a>
            </td>
          <td>
            Bowen Hou, Jinyuan Wu, Diana Y Qiu
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper introduces a framework by which the nonlinear trajectory optimization problem is posed as a path-planning problem in a space liberated of dynamics. In this space, general state constraints for continuous and impulsive control problems are encoded as linear constraints on the native overparameterized variables. This framework is enabled by nonlinear expansion in the vicinity of a reference in terms of fundamental solutions and a minimal nonlinear basis of mixed monomials in problem initial conditions. The former can be computed using state transition tensors, differential algebra, or analytic approaches, and the latter is computed analytically. Nonlinear guidance schemes are proposed taking advantage of this framework, including a successive convex programming scheme for delta-V minimizing trajectory optimization. This work enables a stable, easy to implement, and highly rapid nonlinear guidance implementation without the need for collocation or real-time integration.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4205e56ab04fa7b0b162e8f2690d1051263545db" target='_blank'>
              Rapid nonlinear convex guidance using a monomial method
              </a>
            </td>
          <td>
            Ethan R. Burnett, Francesco Topputo
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In chaos control, one usually seeks to stabilize the unstable periodic orbits (UPOs) that densely inhabit the attractors of many chaotic dynamical systems. These orbits collectively play a significant role in determining the dynamics and properties of chaotic systems and are said to form the skeleton of the associated attractors. While UPOs are insightful tools for analysis, they are naturally unstable and, as such, are difficult to find and computationally expensive to stabilize. An alternative to using UPOs is to approximate them using cupolets. Cupolets, a name derived from chaotic, unstable, periodic, orbit-lets, are a relatively new class of waveforms that represent highly accurate approximations to the UPOs of chaotic systems, but which are generated via a particular control scheme that applies tiny perturbations along Poincaré sections. Originally discovered in an application of secure chaotic communications, cupolets have since gone on to play pivotal roles in a number of theoretical and practical applications. These developments include using cupolets as wavelets for image compression, targeting in dynamical systems, a chaotic analog to quantum entanglement, an abstract reducibility classification, a basis for audio and video compression, and, most recently, their detection in a chaotic neuron model. This review will detail the historical development of cupolets, how they are generated, and their successful integration into theoretical and computational science and will also identify some unanswered questions and future directions for this work.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a248090592163d02636c115d39431e2d9ab4685a" target='_blank'>
              Cupolets: History, Theory, and Applications
              </a>
            </td>
          <td>
            Matthew A. Morena, K. Short
          </td>
          <td>2024-05-13</td>
          <td>Dynamics</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this paper, we consider the problem of reference tracking in uncertain nonlinear systems. A neural State-Space Model (NSSM) is used to approximate the nonlinear system, where a deep encoder network learns the nonlinearity from data, and a state-space component captures the temporal relationship. This transforms the nonlinear system into a linear system in a latent space, enabling the application of model predictive control (MPC) to determine effective control actions. Our objective is to design the optimal controller using limited data from the \textit{target system} (the system of interest). To this end, we employ an implicit model-agnostic meta-learning (iMAML) framework that leverages information from \textit{source systems} (systems that share similarities with the target system) to expedite training in the target system and enhance its control performance. The framework consists of two phases: the (offine) meta-training phase learns a aggregated NSSM using data from source systems, and the (online) meta-inference phase quickly adapts this aggregated model to the target system using only a few data points and few online training iterations, based on local loss function gradients. The iMAML algorithm exploits the implicit function theorem to exactly compute the gradient during training, without relying on the entire optimization path. By focusing solely on the optimal solution, rather than the path, we can meta-train with less storage complexity and fewer approximations than other contemporary meta-learning algorithms. We demonstrate through numerical examples that our proposed method can yield accurate predictive models by adaptation, resulting in a downstream MPC that outperforms several baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2140b914c25c6c89e81a3b8e30f2c87f5d4bcd5d" target='_blank'>
              MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast Adaptation of Neural Predictive Models
              </a>
            </td>
          <td>
            Jiaqi Yan, Ankush Chakrabarty, Alisa Rupenyan, John Lygeros
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="The nonlinear sine-Gordon equation is a prevalent feature in numerous scientific and engineering problems. In this paper, we propose a machine learning-based approach, physics-informed neural networks (PINNs), to investigate and explore the solution of the generalized non-linear sine-Gordon equation, encompassing Dirichlet and Neumann boundary conditions. To incorporate physical information for the sine-Gordon equation, a multiobjective loss function has been defined consisting of the residual of governing partial differential equation (PDE), initial conditions, and various boundary conditions. Using multiple densely connected independent artificial neural networks (ANNs), called feedforward deep neural networks designed to handle partial differential equations, PINNs have been trained through automatic differentiation to minimize a loss function that incorporates the given PDE that governs the physical laws of phenomena. To illustrate the effectiveness, validity, and practical implications of our proposed approach, two computational examples from the nonlinear sine-Gordon are presented. We have developed a PINN algorithm and implemented it using Python software. Various experiments were conducted to determine an optimal neural architecture. The network training was employed by using the current state-of-the-art optimization methods in machine learning known as Adam and L-BFGS-B minimization techniques. Additionally, the solutions from the proposed method are compared with the established analytical solutions found in the literature. The findings show that the proposed method is a computational machine learning approach that is accurate and efficient for solving nonlinear sine-Gordon equations with a variety of boundary conditions as well as any complex nonlinear physical problems across multiple disciplines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2fa94192e360572ae7bfd3cff18118b6634e19da" target='_blank'>
              Exploring Physics-Informed Neural Networks for the Generalized Nonlinear Sine-Gordon Equation
              </a>
            </td>
          <td>
            Alemayehu Tamirie Deresse, T. T. Dufera
          </td>
          <td>2024-04-30</td>
          <td>Applied Computational Intelligence and Soft Computing</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This article develops mathematical formalisms and provides numerical methods for studying the evolution of measures in nonsmooth dynamical systems using the continuity equation. The nonsmooth dynamical system is described by an evolution variational inequality and we derive the continuity equation associated with this system class using three different formalisms. The first formalism consists of using the {superposition principle} to describe the continuity equation for a measure that disintegrates into a probability measure supported on the set of vector fields and another measure representing the distribution of system trajectories at each time instant. The second formalism is based on the regularization of the nonsmooth vector field and describing the measure as the limit of a sequence of measures associated with the regularization parameter. In doing so, we obtain quantitative bounds on the Wasserstein metric between measure solutions of the regularized vector field and the limiting measure associated with the nonsmooth vector field. The third formalism uses a time-stepping algorithm to model a time-discretized evolution of the measures and show that the absolutely continuous trajectories associated with the continuity equation are recovered in the limit as the sampling time goes to zero. We also validate each formalism with numerical examples. For the first formalism, we use polynomial optimization techniques and the moment-SOS hierarchy to obtain approximate moments of the measures. For the second formalism, we illustrate the bounds on the Wasserstein metric for an academic example for which the closed-form expression of the Wasserstein metric can be calculated. For the third formalism, we illustrate the time-stepping based algorithm for measure evolution on an example that shows the effect of the concentration of measures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e9e4499422a19e67ffdc1502cf5b778af6e4bb7" target='_blank'>
              Evolution of Measures in Nonsmooth Dynamical Systems: Formalisms and Computation
              </a>
            </td>
          <td>
            S. Chhatoi, A. Tanwani, Didier Henrion
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="The construction of time of arrival (TOA) operators canonically conjugate to the system Hamiltonian entails finding the solution of a specific second-order partial differential equation called the time kernel equation (TKE). An expanded iterative solution of the TKE has been obtained recently in [Eur. Phys. J. Plus \textbf{138}, 153 (2023)] but is generally intractable to be useful for arbitrary nonlinear potentials. In this work, we provide an exact analytic solution of the TKE for a special class of potentials satisfying a specific separability condition. The solution enables us to investigate the time evolution of the eigenfunctions of the conjugacy-preserving TOA operators (CPTOA) by coarse graining methods and spatial confinement. We show that the eigenfunctions of the constructed operator exhibit unitary arrival at the intended arrival point at a time equal to their corresponding eigenvalue. Moreover, we examine whether there is a discernible difference in the dynamics between the TOA operators constructed by quantization and those independent of quantization for specific interaction potentials. We find that the CPTOA operator possesses better unitary dynamics over the Weyl-quantized one within numerical accuracy. This allows us determine the role of the canonical commutation relation between time and energy on the observed dynamics of time of arrival operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f8ef14c93c6b0befed911692071bf4c7c589e4ed" target='_blank'>
              The role of conjugacy in the dynamics of time of arrival operators
              </a>
            </td>
          <td>
            Dean Alvin L. Pablico, J. J. P. Magadan, Carl Anthony L. Arguelles, E. Galapon
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="In this paper, we provide a theoretical analysis of a type of operator learning method without data reliance based on the classical finite element approximation, which is called the finite element operator network (FEONet). We first establish the convergence of this method for general second-order linear elliptic PDEs with respect to the parameters for neural network approximation. In this regard, we address the role of the condition number of the finite element matrix in the convergence of the method. Secondly, we derive an explicit error estimate for the self-adjoint case. For this, we investigate some regularity properties of the solution in certain function classes for a neural network approximation, verifying the sufficient condition for the solution to have the desired regularity. Finally, we will also conduct some numerical experiments that support the theoretical findings, confirming the role of the condition number of the finite element matrix in the overall convergence.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e847ec31671f17d8234282f710b8d15b3b52992e" target='_blank'>
              Error analysis for finite element operator learning methods for solving parametric second-order elliptic PDEs
              </a>
            </td>
          <td>
            Youngjoon Hong, Seungchan Ko, Jae Yong Lee
          </td>
          <td>2024-04-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="The symplectic eigenvalues play a significant role in the finite-mode quantum information theory, and Williamson’s normal form proves to be a valuable tool in this area. Understanding the symplectic spectrum of a Gaussian Covariance Operator is a crucial task. Recently, in 2018, an infinite-dimensional analogue of Williamson’s Normal form was discovered, which has been instrumental in studying the infinite-mode Gaussian quantum states. However, most existing results pertain to finite-dimensional operators, leaving a dearth of literature in the infinite-dimensional context. The focus of this article is on employing approximation techniques to estimate the symplectic spectrum of certain infinite-dimensional operators. These techniques are well-suited for a particular class of operators, including specific types of infinite-mode Gaussian Covariance Operators. Our approach involves computing the Williamson’s Normal form and deriving bounds for the symplectic spectrum of these operators. As a practical application, we explicitly compute the symplectic spectrum of Gaussian Covariance Operators. Through this research, we aim to contribute to the understanding of symplectic eigenvalues in the context of infinite-dimensional operators, opening new avenues for exploration in quantum information theory and related fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc1092d35c09849a09f90eea0f7e9a3b06ecfb88" target='_blank'>
              On approximating the symplectic spectrum of infinite-dimensional operators
              </a>
            </td>
          <td>
            V. B. K. Kumar, Anmary Tonny
          </td>
          <td>2024-04-01</td>
          <td>Journal of Mathematical Physics</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise. The resulting motion can only be described as a random process instead of a solution curve. Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow. This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods. It presents a numerical procedure based on kernel integral operators, that computes this expectation. In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c51278ccd72965de531b2582e9c1c49ab661474a" target='_blank'>
              Drift estimation in stochastic flows using kernel integral operators
              </a>
            </td>
          <td>
            Suddhasattwa Das
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The motivation behind this study is to overcome the complex mathematical formulation and time-consuming nature of traditional numerical methods used in solving differential equations. It seeks an alternative approach for more efficient and simplified solutions. A Deep Neural Network (DNN) is utilized to understand the intricate correlations between the oscillator’s variables and to precisely capture their dynamics by being trained on a dataset of known oscillator behaviors. In this work, we discuss the main challenge of predicting the behavior of oscillators without depending on complex strategies or time-consuming simulations. The present work proposes a favorable modified form of neural structure to improve the strategy for simulating linear and nonlinear harmonic oscillators from mechanical systems by formulating an ANN as a DNN via an appropriate oscillating activation function. The proposed methodology provides the solutions of linear and nonlinear differential equations (DEs) in differentiable form and is a more accurate approximation as compared to the traditional numerical method. The Van der Pol equation with parametric damping and the Mathieu equation are adopted as illustrations. Experimental analysis shows that our proposed scheme outperforms other numerical methods in terms of accuracy and computational cost. We provide a comparative analysis of the outcomes obtained through our proposed approach and those derived from the LSODA algorithm, utilizing numerical techniques, Adams–Bashforth, and the Backward Differentiation Formula (BDF). The results of this research provide insightful information for engineering applications, facilitating improvements in energy efficiency, and scientific innovation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6535b101b792ba6de30f73af92407e39980aec0b" target='_blank'>
              Oscillator Simulation with Deep Neural Networks
              </a>
            </td>
          <td>
            J. Rahman, Sana Danish, Dianchen Lu
          </td>
          <td>2024-03-23</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations. Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself. We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space. Moreover, we prove a rigorous convergence result with a convergence rate. As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution. One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward. Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c247154ceefdafb75f75cef53cbf294aada8d1d9" target='_blank'>
              Finite-dimensional approximations of push-forwards on locally analytic functionals and truncation of least-squares polynomials
              </a>
            </td>
          <td>
            Isao Ishikawa
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3714b46ce3de90e0001694844b5ec570eab52656" target='_blank'>
              Nonlinear model reduction for operator learning
              </a>
            </td>
          <td>
            Hamidreza Eivazi, Stefan H. A. Wittek, Andreas Rausch
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data. It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications. While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required. Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology. To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems. By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set. Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b989117896302bd39581399a3a8523e2427d1a9" target='_blank'>
              Deep Learning as Ricci Flow
              </a>
            </td>
          <td>
            Anthony Baptista, Alessandro Barp, Tapabrata Chakraborti, Chris Harbron, Ben D. MacArthur, Christopher R. S. Banerji
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This study utilized the Gaussian Processes (GPs) regression framework to establish stochastic error bounds between the actual and predicted state evolution of nonlinear systems. These systems are embedded in the linear parameter-varying (LPV) formulation and controlled using model predictive control (MPC). Our main focus is quantifying the uncertainty of the LPVMPC framework's forward error resulting from scheduling signal estimation mismatch. We compared our stochastic approach with a recent deterministic approach and observed improvements in conservatism and robustness. To validate our analysis and method, we solved the regulator problem of an unbalanced disk.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ff8595a6a1aaeecc229b93d39c77d7cfe4f41df1" target='_blank'>
              Stochastic Error Bounds in Nonlinear Model Predictive Control with Gaussian Processes via Parameter-Varying Embeddings
              </a>
            </td>
          <td>
            Dimitrios S. Karachalios, Hossameldin S. Abbas
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Given a time-series of noisy measured outputs of a dynamical system z[k], k=1...N, the Identifying Regulation with Adversarial Surrogates (IRAS) algorithm aims to find a non-trivial first integral of the system, namely, a scalar function g() such that g(z[i]) = g(z[j]), for all i,j. IRAS has been suggested recently and was used successfully in several learning tasks in models from biology and physics. Here, we give the first rigorous analysis of this algorithm in a specific setting. We assume that the observations admit a linear first integral and that they are contaminated by Gaussian noise. We show that in this case the IRAS iterations are closely related to the self-consistent-field (SCF) iterations for solving a generalized Rayleigh quotient minimization problem. Using this approach, we derive several sufficient conditions guaranteeing local convergence of IRAS to the correct first integral.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/01ba9751006fe3e23f3ffebb271a6a306b7169bc" target='_blank'>
              Analysis of the Identifying Regulation With Adversarial Surrogates Algorithm
              </a>
            </td>
          <td>
            Ron Teichner, Ron Meir, Michael Margaliot
          </td>
          <td>2024-05-05</td>
          <td>IEEE Control Systems Letters</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Neural Ordinary Differential Equations typically struggle to generalize to new dynamical behaviors created by parameter changes in the underlying system, even when the dynamics are close to previously seen behaviors. The issue gets worse when the changing parameters are unobserved, i.e., their value or influence is not directly measurable when collecting data. We introduce Neural Context Flow (NCF), a framework that encodes said unobserved parameters in a latent context vector as input to a vector field. NCFs leverage differentiability of the vector field with respect to the parameters, along with first-order Taylor expansion to allow any context vector to influence trajectories from other parameters. We validate our method and compare it to established Multi-Task and Meta-Learning alternatives, showing competitive performance in mean squared error for in-domain and out-of-distribution evaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scott problems. This study holds practical implications for foundational models in science and related areas that benefit from conditional neural ODEs. Our code is openly available at https://github.com/ddrous/ncflow.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c738e0450649ed6c04ff7e82d993987c381e35a" target='_blank'>
              Neural Context Flows for Learning Generalizable Dynamical Systems
              </a>
            </td>
          <td>
            Roussel Desmond Nzoyem, David A.W. Barton, Tom Deakin
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Resolvent analysis provides a framework to predict coherent spatio-temporal structures of largest linear energy amplification, through a singular value decomposition (SVD) of the resolvent operator, obtained by linearizing the Navier--Stokes equations about a known turbulent mean velocity profile. Resolvent analysis utilizes a Fourier decomposition in time, which has thus-far limited its application to statistically-stationary or time-periodic flows. This work develops a variant of resolvent analysis applicable to time-evolving flows, and proposes a variant that identifies spatio-temporally sparse structures, applicable to either stationary or time-varying mean velocity profiles. Spatio-temporal resolvent analysis is formulated through the incorporation of the temporal dimension to the numerical domain via a discrete time-differentiation operator. Sparsity (which manifests in localisation) is achieved through the addition of an L1-norm penalisation term to the optimisation associated with the SVD. This modified optimization problem can be formulated as a nonlinear eigenproblem, and solved via an inverse power method. We first demonstrate the implementation of the sparse analysis on statistically-stationary turbulent channel flow, and demonstrate that the sparse variant can identify aspects of the physics not directly evident from standard resolvent analysis. This is followed by applying the sparse space-time formulation on systems that are time-varying: a time-periodic turbulent Stokes boundary layer, and then a turbulent channel flow with a sudden change in pressure gradient. We present results demonstrating how the sparsity-promoting variant can either change the quantitative structure of the leading space-time modes to increase their sparsity, or identify entirely different linear amplification mechanisms compared to non-sparse resolvent analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f03e85eaeca443d0e3afa26fb6b312ac36136ef" target='_blank'>
              Sparse space-time resolvent analysis for statistically-stationary and time-varying flows
              </a>
            </td>
          <td>
            Barbara Lopez-Doriga, Eric Ballouz, H. J. Bae, Scott T. M. Dawson
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequencies. The approach presented here can produce {\em multiple} constants of motion from a {\em single}, generic data set.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/165c91a0a7e1b63ea3b1478005a1ff4bc9fcce02" target='_blank'>
              Constants of Motion for Conserved and Non-conserved Dynamics
              </a>
            </td>
          <td>
            Michael F. Zimmer
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Regression on function spaces is typically limited to models with Gaussian process priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3ae485c26a60310aa9df90e2ba95440cb064a85" target='_blank'>
              Universal Functional Regression with Neural Operator Flows
              </a>
            </td>
          <td>
            Yaozhong Shi, Angela F. Gao, Zachary E. Ross, K. Azizzadenesheli
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces. As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks. Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ceae40c347aa83b2fca85752e3356c459c21f769" target='_blank'>
              A rank decomposition for the topological classification of neural representations
              </a>
            </td>
          <td>
            Kosio Beshkov, Gaute T. Einevoll
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="We study a one-parameter family of time-reversible Hamiltonian vector fields in $\mathbb{R}^4$, which has received great attention in the literature. On the one hand, it is due to the role it plays in the context of certain applications in the field of Physics or Engineering and, on the other hand, we especially highlight its relevance within the framework of generic unfoldings of the four-dimensional nilpotent singularity of codimension four. The system exhibits a bifocal equilibrium point for a range of parameter values. The associated two-dimensional invariant manifolds, stable and unstable, fold into the phase space in such a way that they produce intricate patterns. This entangled geometry has previously been called \textit{tentacular geometry}. We consider a three-dimensional level set containing the bifocal equilibrium point to gain insight into the folding behavior of these invariant manifolds. Our method consists of describing the traces left by invariant manifolds when crossing an invariant cross section by the reversibility map. With this new approach, we provide a better understanding of how tentacular geometry evolves with respect to the parameter. Our techniques enables us to link the tentacular geometry on the cross section with the study of cocooning cascades of homoclinic tangencies. Indeed, we present a general theory to extend to $\mathbb{R}^4$ the phenomena related to cocoon bifurcation that classically develop within families of three-dimensional reversible vector fields. On the basis of these results, we conjecture the existence of heteroclinic cycles consisting of two orbits connecting the bifocus with a saddle node periodic orbit.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5399ea8dddfff5bfa24f94dd5379f743c57bb341" target='_blank'>
              Invariant manifolds in a reversible Hamiltonian system: the tentacle-like geometry
              </a>
            </td>
          <td>
            Pablo S. Casas, F'atima Drubi, Santiago Ib'anez
          </td>
          <td>2024-04-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper presents a new boundary-value problem formulation for quantifying uncertainty induced by the presence of small Brownian noise near transversally stable periodic orbits (limit cycles) and quasiperiodic invariant tori of the deterministic dynamical systems obtained in the absence of noise. The formulation uses adjoints to construct a continuous family of transversal hyperplanes that are invariant under the linearized deterministic flow near the limit cycle or quasiperiodic invariant torus. The intersections with each hyperplane of stochastic trajectories that remain near the deterministic cycle or torus over intermediate times may be approximated by a Gaussian distribution whose covariance matrix can be obtained from the solution to the corresponding boundary-value problem. In the case of limit cycles, the analysis improves upon results in the literature through the explicit use of state-space projections, transversality constraints, and symmetry-breaking parameters that ensure uniqueness of the solution despite the lack of hyperbolicity along the limit cycle. These same innovations are then generalized to the case of a quasiperiodic invariant torus of arbitrary dimension. In each case, a closed-form solution to the covariance boundary-value problem is found in terms of a convergent series. The methodology is validated against the results of numerical integration for two examples of stochastically perturbed limit cycles and one example of a stochastically perturbed two-dimensional quasiperiodic invariant torus. Finally, an implementation of the covariance boundary-value problem in the numerical continuation package coco is applied to analyze the small-noise limit near a two-dimensional quasiperiodic invariant torus in a nonlinear deterministic dynamical system in $\mathbb{R}^4$ that does not support closed-form analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3301de8fe4477b187bf60eaf9e1b46e7eb66f04f" target='_blank'>
              Adjoint-Based Projections for Uncertainty Quantification near Stochastically Perturbed Limit Cycles and Tori
              </a>
            </td>
          <td>
            Zaid Ahsan, Harry Dankowicz, Christian Kuehn
          </td>
          <td>2024-04-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model. The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space. Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints. This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution. The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis. To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE). Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics. The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/15f8d797779bc932cdc0bb4020bb6cb273b8cfa5" target='_blank'>
              Preserving Nonlinear Constraints in Variational Flow Filtering Data Assimilation
              </a>
            </td>
          <td>
            Amit N. Subrahmanya, Andrey A. Popov, Reid J. Gomillion, Adrian Sandu
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Learning approximations to smooth target functions of many variables from finite sets of pointwise samples is an important task in scientific computing and its many applications in computational science and engineering. Despite well over half a century of research on high-dimensional approximation, this remains a challenging problem. Yet, significant advances have been made in the last decade towards efficient methods for doing this, commencing with so-called sparse polynomial approximation methods and continuing most recently with methods based on Deep Neural Networks (DNNs). In tandem, there have been substantial advances in the relevant approximation theory and analysis of these techniques. In this work, we survey this recent progress. We describe the contemporary motivations for this problem, which stem from parametric models and computational uncertainty quantification; the relevant function classes, namely, classes of infinite-dimensional, Banach-valued, holomorphic functions; fundamental limits of learnability from finite data for these classes; and finally, sparse polynomial and DNN methods for efficiently learning such functions from finite data. For the latter, there is currently a significant gap between the approximation theory of DNNs and the practical performance of deep learning. Aiming to narrow this gap, we develop the topic of practical existence theory, which asserts the existence of dimension-independent DNN architectures and training strategies that achieve provably near-optimal generalization errors in terms of the amount of training data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36a0439df646365cfb853ff77a6ee369e1b99325" target='_blank'>
              Learning smooth functions in high dimensions: from sparse polynomials to deep neural networks
              </a>
            </td>
          <td>
            Ben Adcock, S. Brugiapaglia, N. Dexter, S. Moraga
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Reduced order models based on the transport of a lower dimensional manifold representation of the thermochemical state, such as Principal Component (PC) transport and Machine Learning (ML) techniques, have been developed to reduce the computational cost associated with the Direct Numerical Simulations (DNS) of reactive flows. Both PC transport and ML normally require an abundance of data to exhibit sufficient predictive accuracy, which might not be available due to the prohibitive cost of DNS or experimental data acquisition. To alleviate such difficulties, similar data from an existing dataset or domain (source domain) can be used to train ML models, potentially resulting in adequate predictions in the domain of interest (target domain). This study presents a novel probabilistic transfer learning (TL) framework to enhance the trust in ML models in correctly predicting the thermochemical state in a lower dimensional manifold and a sparse data setting. The framework uses Bayesian neural networks, and autoencoders, to reduce the dimensionality of the state space and diffuse the knowledge from the source to the target domain. The new framework is applied to one-dimensional freely-propagating flame solutions under different data sparsity scenarios. The results reveal that there is an optimal amount of knowledge to be transferred, which depends on the amount of data available in the target domain and the similarity between the domains. TL can reduce the reconstruction error by one order of magnitude for cases with large sparsity. The new framework required 10 times less data for the target domain to reproduce the same error as in the abundant data scenario. Furthermore, comparisons with a state-of-the-art deterministic TL strategy show that the probabilistic method can require four times less data to achieve the same reconstruction error.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1b40d43f32053fa66703ccc372c93df9eb12e60d" target='_blank'>
              Probabilistic transfer learning methodology to expedite high fidelity simulation of reactive flows
              </a>
            </td>
          <td>
            Bruno S. Soriano, , T. Echekki, Jacqueline H. Chen, Mohammad Khalil
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="There exist many examples of systems which have some symmetries, and which one may monitor with symmetry preserving controls. Since symmetries are preserved along the evolution, full controllability is not possible, and controllability has to be considered inside sets of states with same symmetries. We prove that generic systems with symmetries are controllable in this sense. This result has several applications, for instance: (i) generic controllability of particle systems when the kernel of interaction between particles plays the role of a mean-field control; (ii) generic controllability for families of vector fields on manifolds with boundary; (iii) universal interpolation for neural networks architectures with"generic"self attention-type layers - a type of layers ubiquitous in recent neural networks architectures, e.g., in the Transformers architecture. The tools we develop could help address various other questions of control of equivariant systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/27cc3c9d1854c9bebc495276317483bfc173510a" target='_blank'>
              Generic controllability of equivariant systems and applications to particle systems and neural networks
              </a>
            </td>
          <td>
            Andrei Agrachev, Cyril Letrouit
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Data-driven modelling and scientific machine learning have been responsible for significant advances in determining suitable models to describe data. Within dynamical systems, neural ordinary differential equations (ODEs), where the system equations are set to be governed by a neural network, have become a popular tool for this challenge in recent years. However, less emphasis has been placed on systems that are only partially-observed. In this work, we employ a hybrid neural ODE structure, where the system equations are governed by a combination of a neural network and domain-specific knowledge, together with symbolic regression (SR), to learn governing equations of partially-observed dynamical systems. We test this approach on two case studies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional model of the Lorenz system. We demonstrate that the method is capable of successfully learning the true underlying governing equations of unobserved states within these systems, with robustness to measurement noise.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8e5f91324b2ca816ed10e0d9a1d6565fb12a4a1f" target='_blank'>
              Learning Governing Equations of Unobserved States in Dynamical Systems
              </a>
            </td>
          <td>
            Gevik Grigorian, Sandip V. George, S. Arridge
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches. As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm. Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics. For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps. We extend our results to the $\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e07e4d3cefc57fa4a734ec7754b6c88148469b11" target='_blank'>
              Computing the spectrum and pseudospectrum of infinite-volume operators from local patches
              </a>
            </td>
          <td>
            Paul Hege, Massimo Moscolari, Stefan Teufel
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We extend the theory of spectral submanifolds (SSMs) to general non-autonomous dynamical systems that are either weakly forced or slowly varying. Examples of such systems arise in structural dynamics, fluid-structure interactions, and control problems. The time-dependent SSMs we construct under these assumptions are normally hyperbolic and hence will persist for larger forcing and faster time dependence that are beyond the reach of our precise existence theory. For this reason, we also derive formal asymptotic expansions that, under explicitly verifiable nonresonance conditions, approximate SSMs and their aperiodic anchor trajectories accurately for stronger, faster, or even temporally discontinuous forcing. Reducing the dynamical system to these persisting SSMs provides a mathematically justified model- reduction technique for non-autonomous physical systems whose time dependence is moderate either in magnitude or speed. We illustrate the existence, persistence, and computation of temporally aperiodic SSMs in mechanical examples under chaotic forcing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6c3f5ce22fe0054b44c9c4edf80e87ec55c0d852" target='_blank'>
              Nonlinear model reduction to temporally aperiodic spectral submanifolds.
              </a>
            </td>
          <td>
            George Haller, Roshan S Kaundinya
          </td>
          <td>2024-04-01</td>
          <td>Chaos</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This paper studies the numerical approximation of evolution equations by nonlinear parametrizations $u(t)=\Phi(q(t))$ with time-dependent parameters $q(t)$, which are to be determined in the computation. The motivation comes from approximations in quantum dynamics by multiple Gaussians and approximations of various dynamical problems by tensor networks and neural networks. In all these cases, the parametrization is typically irregular: the derivative $\Phi'(q)$ can have arbitrarily small singular values and may have varying rank. We derive approximation results for a regularized approach in the time-continuous case as well as in time-discretized cases. With a suitable choice of the regularization parameter and the time stepsize, the approach can be successfully applied in irregular situations, even though it runs counter to the basic principle in numerical analysis to avoid solving ill-posed subproblems when aiming for a stable algorithm. Numerical experiments with sums of Gaussians for approximating quantum dynamics and with neural networks for approximating the flow map of a system of ordinary differential equations illustrate and complement the theoretical results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cf4d1a09ab706c58f59e4885953f1932527722b7" target='_blank'>
              Regularized dynamical parametric approximation
              </a>
            </td>
          <td>
            Michael Feischl, Caroline Lasser, Christian Lubich, Jorg Nick
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="In one calculation, adjoint sensitivity analysis provides the gradient of a quantity of interest with respect to all system's parameters. Conventionally, adjoint solvers need to be implemented by differentiating computational models, which can be a cumbersome task and is code-specific. To propose an adjoint solver that is not code-specific, we develop a data-driven strategy. We demonstrate its application on the computation of gradients of long-time averages of chaotic flows. First, we deploy a parameter-aware echo state network (ESN) to accurately forecast and simulate the dynamics of a dynamical system for a range of system's parameters. Second, we derive the adjoint of the parameter-aware ESN. Finally, we combine the parameter-aware ESN with its adjoint version to compute the sensitivities to the system parameters. We showcase the method on a prototypical chaotic system. Because adjoint sensitivities in chaotic regimes diverge for long integration times, we analyse the application of ensemble adjoint method to the ESN. We find that the adjoint sensitivities obtained from the ESN match closely with the original system. This work opens possibilities for sensitivity analysis without code-specific adjoint solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/553a6afc439089894b231b44d32efab776f1e7b8" target='_blank'>
              Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A Data-Driven Approach
              </a>
            </td>
          <td>
            D. E. Ozan, Luca Magri
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we consider direct policy optimization for the linear-quadratic Gaussian (LQG) setting. Over the past few years, it has been recognized that the landscape of stabilizing output-feedback controllers of relevance to LQG has an intricate geometry, particularly as it pertains to the existence of spurious stationary points. In order to address such challenges, in this paper, we first adopt a Riemannian metric for the space of stabilizing full-order minimal output-feedback controllers. We then proceed to prove that the orbit space of such controllers modulo coordinate transformation admits a Riemannian quotient manifold structure. This geometric structure is then used to develop a Riemannian gradient descent for the direct LQG policy optimization. We prove a local convergence guarantee with linear rate and show the proposed approach exhibits significantly faster and more robust numerical performance as compared with ordinary gradient descent for LQG. Subsequently, we provide reasons for this observed behavior; in particular, we argue that optimizing over the orbit space of controllers is the right theoretical and computational setup for direct LQG policy optimization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c5e5ac6797169a586215c869fc9b5f75f42dbe46" target='_blank'>
              Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization
              </a>
            </td>
          <td>
            Spencer Kraisler, M. Mesbahi
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>44</td>
        </tr>

        <tr id="We study the integrability of two-dimensional theories that are obtained by a dimensional reduction of certain four-dimensional gravitational theories describing the coupling of Maxwell fields and neutral scalar fields to gravity in the presence of a potential for the neutral scalar fields. By focusing on a certain solution subspace, we show that a subset of the equations of motion in two dimensions are the compatibility conditions for a modified version of the Breitenlohner-Maison linear system. Subsequently, we study the Liouville integrability of the 2D models encoding the chosen 4D solution subspace from a one-dimensional point of view by constructing Lax pair matrices. In this endeavour, we successfully employ a linear neural network to search for Lax pair matrices for these models, thereby illustrating how machine learning approaches can be effectively implemented to augment the identification of integrable structures in classical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2dc257500269265c06959381facfe5f7dcfa673a" target='_blank'>
              Classical integrability in the presence of a cosmological constant: analytic and machine learning results
              </a>
            </td>
          <td>
            G. L. Cardoso, Dami'an Mayorga Pena, S. Nampuri
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Continuous time recurrent neural networks (CTRNNs) are systems of coupled ordinary differential equations (ODEs) inspired by the structure of neural networks in the brain. CTRNNs are known to be universal dynamical approximators: given a large enough system, the parameters of a CTRNN can be tuned to produce output that is arbitrarily close to that of any other dynamical system. However, in practice, both designing systems of CTRNN to have a certain output, and the reverse-understanding the dynamics of a given system of CTRNN-can be nontrivial. In this article, we describe a method for embedding any specified Turing machine in its entirety into a CTRNN. As such, we describe in detail a continuous time dynamical system that performs arbitrary discrete-state computations. We suggest that in acting as both a continuous time dynamical system and as a computer, the study of such systems can help refine and advance the debate concerning the Computational Hypothesis that cognition is a form of computation and the Dynamical Hypothesis that cognitive systems are dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a4c18d67633c17f70ffc53a4f2853ff51348378c" target='_blank'>
              A Continuous Time Dynamical Turing Machine.
              </a>
            </td>
          <td>
            C. Postlethwaite, Peter Ashwin, Matthew Egbert
          </td>
          <td>2024-05-16</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Many complicated dynamical events may be broken down into simpler pieces and efficiently described by a system that shifts among a variety of conditionally dynamical modes. Building on switching linear dynamical systems, we develop a new model that extends the switching linear dynamical systems for better discovering these dynamical modes. In the proposed model, the linear dynamics of latent variables can be described by a higher-order vector autoregressive process, which makes it feasible to evaluate the higher-order dependency relationships in the dynamics. In addition, the transition of switching states is determined by a stick-breaking logistic regression, overcoming the limitation of a restricted geometric state duration and recovering the symmetric dependency between the switching states and the latent variables from asymmetric relationships. Furthermore, logistic regression evidence potentials can appear as conditionally Gaussian potentials by utilizing the Pólya-gamma augmentation strategy. Filtering and smoothing algorithms and Bayesian inference for parameter learning in the proposed model are presented. The utility and versatility of the proposed model are demonstrated on synthetic data and public functional magnetic resonance imaging data. Our model improves the current methods for learning the switching linear dynamical modes, which will facilitate the identification and assessment of the dynamics of complex systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f180f44aa0cab4e14882a44aef54b3e4ae757688" target='_blank'>
              Bayesian Inference of Recurrent Switching Linear Dynamical Systems with Higher-Order Dependence
              </a>
            </td>
          <td>
            Houxiang Wang, Jiaqing Chen
          </td>
          <td>2024-04-13</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present a generative model that amortises computation for the field around e.g. gravitational or magnetic sources. Exact numerical calculation has either computational complexity $\mathcal{O}(M\times{}N)$ in the number of sources and field evaluation points, or requires a fixed evaluation grid to exploit fast Fourier transforms. Using an architecture where a hypernetwork produces an implicit representation of the field around a source collection, our model instead performs as $\mathcal{O}(M + N)$, achieves accuracy of $\sim\!4\%-6\%$, and allows evaluation at arbitrary locations for arbitrary numbers of sources, greatly increasing the speed of e.g. physics simulations. We also examine a model relating to the physical properties of the output field and develop two-dimensional examples to demonstrate its application. The code for these models and experiments is available at https://github.com/cmt-dtu-energy/hypermagnetics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/70c2a9f4249351e7bf76677c09b508db82644b76" target='_blank'>
              Scalable physical source-to-field inference with hypernetworks
              </a>
            </td>
          <td>
            Berian James, Stefan Pollok, Ignacio Peis, J. Frellsen, Rasmus Bjørk
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="The randomized singular value decomposition (SVD) has become a popular approach to computing cheap, yet accurate, low-rank approximations to matrices due to its efficiency and strong theoretical guarantees. Recent work by Boull\'e and Townsend (FoCM, 2023) presents an infinite-dimensional analog of the randomized SVD to approximate Hilbert-Schmidt operators. However, many applications involve computing low-rank approximations to symmetric positive semi-definite matrices. In this setting, it is well-established that the randomized Nystr{\"o}m approximation is usually preferred over the randomized SVD. This paper explores an infinite-dimensional analog of the Nystr{\"o}m approximation to compute low-rank approximations to non-negative self-adjoint trace-class operators. We present an analysis of the method and, along the way, improve the existing infinite-dimensional bounds for the randomized SVD. Our analysis yields bounds on the expected value and tail bounds for the Nystr{\"o}m approximation error in the operator, trace, and Hilbert-Schmidt norms. Numerical experiments for simple integral operators validate the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a99a1ae9a29584700d070c9ffbf466c118c5409a" target='_blank'>
              Randomized Nyström approximation of non-negative self-adjoint operators
              </a>
            </td>
          <td>
            David Persson, Nicolas Boull'e, Daniel Kressner
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/50cc0f3e7c994d26825a061369cf24c303e71e0a" target='_blank'>
              Learning spiking neuronal networks with artificial neural networks: neural oscillations.
              </a>
            </td>
          <td>
            Ruilin Zhang, Zhongyi Wang, Tianyi Wu, Yuhang Cai, Louis Tao, Zhuocheng Xiao, Yao Li
          </td>
          <td>2024-04-17</td>
          <td>Journal of mathematical biology</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="
 Learned Primal-Dual (LPD) is a deep learning based method for composite optimization problems that is based on unrolling/unfolding the primal-dual hybrid gradient algorithm. While achieving great successes in applications, the mathematical interpretation of LPD as a truncated iterative scheme is not necessarily sufficient to fully understand its properties. In this paper, we study the LPD with a general linear operator. We model the forward propagation of LPD as a system of difference equations and a system of differential equations in discrete- and continuous-time settings (for primal and dual variables/trajectories), which are named discrete-time LPD and continuous-time LPD, respectively. Forward analyses such as stabilities and the convergence of the state variables of the discrete-time LPD to the solution of continuous-time LPD are given. Moreover, we analyze the learning problems with/without regularization terms of both discrete-time and continuous-time LPD from the optimal control viewpoint. We prove convergence results of their optimal solutions with respect to the network state initialization and training data, showing in some sense the topological stability of the learning problems. We also establish convergence from the solution of the discrete-time LPD learning problem to that of the continuous-time LPD learning problem through a piecewise linear extension, under some appropriate assumptions on the space of learnable parameters. This study demonstrates theoretically the robustness of the LPD structure and the associated training process, and can induce some future research and applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e385d7198f01073117e03c2ead3f15483eeca0b6" target='_blank'>
              On dynamical system modeling of Learned Primal-Dual with a linear operator $\mathcal{K}$: Stability and convergence properties
              </a>
            </td>
          <td>
            Jinshu Huang, Yiming Gao, Chunlin Wu
          </td>
          <td>2024-05-10</td>
          <td>Inverse Problems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Model inversion is a fundamental technique in feedforward control. Unstable inverse models present a challenge in that useful feedforward control trajectories cannot be generated by directly propagating them. Stable inversion is a process for generating useful trajectories from unstable inverses by handling their stable and unstable modes separately. Piecewise affine (PWA) systems are a popular framework for modeling complicated dynamics. The primary contributions of this article are closed-form inverse formulas for a general class of PWA models, and stable inversion methods for these models. Both contributions leverage closed-form model representations to prove sufficient conditions for solution existence and uniqueness, and to develop solution computation methods. The result is implementable feedforward control synthesis from PWA models with either stable or unstable inverses. In practice, feedforward control alone may yield substantial tracking errors due to mismatch between the known system model and the unknowable complete system physics. Iterative learning control (ILC) is a technique for achieving robustness to model error in feedforward control. To demonstrate the primary contributions' validity and utility, this article also integrates PWA stable inversion with ILC in simulations based on a physical printhead positioning system.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b53e56d73a6c6cdeabfac2d80dae3096409c8196" target='_blank'>
              Stable Inversion of Piecewise Affine Systems with Application to Feedforward and Iterative Learning Control
              </a>
            </td>
          <td>
            Isaac A. Spiegel, Nard Strijbosch, R. Rozario, Tom Oomen, Kira Barton
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Manifold learning is a field of study in machine learning and statistics that is closely associated with dimensionality reduction algorithmic techniques is gaining popularity these days. There are two types of manifold learning approaches: linear and nonlinear. Principal component analysis (PCA) and multidimensional scaling (MDS) are two examples of linear techniques that have long been staples in the statistician's arsenal for evaluating multivariate data. Nonlinear manifold learning, which encompasses diffusion maps, Laplacian Eigenmaps, Hessian Eigenmaps, Isomap, and local linear embedding, has seen a surge in research effort recently. A few of these methods are nonlinear extensions of linear approaches. A nearest search, the definition of distances or affinities between points (a crucial component of these methods' effectiveness), and an Eigen problem for embedding high-dimensional points into a lower dimensional space make up the algorithmic process of the majority of these techniques. The strengths and weaknesses of the new method are briefly reviewed in this article. In the field of computer graphics, we utilize a particular manifold learning method was first presented in statistics and machine learning to create a global, Spectral-based shape descriptor.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/82375ae6c235229b9849c79c260eab0aac25395d" target='_blank'>
              Analysis of Manifold and its Application
              </a>
            </td>
          <td>
            Gyanvendra Pratap Singh, Shristi Srivastav
          </td>
          <td>2024-05-30</td>
          <td>International Journal of Science and Research Archive</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c792c4fd76204bea7c35e45625c8cacdb7e0afc3" target='_blank'>
              Backpropagation through space, time, and the brain
              </a>
            </td>
          <td>
            B. Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="We use Fourier Neural Operators (FNOs) to study the relation between the modulus and phase of amplitudes in $2\to 2$ elastic scattering at fixed energies. Unlike previous approaches, we do not employ the integral relation imposed by unitarity, but instead train FNOs to discover it from many samples of amplitudes with finite partial wave expansions. When trained only on true samples, the FNO correctly predicts (unique or ambiguous) phases of amplitudes with infinite partial wave expansions. When also trained on false samples, it can rate the quality of its prediction by producing a true/false classifying index. We observe that the value of this index is strongly correlated with the violation of the unitarity constraint for the predicted phase, and present examples where it delineates the boundary between allowed and disallowed profiles of the modulus. Our application of FNOs is unconventional: it involves a simultaneous regression-classification task and emphasizes the role of statistics in ensembles of NOs. We comment on the merits and limitations of the approach and its potential as a new methodology in Theoretical Physics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4a059c9216216318baec94fd6fbf950e34c349f3" target='_blank'>
              Learning S-Matrix Phases with Neural Operators
              </a>
            </td>
          <td>
            V. Niarchos, C. Papageorgakis
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="A large toolbox of numerical schemes for dispersive equations has been established, based on different discretization techniques such as discretizing the variation-of-constants formula (e.g., exponential integrators) or splitting the full equation into a series of simpler subproblems (e.g., splitting methods). In many situations these classical schemes allow a precise and efficient approximation. This, however, drastically changes whenever non-smooth phenomena enter the scene such as for problems at low regularity and high oscillations. Classical schemes fail to capture the oscillatory nature of the solution, and this may lead to severe instabilities and loss of convergence. In this article we review a new class of resonance-based schemes. The key idea in the construction of the new schemes is to tackle and deeply embed the underlying nonlinear structure of resonances into the numerical discretization. As in the continuous case, these terms are central to structure preservation and offer the new schemes strong properties at low regularity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1d4d0ccd89047fc9786193f37629e687c0217dc4" target='_blank'>
              Resonances as a computational tool
              </a>
            </td>
          <td>
            Fr'ed'eric Rousset, Katharina Schratz
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions. MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD. We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3de16e91cc88fd0f9ba210bffa57fcd84c2368b6" target='_blank'>
              MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation
              </a>
            </td>
          <td>
            Akshay Thakur, Souvik Chakraborty
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being well-suited for optimal control design, numerical simulations illustrate that our NN-based technique enjoys very similar performance to state-of-the-art system identification methodologies for hybrid systems and it is competitive on nonlinear benchmarks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b929d17a9033ce7fade68ef2c9fc4e0ab291210" target='_blank'>
              A neural network-based approach to hybrid systems identification for control
              </a>
            </td>
          <td>
            F. Fabiani, Bartolomeo Stellato, Daniele Masti, P. Goulart
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>36</td>
        </tr>

        <tr id="Standard approaches to controlling dynamical systems involve biologically implausible steps such as backpropagation of errors or intermediate model-based system representations. Recent advances in machine learning have shown that"imperfect"feedback of errors during training can yield test performance that is similar to using full backpropagated errors, provided that the two error signals are at least somewhat aligned. Inspired by such methods, we introduce an iterative, spatiotemporally local protocol to learn driving forces and control non-equilibrium dynamical systems using imperfect feedback signals. We present numerical experiments and theoretical justification for several examples. For systems in conservative force fields that are driven by external time-dependent protocols, our update rules resemble a dynamical version of contrastive divergence. We appeal to linear response theory to establish that our imperfect update rules are locally convergent for these conservative systems. For systems evolving under non-conservative dynamics, we derive a new theoretical result that makes possible the control of non-equilibrium steady-state probabilities through simple local update rules. Finally, we show that similar local update rules can also solve dynamical control problems for non-conservative systems, and we illustrate this in the non-trivial example of active nematics. Our updates allow learning spatiotemporal activity fields that pull topological defects along desired trajectories in the active nematic fluid. These imperfect feedback methods are information efficient and in principle biologically plausible, and they can help extend recent methods of decentralized training for physical materials into dynamical settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/82094b89578003d8660077eda173551c4969dba9" target='_blank'>
              Learning to control non-equilibrium dynamics using local imperfect gradients
              </a>
            </td>
          <td>
            Carlos Floyd, Aaron Dinner, Suriyanarayanan Vaikuntanathan
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens. Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet. The main goal of our work is to close this gap and understand the limitations and potentials of this device. It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates. Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams. In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables. This provides an evaluation of the dynamical aperture, Poincar\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes. Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time. The second part investigates the particle dynamics as a function of system parameters. We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations. Each regime is considered in great detail, including the limiting cases of large and small amplitudes. In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bb3718191ea7e071bb47fd0a739ce8e135459644" target='_blank'>
              Dynamics of McMillan mappings II. Axially symmetric map
              </a>
            </td>
          <td>
            T. Zolkin, Brandon Cathey, Sergei Nagaitsev
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="A geometric approach to the integrability and reduction of dynamical systems, both when dealing with systems of differential equations and in classical physics, is developed from a modern perspective. The main ingredients of this analysis are infinitesimal symmetries and tensor fields that are invariant under the given dynamics. A particular emphasis is placed on the existence of alternative invariant volume forms and the associated Jacobi multiplier theory, and then the Hojman symmetry theory is developed as a complement to the Noether theorem and non-Noether constants of motion. We also recall the geometric approach to Sundman infinitesimal time-reparametrisation for autonomous systems of first-order differential equations and some of its applications to integrability, and an analysis of how to define Sundman transformations for autonomous systems of second-order differential equations is proposed, which shows the necessity of considering alternative tangent bundle structures. A short description of alternative tangent structures is provided, and an application to integrability, namely, the linearisability of scalar second-order differential equations under generalised Sundman transformations, is developed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/aafe195805568425a8a07711c74f8566990cd739" target='_blank'>
              A Geometric Approach to the Sundman Transformation and Its Applications to Integrability
              </a>
            </td>
          <td>
            J. Cariñena
          </td>
          <td>2024-05-06</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="Coherent sets are time-dependent regions in the physical space of nonautonomous flows that exhibit little mixing with their neighborhoods, robustly under small random perturbations of the flow. They thus characterize the global long-term transport behavior of the system. We propose a framework to extract such time-dependent families of coherent sets for nonautonomous systems with an ergodic driving dynamics and (small) Brownian noise in physical space. Our construction involves the assembly and analysis of an operator on functions over the augmented space of the associated skew product that, for each fixed state of the driving, propagates distributions on the corresponding physical-space fibre according to the dynamics. This time-dependent operator has the structure of a semigroup (it is called the Mather semigroup), and we show that a spectral analysis of its generator allows for a trajectory-free computation of coherent families, simultaneously for all states of the driving. Additionally, for quasi-periodically driven torus flows, we propose a tailored Fourier discretization scheme for this generator and demonstrate our method by means of three examples of two-dimensional flows.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7253d73cd8e5de2e6f4f2e457a0278813f80797c" target='_blank'>
              Extracting coherent sets in aperiodically driven flows from generators of Mather semigroups
              </a>
            </td>
          <td>
            Robin Chemnitz, Maximilian Engel, P'eter Koltai
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We propose a novel nonlinear bidirectionally coupled heterogeneous chain network whose dynamics evolve in discrete time. The backbone of the model is a pair of popular map-based neuron models, the Chialvo and the Rulkov maps. This model is assumed to proximate the intricate dynamical properties of neurons in the widely complex nervous system. The model is first realized via various nonlinear analysis techniques: fixed point analysis, phase portraits, Jacobian matrix, and bifurcation diagrams. We observe the coexistence of chaotic and period-4 attractors. Various codimension-1 and -2 patterns for example saddle-node, period-doubling, Neimark-Sacker, double Neimark-Sacker, flip- and fold-Neimark Sacker, and 1:1 and 1:2 resonance are also explored. Furthermore, the study employs two synchronization measures to quantify how the oscillators in the network behave in tandem with each other over a long number of iterations. Finally, a time series analysis of the model is performed to investigate its complexity in terms of sample entropy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f5375ce11f3e9ca8219dcac0c58408d4b1b551d6" target='_blank'>
              Dynamical properties of a small heterogeneous chain network of neurons in discrete time
              </a>
            </td>
          <td>
            Indranil Ghosh, Anjana S. Nair, H. O. Fatoyinbo, S. S. Muni
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Both fixed-gain control and adaptive learning architectures aim to mitigate the effects of uncertainties. In particular, fixed-gain control offers more predictable closed-loop system behavior but requires the knowledge of uncertainty bounds. In contrast, while adaptive learning does not necessarily require such knowledge, it often results in less predictable closed-loop system behavior compared to fixed-gain control. To this end, this paper presents a novel symbiotic control framework that offers the strengths of fixed-gain control and adaptive learning architectures. Specifically, this framework synergistically integrates these architectures to mitigate the effects of uncertainties in a more predictable manner as compared to adaptive learning alone and it does not require any knowledge on such uncertainties. Both parametric and nonparametric uncertainties are considered, where we utilize neural networks to approximate the unknown uncertainty basis for the latter case. Counterintuitively, the proposed framework has the ability to achieve a desired level of closed-loop system behavior even with an insufficient number of neurons (e.g., when the neural network approximation error is large) or in the face of injudiciously selected adaptive learning parameters (e.g., high leakage term parameters).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/481298d781d00ff7633736f4e84fb5e00027e0b9" target='_blank'>
              Symbiotic Control of Uncertain Dynamical Systems: Harnessing Synergy Between Fixed-Gain Control and Adaptive Learning Architectures
              </a>
            </td>
          <td>
            T. Yucelen, S. B. Sarsılmaz, E. Yildirim
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Modeling and control of agent-based models is twice cursed by the dimensionality of the problem, as both the number of agents and their state space dimension can be large. Even though the computational barrier posed by a large ensemble of agents can be overcome through a mean field formulation of the control problem, the feasibility of its solution is generally guaranteed only for agents operating in low-dimensional spaces. To circumvent the difficulty posed by the high dimensionality of the state space a kinetic model is proposed, requiring the sampling of high-dimensional, two-agent sub-problems, to evolve the agents' density using a Boltzmann type equation. Such density evolution requires a high-frequency sampling of two-agent optimal control problems, which is efficiently approximated by means of deep neural networks and supervised learning, enabling the fast simulation of high-dimensional, large-scale ensembles of controlled particles. Numerical experiments demonstrate the effectiveness of the proposed approach in the control of consensus and attraction-repulsion dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fc40e443b0c81201f8864bff37a50d5eb1d645f" target='_blank'>
              Control of high-dimensional collective dynamics by deep neural feedback laws and kinetic modelling
              </a>
            </td>
          <td>
            G. Albi, Sara Bicego, D. Kalise
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="For the recently introduced deep learning-powered approach to PDE backstepping control, we present an advancement applicable across all the results developed thus far: approximating the control gain function only (a function of one variable), rather than the entire kernel function of the backstepping transformation (a function of two variables). We introduce this idea on a couple benchmark (unstable) PDEs, hyperbolic and parabolic. We alter the approach of quantifying the effect of the approximation error by replacing a backstepping transformation that employs the approximated kernel (suitable for adaptive control) by a transformation that employs the exact kernel (suitable for gain scheduling). A major simplification in the target system arises, with the perturbation due to the approximation shifting from the domain to the boundary condition. This results in a significant difference in the Lyapunov analysis, which nevertheless results in a guarantee of the stability being retained with the simplified approximation approach. The approach of approximating only the control gain function simplifies the operator being approximated and the training of its neural approximation, with an expected reduction in the neural network size. The price for the savings in approximation is paid through a somewhat more intricate Lyapunov analysis, in higher Sobolev spaces for some PDEs, as well as some restrictions on initial conditions that result from higher Sobolev spaces. While the proposed approach appears inapplicable to uses in adaptive control, it is almost certainly applicable in gain scheduling applications of neural operator-approximated PDE backstepping controllers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b8dc05fe50a3a0e909bcccb706cd06a5210bfabd" target='_blank'>
              Gain-Only Neural Operator Approximators of PDE Backstepping Controllers
              </a>
            </td>
          <td>
            R. Vázquez, Miroslav Krstic
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="In this paper, we apply a novel approach based on physics-informed neural networks to the computation of quasinormal modes of black hole solutions in modified gravity. In particular, we focus on the case of Einstein-scalar-Gauss-Bonnet theory, with several choices of the coupling function between the scalar field and the Gauss-Bonnet invariant. This type of calculation introduces a number of challenges with respect to the case of General Relativity, mainly due to the extra complexity of the perturbation equations and to the fact that the background solution is known only numerically. The solution of these perturbation equations typically requires sophisticated numerical techniques that are not easy to develop in computational codes. We show that physics-informed neural networks have an accuracy which is comparable to traditional numerical methods in the case of numerical backgrounds, while being very simple to implement. Additionally, the use of GPU parallelization is straightforward thanks to the use of standard machine learning environments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a02f352df1247d21253b324c3e64e5c393f97036" target='_blank'>
              Quasinormal Modes in Modified Gravity using Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            Raimon Luna, D. Doneva, Jos'e A. Font, Jr-Hua Lien, S. Yazadjiev
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="Neural networks have become a widely adopted tool for tackling a variety of problems in machine learning and artificial intelligence. In this contribution we use the mathematical framework of local stability analysis to gain a deeper understanding of the learning dynamics of feed forward neural networks. Therefore, we derive equations for the tangent operator of the learning dynamics of three-layer networks learning regression tasks. The results are valid for an arbitrary numbers of nodes and arbitrary choices of activation functions. Applying the results to a network learning a regression task, we investigate numerically, how stability indicators relate to the final training-loss. Although the specific results vary with different choices of initial conditions and activation functions, we demonstrate that it is possible to predict the final training loss, by monitoring finite-time Lyapunov exponents or covariant Lyapunov vectors during the training process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/870465bd2bfb984efc8bc1293ffbd4dda5049468" target='_blank'>
              On the weight dynamics of learning networks
              </a>
            </td>
          <td>
            Nahal Sharafi, Christoph Martin, Sarah Hallerberg
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Symbolic Regression (SR) is a widely studied field of research that aims to infer symbolic expressions from data. A popular approach for SR is the Sparse Identification of Nonlinear Dynamical Systems (\sindy) framework, which uses sparse regression to identify governing equations from data. This study introduces an enhanced method, Nested SINDy, that aims to increase the expressivity of the SINDy approach thanks to a nested structure. Indeed, traditional symbolic regression and system identification methods often fail with complex systems that cannot be easily described analytically. Nested SINDy builds on the SINDy framework by introducing additional layers before and after the core SINDy layer. This allows the method to identify symbolic representations for a wider range of systems, including those with compositions and products of functions. We demonstrate the ability of the Nested SINDy approach to accurately find symbolic expressions for simple systems, such as basic trigonometric functions, and sparse (false but accurate) analytical representations for more complex systems. Our results highlight Nested SINDy's potential as a tool for symbolic regression, surpassing the traditional SINDy approach in terms of expressivity. However, we also note the challenges in the optimization process for Nested SINDy and suggest future research directions, including the designing of a more robust methodology for the optimization process. This study proves that Nested SINDy can effectively discover symbolic representations of dynamical systems from data, offering new opportunities for understanding complex systems through data-driven methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5ef8aa10b07c62bde12d142106068d2ffd9e7414" target='_blank'>
              Generalizing the SINDy approach with nested neural networks
              </a>
            </td>
          <td>
            Camilla Fiorini, Cl'ement Flint, Louis Fostier, Emmanuel Franck, Reyhaneh Hashemi, Victor Michel-Dansac, Wassim Tenachi
          </td>
          <td>2024-04-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="There exist endless examples of dynamical systems with vast available data and unsatisfying mathematical descriptions. Sparse regression applied to symbolic libraries has quickly emerged as a powerful tool for learning governing equations directly from data; these learned equations balance quantitative accuracy with qualitative simplicity and human interpretability. Here, I present a general purpose, model agnostic sparse regression algorithm that extends a recently proposed exhaustive search leveraging iterative Singular Value Decompositions (SVD). This accelerated scheme, Scalable Pruning for Rapid Identification of Null vecTors (SPRINT), uses bisection with analytic bounds to quickly identify optimal rank-1 modifications to null vectors. It is intended to maintain sensitivity to small coefficients and be of reasonable computational cost for large symbolic libraries. A calculation that would take the age of the universe with an exhaustive search but can be achieved in a day with SPRINT.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8f8ccf22995f032ef4f3f34e6540383da7d52b4c" target='_blank'>
              Scalable Sparse Regression for Model Discovery: The Fast Lane to Insight
              </a>
            </td>
          <td>
            Matthew Golden
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This article presents an innovative approach to integrating port-Hamiltonian systems with neural network architectures, transitioning from deterministic to stochastic models. The study presents novel mathematical formulations and computational models that extend the understanding of dynamical systems under uncertainty and complex interactions. It emphasizes the significant progress in learning and predicting the dynamics of non-autonomous systems using port-Hamiltonian neural networks (pHNNs). It also explores the implications of stochastic neural networks in various dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a8c88fc5b355006989399caafa75200e15dbb217" target='_blank'>
              Integrating Port-Hamiltonian Systems with Neural Networks: From Deterministic to Stochastic Frameworks
              </a>
            </td>
          <td>
            L. D. Persio, Matthias Ehrhardt, Sofia Rizzotto
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In biochemical modeling, some foundational systems can exhibit sudden and profound behavioral shifts, such as the cellular signaling pathway models, in which the physiological responses promptly react to environmental changes, resulting in steep changes in their dynamic model trajectories. These steep changes are one of the major challenges in biochemical modeling governed by nonlinear differential equations. One promising way to tackle this challenge is converting the input data from the time domain to the frequency domain through Fourier Neural Operators, which enhances the ability to analyze data periodicity and regularity. However, the effectiveness of these Fourier based methods diminishes in scenarios with complex abrupt switches. To address this limitation, an innovative Multiscale Attention Wavelet Neural Operator (MAWNO) method is proposed in this paper, which comprehensively combines the attention mechanism with the versatile wavelet transforms to effectively capture these abrupt switches. Specifically, the wavelet transform scrutinizes data across multiple scales to extract the characteristics of abrupt signals into wavelet coefficients, while the self-attention mechanism is adeptly introduced to enhance the wavelet coefficients in high-frequency signals that can better characterize the abrupt switches. Experimental results substantiate MAWNO’s supremacy in terms of accuracy on three classical biochemical models featuring periodic and steep trajectories. https://github.com/SUDERS/MAWNO.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b93eb74da4845805ef08081926bc92082747ed15" target='_blank'>
              Multiscale Attention Wavelet Neural Operator for Capturing Steep Trajectories in Biochemical Systems
              </a>
            </td>
          <td>
            Jiayang Su, Junbo Ma, Songyang Tong, Enze Xu, Minghan Chen
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Uncertainty quantification (UQ) in scientific machine learning (SciML) combines the powerful predictive power of SciML with methods for quantifying the reliability of the learned models. However, two major challenges remain: limited interpretability and expensive training procedures. We provide a new interpretation for UQ problems by establishing a new theoretical connection between some Bayesian inference problems arising in SciML and viscous Hamilton-Jacobi partial differential equations (HJ PDEs). Namely, we show that the posterior mean and covariance can be recovered from the spatial gradient and Hessian of the solution to a viscous HJ PDE. As a first exploration of this connection, we specialize to Bayesian inference problems with linear models, Gaussian likelihoods, and Gaussian priors. In this case, the associated viscous HJ PDEs can be solved using Riccati ODEs, and we develop a new Riccati-based methodology that provides computational advantages when continuously updating the model predictions. Specifically, our Riccati-based approach can efficiently add or remove data points to the training set invariant to the order of the data and continuously tune hyperparameters. Moreover, neither update requires retraining on or access to previously incorporated data. We provide several examples from SciML involving noisy data and \textit{epistemic uncertainty} to illustrate the potential advantages of our approach. In particular, this approach's amenability to data streaming applications demonstrates its potential for real-time inferences, which, in turn, allows for applications in which the predicted uncertainty is used to dynamically alter the learning process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4134838398ace7c1614eb021aca6dc028e137505" target='_blank'>
              Leveraging viscous Hamilton-Jacobi PDEs for uncertainty quantification in scientific machine learning
              </a>
            </td>
          <td>
            Zongren Zou, Tingwei Meng, Paula Chen, J. Darbon, G. Karniadakis
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>125</td>
        </tr>

        <tr id="One of the main theoretical challenges in learning dynamical systems from data is providing upper bounds on the generalization error, that is, the difference between the expected prediction error and the empirical prediction error measured on some finite sample. In machine learning, a popular class of such bounds are the so-called Probably Approximately Correct (PAC) bounds. In this paper, we derive a PAC bound for stable continuous-time linear parameter-varying (LPV) systems. Our bound depends on the H2 norm of the chosen class of the LPV systems, but does not depend on the time interval for which the signals are considered.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/06ee4d9bca15b8e5ff46993fc4598f015c786bb2" target='_blank'>
              A finite-sample generalization bound for stable LPV systems
              </a>
            </td>
          <td>
            Daniel Racz, Martin Gonzalez, M. Petreczky, A. Benczúr, B. Daróczy
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="Computer simulations of the reaction of neurons to electric stimulation can help to improve the understanding of the mechanisms behind deep brain stimulation. This is necessary to develop better treatments for patients who suffer from Parkinson’s disease, epilepsy, or other disorders. Since detailed and accurate computer simulations of neurons are computationally expensive, different methods are available to reduce this complexity. In this paper, we aim to reduce the computational complexity of a linear finite-element model of a neuron, which is placed atop a planar electrode, by applying three different model order reduction methods. Precisely, we use Krylov subspace-based model order reduction, proper orthogonal decomposition, and operator inference to obtain reduced models of different orders. Furthermore, we compare the quality of the obtained reduced-order models with the full-size finite-element model. Additionally, we compare the computational (CPU) time to construct the different reduced-order models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e9aeab7ff1de69e56e1ad557fc6e5f1251ecd69d" target='_blank'>
              Comparison of Model Order Reduction Methods for a Linear Finite Element Model of an Electrically Stimulated Neuron
              </a>
            </td>
          <td>
            Ulrike Fitzer, Pawan Goyal, Arwed Schuetz, I. Zawra, Dennis Hohlfeld, Tamara Bechtold
          </td>
          <td>2024-04-07</td>
          <td>2024 25th International Conference on Thermal, Mechanical and Multi-Physics Simulation and Experiments in Microelectronics and Microsystems (EuroSimE)</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d40082e5be2a7157a0b7aec673fb42471fdc321a" target='_blank'>
              Capturing Shock Waves by Relaxation Neural Networks
              </a>
            </td>
          <td>
            Nan Zhou, Zheng Ma
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson geometry. We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7dfed0d91681828f8554f5547f7cccf87a7151e7" target='_blank'>
              Designing Poisson Integrators Through Machine Learning
              </a>
            </td>
          <td>
            M. Vaquero, David Mart'in de Diego, Jorge Cort'es
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="State estimation of nonlinear dynamical systems has long aimed to balance accuracy, computational efficiency, robustness, and reliability. The rapid evolution of various industries has amplified the demand for estimation frameworks that satisfy all these factors. This study introduces a neuromorphic approach for robust filtering of nonlinear dynamical systems: SNN-EMSIF (spiking neural network-extended modified sliding innovation filter). SNN-EMSIF combines the computational efficiency and scalability of SNNs with the robustness of EMSIF, an estimation framework designed for nonlinear systems with zero-mean Gaussian noise. Notably, the weight matrices are designed according to the system model, eliminating the need for a learning process. The framework's efficacy is evaluated through comprehensive Monte Carlo simulations, comparing SNN-EMSIF with EKF and EMSIF. Additionally, it is compared with SNN-EKF in the presence of modeling uncertainties and neuron loss, using RMSEs as a metric. The results demonstrate the superior accuracy and robustness of SNN-EMSIF. Further analysis of runtimes and spiking patterns reveals an impressive reduction of 85% in emitted spikes compared to possible spikes, highlighting the computational efficiency of SNN-EMSIF. This framework offers a promising solution for robust estimation in nonlinear dynamical systems, opening new avenues for efficient and reliable estimation in various industries that can benefit from neuromorphic computing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4597b269ad69fee485fb527be1df63b665e3950d" target='_blank'>
              Neuromorphic Robust Estimation of Nonlinear Dynamical Systems Applied to Satellite Rendezvous
              </a>
            </td>
          <td>
            Reza Ahmadvand, S. S. Sharif, Y. Banad
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Solving high-dimensional dynamical systems in multi-query or real-time applications requires efficient surrogate modelling techniques, as e.g., achieved via model order reduction (MOR). If these systems are Hamiltonian systems their physical structure should be preserved during the reduction, which can be ensured by applying symplectic basis generation techniques such as the complex SVD (cSVD). Recently, randomized symplectic methods such as the randomized complex singular value decomposition (rcSVD) have been developed for a more efficient computation of symplectic bases that preserve the Hamiltonian structure during MOR. In the current paper, we present two error bounds for the rcSVD basis depending on the choice of hyperparameters and show that with a proper choice of hyperparameters, the projection error of rcSVD is at most a constant factor worse than the projection error of cSVD. We provide numerical experiments that demonstrate the efficiency of randomized symplectic basis generation and compare the bounds numerically.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/784185a6757f45f1fb6a75360f444570984fe1d6" target='_blank'>
              Error Analysis of Randomized Symplectic Model Order Reduction for Hamiltonian systems
              </a>
            </td>
          <td>
            Robin Herkert, Patrick Buchfink, B. Haasdonk, Johannes Rettberg, Jorg Fehr
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="We consider a distributed optimal control problem subject to a parabolic evolution equation as constraint. The control will be considered in the energy norm of the anisotropic Sobolev space $[H_{0;,0}^{1,1/2}(Q)]^\ast$, such that the state equation of the partial differential equation defines an isomorphism onto $H^{1,1/2}_{0;0,}(Q)$. Thus, we can eliminate the control from the tracking type functional to be minimized, to derive the optimality system in order to determine the state. Since the appearing operator induces an equivalent norm in $H_{0;0,}^{1,1/2}(Q)$, we will replace it by a computable realization of the anisotropic Sobolev norm, using a modified Hilbert transformation. We are then able to link the cost or regularization parameter $\varrho>0$ to the distance of the state and the desired target, solely depending on the regularity of the target. For a conforming space-time finite element discretization, this behavior carries over to the discrete setting, leading to an optimal choice $\varrho = h_x^2$ of the regularization parameter $\varrho$ to the spatial finite element mesh size $h_x$. Using a space-time tensor product mesh, error estimates for the distance of the computable state to the desired target are derived. The main advantage of this new approach is, that applying sparse factorization techniques, a solver of optimal, i.e., almost linear, complexity is proposed and analyzed. The theoretical results are complemented by numerical examples, including discontinuous and less regular targets. Moreover, this approach can be applied also to optimal control problems subject to non-linear state equations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c19489ba0d276488c896ca9c62c5fe4a573774bc" target='_blank'>
              Optimal complexity solution of space-time finite element systems for state-based parabolic distributed optimal control problems
              </a>
            </td>
          <td>
            Richard Löscher, Michael Reichelt, Olaf Steinbach
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4302593601e57a1c4cb487d871723b9d11a8e2b2" target='_blank'>
              PID Control-Based Self-Healing to Improve the Robustness of Large Language Models
              </a>
            </td>
          <td>
            Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="In this paper we consider adaptive deep neural network approximation for stochastic dynamical systems. Based on the Liouville equation associated with the stochastic dynamical systems, a new temporal KRnet (tKRnet) is proposed to approximate the probability density functions (PDFs) of the state variables. The tKRnet gives an explicit density model for the solution of the Liouville equation, which alleviates the curse of dimensionality issue that limits the application of traditional grid based numerical methods. To efficiently train the tKRnet, an adaptive procedure is developed to generate collocation points for the corresponding residual loss function, where samples are generated iteratively using the approximate density function at each iteration. A temporal decomposition technique is also employed to improve the long-time integration. Theoretical analysis of our proposed method is provided, and numerical examples are presented to demonstrate its performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/827723bda19209701daa5c4d36f6625034285087" target='_blank'>
              Adaptive deep density approximation for stochastic dynamical systems
              </a>
            </td>
          <td>
            Junjie He, Qifeng Liao, Xiaoliang Wan
          </td>
          <td>2024-05-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Neural networks are playing a crucial role in everyday life, with the most modern generative models able to achieve impressive results. Nonetheless, their functioning is still not very clear, and several strategies have been adopted to study how and why these model reach their outputs. A common approach is to consider the data in an Euclidean settings: recent years has witnessed instead a shift from this paradigm, moving thus to more general framework, namely Riemannian Geometry. Two recent works introduced a geometric framework to study neural networks making use of singular Riemannian metrics. In this paper we extend these results to convolutional, residual and recursive neural networks, studying also the case of non-differentiable activation functions, such as ReLU. We illustrate our findings with some numerical experiments on classification of images and thermodynamic problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/880758e6bd878b8074fac2a2b2c0868c61d52a64" target='_blank'>
              A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes
              </a>
            </td>
          <td>
            A. Benfenati, A. Marta
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Separating relevant and irrelevant information is key to any modeling process or scientific inquiry. Theoretical physics offers a powerful tool for achieving this in the form of the renormalization group (RG). Here we demonstrate a practical approach to performing Wilsonian RG in the context of Gaussian Process (GP) Regression. We systematically integrate out the unlearnable modes of the GP kernel, thereby obtaining an RG flow of the Gaussian Process in which the data plays the role of the energy scale. In simple cases, this results in a universal flow of the ridge parameter, which becomes input-dependent in the richer scenario in which non-Gaussianities are included. In addition to being analytically tractable, this approach goes beyond structural analogies between RG and neural networks by providing a natural connection between RG flow and learnable vs. unlearnable modes. Studying such flows may improve our understanding of feature learning in deep neural networks, and identify potential universality classes in these models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ffb0c50aedbc2b6f786ab5dbdc2e8f25674ee4d" target='_blank'>
              Wilsonian Renormalization of Neural Network Gaussian Processes
              </a>
            </td>
          <td>
            Jessica N. Howard, Ro Jefferson, Anindita Maiti, Z. Ringel
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods. We find that selecting improved optimization algorithms significantly enhances the accuracy of the results. Simple modifications to the loss function may also improve precision, offering an additional avenue for enhancement. Despite optimization algorithms having a greater impact on convergence than adjustments to the loss function, practical considerations often favor tweaking the latter due to ease of implementation. On a global scale, the integration of an enhanced optimizer and a marginally adjusted loss function enables a reduction in the loss function by several orders of magnitude across diverse physical problems. Consequently, our results obtained using compact networks (typically comprising 2 or 3 layers of 20-30 neurons) achieve accuracies comparable to finite difference schemes employing thousands of grid points. This study encourages the continued advancement of PINNs and associated optimization techniques for broader applications across various fields.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c2f96d96141117af42c065f364f2403392709a2" target='_blank'>
              Unveiling the optimization process of Physics Informed Neural Networks: How accurate and competitive can PINNs be?
              </a>
            </td>
          <td>
            Jorge F. Urb'an, P. Stefanou, Jos'e A. Pons
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Recent work of three of the authors showed that the operator which maps the local density of states of a one-dimensional untwisted bilayer material to the local density of states of the same bilayer material at non-zero twist, known as the twist operator, can be learned by a neural network. In this work, we first provide a mathematical formulation of that work, making the relevant models and operator learning problem precise. We then prove that the operator learning problem is well-posed for a family of one-dimensional models. To do this, we first prove existence and regularity of the twist operator by solving an inverse problem. We then invoke the universal approximation theorem for operators to prove existence of a neural network capable of approximating the twist operator.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3bbb1080fad379bbbbc323cc53047f32ed30dc41" target='_blank'>
              Learning the local density of states of a bilayer moir\'e material in one dimension
              </a>
            </td>
          <td>
            Diyi Liu, Alexander B. Watson, Stephen Carr, Mitchell Luskin
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The scientific machine learning (SciML) field has introduced a new class of models called physics-informed neural networks (PINNs). These models incorporate domain-specific knowledge as soft constraints on a loss function and use machine learning techniques to train the model. Although PINN models have shown promising results for simple problems, they are prone to failure when moderate level of complexities are added to the problems. We demonstrate that the existing baseline models, in particular PINN and evolutionary sampling (Evo), are unable to capture the solution to differential equations with convection, reaction, and diffusion operators when the imposed initial condition is non-trivial. We then propose a promising solution to address these types of failure modes. This approach involves coupling Curriculum learning with the baseline models, where the network first trains on PDEs with simple initial conditions and is progressively exposed to more complex initial conditions. Our results show that we can reduce the error by 1 – 2 orders of magnitude with our proposed method compared to regular PINN and Evo.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe791350fe483fbc36ae375ebc28ffbc5128c20e" target='_blank'>
              Sequencing Initial Conditions in Physics-Informed Neural Networks
              </a>
            </td>
          <td>
            S. Hooshyar, Arash Elahi
          </td>
          <td>2024-03-26</td>
          <td>Journal of Chemistry and Environment</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2ebcd3b939bc9932486a07358e9a732083b19f0" target='_blank'>
              Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes
              </a>
            </td>
          <td>
            Paul G. Beckman, Christopher J. Geoga
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc227c83d593a317b47926de7d4a6905d2fc78a4" target='_blank'>
              Learning Deep Dynamical Systems using Stable Neural ODEs
              </a>
            </td>
          <td>
            Andreas Sochopoulos, M. Gienger, S. Vijayakumar
          </td>
          <td>2024-04-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>46</td>
        </tr>

        <tr id="Besides classical feed-forward neural networks, also neural ordinary differential equations (neural ODEs) gained particular interest in recent years. Neural ODEs can be interpreted as an infinite depth limit of feed-forward or residual neural networks. We study the input-output dynamics of finite and infinite depth neural networks with scalar output. In the finite depth case, the input is a state associated to a finite number of nodes, which maps under multiple non-linear transformations to the state of one output node. In analogy, a neural ODE maps a linear transformation of the input to a linear transformation of its time-$T$ map. We show that depending on the specific structure of the network, the input-output map has different properties regarding the existence and regularity of critical points. These properties can be characterized via Morse functions, which are scalar functions, where every critical point is non-degenerate. We prove that critical points cannot exist, if the dimension of the hidden layer is monotonically decreasing or the dimension of the phase space is smaller or equal to the input dimension. In the case that critical points exist, we classify their regularity depending on the specific architecture of the network. We show that each critical point is non-degenerate, if for finite depth neural networks the underlying graph has no bottleneck, and if for neural ODEs, the linear transformations used have full rank. For each type of architecture, the proven properties are comparable in the finite and in the infinite depth case. The established theorems allow us to formulate results on universal embedding, i.e.\ on the exact representation of maps by neural networks and neural ODEs. Our dynamical systems viewpoint on the geometric structure of the input-output map provides a fundamental understanding, why certain architectures perform better than others.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/94c846422a6136b9971fe1195656c66dd1af58fe" target='_blank'>
              Analysis of the Geometric Structure of Neural Networks and Neural ODEs via Morse Functions
              </a>
            </td>
          <td>
            Christian Kuehn, Sara-Viola Kuntz
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Any continuous curve in a higher dimensional space can be considered a trajectory that can be parameterized by a single variable, usually taken as time. It is well known that a continuous curve can have a fractional dimensionality, which can be estimated using already standard algorithms. However, characterizing a trajectory from an entropic perspective is far less developed. The search for such characterization leads us to use chain coding to discretize the description of a curve. Calculating the entropy density and entropy-related magnitudes from the resulting finite alphabet code becomes straightforward. In such a way, the entropy of a trajectory can be defined and used as an effective tool to assert creativity and pattern formation from a Shannon perspective. Applying the procedure to actual experimental physiological data and modelled trajectories of astronomical dynamics proved the robustness of the entropic characterization in a wealth of trajectories of different origins and the insight that can be gained from its use.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5463e14a541d4fcb1e7b407da1da64fbc2fd9346" target='_blank'>
              Trajectory analysis through entropy characterization over coded representation
              </a>
            </td>
          <td>
            Roxana Pena-Mendieta, Ania Mesa-Rodr'iguez, Ernesto Estevez-Rams, D. Estevez-Moya, D. Kunka
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/14fdab35cc6288083a38a92392af3f1da0b95a90" target='_blank'>
              KAN: Kolmogorov-Arnold Networks
              </a>
            </td>
          <td>
            Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljavci'c, Thomas Y. Hou, Max Tegmark
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>9</td>
          <td>1</td>
        </tr>

        <tr id="Hagedorn functions are carefully constructed generalizations of Hermite functions to the setting of many-dimensional squeezed and coupled harmonic systems. Wavepackets formed by superpositions of Hagedorn functions have been successfully used to solve the time-dependent Schr\"{o}dinger equation exactly in harmonic systems and variationally in anharmonic systems. For evaluating typical observables, such as position or kinetic energy, it is sufficient to consider orthonormal Hagedorn functions with a single Gaussian center. Here, we instead derive various relations between Hagedorn bases associated with different Gaussians, including their overlaps, which are necessary for evaluating quantities nonlocal in time, such as time correlation functions needed for computing spectra. First, we use the Bogoliubov transformation to obtain commutation relations between the ladder operators associated with different Gaussians. Then, instead of using numerical quadrature, we employ these commutation relations to derive exact recurrence relations for the overlap integrals between Hagedorn functions with different Gaussian centers. Finally, we present numerical experiments that demonstrate the accuracy and efficiency of our algebraic method as well as its suitability to treat problems in spectroscopy and chemical dynamics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/683aa5d6364befede3a234d7e93ba2d9d9d6da46" target='_blank'>
              On Hagedorn wavepackets associated with different Gaussians
              </a>
            </td>
          <td>
            Jivr'i J. L. Van'ivcek, Zhan Tong Zhang
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>3</td>
        </tr>

        <tr id="This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0a49a190498e9f592e696b72203f0810466fe6e3" target='_blank'>
              Manifold-Guided Lyapunov Control with Diffusion Models
              </a>
            </td>
          <td>
            Amartya Mukherjee, Thanin Quartz, Jun Liu
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems. Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role. Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods. However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms. We propose a general framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices. Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability. Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms. The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter. Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a17bd07c7d0086b89607f16256d71eb9567787cf" target='_blank'>
              A general error analysis for randomized low-rank approximation with application to data assimilation
              </a>
            </td>
          <td>
            Alexandre Scotto Di Perrotolo, Youssef Diouane, S. Gurol, Xavier Vasseur
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Physics-guided neural networks (PGNN) is an effective tool that combines the benefits of data-driven modeling with the interpretability and generalization of underlying physical information. However, for a classical PGNN, the penalization of the physics-guided part is at the output level, which leads to a conservative result as systems with highly similar state-transition functions, i.e. only slight differences in parameters, can have significantly different time-series outputs. Furthermore, the classical PGNN cost function regularizes the model estimate over the entire state space with a constant trade-off hyperparameter. In this paper, we introduce a novel model augmentation strategy for nonlinear state-space model identification based on PGNN, using a weighted function regularization (W-PGNN). The proposed approach can efficiently augment the prior physics-based state-space models based on measurement data. A new weighted regularization term is added to the cost function to penalize the difference between the state and output function of the baseline physics-based and final identified model. This ensures the estimated model follows the baseline physics model functions in regions where the data has low information content, while placing greater trust in the data when a high informativity is present. The effectiveness of the proposed strategy over the current PGNN method is demonstrated on a benchmark example.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9732330db3e9f3fa89caefb8ac538d9f0a8807e6" target='_blank'>
              Physics-Guided State-Space Model Augmentation Using Weighted Regularized Neural Networks
              </a>
            </td>
          <td>
            Yuhan Liu, Roland T'oth, M. Schoukens
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Data-driven predictive control (DPC) has recently gained popularity as an alternative to model predictive control (MPC). Amidst the surge in proposed DPC frameworks, upon closer inspection, many of these frameworks are more closely related (or perhaps even equivalent) to each other than it may first appear. We argue for a more formal characterization of these relationships so that results can be freely transferred from one framework to another, rather than being uniquely attributed to a particular framework. We demonstrate this idea by examining the connection between $\gamma$-DDPC and the original DeePC formulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0746be015cf60990a7af8b7daa792611fc9aef35" target='_blank'>
              Towards a unifying framework for data-driven predictive control with quadratic regularization
              </a>
            </td>
          <td>
            Manuel Klädtke, M. S. Darup
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as different extremal cases of this general setup: dynamic programming, Monte Carlo methods, temporal difference learning, and deep RL. We see this as strong evidence that this approach is a natural one and believe that it will be a fruitful way to think about RL in the future.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b43e486e12f2687a86042a0bc22e1ba9a31dce5c" target='_blank'>
              Reinforcement Learning in Categorical Cybernetics
              </a>
            </td>
          <td>
            Jules Hedges, Riu Rodr'iguez Sakamoto
          </td>
          <td>2024-04-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The parametrized post-Einsteinian (ppE) framework and its variants are widely used to probe gravity through gravitational-wave tests that apply to a large class of theories beyond general relativity. However, the ppE framework is not truly theory-agnostic as it only captures certain types of deviations from general relativity: those that admit a post-Newtonian series representation in the inspiral of coalescencing compact objects. Moreover, each type of deviation in the ppE framework has to be tested separately, making the whole process computationally inefficient and expensive, possibly obscuring the theoretical interpretation of potential deviations that could be detected in the future. We here present the neural post-Einsteinian (npE) framework, an extension of the ppE formalism that overcomes the above weaknesses using deep-learning neural networks. The core of the npE framework is a variantional autoencoder that maps the discrete ppE theories into a continuous latent space in a well-organized manner. This design enables the npE framework to test many theories simultaneously and to select the theory that best describes the observation in a single parameter estimation run. The smooth extension of the ppE parametrization also allows for more general types of deviations to be searched for with the npE model. We showcase the application of the new npE framework to future tests of general relativity with the fifth observing run of the LIGO-Virgo-KAGRA collaboration. In particular, the npE framework is demonstrated to efficiently explore modifications to general relativity beyond what can be mapped by the ppE framework, including modifications coming from higher-order curvature corrections to the Einstein-Hilbert action at high post-Newtonian order, and dark-photon interactions in possibly hidden sectors of matter that do not admit a post-Newtonian representation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7d92e4cabbf132a07583b8404fbe0718f4c87c93" target='_blank'>
              Neural Post-Einsteinian Framework for Efficient Theory-Agnostic Tests of General Relativity with Gravitational Waves
              </a>
            </td>
          <td>
            Yiqi Xie, Deep Chatterjee, Gautham Narayan, Nicol'as Yunes
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Model-based reinforcement learning is an effective approach for controlling an unknown system. It is based on a longstanding pipeline familiar to the control community in which one performs experiments on the environment to collect a dataset, uses the resulting dataset to identify a model of the system, and finally performs control synthesis using the identified model. As interacting with the system may be costly and time consuming, targeted exploration is crucial for developing an effective control-oriented model with minimal experimentation. Motivated by this challenge, recent work has begun to study finite sample data requirements and sample efficient algorithms for the problem of optimal exploration in model-based reinforcement learning. However, existing theory and algorithms are limited to model classes which are linear in the parameters. Our work instead focuses on models with nonlinear parameter dependencies, and presents the first finite sample analysis of an active learning algorithm suitable for a general class of nonlinear dynamics. In certain settings, the excess control cost of our algorithm achieves the optimal rate, up to logarithmic factors. We validate our approach in simulation, showcasing the advantage of active, control-oriented exploration for controlling nonlinear systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7cf97c150f8a0cc3bcad648019e5d858ef78b7ef" target='_blank'>
              Active Learning for Control-Oriented Identification of Nonlinear Systems
              </a>
            </td>
          <td>
            Bruce D. Lee, Ingvar M. Ziemann, George J. Pappas, Nikolai Matni
          </td>
          <td>2024-04-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="In this study, we are concerned with nonlinear model predictive control (NMPC) schemes that, through the linear parameter-varying (LPV) formulation, nonlinear systems can be embedded and with a sequential quadratic program (SQP) can provide efficient solutions for the NMPC. We revisit the different constrained optimization formulations known as simultaneous and sequential approaches tailored with the LPV predictor, constituting, in general, a nonlinear program (NLP). The derived NLPs are represented through the Lagrangian formulation, which enforces the Karush-Kuhn-Tucker (KKT) optimality conditions for the optimization problem to be solvable. The main novelty suggests that the problem can still be efficiently solvable with a significantly lower computational load by approximating certain terms to reduce the computational burden. The proposed method is compared with other state-of-the-art approaches on standard performance measures. Moreover, we provide convergence analysis that can assert further theoretical guarantees in control, such as stability and recursive feasibility. Finally, the method is tested through well-studied control benchmarks such as the forced Van der Pol oscillator and the dynamic unicycle.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c558f1a25a48601a2e65ab8e37a9dc2be9c04c35" target='_blank'>
              Efficient Nonlinear Model Predictive Control by Leveraging Linear Parameter-Varying Embedding and Sequential Quadratic Programming
              </a>
            </td>
          <td>
            Dimitrios S. Karachalios, Hossameldin S. Abbas
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Dynamics of many complex systems, from weather and climate to spread of infectious diseases, can be described by partial differential equations (PDEs). Such PDEs involve unknown function(s), partial derivatives, and typically multiple independent variables. The traditional numerical methods for solving PDEs assume that the data are observed on a regular grid. However, in many applications, for example, weather and air pollution monitoring delivered by the arbitrary located weather stations of the National Weather Services, data records are irregularly spaced. Furthermore, in problems involving prediction analytics such as forecasting wildfire smoke plumes, the primary focus may be on a set of irregular locations associated with urban development. In recent years, deep learning (DL) methods and, in particular, graph neural networks (GNNs) have emerged as a new promising tool that can complement traditional PDE solvers in scenarios of the irregular spaced data, contributing to the newest research trend of physics informed machine learning (PIML). However, most existing PIML methods tend to be limited in their ability to describe higher dimensional structural properties exhibited by real world phenomena, especially, ones that live on manifolds. To address this fundamental challenge, we bring the elements of the Hodge theory and, in particular, simplicial convolution defined on the Hodge Laplacian to the emerging nexus of DL and PDEs. In contrast to conventional Laplacian and the associated convolution operation, the simplicial convolution allows us to rigorously describe diffusion across higher order structures and to better approximate the complex underlying topology and geometry of the data. The new approach, Simplicial Neural Networks for Partial Differential Equations (SNN PDE) offers a computationally efficient yet effective solution for time dependent PDEs. Our studies of a broad range of synthetic data and wildfire processes demonstrate that SNN PDE improves upon state of the art baselines in handling unstructured grids and irregular time intervals of complex physical systems and offers competitive forecasting capabilities for weather and air quality forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b2928c9c56daf45d01b8df307cb7baf81ffbc7bc" target='_blank'>
              SNN-PDE: Learning Dynamic PDEs from Data with Simplicial Neural Networks
              </a>
            </td>
          <td>
            J. Choi, Yuzhou Chen, Huikyo Lee, Hyun Kim, Yulia R. Gel
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Widespread deployment of societal-scale machine learning systems necessitates a thorough understanding of the resulting long-term effects these systems have on their environment, including loss of trustworthiness, bias amplification, and violation of AI safety requirements. We introduce a repeated learning process to jointly describe several phenomena attributed to unintended hidden feedback loops, such as error amplification, induced concept drift, echo chambers and others. The process comprises the entire cycle of obtaining the data, training the predictive model, and delivering predictions to end-users within a single mathematical model. A distinctive feature of such repeated learning setting is that the state of the environment becomes causally dependent on the learner itself over time, thus violating the usual assumptions about the data distribution. We present a novel dynamical systems model of the repeated learning process and prove the limiting set of probability distributions for positive and negative feedback loop modes of the system operation. We conduct a series of computational experiments using an exemplary supervised learning problem on two synthetic data sets. The results of the experiments correspond to the theoretical predictions derived from the dynamical model. Our results demonstrate the feasibility of the proposed approach for studying the repeated learning processes in machine learning systems and open a range of opportunities for further research in the area.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bbdf526001c151c67507de963a0c683064b1f630" target='_blank'>
              A Mathematical Model of the Hidden Feedback Loop Effect in Machine Learning Systems
              </a>
            </td>
          <td>
            Andrey Veprikov, Alexander Afanasiev, Anton Khritankov
          </td>
          <td>2024-05-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/081eb8781725e560f597b01c624fe65618c3c0f8" target='_blank'>
              Kolmogorov-Arnold Networks (KANs) for Time Series Analysis
              </a>
            </td>
          <td>
            Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This paper details how the Bayesian-network structure of the posterior distribution of state-space models can be exploited to build improved parameterizations for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution-smoother; each resulting in a different parameter estimation method. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization methods, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using and embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and be work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b901b2ba77f44c7b2e434be5f4559d0d932e3815" target='_blank'>
              Bayesian Networks for Variational System Identification
              </a>
            </td>
          <td>
            Dimas Abreu Archanjo Dutra
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modern space missions with uncrewed spacecraft require robust trajectory design to connect multiple chaotic orbits by small controls. To address this issue, we propose a novel control scheme to design robust trajectories by leveraging a geometrical structure in chaotic zones, known as {\it lobe}. Our scheme shows that appropriately selected lobes reveal possible paths to traverse chaotic zones in a short time. The effectiveness of our method is demonstrated through trajectory design in both the standard map and Hill's equation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/142a444ea4b3232b341379dc803a8b43d92cd2f4" target='_blank'>
              Designing robust trajectories by lobe dynamics in low-dimensional Hamiltonian systems
              </a>
            </td>
          <td>
            Naoki Hiraiwa, Mai Bando, Isaia Nisoli, Yuzuru Sato
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Finding low-dimensional interpretable models of complex physical fields such as turbulence remains an open question, 80 years after the pioneer work of Kolmogorov. Estimating high-dimensional probability distributions from data samples suffers from an optimization and an approximation curse of dimensionality. It may be avoided by following a hierarchic probability flow from coarse to fine scales. This inverse renormalization group is defined by conditional probabilities across scales, renormalized in a wavelet basis. For a $\varphi^4$ scalar potential, sampling these hierarchic models avoids the critical slowing down at the phase transition. An outstanding issue is to also approximate non-Gaussian fields having long-range interactions in space and across scales. We introduce low-dimensional models with robust multiscale approximations of high order polynomial energies. They are calculated with a second wavelet transform, which defines interactions over two hierarchies of scales. We estimate and sample these wavelet scattering models to generate 2D vorticity fields of turbulence, and images of dark matter densities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d65e2612cc3968688394a7660934b8780d0f7e26" target='_blank'>
              Hierarchic Flows to Estimate and Sample High-dimensional Probabilities
              </a>
            </td>
          <td>
            Etienne Lempereur, Stéphane Mallat
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We solve high-dimensional steady-state Fokker-Planck equations on the whole space by applying tensor neural networks. The tensor networks are a tensor product of one-dimensional feedforward networks or a linear combination of several selected radial basis functions. The use of tensor feedforward networks allows us to efficiently exploit auto-differentiation in major Python packages while using radial basis functions can fully avoid auto-differentiation, which is rather expensive in high dimensions. We then use the physics-informed neural networks and stochastic gradient descent methods to learn the tensor networks. One essential step is to determine a proper truncated bounded domain or numerical support for the Fokker-Planck equation. To better train the tensor radial basis function networks, we impose some constraints on parameters, which lead to relatively high accuracy. We demonstrate numerically that the tensor neural networks in physics-informed machine learning are efficient for steady-state Fokker-Planck equations from two to ten dimensions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/669fdb54ae870354c2070ef0ca8c6b182864e6a3" target='_blank'>
              Tensor neural networks for high-dimensional Fokker-Planck equations
              </a>
            </td>
          <td>
            Taorui Wang, Zheyuan Hu, Kenji Kawaguchi, Zhongqiang Zhang, G. Karniadakis
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>125</td>
        </tr>

        <tr id="The ability to automatically discover interpretable mathematical models from data could forever change how we model soft matter systems. For convex discovery problems with a unique global minimum, model discovery is well-established. It uses a classical top-down approach that first calculates a dense parameter vector, and then sparsifies the vector by gradually removing terms. For non-convex discovery problems with multiple local minima, this strategy is infeasible since the initial parameter vector is generally non-unique. Here we propose a novel bottom-up approach that starts with a sparse single-term vector, and then densifies the vector by systematically adding terms. Along the way, we discover models of gradually increasing complexity, a strategy that we call best-in-class modeling. To identify successful candidate terms, we reverse-engineer a library of sixteen functional building blocks that integrate a century of knowledge in material modeling with recent trends in machine learning and artificial intelligence. Yet, instead of solving the discrete combinatorial problem with 65,536 possible combinations of terms, best-in-class modeling starts with the best one-term model and iteratively repeats adding terms, until the objective function meets a user-defined convergence criterion. Strikingly, we achieve good convergence with only one or two terms. We illustrate the best-in-class one- and two-term models for a variety of soft matter systems including rubber, brain, artificial meat, skin, and arteries. Our discovered models display distinct and unexpected features for each family of materials, and suggest that best-in-class modeling is an efficient, robust, and easy-to-use strategy to discover the mechanical signatures of traditional and unconventional soft materials. We anticipate that our technology will generalize naturally to other classes of natural and man-made soft matter.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/200655dc975e870e69368b9f30fdb78bdcd351ea" target='_blank'>
              Best-in-class modeling: A novel strategy to discover constitutive models for soft matter systems
              </a>
            </td>
          <td>
            K. Linka, E. Kuhl
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="Generative modeling aims at producing new datapoints whose statistical properties resemble the ones in a training dataset. In recent years, there has been a burst of machine learning techniques and settings that can achieve this goal with remarkable performances. In most of these settings, one uses the training dataset in conjunction with noise, which is added as a source of statistical variability and is essential for the generative task. Here, we explore the idea of using internal chaotic dynamics in high-dimensional chaotic systems as a way to generate new datapoints from a training dataset. We show that simple learning rules can achieve this goal within a set of vanilla architectures and characterize the quality of the generated datapoints through standard accuracy measures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5a17d09d7112969a8c4684adf34d039b23569e5d" target='_blank'>
              Generative modeling through internal high-dimensional chaotic activity
              </a>
            </td>
          <td>
            Samantha J. Fournier, Pierfrancesco Urbani
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>25</td>
        </tr>

        <tr id="We establish that stabilization of a class of linear, hyperbolic partial differential equations (PDEs) with a large (nevertheless finite) number of components, can be achieved via employment of a backstepping-based control law, which is constructed for stabilization of a continuum version (i.e., as the number of components tends to infinity) of the PDE system. This is achieved by proving that the exact backstepping kernels, constructed for stabilization of the large-scale system, can be approximated (in certain sense such that exponential stability is preserved) by the backstepping kernels constructed for stabilization of a continuum version (essentially an infinite ensemble) of the original PDE system. The proof relies on construction of a convergent sequence of backstepping kernels that is defined such that each kernel matches the exact backstepping kernels (derived based on the original, large-scale system), in a piecewise constant manner with respect to an ensemble variable; while showing that they satisfy the continuum backstepping kernel equations. We present a numerical example that reveals that complexity of computation of stabilizing backstepping kernels may not scale with the number of components of the PDE state, when the kernels are constructed on the basis of the continuum version, in contrast to the case in which they are constructed on the basis of the original, large-scale system. In addition, we formally establish the connection between the solutions to the large-scale system and its continuum counterpart. Thus, this approach can be useful for design of computationally tractable, stabilizing backstepping-based control laws for large-scale PDE systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b82c2d533a88a32e01f9080c356d508cb218c5d1" target='_blank'>
              Stabilization of a Class of Large-Scale Systems of Linear Hyperbolic PDEs via Continuum Approximation of Exact Backstepping Kernels
              </a>
            </td>
          <td>
            Jukka-Pekka Humaloja, N. Bekiaris-Liberis
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>30</td>
        </tr>

        <tr id="While effective in practice, iterative methods for solving large systems of linear equations can be significantly affected by problem-dependent condition number quantities. This makes characterizing their time complexity challenging, particularly when we wish to make comparisons between deterministic and stochastic methods, that may or may not rely on preconditioning and/or fast matrix multiplication. In this work, we consider a fine-grained notion of complexity for iterative linear solvers which we call the spectral tail condition number, $\kappa_\ell$, defined as the ratio between the $\ell$th largest and the smallest singular value of the matrix representing the system. Concretely, we prove the following main algorithmic result: Given an $n\times n$ matrix $A$ and a vector $b$, we can find $\tilde{x}$ such that $\|A\tilde{x}-b\|\leq\epsilon\|b\|$ in time $\tilde{O}(\kappa_\ell\cdot n^2\log 1/\epsilon)$ for any $\ell = O(n^{\frac1{\omega-1}})=O(n^{0.729})$, where $\omega \approx 2.372$ is the current fast matrix multiplication exponent. This guarantee is achieved by Sketch-and-Project with Nesterov's acceleration. Some of the implications of our result, and of the use of $\kappa_\ell$, include direct improvement over a fine-grained analysis of the Conjugate Gradient method, suggesting a stronger separation between deterministic and stochastic iterative solvers; and relating the complexity of iterative solvers to the ongoing algorithmic advances in fast matrix multiplication, since the bound on $\ell$ improves with $\omega$. Our main technical contributions are new sharp characterizations for the first and second moments of the random projection matrix that commonly arises in sketching algorithms, building on a combination of techniques from combinatorial sampling via determinantal point processes and Gaussian universality results from random matrix theory.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/234eca2418b8854174a2a3b605c383088fd36dea" target='_blank'>
              Fine-grained Analysis and Faster Algorithms for Iteratively Solving Linear Systems
              </a>
            </td>
          <td>
            Michal Derezi'nski, Daniel LeJeune, Deanna Needell, E. Rebrova
          </td>
          <td>2024-05-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Many supervised learning tasks have intrinsic symmetries, such as translational and rotational symmetry in image classifications. These symmetries can be exploited to enhance performance. We formulate the symmetry constraints into a concise mathematical form. We design two ways to adopt the constraints into the cost function, thereby shaping the cost landscape in favour of parameter choices which respect the given symmetry. Unlike methods that alter the neural network circuit ansatz to impose symmetry, our method only changes the classical post-processing of gradient descent, which is simpler to implement. We call the method symmetry-guided gradient descent (SGGD). We illustrate SGGD in entanglement classification of Werner states and in a binary classification task in a 2-D feature space. In both cases, the results show that SGGD can accelerate the training, improve the generalization ability, and remove vanishing gradients, especially when the training data is biased.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e7d7dd7ced77cb9c35f85cea021da793348da645" target='_blank'>
              Symmetry-guided gradient descent for quantum neural networks
              </a>
            </td>
          <td>
            Ka Bian, Shitao Zhang, Fei Meng, Wen Zhang, Oscar Dahlsten
          </td>
          <td>2024-04-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In recent years, machine learning algorithms, especially deep learning, have shown promising prospects in solving Partial Differential Equations (PDEs). However, as the dimension increases, the relationship and interaction between variables become more complex, and existing methods are difficult to provide fast and interpretable solutions for high-dimensional PDEs. To address this issue, we propose a genetic programming symbolic regression algorithm based on transfer learning and automatic differentiation to solve PDEs. This method uses genetic programming to search for a mathematically understandable expression and combines automatic differentiation to determine whether the search result satisfies the PDE and boundary conditions to be solved. To overcome the problem of slow solution speed caused by large search space, we propose a transfer learning mechanism that transfers the structure of one-dimensional PDE analytical solution to the form of high-dimensional PDE solution. We tested three representative types of PDEs, and the results showed that our proposed method can obtain reliable and human-understandable real solutions or algebraic equivalent solutions of PDEs, and the convergence speed is better than the compared methods. Code of this project is at https://github.com/grassdeerdeer/HD-TLGP.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/90f5a0d85d18647ad1593dea2c51fdadb0af8c62" target='_blank'>
              An Interpretable Approach to the Solutions of High-Dimensional Partial Differential Equations
              </a>
            </td>
          <td>
            Lulu Cao, Yufei Liu, Zhenzhong Wang, Dejun Xu, Kai Ye, Kay Chen Tan, Min Jiang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we introduce a type of tensor neural network based machine learning method to solve elliptic multiscale problems. Based on the special structure, we can do the direct and highly accurate high dimensional integrations for the tensor neural network functions without Monte Carlo process. Here, with the help of homogenization techniques, the multiscale problem is first transformed to the high dimensional limit problem with reasonable accuracy. Then, based on the tensor neural network, we design a type of machine learning method to solve the derived high dimensional limit problem. The proposed method in this paper brings a new way to design numerical methods for computing more general multiscale problems with high accuracy. Several numerical examples are also provided to validate the accuracy of the proposed numerical methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9f593f88fe7f1474c8885c044343e462e518561a" target='_blank'>
              Tensor Neural Network Based Machine Learning Method for Elliptic Multiscale Problems
              </a>
            </td>
          <td>
            Zhongshuo Lin, Haochen Liu, Hehu Xie
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets. These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups. Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory. They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries. In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups). Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning. We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles. The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6140d9bf7e31537e13dcf3cf1b9d37e9b71d1eec" target='_blank'>
              Kuramoto Oscillators and Swarms on Manifolds for Geometry Informed Machine Learning
              </a>
            </td>
          <td>
            Vladimir Jacimovic
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Efficiently modeling spatio-temporal (ST) physical processes and observations presents a challenging problem for the deep learning community. Many recent studies have concentrated on meticulously reconciling various advantages, leading to designed models that are neither simple nor practical. To address this issue, this paper presents a systematic study on existing shortcomings faced by off-the-shelf models, including lack of local fidelity, poor prediction performance over long time-steps, low scalability, and inefficiency. To systematically address the aforementioned problems, we propose an EarthFarseer, a concise framework that combines parallel local convolutions and global Fourier-based transformer architectures, enabling dynamically capture the local-global spatial interactions and dependencies. EarthFarseer also incorporates a multi-scale fully convolutional and Fourier architectures to efficiently and effectively capture the temporal evolution. Our proposal demonstrates strong adaptability across various tasks and datasets, with fast convergence and better local fidelity in long time-steps predictions. Extensive experiments and visualizations over eight human society physical and natural physical datasets demonstrates the state-of-the-art performance of EarthFarseer. We release our code at https://github.com/easylearningscores/EarthFarseer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/64a046a9175276e6860a548f090efde51193e84f" target='_blank'>
              Earthfarsser: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model
              </a>
            </td>
          <td>
            Hao Wu, Yuxuan Liang, Wei Xiong, Zhengyang Zhou, Wei Huang, Shilong Wang, Kun Wang
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>1</td>
          <td>3</td>
        </tr>

        <tr id="Signature kernels are at the core of several machine learning algorithms for analysing multivariate time series. The kernel of two bounded variation paths (such as piecewise linear interpolations of time series data) is typically computed by solving a Goursat problem for a hyperbolic partial differential equation (PDE) in two independent time variables. However, this approach becomes considerably less practical for highly oscillatory input paths, as they have to be resolved at a fine enough scale to accurately recover their signature kernel, resulting in significant time and memory complexities. To mitigate this issue, we first show that the signature kernel of a broader class of paths, known as \emph{smooth rough paths}, also satisfies a PDE, albeit in the form of a system of coupled equations. We then use this result to introduce new algorithms for the numerical approximation of signature kernels. As bounded variation paths (and more generally geometric $p$-rough paths) can be approximated by piecewise smooth rough paths, one can replace the PDE with rapidly varying coefficients in the original Goursat problem by an explicit system of coupled equations with piecewise constant coefficients derived from the first few iterated integrals of the original input paths. While this approach requires solving more equations, they do not require looking back at the complex and fine structure of the initial paths, which significantly reduces the computational complexity associated with the analysis of highly oscillatory time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d466579deec9a59d524ac0981d5812dda5d235c7" target='_blank'>
              A High Order Solver for Signature Kernels
              </a>
            </td>
          <td>
            M. Lemercier, Terry Lyons
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>9</td>
        </tr>

        <tr id="A large part of modern research, especially in the broad field of complex systems, relies on the numerical integration of PDEs, with and without stochastic noise. This is usually done with eiher in- house made codes or external packages like MATLAB, Mathematica, Fenicsx, OpenFOAM, Dedalus, and others. These packages rarely offer a good combination of speed, generality, and the option to easily add stochasticity to the system, while in-house codes depend on certain expertise to obtain good performance, and are usually written for each specific use case, sacrificing modularity and reusability. This paper introduces a package written in CUDA C++, thus enabling by default gpu acceleration, that performs pseudo-spectral integration of generic stochastic PDEs in flat lattices in one, two and three dimensions. This manuscript describes how the basic functionality of cuPSS, with an example and benchmarking, showing that cuPSS offers a considerable improvement in speed over other popular finite-difference and spectral solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fe3d36e5d7d4496386dd2bf557c35ebbea744e9f" target='_blank'>
              cuPSS: a package for pseudo-spectral integration of stochastic PDEs
              </a>
            </td>
          <td>
            Fernando Caballero
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We discuss Hamiltonian and Liouvillian learning for analog quantum simulation from non-equilibrium quench dynamics in the limit of weakly dissipative many-body systems. We present various strategies to learn the operator content of the Hamiltonian and the Lindblad operators of the Liouvillian. We compare different ans\"atze based on an experimentally accessible"learning error"which we consider as a function of the number of runs of the experiment. Initially, the learning error decreasing with the inverse square root of the number of runs, as the error in the reconstructed parameters is dominated by shot noise. Eventually the learning error remains constant, allowing us to recognize missing ansatz terms. A central aspect of our approach is to (re-)parametrize ans\"atze by introducing and varying the dependencies between parameters. This allows us to identify the relevant parameters of the system, thereby reducing the complexity of the learning task. Importantly, this (re-)parametrization relies solely on classical post-processing, which is compelling given the finite amount of data available from experiments. A distinguishing feature of our approach is the possibility to learn the Hamiltonian, without the necessity of learning the complete Liouvillian, thus further reducing the complexity of the learning task. We illustrate our method with two, experimentally relevant, spin models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/690545a3d51683d051059ba60a9d51c6979db019" target='_blank'>
              Hamiltonian and Liouvillian learning in weakly-dissipative quantum many-body systems
              </a>
            </td>
          <td>
            Tobias Olsacher, Tristan Kraft, C. Kokail, Barbara Kraus, Peter Zoller
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="In this article, we consider the correction of metric matrices in quasi-Newton methods (QNM) from the perspective of machine learning theory. Based on training information for estimating the matrix of the second derivatives of a function, we formulate a quality functional and minimize it by using gradient machine learning algorithms. We demonstrate that this approach leads us to the well-known ways of updating metric matrices used in QNM. The learning algorithm for finding metric matrices performs minimization along a system of directions, the orthogonality of which determines the convergence rate of the learning process. The degree of learning vectors’ orthogonality can be increased both by choosing a QNM and by using additional orthogonalization methods. It has been shown theoretically that the orthogonality degree of learning vectors in the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is higher than in the Davidon–Fletcher–Powell (DFP) method, which determines the advantage of the BFGS method. In our paper, we discuss some orthogonalization techniques. One of them is to include iterations with orthogonalization or an exact one-dimensional descent. As a result, it is theoretically possible to detect the cumulative effect of reducing the optimization space on quadratic functions. Another way to increase the orthogonality degree of learning vectors at the initial stages of the QNM is a special choice of initial metric matrices. Our computational experiments on problems with a high degree of conditionality have confirmed the stated theoretical assumptions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d1a6267efb1747cb7eb0d9964e4bb33c18b17735" target='_blank'>
              Machine Learning in Quasi-Newton Methods
              </a>
            </td>
          <td>
            Vladimit N. Krutikov, Elena Tovbis, P. Stanimirović, L. Kazakovtsev, Darjan Karabašević
          </td>
          <td>2024-04-05</td>
          <td>Axioms</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We present an algorithm for constructing efficient surrogate frequency‐domain models of (nonlinear) parametric dynamical systems in a non‐intrusive way. To capture the dependence of the underlying system on frequency and parameters, our proposed approach combines rational approximation and smooth interpolation. In the approximation effort, locally adaptive sparse grids are applied to effectively explore the parameter domain even if the number of parameters is modest or high. Adaptivity is also employed to build rational approximations that efficiently capture the frequency dependence of the problem. These two features enable our method to build surrogate models that achieve a user‐prescribed approximation accuracy, without wasting resources in “oversampling” the frequency and parameter domains. Thanks to its non‐intrusiveness, our proposed method, as opposed to projection‐based techniques for model order reduction, can be applied regardless of the complexity of the underlying physical model. Notably, our algorithm for adaptive sampling can be used even when prior knowledge of the problem structure is not available. To showcase the effectiveness of our approach, we apply it in the study of an aerodynamic bearing. Our method allows us to build surrogate models that adequately identify the bearing's behavior with respect to both design and operational parameters, while still achieving significant speedups.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/071dd64f8fb372d9b51bd3b285c8518554a143a0" target='_blank'>
              Plug‐and‐play adaptive surrogate modeling of parametric nonlinear dynamics in frequency domain
              </a>
            </td>
          <td>
            Phillip Huwiler, Davide Pradovera, Jürg Schiffmann
          </td>
          <td>2024-04-16</td>
          <td>International Journal for Numerical Methods in Engineering</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations. In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time. We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures. The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure. We conduct a numerical analysis of this approximation and investigate optimal parameter choices. Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy. The code and all trained networks are provided on GitHub https://github.com/ScSteffen/neuralEntropyClosures and https://github.com/CSMMLab/KiT-RT.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4c1c4dec5fd037121e304bf387cacc229dc80951" target='_blank'>
              Structure-preserving neural networks for the regularized entropy-based closure of the Boltzmann moment system
              </a>
            </td>
          <td>
            Steffen Schotthofer, M. P. Laiu, Martin Frank, C. Hauck
          </td>
          <td>2024-04-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="The FitzHugh–Nagumo model has been used empirically to model certain types of neuronal activities. It is also a non-linear dynamical system applicable to chemical kinetics, population dynamics, epidemiology and pattern formation. In the literature, many approaches have been proposed to study its dynamics. In this paper, initially, we have employed cutting-edge tools from discrete dynamics for discretization and fixed points. It has been proven that an exact discrete scheme exists for this paradigm. This project also considers the phase space and integral surfaces of these evolutionary equations. In addition, it carries out a thorough symmetry analysis of this reaction diffusion system to find equivalent systems. Moreover, steady-state solutions are obtained using ansatzes for traveling wave solutions. The existence of infinite traveling wave solutions has also been proven. Yet again, this investigation establishes the potential of symmetry methods to unravel non-linearity. Finally, singular perturbation theory has been employed to obtain analytical approximations and to study stability in different parameter regimes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecb93780bd3dfbba9d6d7759840de5e98c836b6f" target='_blank'>
              Nonstandard Nearly Exact Analysis of the FitzHugh–Nagumo Model
              </a>
            </td>
          <td>
            Sergei D. Odintsov, Marek Berezowski, Mujahid Abbas, Eddy Kwessi
          </td>
          <td>2024-05-09</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This work is on a user-friendly reduced basis method for solving a family of parametric PDEs by preconditioned Krylov subspace methods including the conjugate gradient method, generalized minimum residual method, and bi-conjugate gradient method. The proposed methods use a preconditioned Krylov subspace method for a high-fidelity discretization of one parameter instance to generate orthogonal basis vectors of the reduced basis subspace. Then large-scale discrete parameter-dependent problems are approximately solved in the low-dimensional Krylov subspace. As shown in the theory and experiments, only a small number of Krylov subspace iterations are needed to simultaneously generate approximate solutions of a family of high-fidelity and large-scale systems in the reduced basis subspace. This reduces the computational cost dramatically because (1) to construct the reduced basis vectors, we only solve one large-scale problem in the high-fidelity level; and (2) the family of large-scale problems restricted to the reduced basis subspace have much smaller sizes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5ddf7e05758a09b26c59d056eee71f5ef64f32d2" target='_blank'>
              Reduced Krylov Basis Methods for Parametric Partial Differential Equations
              </a>
            </td>
          <td>
            Yuwen Li, L. Zikatanov, Cheng Zuo
          </td>
          <td>2024-05-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="This paper explores the extension of dimension reduction (DR) techniques to the multi-dimension case by using the Einstein product. Our focus lies on graph-based methods, encompassing both linear and nonlinear approaches, within both supervised and unsupervised learning paradigms. Additionally, we investigate variants such as repulsion graphs and kernel methods for linear approaches. Furthermore, we present two generalizations for each method, based on single or multiple weights. We demonstrate the straightforward nature of these generalizations and provide theoretical insights. Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca293b87e8603a407cbaf291f1ab2232f88e0a64" target='_blank'>
              Higher order multi-dimension reduction methods via Einstein-product
              </a>
            </td>
          <td>
            Alaeddine Zahir, K. Jbilou, A. Ratnani
          </td>
          <td>2024-03-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="This paper explores the efficacy of diffusion-based generative models as neural operators for partial differential equations (PDEs). Neural operators are neural networks that learn a mapping from the parameter space to the solution space of PDEs from data, and they can also solve the inverse problem of estimating the parameter from the solution. Diffusion models excel in many domains, but their potential as neural operators has not been thoroughly explored. In this work, we show that diffusion-based generative models exhibit many properties favourable for neural operators, and they can effectively generate the solution of a PDE conditionally on the parameter or recover the unobserved parts of the system. We propose to train a single model adaptable to multiple tasks, by alternating between the tasks during training. In our experiments with multiple realistic dynamical systems, diffusion models outperform other neural operators. Furthermore, we demonstrate how the probabilistic diffusion model can elegantly deal with systems which are only partially identifiable, by producing samples corresponding to the different possible solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/93f7dd73bdb8cf078d6f19120987ab3c21100bc5" target='_blank'>
              Diffusion models as probabilistic neural operators for recovering unobserved states of dynamical systems
              </a>
            </td>
          <td>
            Katsiaryna Haitsiukevich, O. Poyraz, Pekka Marttinen, Alexander Ilin
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability. We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning. Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability. This allows for a straightforward identification of the sources of power-law scaling in model performance. We compute the generalization error of a broad class of random feature models. We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator. Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates. These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting. We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting. Our results extend and provide a unifying perspective on earlier models of neural scaling laws.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7b56a65ad87336d6e0dd43f5565184fe6b561e26" target='_blank'>
              Scaling and renormalization in high-dimensional regression
              </a>
            </td>
          <td>
            Alexander Atanasov, Jacob A. Zavatone-Veth, C. Pehlevan
          </td>
          <td>2024-05-01</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>24</td>
        </tr>

        <tr id="This paper designs novel nonparametric Bellman mappings in reproducing kernel Hilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings benefit from the rich approximating properties of RKHSs, adopt no assumptions on the statistics of the data owing to their nonparametric nature, require no knowledge on transition probabilities of Markov decision processes, and may operate without any training data. Moreover, they allow for sampling on-the-fly via the design of trajectory samples, re-use past test data via experience replay, effect dimensionality reduction by random Fourier features, and enable computationally lightweight operations to fit into efficient online or time-adaptive learning. The paper offers also a variational framework to design the free parameters of the proposed Bellman mappings, and shows that appropriate choices of those parameters yield several popular Bellman-mapping designs. As an application, the proposed mappings are employed to offer a novel solution to the problem of countering outliers in adaptive filtering. More specifically, with no prior information on the statistics of the outliers and no training data, a policy-iteration algorithm is introduced to select online, per time instance, the ``optimal'' coefficient p in the least-mean-p-power-error method. Numerical tests on synthetic data showcase, in most of the cases, the superior performance of the proposed solution over several RL and non-RL schemes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/07b978cc5ce476017d504ec76c0a67d801d77dd3" target='_blank'>
              Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering
              </a>
            </td>
          <td>
            Yuki Akiyama, Minh Vu, Konstantinos Slavakis
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Approximations in computing model likelihoods with continuous normalizing flows (CNFs) hinder the use of these models for importance sampling of Boltzmann distributions, where exact likelihoods are required. In this work, we present Verlet flows, a class of CNFs on an augmented state-space inspired by symplectic integrators from Hamiltonian dynamics. When used with carefully constructed Taylor-Verlet integrators, Verlet flows provide exact-likelihood generative models which generalize coupled flow architectures from a non-continuous setting while imposing minimal expressivity constraints. On experiments over toy densities, we demonstrate that the variance of the commonly used Hutchinson trace estimator is unsuitable for importance sampling, whereas Verlet flows perform comparably to full autograd trace computations while being significantly faster.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/75ec4939ac09af2623fa3e61fe435cc9cdb348d5" target='_blank'>
              Verlet Flows: Exact-Likelihood Integrators for Flow-Based Generative Models
              </a>
            </td>
          <td>
            Ezra Erives, Bowen Jing, T. Jaakkola
          </td>
          <td>2024-05-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>94</td>
        </tr>

        <tr id="We study the causal dynamics of an embedded null horizon foliated by marginally outer trapped surfaces (MOTS) for a locally rotationally symmetric background spacetime subjected to linear perturbations. We introduce a simple procedure which characterizes the transition of the causal character of the null horizon. We apply our characterization scheme to non-dissipative perturbations of the Schwarzschild and spatially homogeneous backgrounds. For the latter, a linear equation of state was imposed. Assuming a harmonic decomposition of the linearized field equations, we clarify the variables of a formal solution to the linearized system that determine how the null horizon evolves. For both classes of backgrounds, the shear and vorticity 2-vectors are essential to the characterization, and their roles are made precise. Finally, we discuss aspects of the relationship between the characterizing conditions. Various properties related to the self-adjointness of the MOTS stability operator are extensively discussed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc2e047ac4d83c30453050c536d291a8c56488b8" target='_blank'>
              Causal dynamics of null horizons under linear perturbations
              </a>
            </td>
          <td>
            P. Dunsby, Seoktae Koh, A. Sherif
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="Recent studies show that transformer-based architectures emulate gradient descent during a forward pass, contributing to in-context learning capabilities - an ability where the model adapts to new tasks based on a sequence of prompt examples without being explicitly trained or fine tuned to do so. This work investigates the generalization properties of a single step of gradient descent in the context of linear regression with well-specified models. A random design setting is considered and analytical expressions are derived for the statistical properties and bounds of generalization error in a non-asymptotic (finite sample) setting. These expressions are notable for avoiding arbitrary constants, and thus offer robust quantitative information and scaling relationships. These results are contrasted with those from classical least squares regression (for which analogous finite sample bounds are also derived), shedding light on systematic and noise components, as well as optimal step sizes. Additionally, identities involving high-order products of Gaussian random matrices are presented as a byproduct of the analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f4b24b0d9937ff37347f0bd93a295601f16546f2" target='_blank'>
              Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression
              </a>
            </td>
          <td>
            Karthik Duraisamy
          </td>
          <td>2024-05-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theory. Additionally, the results illustrate a strong connection between the learned proximal metrics and active constraints at the optima, leading to an interpretation in which the learning of proximal metrics can be viewed as a form of active set learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ef9e9b5a25df49d547c7d8883144219f5ec46cd7" target='_blank'>
              Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming
              </a>
            </td>
          <td>
            Ethan King, James Kotary, Ferdinando Fioretto, Ján Drgoňa
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22b9ae11510f83836b2e273b94380a5760a72dc2" target='_blank'>
              Tensor Network-Constrained Kernel Machines as Gaussian Processes
              </a>
            </td>
          <td>
            Frederiek Wesel, Kim Batselier
          </td>
          <td>2024-03-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We investigate the learning of implicit neural representation (INR) using an overparameterized multilayer perceptron (MLP) via a novel nonparametric teaching perspective. The latter offers an efficient example selection framework for teaching nonparametrically defined (viz. non-closed-form) target functions, such as image functions defined by 2D grids of pixels. To address the costly training of INRs, we propose a paradigm called Implicit Neural Teaching (INT) that treats INR learning as a nonparametric teaching problem, where the given signal being fitted serves as the target function. The teacher then selects signal fragments for iterative training of the MLP to achieve fast convergence. By establishing a connection between MLP evolution through parameter-based gradient descent and that of function evolution through functional gradient descent in nonparametric teaching, we show for the first time that teaching an overparameterized MLP is consistent with teaching a nonparametric learner. This new discovery readily permits a convenient drop-in of nonparametric teaching algorithms to broadly enhance INR training efficiency, demonstrating 30%+ training time savings across various input modalities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/59fce91bd2c91e64ffd4ab503cf91572a94e7bd0" target='_blank'>
              Nonparametric Teaching of Implicit Neural Representations
              </a>
            </td>
          <td>
            Chen Zhang, , Jason Chun Lok Li, Yik-Chung Wu, Ngai Wong
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="A promising step from linear towards nonlinear data-driven control is via the design of controllers for linear parameter-varying (LPV) systems, which are linear systems whose parameters are varying along a measurable scheduling signal. However, the interplay between uncertainty arising from corrupted data and the parameter-varying nature of these systems impacts the stability analysis, and limits the generalization of well-understood data-driven methods for linear time-invariant systems. In this work, we decouple this interplay using a recently developed variant of the Fundamental Lemma for LPV systems and the viewpoint of data-informativity, in combination with biquadratic Lyapunov forms. Together, these allow us to develop novel linear matrix inequality conditions for the existence of scheduling-dependent Lyapunov functions, incorporating the intrinsic nonlinearity. Appealingly, these results are stated purely in terms of the collected data and bounds on the noise, and they are computationally favorable to check.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b730a69e3aac8462117e84aa38473769811d6c65" target='_blank'>
              Decoupling parameter variation from noise: Biquadratic Lyapunov forms in data-driven LPV control
              </a>
            </td>
          <td>
            C. Verhoek, J. Eising, Florian Dörfler, Roland T'oth
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Solving high-dimensional partial differential equations necessitates methods free of exponential scaling in the dimension of the problem. This work introduces a tensor network approach for the Kolmogorov backward equation via approximating directly the Markov operator. We show that the high-dimensional Markov operator can be obtained under a functional hierarchical tensor (FHT) ansatz with a hierarchical sketching algorithm. When the terminal condition admits an FHT ansatz, the proposed operator outputs an FHT ansatz for the PDE solution through an efficient functional tensor network contraction procedure. In addition, the proposed operator-based approach also provides an efficient way to solve the Kolmogorov forward equation when the initial distribution is in an FHT ansatz. We apply the proposed approach successfully to two challenging time-dependent Ginzburg-Landau models with hundreds of variables.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1cf099a3b1f5822a347ab3adac01ba73485f33e7" target='_blank'>
              Solving high-dimensional Kolmogorov backward equations with functional hierarchical tensor operators
              </a>
            </td>
          <td>
            Xun Tang, Leah Collis, Lexing Ying
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id=". Scattered data interpolation aims to reconstruct a continuous (smooth) function that approximates the underlying function by ﬁtting (meshless) data points. There are extensive applications of scattered data interpolation in computer graphics, ﬂuid dynamics, inverse kinematics, machine learning, etc. In this paper, we consider a novel generalized Mercel kernel in the reproducing kernel Banach space for scattered data interpolation. The system of interpolation equations is formulated as a multilinear sys-tem with a structural tensor, which is an absolutely and uniformly convergent inﬁnite series of symmetric rank-one tensors. Then we design a fast numerical method for computing the product of the structural tensor and any vector in arbitrary precision. Whereafter, a scalable optimization approach equipped with limited-memory BFGS and Wolfe line-search techniques is customized for solving these multilinear systems. Using the Łojasiewicz inequality, we prove that the proposed scalable optimization approach is a globally convergent algorithm and possesses a linear or sublinear convergence rate. Numerical experiments illustrate that the proposed scalable optimization approach can improve the accuracy of interpolation ﬁtting and computational efﬁciency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/440cce0394a5ce313aae5c9e7f60b66052e7b553" target='_blank'>
              A Scalable Optimization Approach for the Multilinear System Arising from Scattered Data Interpolation
              </a>
            </td>
          <td>
            Yannan Chen, Kaidong Fu, Can Li and Qi Ye
          </td>
          <td>2024-05-01</td>
          <td>CSIAM Transactions on Applied Mathematics</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Algorithms developed to solve many-body quantum problems, like tensor networks, can turn into powerful quantum-inspired tools to tackle problems in the classical domain. In this work, we focus on matrix product operators, a prominent numerical technique to study many-body quantum systems, especially in one dimension. It has been previously shown that such a tool can be used for classification, learning of deterministic sequence-to-sequence processes and of generic quantum processes. We further develop a matrix product operator algorithm to learn probabilistic sequence-to-sequence processes and apply this algorithm to probabilistic cellular automata. This new approach can accurately learn probabilistic cellular automata processes in different conditions, even when the process is a probabilistic mixture of different chaotic rules. In addition, we find that the ability to learn these dynamics is a function of the bit-wise difference between the rules and whether one is much more likely than the other.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8110beec24b4ccfaab141e29148faf176cdc0802" target='_blank'>
              Tensor-Networks-based Learning of Probabilistic Cellular Automata Dynamics
              </a>
            </td>
          <td>
            Heitor P. Casagrande, Bo Xing, William J. Munro, Chu Guo, Dario Poletti
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equation, into data-driven neural models for motion prediction in off-road driving. Our experiments showed that PhysORD can accurately predict vehicle motion and tolerate external disturbance by modeling uncertainties. It outperforms existing methods both in accuracy and efficiency and demonstrates data-efficient learning and generalization ability in long-term prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d119b5e0192d10508a2178015de9283c30ffdaf6" target='_blank'>
              PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving
              </a>
            </td>
          <td>
            Zhipeng Zhao, Bowen Li, Yi Du, Taimeng Fu, Chen Wang
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from observed data. In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency (RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. We compute RSE up to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval, bilinear sensing, and matrix completion. In all cases, the RSE quantifies the compatibility between the covariance of the population data and the latent model parameter. Interestingly, we observe a precise reciprocal relationship between RSE and the intrinsic complexity/sensitivity of the problem instance, paralleling the classical Eckart-Young theorem in numerical analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b3915b29a108ab610d7bca598d33dcca619b0283" target='_blank'>
              The radius of statistical efficiency
              </a>
            </td>
          <td>
            Joshua Cutler, M. D'iaz, D. Drusvyatskiy
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="We introduce the concept of imprecise Markov semigroup. It allows us to see Markov chains and processes with imprecise transition probabilities as (a collection of diffusion) operators, and thus to unlock techniques from geometry, functional analysis, and (high dimensional) probability to study their ergodic behavior. We show that, if the initial distribution of an imprecise Markov semigroup is known and invariant, under some conditions that also involve the geometry of the state space, eventually the ambiguity around the transition probability fades. We call this property ergodicity of the imprecise Markov semigroup, and we relate it to the classical (Birkhoff's) notion of ergodicity. We prove ergodicity both when the state space is Euclidean or a Riemannian manifold, and when it is an arbitrary measurable space. The importance of our findings for the fields of machine learning and computer vision is also discussed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/381b5d3cbf4bfb0247a8c0ca47e6714d9c344b32" target='_blank'>
              Imprecise Markov Semigroups and their Ergodicity
              </a>
            </td>
          <td>
            Michele Caprio
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Model Predictive Control (MPC) is a method to control nonlinear systems with guaranteed stability and constraint satisfaction but suffers from high computation times. Approximate MPC (AMPC) with neural networks (NNs) has emerged to address this limitation, enabling deployment on resource-constrained embedded systems. However, when tuning AMPCs for real-world systems, large datasets need to be regenerated and the NN needs to be retrained at every tuning step. This work introduces a novel, parameter-adaptive AMPC architecture capable of online tuning without recomputing large datasets and retraining. By incorporating local sensitivities of nonlinear programs, the proposed method not only mimics optimal MPC inputs but also adjusts to changes in physical parameters of the model using linear predictions while still guaranteeing stability. We showcase the effectiveness of parameter-adaptive AMPC by controlling the swing-ups of two different real cartpole systems with a severely resource-constrained microcontroller (MCU). We use the same NN across both system instances that have different parameters. This work not only represents the first experimental demonstration of AMPC for fast-moving systems on low-cost MCUs to the best of our knowledge, but also showcases generalization across system instances and variations through our parameter-adaptation method. Taken together, these contributions represent a marked step toward the practical application of AMPC in real-world systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ce2086c359b1782c3adbe825ce48d2bd52bc092c" target='_blank'>
              Parameter-Adaptive Approximate MPC: Tuning Neural-Network Controllers without Re-Training
              </a>
            </td>
          <td>
            Henrik Hose, Alexander Gräfe, Sebastian Trimpe
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Stochastic reaction networks are widely used in the modeling of stochastic systems across diverse domains such as biology, chemistry, physics, and ecology. However, the comprehension of the dynamic behaviors inherent in stochastic reaction networks is a formidable undertaking, primarily due to the exponential growth in the number of possible states or trajectories as the state space dimension increases. In this study, we introduce a knowledge distillation method based on reinforcement learning principles, aimed at compressing the dynamical knowledge encoded in stochastic reaction networks into a singular neural network construct. The trained neural network possesses the capability to accurately predict the state conditional joint probability distribution that corresponds to the given query contexts, when prompted with rate parameters, initial conditions, and time values. This obviates the need to track the dynamical process, enabling the direct estimation of normalized state and trajectory probabilities, without necessitating the integration over the complete state space. By applying our method to representative examples, we have observed a high degree of accuracy in both multimodal and high-dimensional systems. Additionally, the trained neural network can serve as a foundational model for developing efficient algorithms for parameter inference and trajectory ensemble generation. These results collectively underscore the efficacy of our approach as a universal means of distilling knowledge from stochastic reaction networks. Importantly, our methodology also spotlights the potential utility in harnessing a singular, pretrained, large-scale model to encapsulate the solution space underpinning a wide spectrum of stochastic dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9ceacaafe31a2cf6dd9a5da06470e4a7369110d8" target='_blank'>
              Distilling dynamical knowledge from stochastic reaction networks.
              </a>
            </td>
          <td>
            Chuanbo Liu, Jin Wang
          </td>
          <td>2024-03-26</td>
          <td>Proceedings of the National Academy of Sciences of the United States of America</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Understanding how neural systems efficiently process information through distributed representations is a fundamental challenge at the interface of neuroscience and machine learning. Recent approaches analyze the statistical and geometrical attributes of neural representations as population-level mechanistic descriptors of task implementation. In particular, manifold capacity has emerged as a promising framework linking population geometry to the separability of neural manifolds. However, this metric has been limited to linear readouts. Here, we propose a theoretical framework that overcomes this limitation by leveraging contextual input information. We derive an exact formula for the context-dependent capacity that depends on manifold geometry and context correlations, and validate it on synthetic and real data. Our framework's increased expressivity captures representation untanglement in deep networks at early stages of the layer hierarchy, previously inaccessible to analysis. As context-dependent nonlinearity is ubiquitous in neural systems, our data-driven and theoretically grounded approach promises to elucidate context-dependent computation across scales, datasets, and models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eeb127e6d37330d3773dc9829d77a67179be68f0" target='_blank'>
              Nonlinear classification of neural manifolds with contextual information
              </a>
            </td>
          <td>
            Francesca Mignacco, Chi-Ning Chou, SueYeon Chung
          </td>
          <td>2024-05-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="We introduce diffusion geometry as a new framework for geometric and topological data analysis. Diffusion geometry uses the Bakry-Emery $\Gamma$-calculus of Markov diffusion operators to define objects from Riemannian geometry on a wide range of probability spaces. We construct statistical estimators for these objects from a sample of data, and so introduce a whole family of new methods for geometric data analysis and computational geometry. This includes vector fields and differential forms on the data, and many of the important operators in exterior calculus. Unlike existing methods like persistent homology and local principal component analysis, diffusion geometry is explicitly related to Riemannian geometry, and is significantly more robust to noise, significantly faster to compute, provides a richer topological description (like the cup product on cohomology), and is naturally vectorised for statistics and machine learning. We find that diffusion geometry outperforms multiparameter persistent homology as a biomarker for real and simulated tumour histology data and can robustly measure the manifold hypothesis by detecting singularities in manifold-like data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b06354ead94e9512914055600e5bf1ff5dd67ba6" target='_blank'>
              Diffusion Geometry
              </a>
            </td>
          <td>
            Iolo Jones
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We present a dynamical system formulation for inhomogeneous LRS-II spacetimes using the covariant 1+1+2 decomposition approach. Our approach describes the LRS-II dynamics from the point of view of a comoving observer. Promoting the covariant radial derivatives of the covariant dynamical quantities to new dynamical variables and utilizing the commutation relation between the covariant temporal and radial derivatives, we were able to construct an autonomous system of first-order ordinary differential equations along with some purely algebraic constraints. Using our dynamical system formulation we found several interesting features in the LRS-II phase space with dust, one of them being that the homogeneous solutions constitute an invariant submanifold. For the particular case of LTB, we were also able to recover the previously known result that an expanding LTB tends to Milne in the absence of a cosmological constant, providing a potential validation of our formalism.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dabebd34feb1ae40af13eec9da4626885c987c2a" target='_blank'>
              A dynamical systems formulation for inhomogeneous LRS-II spacetimes
              </a>
            </td>
          <td>
            Saikat Chakraborty, P. Dunsby, Rituparno Goswami, Amare Abebe
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>26</td>
        </tr>

        <tr id="Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels. In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches. Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible. We apply the method to quantum process tomography and a quantum machine learning problem. Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography. Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5cd381c451dada5cfca98c8d95c1be5abd4d1c7" target='_blank'>
              Regularization of Riemannian optimization: Application to process tomography and quantum machine learning
              </a>
            </td>
          <td>
            Felix Soest, K. Beyer, W. Strunz
          </td>
          <td>2024-04-30</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="This paper deals with a class of neural SDEs and studies the limiting behavior of the associated sampled optimal control problems as the sample size grows to infinity. The neural SDEs with N samples can be linked to the N-particle systems with centralized control. We analyze the Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and establish regularity results which are uniform in N. The uniform regularity estimates are obtained by the stochastic maximum principle and the analysis of a backward stochastic Riccati equation. Using these uniform regularity results, we show the convergence of the minima of objective functionals and optimal parameters of the neural SDEs as the sample size N tends to infinity. The limiting objects can be identified with suitable functions defined on the Wasserstein space of Borel probability measures. Furthermore, quantitative algebraic convergence rates are also obtained.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/57f3f65458cfdf0a4a5311bb7d148e79802fadd4" target='_blank'>
              Convergence analysis of controlled particle systems arising in deep learning: from finite to infinite sample size
              </a>
            </td>
          <td>
            Huafu Liao, Alp'ar R. M'esz'aros, Chenchen Mou, Chao Zhou
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Mean-field control (MFC) problems aim to find the optimal policy to control massive populations of interacting agents. These problems are crucial in areas such as economics, physics, and biology. We consider the non-local setting, where the interactions between agents are governed by a suitable kernel. For $N$ agents, the interaction cost has $\mathcal{O}(N^2)$ complexity, which can be prohibitively slow to evaluate and differentiate when $N$ is large. To this end, we propose an efficient primal-dual algorithm that utilizes basis expansions of the kernels. The basis expansions reduce the cost of computing the interactions, while the primal-dual methodology decouples the agents at the expense of solving for a moderate number of dual variables. We also demonstrate that our approach can further be structured in a multi-resolution manner, where we estimate optimal dual variables using a moderate $N$ and solve decoupled trajectory optimization problems for large $N$. We illustrate the effectiveness of our method on an optimal control of 5000 interacting quadrotors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4f2ca48cf67d512b3c58916552379527d295c342" target='_blank'>
              Kernel Expansions for High-Dimensional Mean-Field Control with Non-local Interactions
              </a>
            </td>
          <td>
            Alexander Vidal, Samy Wu Fung, Stanley Osher, Luis Tenorio, L. Nurbekyan
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Consider the global optimisation of a function $U$ defined on a finite set $V$ endowed with an irreducible and reversible Markov generator.By integration, we extend $U$ to the set $\mathcal{P}(V)$ of probability distributions on $V$ and we penalise it with a time-dependent generalised entropy functional.Endowing $\mathcal{P}(V)$ with a Maas' Wasserstein-type Riemannian structure, enables us to consider an associated time-inhomogeneous gradient descent algorithm.There are several ways to interpret this $\cP(V)$-valued dynamical system as the time-marginal laws of a time-inhomogeneous non-linear Markov process taking values in $V$, each of them allowing for interacting particle approximations.This procedure extends to the discrete framework the continuous state space swarm algorithm approach of Bolte, Miclo and Villeneuve \cite{Bolte}, but here we go further by considering more general generalised entropy functionals for which functional inequalities can be proven.Thus in the full generality of the above finite framework, we give conditions on the underlying time dependence ensuring the convergence of the algorithm toward laws supported by the set of global minima of $U$.Numerical simulations illustrate that one has to be careful about the choice of the time-inhomogeneous non-linear Markov process interpretation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/759723d7b06217834debd87724a5f9b1c02a6801" target='_blank'>
              Swarm dynamics for global optimisation on finite sets
              </a>
            </td>
          <td>
            Laurent Miclo, Nhat-Thang Le
          </td>
          <td>2024-04-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Active learning optimizes the exploration of large parameter spaces by strategically selecting which experiments or simulations to conduct, thus reducing resource consumption and potentially accelerating scientific discovery. A key component of this approach is a probabilistic surrogate model, typically a Gaussian Process (GP), which approximates an unknown functional relationship between control parameters and a target property. However, conventional GPs often struggle when applied to systems with discontinuities and non-stationarities, prompting the exploration of alternative models. This limitation becomes particularly relevant in physical science problems, which are often characterized by abrupt transitions between different system states and rapid changes in physical property behavior. Fully Bayesian Neural Networks (FBNNs) serve as a promising substitute, treating all neural network weights probabilistically and leveraging advanced Markov Chain Monte Carlo techniques for direct sampling from the posterior distribution. This approach enables FBNNs to provide reliable predictive distributions, crucial for making informed decisions under uncertainty in the active learning setting. Although traditionally considered too computationally expensive for 'big data' applications, many physical sciences problems involve small amounts of data in relatively low-dimensional parameter spaces. Here, we assess the suitability and performance of FBNNs with the No-U-Turn Sampler for active learning tasks in the 'small data' regime, highlighting their potential to enhance predictive accuracy and reliability on test functions relevant to problems in physical sciences.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ebe4e8caad7fd908989b7e37a05fb880b373a0e4" target='_blank'>
              Active Learning with Fully Bayesian Neural Networks for Discontinuous and Nonstationary Data
              </a>
            </td>
          <td>
            Maxim Ziatdinov
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Real-world data typically exhibit aleatoric uncertainty which has to be considered during data-driven decision-making to assess the confidence of the decision provided by machine learning models. To propagate aleatoric uncertainty represented by probability distributions (PDs) through neural networks (NNs), both sampling-based and function approximation-based methods have been proposed. However, these methods suffer from significant approximation errors and are not able to accurately represent predictive uncertainty in the NN output. In this paper, we present a novel method, Piecewise Linear Transformation (PLT), for propagating PDs through NNs with piecewise linear activation functions (e.g., ReLU NNs). PLT does not require sampling or specific assumptions about the PDs. Instead, it harnesses the piecewise linear structure of such NNs to determine the propagated PD in the output space. In this way, PLT supports the accurate quantification of predictive uncertainty based on the criterion exactness of the propagated PD. We assess this exactness in theory by showing error bounds for our propagated PD. Further, our experimental evaluation validates that PLT outperforms competing methods on publicly available real-world classification and regression datasets regarding exactness. Thus, the PDs propagated by PLT allow to assess the uncertainty of the provided decisions, offering valuable support.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5743523253d12872c791157a2a0dba2ac20a6d2e" target='_blank'>
              Piecewise Linear Transformation - Propagating Aleatoric Uncertainty in Neural Networks
              </a>
            </td>
          <td>
            Thomas Krapf, Michael Hagn, Paul Miethaner, Alexander Schiller, Lucas Luttner, Bernd Heinrich
          </td>
          <td>2024-03-24</td>
          <td>DBLP</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We propose a new neural network based large eddy simulation framework for the incompressible Navier-Stokes equations based on the paradigm"discretize first, filter and close next". This leads to full model-data consistency and allows for employing neural closure models in the same environment as where they have been trained. Since the LES discretization error is included in the learning process, the closure models can learn to account for the discretization. Furthermore, we introduce a new divergence-consistent discrete filter defined through face-averaging. The new filter preserves the discrete divergence-free constraint by construction, unlike general discrete filters such as volume-averaging filters. We show that using a divergence-consistent LES formulation coupled with a convolutional neural closure model produces stable and accurate results for both a-priori and a-posteriori training, while a general (divergence-inconsistent) LES model requires a-posteriori training or other stability-enforcing measures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e34f7708df27471c299e81a622396a72054cae2e" target='_blank'>
              Discretize first, filter next: learning divergence-consistent closure models for large-eddy simulation
              </a>
            </td>
          <td>
            S. Agdestein, Benjamin Sanderse
          </td>
          <td>2024-03-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This contribution extends the localized training approach, traditionally employed for multiscale problems and parameterized partial differential equations (PDEs) featuring locally heterogeneous coefficients, to the class of linear, positive symmetric operators, known as Friedrichs' operators. Considering a local subdomain with corresponding oversampling domain we prove the compactness of the transfer operator which maps boundary data to solutions on the interior domain. While a Caccioppoli-inequality quantifying the energy decay to the interior holds true for all Friedrichs' systems, showing a compactness result for the graph-spaces hosting the solution is additionally necessary. We discuss the mixed formulation of a convection-diffusion-reaction problem where the necessary compactness result is obtained by the Picard-Weck-Weber theorem. Our numerical results, focusing on a scenario involving heterogeneous diffusion fields with multiple high-conductivity channels, demonstrate the effectiveness of the proposed method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97fc4f68a1e50a7e310c833837d3f9043734f822" target='_blank'>
              Construction of local reduced spaces for Friedrichs' systems via randomized training
              </a>
            </td>
          <td>
            C. Engwer, Mario Ohlberger, Lukas Renelt
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a909ef40e38eda1773cb3a2f56486e02a75e03f0" target='_blank'>
              Existence of reservoir with finite-dimensional output for universal reservoir computing
              </a>
            </td>
          <td>
            Shuhei Sugiura, Ryo Ariizumi, Toru Asai, Shun-Ichi Azuma
          </td>
          <td>2024-04-11</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Recent connections in the adaptive control literature to continuous-time analogs of Nesterov's accelerated gradient method have led to the development of new real-time adaptation laws based on accelerated gradient methods. However, previous results assume that the system's uncertainties are linear-in-the-parameters (LIP). To compensate for non-LIP uncertainties, our preliminary results developed a neural network (NN)-based accelerated gradient adaptive controller to achieve trajectory tracking for nonlinear systems; however, the development and analysis only considered single-hidden-layer NNs. In this article, a generalized deep NN (DNN) architecture with an arbitrary number of hidden layers is considered, and a new DNN-based accelerated gradient adaptation scheme is developed to generate estimates of all the DNN weights in real-time. A nonsmooth Lyapunov-based analysis is used to guarantee the developed accelerated gradient-based DNN adaptation design achieves global asymptotic tracking error convergence for general nonlinear control affine systems subject to unknown (non-LIP) drift dynamics and exogenous disturbances. A comprehensive set of simulation studies are conducted on a two-state nonlinear system, a robotic manipulator, and a complex 20-D nonlinear system to demonstrate the improved performance of the developed method. Our simulation studies demonstrate enhanced tracking and function approximation performance from both DNN architectures and accelerated gradient adaptation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e6577dd55a18708bb5032a658a9405e8d5028549" target='_blank'>
              Accelerated Gradient Approach For Deep Neural Network-Based Adaptive Control of Unknown Nonlinear Systems.
              </a>
            </td>
          <td>
            Duc M. Le, O. Patil, Cristian F. Nino, Warren E. Dixon
          </td>
          <td>2024-05-14</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="We study the existence of infinite-dimensional invariant tori in a mechanical system of infinitely many rotators weakly interacting with each other. We consider explicitly interactions depending only on the angles, with the aim of discussing in a simple case the analyticity properties to be required on the perturbation of the integrable system in order to ensure the persistence of a large measure set of invariant tori with finite energy. The proof we provide of the persistence of the invariant tori implements the Renormalization Group scheme based on the tree formalism -- i.e. the graphical representation of the solutions of the equations of motion in terms of trees -- which has been widely used in finite-dimensional problems. The method is very effectual and flexible: it naturally extends, once the functional setting has been fixed, to the infinite-dimensional case with only minor technical-natured adaptations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8ddf4967123470663d2dcc80f5d0211ba1c21930" target='_blank'>
              Maximal tori in infinite-dimensional Hamiltonian systems: a Renormalization Group approach
              </a>
            </td>
          <td>
            L. Corsi, Guido Gentile, M. Procesi
          </td>
          <td>2024-04-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucidating hidden structures in complex physical systems while offering practical applications in various domains requiring accurate short-term forecasting capabilities.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/263a32e783722b09eefe90e0bbdf88a61c93c0c4" target='_blank'>
              A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours
              </a>
            </td>
          <td>
            Aarush Sinha
          </td>
          <td>2024-03-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this work, we investigate a numerical procedure for recovering a space-dependent diffusion coefficient in a (sub)diffusion model from the given terminal data, and provide a rigorous numerical analysis of the procedure. By exploiting decay behavior of the observation in time, we establish a novel H{\"o}lder type stability estimate for a large terminal time $T$. This is achieved by novel decay estimates of the (fractional) time derivative of the solution. To numerically recover the diffusion coefficient, we employ the standard output least-squares formulation with an $H^1(\Omega)$-seminorm penalty, and discretize the regularized problem by the Galerkin finite element method with continuous piecewise linear finite elements in space and backward Euler convolution quadrature in time. Further, we provide an error analysis of discrete approximations, and prove a convergence rate that matches the stability estimate. The derived $L^2(\Omega)$ error bound depends explicitly on the noise level, regularization parameter and discretization parameter(s), which gives a useful guideline of the \textsl{a priori} choice of discretization parameters with respect to the noise level in practical implementation. The error analysis is achieved using the conditional stability argument and discrete maximum-norm resolvent estimates. Several numerical experiments are also given to illustrate and complement the theoretical analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2563fe6c41b76a0a0268761a50f339208533c3a1" target='_blank'>
              Numerical Recovery of the Diffusion Coefficient in Diffusion Equations from Terminal Measurement
              </a>
            </td>
          <td>
            Bangti Jin, Xiliang Lu, Qimeng Quan, 
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>41</td>
        </tr>

        <tr id="Nonlinear systems widely exist in real‐word applications and the research for these systems has enjoyed a long and fruitful history, including the system identification community. However, the modeling for nonlinear systems is often quite challenging and still remains many unresolved questions. This article considers the online identification issue of Hammerstein systems, whose nonlinear static function is modeled by a B‐spline network. First, the identification model of the studied system is constructed using the bilinear parameter decomposition model. Second, the online recursive algorithms are proposed to find the estimates using the moving data window and the particle swarm optimization procedure, and show that these estimates converge to their true values with a low computational burden. Numerical examples are also given to test the effectiveness of the proposed algorithms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d09ab0971243457853dad8483bc7bd474b82dcea" target='_blank'>
              Online identification of Hammerstein systems with B‐spline networks
              </a>
            </td>
          <td>
            Yanjiao Wang, Yiting Liu, Jiehao Chen, Shihua Tang, Muqing Deng
          </td>
          <td>2024-03-27</td>
          <td>International Journal of Adaptive Control and Signal Processing</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Replicating chaotic characteristics of non-linear dynamics by machine learning (ML) has recently drawn wide attentions. In this work, we propose that a ML model, trained to predict the state one-step-ahead from several latest historic states, can accurately replicate the bifurcation diagram and the Lyapunov exponents of discrete dynamic systems. The characteristics for different values of the hyper-parameters are captured universally by a single ML model, while the previous works considered training the ML model independently by fixing the hyper-parameters to be specific values. Our benchmarks on the one- and two-dimensional Logistic maps show that variational quantum circuit can reproduce the long-term characteristics with higher accuracy than the long short-term memory (a well-recognized classical ML model). Our work reveals an essential difference between the ML for the chaotic characteristics and that for standard tasks, from the perspective of the relation between performance and model complexity. Our results suggest that quantum circuit model exhibits potential advantages on mitigating over-fitting, achieving higher accuracy and stability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3398552790736a431bd98092b77b141dded167f" target='_blank'>
              Universal replication of chaotic characteristics by classical and quantum machine learning
              </a>
            </td>
          <td>
            Shengxing Bai, Shi-Ju Ran
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Willems' fundamental lemma enables a trajectory-based characterization of linear systems through data-based Hankel matrices. However, in the presence of measurement noise, we ask: Is this noisy Hankel-based model expressive enough to re-identify itself? In other words, we study the output prediction accuracy from recursively applying the same persistently exciting input sequence to the model. We find an asymptotic connection to this self-consistency question in terms of the amount of data. More importantly, we also connect this question to the depth (number of rows) of the Hankel model, showing the simple act of reconfiguring a finite dataset significantly improves accuracy. We apply these insights to find a parsimonious depth for LQR problems over the trajectory space.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a5a8c3dd68d42cacef8f099fb744c61a43353a85" target='_blank'>
              Deep Hankel matrices with random elements
              </a>
            </td>
          <td>
            Nathan P. Lawrence, Philip D. Loewen, Shuyuan Wang, M. Forbes, R. B. Gopaluni
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Multistep prediction of high-dimensional time series is an essential and challenging task. In this study, we propose an integrated reservoir predictor for making accurate and robust multistep-ahead forecasts based on short-term high-dimensional time series. Initially, a conjugated pair of Spatiotemporal Information (STI) equations is derived using Takens’ embedding theory to transform the spatial information of high-dimensional variables into one-dimensional temporal information of the target variable and vice versa. Next, by exploiting reservoir networks, reservoir-based STI equations are established to efficiently capture nonlinear dynamics of the target system with only linear optimization. Then, through an integration phase, the integrated reservoir predictor can output precise and robust predictions of the multistep-ahead states of any target variable. The integrated reservoir predictor outperforms some other prediction methods (including reservoir computing, long-short-term-memory network, convolutional neural network and support vector regression), when applied to classical dynamic systems (e.g. 60D double scroll model, 40D Lorenz 96 model, and 60D Rössler model) and real-world datasets (solar generation data and PM2.5 concentration records), as indicated by evaluation metrics such as Pearson correlation coefficients exceeding 0.9 and root-mean-square errors below 0.3, even in the presence of noise in training data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5e705f69f9f7f12f5932f99605da4b5cc122e510" target='_blank'>
              An Integrated Reservoir Predictor Based on Spatiotemporal Information Transformation
              </a>
            </td>
          <td>
            Na Yang, Renhao Hong, Pei Chen, Zhengrong Liu
          </td>
          <td>2024-04-09</td>
          <td>Int. J. Bifurc. Chaos</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="We investigate matrix-weighted bounds for the sublinear non-kernel operators considered by F. Bernicot, D. Frey, and S. Petermichl. We extend their result to sublinear operators acting upon vector-valued functions. First, we dominate these operators by bilinear convex body sparse forms, adapting a recent general principle due to T. Hyt\"onen. Then we use this domination to derive matrix-weighted bounds, adapting arguments of F. Nazarov, S. Petermichl, S. Treil, and A. Volberg. Our requirements on the weight are formulated in terms of two-exponent matrix Muckenhoupt conditions, which surprisingly exhibit a rich structure that is absent in the scalar case. Consequently, we deduce that our matrix-weighted bounds improve the ones that were recently obtained by A. Laukkarinen. The methods we use are flexible, which allows us to complement our results with a limited range extrapolation theorem for matrix weights, extending the results of P. Auscher and J. M. Martell, as well as M. Bownik and D. Cruz-Uribe.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00dd12588aaef6318714867d35cfd92893d5ab4b" target='_blank'>
              Matrix-weighted estimates beyond Calder\'on-Zygmund theory
              </a>
            </td>
          <td>
            S. Kakaroumpas, Thu Hien Nguyen, Dimitris Vardakis
          </td>
          <td>2024-04-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In this paper, we present an abstract framework to obtain convergence rates for the approximation of random evolution equations corresponding to a random family of forms determined by finite-dimensional noise. The full discretisation error in space, time, and randomness is considered, where polynomial chaos expansion (PCE) is used for the semi-discretisation in randomness. The main result are regularity conditions on the random forms under which convergence of polynomial order in randomness is obtained depending on the smoothness of the coefficients and the Sobolev regularity of the initial value. In space and time, the same convergence rates as in the deterministic setting are achieved. To this end, we derive error estimates for vector-valued PCE as well as a quantified version of the Trotter-Kato theorem for form-induced semigroups.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/754f8ae1f0c7cd65450365e9e466a1658dcbbe79" target='_blank'>
              Approximation of Random Evolution Equations
              </a>
            </td>
          <td>
            Katharina Klioba, Christian Seifert
          </td>
          <td>2024-04-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="To solve problems in domains such as filtering, optimization, and posterior sampling, interacting-particle methods have recently received much attention. These parallelizable and often gradient-free algorithms use an ensemble of particles that evolve in time, based on a combination of well-chosen dynamics and interaction between the particles. For computationally expensive dynamics -- for example, dynamics that solve inverse problems with an expensive forward model -- the cost of attaining a high accuracy quickly becomes prohibitive. We exploit a hierarchy of approximations to this forward model and apply multilevel Monte Carlo (MLMC) techniques, improving the asymptotic cost-to-error relation. More specifically, we use MLMC at each time step to estimate the interaction term within a single, globally-coupled ensemble. This technique was proposed by Hoel et al. in the context of the ensemble Kalman filter; the goal of the present paper is to study its applicability to a general framework of interacting-particle methods. After extending the algorithm and its analysis to a broad set of methods with fixed numbers of time steps, we motivate the application of the method to the class of algorithms with an infinite time horizon, which includes popular methods such as ensemble Kalman algorithms for optimization and sampling. Numerical tests confirm the improved asymptotic scaling of the multilevel approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83f0694bf03f146ba6f88c4cdf323e51cc8069f4" target='_blank'>
              Single-ensemble multilevel Monte Carlo for discrete interacting-particle methods
              </a>
            </td>
          <td>
            Arne Bouillon, Toon Ingelaere, Giovanni Samaey
          </td>
          <td>2024-05-16</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="Our work is part of the close link between continuous-time dissipative dynamical systems and optimization algorithms, and more precisely here, in the stochastic setting. We aim to study stochastic convex minimization problems through the lens of stochastic inertial differential inclusions that are driven by the subgradient of a convex objective function. This will provide a general mathematical framework for analyzing the convergence properties of stochastic second-order inertial continuous-time dynamics involving vanishing viscous damping and measurable stochastic subgradient selections. Our chief goal in this paper is to develop a systematic and unified way that transfers the properties recently studied for first-order stochastic differential equations to second-order ones involving even subgradients in lieu of gradients. This program will rely on two tenets: time scaling and averaging, following an approach recently developed in the literature by one of the co-authors in the deterministic case. Under a mild integrability assumption involving the diffusion term and the viscous damping, our first main result shows that almost surely, there is weak convergence of the trajectory towards a minimizer of the objective function and fast convergence of the values and gradients. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex, strongly convex, and (local) Polyak-Lojasiewicz case. Finally, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f951f4efa58b17f6388d5d737d0043ace7f3e13d" target='_blank'>
              Stochastic Inertial Dynamics Via Time Scaling and Averaging
              </a>
            </td>
          <td>
            Rodrigo Maulen-Soto, Jalal Fadili, H. Attouch, Peter Ochs
          </td>
          <td>2024-03-25</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>49</td>
        </tr>

        <tr id="Entropy is a central concept in physics, but can be challenging to calculate even for systems that are easily simulated. This is exacerbated out of equilibrium, where generally little is known about the distribution characterizing simulated configurations. However, modern machine learning algorithms can estimate the probability density characterizing an ensemble of images, given nothing more than sample images assumed to be drawn from this distribution. We show that by mapping system configurations to images, such approaches can be adapted to the efficient estimation of the density, and therefore the entropy, from simulated or experimental data. We then use this idea to obtain entropic limit cycles in a kinetic Ising model driven by an oscillating magnetic field. Despite being a global probe, we demonstrate that this allows us to identify and characterize stochastic dynamics at parameters near the dynamical phase transition.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc5c2639eefdb691a6b558afc70620258eb7d1be" target='_blank'>
              Nonequilibrium entropy from density estimation
              </a>
            </td>
          <td>
            Samuel D. Gelman, Guy Cohen
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay. In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity. Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships. To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains. The convergence of the loss function is proven. Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9a52977fee60d7b3615721c368a708f015928933" target='_blank'>
              Double-activation neural network for solving parabolic equations with time delay
              </a>
            </td>
          <td>
            Qiumei Huang, Qiao Zhu
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Computing distances on Riemannian manifolds is a challenging problem with numerous applications, from physics, through statistics, to machine learning. In this paper, we introduce the metric-constrained Eikonal solver to obtain continuous, differentiable representations of distance functions on manifolds. The differentiable nature of these representations allows for the direct computation of globally length-minimising paths on the manifold. We showcase the use of metric-constrained Eikonal solvers for a range of manifolds and demonstrate the applications. First, we demonstrate that metric-constrained Eikonal solvers can be used to obtain the Fr\'echet mean on a manifold, employing the definition of a Gaussian mixture model, which has an analytical solution to verify the numerical results. Second, we demonstrate how the obtained distance function can be used to conduct unsupervised clustering on the manifold -- a task for which existing approaches are computationally prohibitive. This work opens opportunities for distance computations on manifolds.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5fb91b477e037f512274e4ee3a3298566bed1fc0" target='_blank'>
              Computing distances and means on manifolds with a metric-constrained Eikonal approach
              </a>
            </td>
          <td>
            Daniel Kelshaw, Luca Magri
          </td>
          <td>2024-04-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Quantitative models of sequence-function relationships, which describe how biological sequences encode functional activities, are ubiquitous in modern biology. One important aspect of these models is that they commonly exhibit gauge freedoms, i.e., directions in parameter space that do not affect model predictions. In physics, gauge freedoms arise when physical theories are formulated in ways that respect fundamental symmetries. However, the connections that gauge freedoms in models of sequence-function relationships have to the symmetries of sequence space have yet to be systematically studied. Here we study the gauge freedoms of models that respect a specific symmetry of sequence space: the group of position-specific character permutations. We find that gauge freedoms arise when the transformations of model parameters that compensate for these symmetry transformations are described by redundant irreducible matrix representations. Based on this finding, we describe an “embedding distillation” procedure that enables analytic calculation of the dimension of the space of gauge freedoms, as well as efficient computation of a sparse basis for this space. Finally, we show that the ability to interpret model parameters as quantifying allelic effects places strong constraints on the form that models can take, and in particular show that all nontrivial equivariant models of allelic effects must exhibit gauge freedoms. Our work thus advances the understanding of the relationship between symmetries and gauge freedoms in quantitative models of sequence-function relationships.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bed3a3b2dfb33293936815a4c9f93157af5ca5c2" target='_blank'>
              Symmetry, gauge freedoms, and the interpretability of sequence-function relationships
              </a>
            </td>
          <td>
            Anna Pósfai, David M. McCandlish, J. Kinney
          </td>
          <td>2024-05-13</td>
          <td>bioRxiv</td>
          <td>1</td>
          <td>23</td>
        </tr>

        <tr id="In this paper, we consider a trigonometric polynomial reconstruction of continuous periodic functions from their noisy values at equidistant nodes of the unit circle by a regularized least squares method. We indicate that the constructed trigonometric polynomial can be determined in explicit due to the exactness of trapezoidal rule. Then a concrete error bound is derived based on the estimation of the Lebesgue constant. In particular, we analyze three regularization parameter choice strategies: Morozov's discrepancy principal, L-curve and generalized cross-validation. Finally, numerical examples are given to perform that well chosen parameters by above strategy can improve approximation quality.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afa8cb90fc2a0d9546bbce99f5ee1ebac8d9d76c" target='_blank'>
              Parameter choice strategies for regularized least squares approximation of noisy continuous functions on the unit circle
              </a>
            </td>
          <td>
            Congpei An, Mou Cai
          </td>
          <td>2024-03-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Simulation of conditioned diffusion processes is an essential tool in inference for stochastic processes, data imputation, generative modelling, and geometric statistics. Whilst simulating diffusion bridge processes is already difficult on Euclidean spaces, when considering diffusion processes on Riemannian manifolds the geometry brings in further complications. In even higher generality, advancing from Riemannian to sub-Riemannian geometries introduces hypoellipticity, and the possibility of finding appropriate explicit approximations for the score of the diffusion process is removed. We handle these challenges and construct a method for bridge simulation on sub-Riemannian manifolds by demonstrating how recent progress in machine learning can be modified to allow for training of score approximators on sub-Riemannian manifolds. Since gradients dependent on the horizontal distribution, we generalise the usual notion of denoising loss to work with non-holonomic frames using a stochastic Taylor expansion, and we demonstrate the resulting scheme both explicitly on the Heisenberg group and more generally using adapted coordinates. We perform numerical experiments exemplifying samples from the bridge process on the Heisenberg group and the concentration of this process for small time.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0803d6af6d30c48b58c31bf11f6d806924a785d9" target='_blank'>
              Score matching for sub-Riemannian bridge sampling
              </a>
            </td>
          <td>
            E. Grong, Karen Habermann, Stefan Sommer
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Robust integration of physical knowledge and data is key to improve computational simulations, such as Earth system models. Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification. Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive. Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models. In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework. These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis. Likewise, one can frame data assimilation as observation-conditioned state calibration. In this work, we propose SLAMS: Score-based Latent Assimilation in Multimodal Setting. Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally. Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings. To our knowledge, our work is the first to apply deep generative framework for multimodal data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models. Our code is available at: https://github.com/yongquan-qu/SLAMS">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc3e0b8ce89e1b4701b2bee561c38e13abec072e" target='_blank'>
              Deep Generative Data Assimilation in Multimodal Setting
              </a>
            </td>
          <td>
            Yongquan Qu, Juan Nathaniel, Shuolin Li, Pierre Gentine
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>2</td>
        </tr>

        <tr id="We study the limiting dynamics of a large class of noisy gradient descent systems in the overparameterized regime. In this regime the set of global minimizers of the loss is large, and when initialized in a neighbourhood of this zero-loss set a noisy gradient descent algorithm slowly evolves along this set. In some cases this slow evolution has been related to better generalisation properties. We characterize this evolution for the broad class of noisy gradient descent systems in the limit of small step size. Our results show that the structure of the noise affects not just the form of the limiting process, but also the time scale at which the evolution takes place. We apply the theory to Dropout, label noise and classical SGD (minibatching) noise, and show that these evolve on different two time scales. Classical SGD even yields a trivial evolution on both time scales, implying that additional noise is required for regularization. The results are inspired by the training of neural networks, but the theorems apply to noisy gradient descent of any loss that has a non-trivial zero-loss set.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/534932034316cdaaacddd5a8407cfd33520a086f" target='_blank'>
              Singular-limit analysis of gradient descent with noise injection
              </a>
            </td>
          <td>
            Anna Shalova, Andr'e Schlichting, M. Peletier
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="The Fourier neural operator (FNO) framework is applied to the large eddy simulation (LES) of three-dimensional compressible Rayleigh-Taylor (RT) turbulence with miscible fluids at Atwood number $A_t=0.5$, stratification parameter $Sr=1.0$, and Reynolds numbers $Re=10000$ and 30000. The FNO model is first used for predicting three-dimensional compressible turbulence. The different magnitudes of physical fields are normalized using root mean square values for an easier training of FNO models. In the \emph{a posteriori} tests, the FNO model outperforms the velocity gradient model (VGM), the dynamic Smagorinsky model (DSM), and implicit large eddy simulation (ILES) in predicting various statistical quantities and instantaneous structures, and is particularly superior to traditional LES methods in predicting temperature fields and velocity divergence. Moreover, the computational efficiency of the FNO model is much higher than that of traditional LES methods. FNO models trained with short-time, low Reynolds number data exhibit a good generalization performance on longer-time predictions and higher Reynolds numbers in the \emph{a posteriori} tests.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/55f9e00431f90298219754a08230b6c8d18eb616" target='_blank'>
              Fourier neural operator for large eddy simulation of compressible Rayleigh-Taylor turbulence
              </a>
            </td>
          <td>
            Tengfei Luo, Zhijie Li, Zelong Yuan, W. Peng, Tianyuan Liu, L. Wang, Jianchun Wang
          </td>
          <td>2024-04-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations and adversarial attacks, as well as the stability and safety of systems with neural network controllers. Therefore, estimation of tight bounds on the Lipschitz constant of neural networks is a well-studied topic. However, typical approaches involve solving a large matrix verification problem, the computational cost of which grows significantly for deeper networks. In this letter, we provide a compositional approach to estimate Lipschitz constants for deep feedforward neural networks by obtaining an exact decomposition of the large matrix verification problem into smaller sub-problems. We further obtain a closed-form solution that applies to most common neural network activation functions, which will enable rapid robustness and stability certificates for neural networks deployed in online control settings. Finally, we demonstrate through numerical experiments that our approach provides a steep reduction in computation time while yielding Lipschitz bounds that are very close to those achieved by state-of-the-art approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/813a16e601a432ebd25dd2b3686aa87a5c4fca39" target='_blank'>
              Compositional Estimation of Lipschitz Constants for Deep Neural Networks
              </a>
            </td>
          <td>
            Yuezhu Xu, S. Sivaranjani
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space. More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/811bf918f6e0b636d02b1e042c162f22f48eca6c" target='_blank'>
              Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint
              </a>
            </td>
          <td>
            Vincent Perrier
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We investigate the complexities of the McKean-Vlasov optimal control problem, exploring its various formulations such as the strong and weak formulations, as well as both Markovian and non-Markovian setups within financial markets. Furthermore, we examine scenarios where the law governing the control process impacts the dynamics of options. By conceptualizing controls as probability measures on a fitting canonical space with filtrations, we unlock the potential to devise classical measurable selection methods, conditioning strategies, and concatenation arguments within this innovative framework. These tools enable us to establish the dynamic programming principle under a wide range of conditions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44c2cc178797305b853ef3bd213acd3645420148" target='_blank'>
              Path integral control under McKean-Vlasov dynamics
              </a>
            </td>
          <td>
            Timothy Bennett
          </td>
          <td>2024-04-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The quantum Fourier transform (QFT), which can be viewed as a reindexing of the discrete Fourier transform (DFT), has been shown to be compressible as a low-rank matrix product operator (MPO) or quantized tensor train (QTT) operator. However, the original proof of this fact does not furnish a construction of the MPO with a guaranteed error bound. Meanwhile, the existing practical construction of this MPO, based on the compression of a quantum circuit, is not as efficient as possible. We present a simple closed-form construction of the QFT MPO using the interpolative decomposition, with guaranteed near-optimal compression error for a given rank. This construction can speed up the application of the QFT and the DFT, respectively, in quantum circuit simulations and QTT applications. We also connect our interpolative construction to the approximate quantum Fourier transform (AQFT) by demonstrating that the AQFT can be viewed as an MPO constructed using a different interpolation scheme.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d34bca7fcdc817c9d79147bb7505a60fbc11223a" target='_blank'>
              Direct interpolative construction of the discrete Fourier transform as a matrix product operator
              </a>
            </td>
          <td>
            Jielun Chen, Michael Lindsey
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The zonotope containment problem, i.e., whether one zonotope is contained in another, is a central problem in control theory to compute invariant sets, obtain fixed points of reachable sets, detect faults, and robustify controllers. Despite the inherent co-NP-hardness of this problem, an approximation algorithm developed by S. Sadraddini and R. Tedrake has gained widespread recognition for its swift execution and consistent reliability in practical scenarios. In our study, we substantiate the precision of the algorithm with a definitive proof, elucidating the empirical accuracy observed in practice. Our proof hinges on establishing a connection between the containment problem and the computation of matrix norms, thereby enabling the extension of the approximation algorithm to encompass ellipsotopes, a broader class of sets derived from zonotopes. Moreover, we explore the computational complexity of the ellipsotope containment problem, focusing on approximability. Finally, we present new methods to calculate robust control invariant sets for linear dynamical systems, demonstrating the practical relevance of approximations to the ellipsotope containment problem.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc30bf2b4d9b8101764d3c2e422b58734d6fff3d" target='_blank'>
              Approximability of the Containment Problem for Zonotopes and Ellipsotopes
              </a>
            </td>
          <td>
            Adrian Kulmburg, Lukas Schafer, Matthias Althoff
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="The stable numerical integration of shocks in compressible flow simulations relies on the reduction or elimination of Gibbs phenomena (unstable, spurious oscillations). A popular method to virtually eliminate Gibbs oscillations caused by numerical discretization in under-resolved simulations is to use a flux limiter. A wide range of flux limiters has been studied in the literature, with recent interest in their optimization via machine learning methods trained on high-resolution datasets. The common use of flux limiters in numerical codes as plug-and-play blackbox components makes them key targets for design improvement. Moreover, while aleatoric (inherent randomness) and epistemic (lack of knowledge) uncertainty is commonplace in fluid dynamical systems, these effects are generally ignored in the design of flux limiters. Even for deterministic dynamical models, numerical uncertainty is introduced via coarse-graining required by insufficient computational power to solve all scales of motion. Here, we introduce a conceptually distinct type of flux limiter that is designed to handle the effects of randomness in the model and uncertainty in model parameters. This new, {\it probabilistic flux limiter}, learned with high-resolution data, consists of a set of flux limiting functions with associated probabilities, which define the frequencies of selection for their use. Using the example of Burgers' equation, we show that a machine learned, probabilistic flux limiter may be used in a shock capturing code to more accurately capture shock profiles. In particular, we show that our probabilistic flux limiter outperforms standard limiters, and can be successively improved upon (up to a point) by expanding the set of probabilistically chosen flux limiting functions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/04d9f0a92b077f68a565391c1b9980829fdf7bdd" target='_blank'>
              Probabilistic Flux Limiters
              </a>
            </td>
          <td>
            Nga Nguyen-Fotiadis, Robert Chiodi, Michael McKerns, Daniel Livescu, Andrew Sornborger
          </td>
          <td>2024-05-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="As one kind important phase field equations, Cahn-Hilliard equations contain spatial high order derivatives, strong nonlinearities, and even singularities. When using the physics informed neural network (PINN) to simulate the long time evolution, it is necessary to decompose the time domain to capture the transition of solutions in different time. Moreover, the baseline PINN can't maintain the mass conservation property for the equations. We propose a mass-preserving spatio-temporal adaptive PINN. This method adaptively dividing the time domain according to the rate of energy decrease, and solves the Cahn-Hilliard equation in each time step using an independent neural network. To improve the prediction accuracy, spatial adaptive sampling is employed in the subdomain to select points with large residual value and add them to the training samples. Additionally, a mass constraint is added to the loss function to compensate the mass degradation problem of the PINN method in solving the Cahn-Hilliard equations. The mass-preserving spatio-temporal adaptive PINN is employed to solve a series of numerical examples. These include the Cahn-Hilliard equations with different bulk potentials, the three dimensional Cahn-Hilliard equation with singularities, and the set of Cahn-Hilliard equations. The numerical results demonstrate the effectiveness of the proposed algorithm.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3067a09d2763478dfa3a700261e63c35c97eb92" target='_blank'>
              Mass-preserving Spatio-temporal adaptive PINN for Cahn-Hilliard equations with strong nonlinearity and singularity
              </a>
            </td>
          <td>
            Qiumei Huang, Jiaxuan Ma, Xu Zhen
          </td>
          <td>2024-04-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper considers the observer design problem for discrete-time nonlinear dynamical systems with sampled measurement data. Earlier, the recently proposed Iteratively Preconditioned Gradient-Descent (IPG) observer, a Newton-type observer, has been empirically shown to have improved robustness against measurement noise than the prominent nonlinear observers, a property that other Newton-type observers lack. However, no theoretical guarantees on the convergence of the IPG observer were provided. This paper presents a rigorous convergence analysis of the IPG observer for a class of nonlinear systems in deterministic settings, proving its local linear convergence to the actual trajectory. Our assumptions are standard in the existing literature of Newton-type observers, and the analysis further confirms the relation of the IPG observer with the Newton observer, which was only hypothesized earlier.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2c99a1ecf08ec5e38db2283a9efc91ed04a1153e" target='_blank'>
              On Convergence of the Iteratively Preconditioned Gradient-Descent (IPG) Observer
              </a>
            </td>
          <td>
            Kushal Chakrabarti, Nikhil Chopra
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/787a961597154d596e3b13fd172d4915fd075927" target='_blank'>
              An Analytic Solution to Covariance Propagation in Neural Networks
              </a>
            </td>
          <td>
            Oren Wright, Yorie Nakahira, Jos'e M. F. Moura
          </td>
          <td>2024-03-24</td>
          <td>DBLP, ArXiv</td>
          <td>1</td>
          <td>10</td>
        </tr>

        <tr id="We propose a counterpart of the classical Rollnik-class of potentials for fractional and massive relativistic Laplacians, and describe this space in terms of appropriate Riesz potentials. These definitions rely on precise resolvent estimates. We show that Coulomb-type potentials are elements of fractional Rollnik-class up to but not including the critical singularity of the Hardy potential. For the operators with fractional exponent $\alpha = 1$ there exists no fractional Rollnik potential, however, in low dimensions we make sense of these classes as limiting cases by using $\Gamma$-convergence. In a second part of the paper we derive detailed results on the self-adjointness and spectral properties of relativistic Schr\"odinger operators obtained under perturbations by fractional Rollnik potentials. We also define an extended fractional Rollnik-class which is the maximal space for the Hilbert-Schmidt property of the related Birman-Schwinger operators.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ae0951ef8b5afa44f27242cdd4ef0ce24ec7bac4" target='_blank'>
              Special potentials for relativistic Laplacians I: Fractional Rollnik-class
              </a>
            </td>
          <td>
            Giacomo Ascione, Atsuhide Ishida, J'ozsef LHorinczi
          </td>
          <td>2024-05-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="A Tikhonov regularized inertial primal\mbox{-}dual dynamical system with time scaling and vanishing damping is proposed for solving a linearly constrained convex optimization problem in Hilbert spaces. The system under consideration consists of two coupled second order differential equations and its convergence properties depend upon the decaying speed of the product of the time scaling parameter and the Tikhonov regularization parameter (named the rescaled regularization parameter) to zero. When the rescaled regularization parameter converges rapidly to zero, the system enjoys fast convergence rates of the primal-dual gap, the feasibility violation, the objective residual, and the gradient norm of the objective function along the trajectory, and the weak convergence of the trajectory to a primal-dual solution of the linearly constrained convex optimization problem. When the rescaled regularization parameter converges slowly to zero, the generated primal trajectory converges strongly to the minimal norm solution of the problem under suitable conditions. Finally, numerical experiments are performed to illustrate the theoretical findings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/493bbc24e7a362645df3137d454d329dd81822d2" target='_blank'>
              Fast convergence rates and trajectory convergence of a Tikhonov regularized inertial primal\mbox{-}dual dynamical system with time scaling and vanishing damping
              </a>
            </td>
          <td>
            Ting-Ting Zhu, Rong Hu, Yaoli Fang
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Networked dynamical systems are widely used as formal models of real-world cascading phenomena, such as the spread of diseases and information. Prior research has addressed the problem of learning the behavior of an unknown dynamical system when the underlying network has a single layer. In this work, we study the learnability of dynamical systems over multilayer networks, which are more realistic and challenging. First, we present an efficient PAC learning algorithm with provable guarantees to show that the learner only requires a small number of training examples to infer an unknown system. We further provide a tight analysis of the Natarajan dimension which measures the model complexity. Asymptotically, our bound on the Nararajan dimension is tight for almost all multilayer graphs. The techniques and insights from our work provide the theoretical foundations for future investigations of learning problems for multilayer dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4d4c0063e47ca85cf9c10b3ade6904eaa76704c7" target='_blank'>
              Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks
              </a>
            </td>
          <td>
            Zirou Qiu, Abhijin Adiga, M. Marathe, S. S. Ravi, D. Rosenkrantz, R. Stearns, Anil Vullikanti
          </td>
          <td>2024-05-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>55</td>
        </tr>

        <tr id="In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization. The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions. Further local linear (and potentially superlinear) convergence is shown under standard additional conditions. The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems. These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters. The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements. We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines. We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/695ed3de01e391e3338f9c30f41c35390429161d" target='_blank'>
              An efficient active-set method with applications to sparse approximations and risk minimization
              </a>
            </td>
          <td>
            Spyridon Pougkakiotis, J. Gondzio, Dionysis Kalogerias
          </td>
          <td>2024-05-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Approximation ability is one of the most important topics in the field of neural networks (NNs). Feedforward NNs, activated by rectified linear units and some of their specific smoothed versions, provide universal approximators to convex as well as continuous functions. However, most of these networks are investigated empirically, or their characteristics are analyzed based on specific operation rules. Moreover, an adequate level of interpretability of the networks is missing as well. In this work, we propose a class of new network architecture, built with reusable neural modules (functional blocks), to supply differentiable and interpretable approximators for convex and continuous target functions. Specifically, first, we introduce a concrete model construction mechanism with particular blocks based on differentiable programming and the composition essence of the max operator, extending the scope of existing activation functions. Moreover, explicit block diagrams are provided for a clear understanding of the external architecture and the internal processing mechanism. Subsequently, the approximation behavior of the proposed network to convex functions and continuous functions is rigorously proved as well, by virtue of mathematical induction. Finally, plenty of numerical experiments are conducted on a wide variety of problems, which exhibit the effectiveness and the superiority of the proposed model over some existing ones.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb22e431201f0753e058726fdf058e8f07404eed" target='_blank'>
              Universal Approximation Abilities of a Modular Differentiable Neural Network.
              </a>
            </td>
          <td>
            Jian Wang, Shujun Wu, Huaqing Zhang, Bin Yuan, Caili Dai, Nikhil R. Pal
          </td>
          <td>2024-04-03</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Dimensionality reduction often serves as the first step toward a minimalist understanding of physical systems as well as the accelerated simulations of them. In particular, neural network-based nonlinear dimensionality reduction methods, such as autoencoders, have shown promising outcomes in uncovering collective variables (CVs). However, the physical meaning of these CVs remains largely elusive. In this work, we constructed a framework that (1) determines the optimal number of CVs needed to capture the essential molecular motions using an ensemble of hierarchical autoencoders and (2) provides topology-based interpretations to the autoencoder-learned CVs with Morse-Smale complex and sublevelset persistent homology. This approach was exemplified using a series of n-alkanes and can be regarded as a general, explainable nonlinear dimensionality reduction method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b589f55faeefb8ff1346474b39d4c4818b853722" target='_blank'>
              Interpretation of autoencoder-learned collective variables using Morse-Smale complex and sublevelset persistent homology: An application on molecular trajectories.
              </a>
            </td>
          <td>
            Shao-Chun Lee, Y. Z
          </td>
          <td>2024-04-09</td>
          <td>The Journal of chemical physics</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Stochastic physics is a central pillar of modern research in many fields, but is rarely presented to undergrad students in a hands-on experiment. Here, we demonstrate how a human-scale, simple, and affordable experimental setup can be used to fill this gap, and to illustrate many advanced concepts in a step-by-step approach. Based on a metal wire (such as a guitar string), our setup facilitates the observation of fluctuating dynamics in the time domain, the frequency spectrum, and in the rotating phase space. The latter allows introducing time-dependent cross-correlations between the sine and cosine quadratures of the stochastic motion, which feature deterministic order even in the absence of any deterministic forces.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c28b7759ee23c42d37c504a441259e0b7515c694" target='_blank'>
              The Stochastic Guitar
              </a>
            </td>
          <td>
            Andreas Eggenberger, Alexander Eichler
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Soft actuators, distinguished by their complex nonlinear behavior, are difficult to model analytically and cumbersome to prototype. Finite element (FE) models allow for more efficient behavioral prediction, but often require onerous setup, especially for large systems. We present a physics-informed neural network model formed by combining a low fidelity analytical model and input-convex neural networks to learn an underlying energy potential for the actuator from experimental and finite element simulation data. In doing this, the neural network can provide sufficiently accurate predictions about systems made up of multiple units, essentially scaling the model from a single unit to an assembly of many. To test this concept, we compare predictions of the deformation of a 5-actuator system from an FE model and from the physics-informed neural network. The neural network, which provides a prediction similar in accuracy to the FE equivalent, can more easily be adjusted to execute systems of greater quantities of units without drastic increases in computational consumption. In this way, we can scale our predictive understanding with adequate accuracy without compounding resources.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/97898a8ef39e378f477e3b2603114c706798ea97" target='_blank'>
              Physics-Informed Neural Network for Scalable Soft Multi-Actuator Systems
              </a>
            </td>
          <td>
            Carly Mendenhall, Jonathan Hardan, Trysta D. Chiang, Laura H. Blumenschein, A. B. Tepole
          </td>
          <td>2024-04-14</td>
          <td>2024 IEEE 7th International Conference on Soft Robotics (RoboSoft)</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ee4b3249380d73a27acd2244bb01a97c229d9bc" target='_blank'>
              The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks
              </a>
            </td>
          <td>
            Lucius Bushnaq, Stefan Heimersheim Nicholas Goldowsky-Dill, Dan Braun, Jake Mendel, Kaarel Hanni, Avery Griffin, Jorn Stohler, Magdalena Wache, Marius Hobbhahn
          </td>
          <td>2024-05-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ad872bc3ab6d5c5e92c15d8f70fc4da03092cc7" target='_blank'>
              How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression
              </a>
            </td>
          <td>
            Lucas Kook, Chris Kolb, Philipp Schiele, Daniel Dold, Marcel Arpogaus, Cornelius Fritz, Philipp F. M. Baumann, Philipp Kopper, Tobias Pielok, Emilio Dorigatti, David Rugamer
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Increasing effort is put into the development of methods for learning mechanistic models from data. This task entails not only the accurate estimation of parameters, but also a suitable model structure. Recent work on the discovery of dynamical systems formulates this problem as a linear equation system. Here, we explore several simulation-based optimization approaches, which allow much greater freedom in the objective formulation and weaker conditions on the available data. We show that even for relatively small stochastic population models, simultaneous estimation of parameters and structure poses major challenges for optimization procedures. Particularly, we investigate the application of the local stochastic gradient descent method, commonly used for training machine learning models. We demonstrate accurate estimation of models but find that enforcing the inference of parsimonious, interpretable models drastically increases the difficulty. We give an outlook on how this challenge can be overcome.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/69f3306c5d346d91ce086b55f87d085d98c721dd" target='_blank'>
              Towards Learning Stochastic Population Models by Gradient Descent
              </a>
            </td>
          <td>
            J. N. Kreikemeyer, Philipp Andelfinger, A. Uhrmacher
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: Dropout, DropConnect, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8238b73c146d1a68f496f60a928271d8400e362e" target='_blank'>
              Information-Theoretic Generalization Bounds for Deep Neural Networks
              </a>
            </td>
          <td>
            Haiyun He, Christina Lee Yu, Ziv Goldfeld
          </td>
          <td>2024-04-04</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>18</td>
        </tr>

        <tr id="The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems. GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks. This work studies a GGN method for optimizing a two-layer neural network with explicit regularization. In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem. This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance. We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters. Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network. The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7fa9754d09446870ffbb8c78af1c076d43eba2d7" target='_blank'>
              Regularized Gauss-Newton for Optimizing Overparameterized Neural Networks
              </a>
            </td>
          <td>
            Adeyemi Damilare Adeoye, Philipp Christian Petersen, Alberto Bemporad
          </td>
          <td>2024-04-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Auto- and cross-spectral density functions for dynamic {random} fields and power are derived. These are based on first- and second-order Pad\'{e} approximants of correlation functions expanded in terms of spectral moments. The second-order approximant permits a characterization of stir noise observable {at high stir frequencies in the autospectral density}. A relationship between stir imperfection and spectral kurtosis is established. For the latter, lower bounds are established. A novel alternative measure of correlation time for mean-square differentiable fields is introduced as the lag at the first point of inflection in the autocorrelation function. A hierarchy of Pad\'{e} deviation coefficients is constructed that quantify imperfections of correlations and spectra with increasing accuracy and range of lags. Analytical models of the spectral densities are derived and their asymptotic behaviour is analyzed. The theoretical spectral density for the electric field as an input quantity is compared with that for power as the measurand. For the latter, its inverted-S shape conforms to experimentally observed stir-spectral power densities. The effect of additive noise on the stir autocorrelation and spectral density functions is quantified.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3029c2258f6ebdda9465e37fc3f2e395d27ed1c0" target='_blank'>
              Correlation and Spectral Density Functions in Mode-Stirred Reverberation—I. Theory
              </a>
            </td>
          <td>
            Luk R. Arnaut
          </td>
          <td>2024-04-02</td>
          <td>IEEE Transactions on Electromagnetic Compatibility</td>
          <td>1</td>
          <td>4</td>
        </tr>

        <tr id="Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control benchmarks. The underlying coarse action space discretizations often yield favourable exploration characteristics while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and energy efficiency, but action costs can be detrimental to exploration during early training. In this work, we aim to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution, taking advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that yield surprisingly strong performance on continuous control tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ea24172e7917c94dcc8d5f5794b0ae23a2cf5e6" target='_blank'>
              Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution
              </a>
            </td>
          <td>
            Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela Rus
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Irregularly sampled time series with missing values are often observed in multiple real-world applications such as healthcare, climate and astronomy. They pose a significant challenge to standard deep learn- ing models that operate only on fully observed and regularly sampled time series. In order to capture the continuous dynamics of the irreg- ular time series, many models rely on solving an Ordinary Differential Equation (ODE) in the hidden state. These ODE-based models tend to perform slow and require large memory due to sequential operations and a complex ODE solver. As an alternative to complex ODE-based mod- els, we propose a family of models called Functional Latent Dynamics (FLD). Instead of solving the ODE, we use simple curves which exist at all time points to specify the continuous latent state in the model. The coefficients of these curves are learned only from the observed values in the time series ignoring the missing values. Through extensive experi- ments, we demonstrate that FLD achieves better performance compared to the best ODE-based model while reducing the runtime and memory overhead. Specifically, FLD requires an order of magnitude less time to infer the forecasts compared to the best performing forecasting model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2a5b8158db29854109275cb5c3fbcf47c080c1c" target='_blank'>
              Functional Latent Dynamics for Irregularly Sampled Time Series Forecasting
              </a>
            </td>
          <td>
            Christian Klotergens, Vijaya Krishna Yalavarthi, Maximilian Stubbemann, Lars Schmidt-Thieme
          </td>
          <td>2024-05-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="We revisit the unified two-timescale Q-learning algorithm as initially introduced by Angiuli et al. \cite{angiuli2022unified}. This algorithm demonstrates efficacy in solving mean field game (MFG) and mean field control (MFC) problems, simply by tuning the ratio of two learning rates for mean field distribution and the Q-functions respectively. In this paper, we provide a comprehensive theoretical explanation of the algorithm's bifurcated numerical outcomes under fixed learning rates. We achieve this by establishing a diagram that correlates continuous-time mean field problems to their discrete-time Q-function counterparts, forming the basis of the algorithm. Our key contribution lies in the construction of a Lyapunov function integrating both mean field distribution and Q-function iterates. This Lyapunov function facilitates a unified convergence of the algorithm across the entire spectrum of learning rates, thus providing a cohesive framework for analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3a4336a84541b93f8513d18af194216142d32d6" target='_blank'>
              Why does the two-timescale Q-learning converge to different mean field solutions? A unified convergence analysis
              </a>
            </td>
          <td>
            Jing An, Jian-wei Lu, Yue Wu, Yang Xiang
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In this paper, we extend our previous work on the power series method for computing backstepping kernels. Our first contribution is the development of initial steps towards a MATLAB toolbox dedicated to backstepping kernel computation. This toolbox would exploit MATLAB's linear algebra and sparse matrix manipulation features for enhanced efficiency; our initial findings show considerable improvements in computational speed with respect to the use of symbolical software without loss of precision at high orders. Additionally, we tackle limitations observed in our earlier work, such as slow convergence (due to oscillatory behaviors) and non-converging series (due to loss of analiticity at some singular points). To overcome these challenges, we introduce a technique that mitigates this behaviour by computing the expansion at different points, denoted as localized power series. This approach effectively navigates around singularities, and can also accelerates convergence by using more local approximations. Basic examples are provided to demonstrate these enhancements. Although this research is still ongoing, the significant potential and simplicity of the method already establish the power series approach as a viable and versatile solution for solving backstepping kernel equations, benefiting both novel and experienced practitioners in the field. We anticipate that these developments will be particularly beneficial in training the recently introduced neural operators that approximate backstepping kernels and gains.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2262d93f9336a0152983b93ce73bf515f56b44e" target='_blank'>
              Towards a MATLAB Toolbox to compute backstepping kernels using the power series method
              </a>
            </td>
          <td>
            Xin Lin, R. Vázquez, Miroslav Krstic
          </td>
          <td>2024-03-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>32</td>
        </tr>

        <tr id="We investigate the precision of the numerical implementation of the functional renormalization group based on extracting the eigenvalues from the linearized RG transformation. For this purpose, we implement the LPA and $O(\partial^2)$ orders of the derivative expansion for the three-dimensional $O(N)$ models with $N~\in~\{1,2,3\}$. We identify several categories of numerical error and devise simple tests to track their magnitude as functions of numerical parameters. Our numerical schemes converge properly and are characterized by errors of several orders of magnitude smaller than the error bars of the derivative expansion for these models. We highlight situations in which our methods cease to converge, most often due to rounding errors. In particular, we observe an impaired convergence of the discretization scheme when the $\tilde \rho$ grid is cut off at the value $\tilde \rho_{\text{Max}}$ smaller than $3.5$ times the local potential minimum. The program performing the numerical calculations for this study is shared as an open-source library accessible for review and reuse.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/27793222b57bb78c9e9803aedd0b3fac8480c9cc" target='_blank'>
              Numerical Accuracy of the Derivative-Expansion-Based Functional Renormalization Group
              </a>
            </td>
          <td>
            Andrzej Chlebicki
          </td>
          <td>2024-04-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We develop a thermodynamic theory for machine learning (ML) systems. Similar to physical thermodynamic systems which are characterized by energy and entropy, ML systems possess these characteristics as well. This comparison inspire us to integrate the concept of temperature into ML systems grounded in the fundamental principles of thermodynamics, and establish a basic thermodynamic framework for machine learning systems with non-Boltzmann distributions. We introduce the concept of states within a ML system, identify two typical types of state, and interpret model training and refresh as a process of state phase transition. We consider that the initial potential energy of a ML system is described by the model's loss functions, and the energy adheres to the principle of minimum potential energy. For a variety of energy forms and parameter initialization methods, we derive the temperature of systems during the phase transition both analytically and asymptotically, highlighting temperature as a vital indicator of system data distribution and ML training complexity. Moreover, we perceive deep neural networks as complex heat engines with both global temperature and local temperatures in each layer. The concept of work efficiency is introduced within neural networks, which mainly depends on the neural activation functions. We then classify neural networks based on their work efficiency, and describe neural networks as two types of heat engines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b562793a90ace11e972615234c347a06107e2828" target='_blank'>
              On the Temperature of Machine Learning Systems
              </a>
            </td>
          <td>
            Dong Zhang
          </td>
          <td>2024-04-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="A tremendous range of design tasks in materials, physics, and biology can be formulated as finding the optimum of an objective function depending on many parameters without knowing its closed-form expression or the derivative. Traditional derivative-free optimization techniques often rely on strong assumptions about objective functions, thereby failing at optimizing non-convex systems beyond 100 dimensions. Here, we present a tree search method for derivative-free optimization that enables accelerated optimal design of high-dimensional complex systems. Specifically, we introduce stochastic tree expansion, dynamic upper confidence bound, and short-range backpropagation mechanism to evade local optimum, iteratively approximating the global optimum using machine learning models. This development effectively confronts the dimensionally challenging problems, achieving convergence to global optima across various benchmark functions up to 2,000 dimensions, surpassing the existing methods by 10- to 20-fold. Our method demonstrates wide applicability to a wide range of real-world complex systems spanning materials, physics, and biology, considerably outperforming state-of-the-art algorithms. This enables efficient autonomous knowledge discovery and facilitates self-driving virtual laboratories. Although we focus on problems within the realm of natural science, the advancements in optimization techniques achieved herein are applicable to a broader spectrum of challenges across all quantitative disciplines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/383c030cc1d9a568808e59e7f90cf8e0ef70b283" target='_blank'>
              Derivative-free tree optimization for complex systems
              </a>
            </td>
          <td>
            Ye Wei, Bo Peng, Ruiwen Xie, Yangtao Chen, Yu Qin, Peng Wen, Stefan Bauer, Po-Yen Tung
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Feedback optimization enables autonomous optimality seeking of a dynamical system through its closed-loop interconnection with iterative optimization algorithms. Among various iteration structures, model-based approaches require the input-output sensitivity of the system to construct gradients, whereas model-free approaches bypass this need by estimating gradients from real-time evaluations of the objective. These approaches own complementary benefits in sample efficiency and accuracy against model mismatch, i.e., errors of sensitivities. To achieve the best of both worlds, we propose gray-box feedback optimization controllers, featuring systematic incorporation of approximate sensitivities into model-free updates via adaptive convex combination. We quantify conditions on the accuracy of the sensitivities that render the gray-box approach preferable. We elucidate how the closed-loop performance is determined by the number of iterations, the problem dimension, and the cumulative effect of inaccurate sensitivities. The proposed controller contributes to a balanced closed-loop behavior, which retains provable sample efficiency and optimality guarantees for nonconvex problems. We further develop a running gray-box controller to handle constrained time-varying problems with changing objectives and steady-state maps.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c4e00a07039a7b16e2cf62ef7aac644c0add818c" target='_blank'>
              Gray-Box Nonlinear Feedback Optimization
              </a>
            </td>
          <td>
            Zhiyu He, S. Bolognani, Michael Muehlebach, Florian Dorfler
          </td>
          <td>2024-04-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>35</td>
        </tr>

        <tr id="Following arXiv:2303.02992, we develop an approach to the Hamiltonian theory of normal forms based on continuous averaging. We concentrate on the case of normal forms near an elliptic singular point, but unlike arXiv:2303.02992 we do not assume that frequences of the linearized system are nonresonant. We study analytic properties of the normalization procedure. In particular we show that in the case of a codimension one resonance an analytic Hamiltonian function may be reduced to a normal form up to an exponentially small reminder with explicit estimates of the reminder and the analyticity domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b7b8075917a1a33d24a24b339438b7570fcab468" target='_blank'>
              Normalization flow in the presence of a resonance
              </a>
            </td>
          <td>
            Dmitry Treschev
          </td>
          <td>2024-04-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The intersection between classical data assimilation methods and novel machine learning techniques has attracted significant interest in recent years. Here we explore another promising solution in which diffusion models are used to formulate a robust nonlinear ensemble filter for sequential data assimilation. Unlike standard machine learning methods, the proposed \textit{Ensemble Score Filter (EnSF)} is completely training-free and can efficiently generate a set of analysis ensemble members. In this study, we apply the EnSF to a surface quasi-geostrophic model and compare its performance against the popular Local Ensemble Transform Kalman Filter (LETKF), which makes Gaussian assumptions on the posterior distribution. Numerical tests demonstrate that EnSF maintains stable performance in the absence of localization and for a variety of experimental settings. We find that EnSF achieves competitive performance relative to LETKF in the case of linear observations, but leads to significant advantages when the state is nonlinearly observed and the numerical model is subject to unexpected shocks. A spectral decomposition of the analysis results shows that the largest improvements over LETKF occur at large scales (small wavenumbers) where LETKF lacks sufficient ensemble spread. Overall, this initial application of EnSF to a geophysical model of intermediate complexity is very encouraging, and motivates further developments of the algorithm for more realistic problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/55f4b9138b3b5ccddebc896d745d1fbef3eeb083" target='_blank'>
              Nonlinear ensemble filtering with diffusion models: Application to the surface quasi-geostrophic dynamics
              </a>
            </td>
          <td>
            Feng Bao, H. Chipilski, Siming Liang, Guannan Zhang, Jeffrey S.Whitaker
          </td>
          <td>2024-04-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Here we test the Stochastic Dynamic Operator (SDO) as a new framework for describing physiological signal dynamics relative to spiking or stimulus events. The SDO is a natural extension of existing spike-triggered averaging (STA), or stimulus-triggered averaging, methods currently used in neural analysis. It extends the classic STA to cover state-dependent and probabilistic responses where STA may fail. SDO methods are more sensitive and specific than the STA for identifying state-dependent relationships in simulated data. We have tested SDO analysis for interactions between electrophysiological recordings of spinal interneurons, single motor units, and aggregate muscle electromyograms (EMG) of major muscles in the spinal frog hindlimb. When predicting target signal behavior relative to spiking events, the SDO framework outperformed or matched classical spike-triggered averaging methods. SDO analysis permits more complicated spike-signal relationships to be captured, analyzed, and interpreted visually and intuitively. SDO methods can be applied at different scales of interest where spike-triggered averaging methods are currently employed, and beyond, from single neurons to gross motor behaviors. SDOs may be readily generated and analyzed using the provided SDO Analysis Toolkit. We anticipate this method will be broadly useful for describing dynamical signal behavior and uncovering state-dependent relationships of stochastic signals relative to discrete event times. SIGNIFICANCE Here the authors introduce new tools and demonstrate data analysis using a new probabilistic and state-dependent technique, which is an expansion and extension of the classical spike-triggered average, the Stochastic Dynamic Operator. Stochastic Dynamic Operator methods extend application into domains where classical spike triggered averages fail, capture more information on spike correlations, and match or outperform the spike-triggered average when generating predictions of signal amplitude based on spiking events. A data and code package toolkit for utilizing and interpreting Stochastic Dynamic Operator methods is provided together with example analyses. Both the method and the associated toolkit are thus expected to be broadly useful in research domains where the spike triggered average is currently used for analysis, and beyond.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/78bda4f2d6769adf0030359a4a589820d74e9eb1" target='_blank'>
              A Stochastic Dynamic Operator framework that improves the precision of analysis and prediction relative to the classical spike-triggered average method, extending the toolkit
              </a>
            </td>
          <td>
            Trevor S. Smith, Maryam Abolfath-Beygi, Terence D. Sanger, S. Giszter
          </td>
          <td>2024-05-12</td>
          <td>bioRxiv</td>
          <td>0</td>
          <td>33</td>
        </tr>

        <tr id="We study minimax optimization problems defined over infinite-dimensional function classes. In particular, we restrict the functions to the class of overparameterized two-layer neural networks and study (i) the convergence of the gradient descent-ascent algorithm and (ii) the representation learning of the neural network. As an initial step, we consider the minimax optimization problem stemming from estimating a functional equation defined by conditional expectations via adversarial estimation, where the objective function is quadratic in the functional space. For this problem, we establish convergence under the mean-field regime by considering the continuous-time and infinite-width limit of the optimization dynamics. Under this regime, gradient descent-ascent corresponds to a Wasserstein gradient flow over the space of probability measures defined over the space of neural network parameters. We prove that the Wasserstein gradient flow converges globally to a stationary point of the minimax objective at a $\mathcal{O}(T^{-1} + \alpha^{-1} ) $ sublinear rate, and additionally finds the solution to the functional equation when the regularizer of the minimax objective is strongly convex. Here $T$ denotes the time and $\alpha$ is a scaling parameter of the neural network. In terms of representation learning, our results show that the feature representation induced by the neural networks is allowed to deviate from the initial one by the magnitude of $\mathcal{O}(\alpha^{-1})$, measured in terms of the Wasserstein distance. Finally, we apply our general results to concrete examples including policy evaluation, nonparametric instrumental variable regression, and asset pricing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/581824ad64a4f9d8b30508fe3a9e100e37dbfc06" target='_blank'>
              A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to Functional Conditional Moment Equations
              </a>
            </td>
          <td>
            Yuchen Zhu, Yufeng Zhang, Zhaoran Wang, Zhuoran Yang, Xiaohong Chen
          </td>
          <td>2024-04-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="To describe the behavior of Markov models as parameters are varied, I show how to embed the space of Markov models within a Minkowski space. This embedding maintains the inherent distance between different instances of the model. The coordinates of this embedding emerge from the symmetrized Kullback-Leibler divergence and are expressed in terms of thermodynamic quantities, organizing the Minkowski space into equilibrium and nonequilibrium components. With this approach, we can visualize models' behaviors and gain a thermodynamic interpretation of information geometric concepts, even in far-from-equilibrium scenarios. I illustrate this approach using an analytically solvable molecular motor model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7ac9bdbe494e68941615e35c270584fb5c9ece8d" target='_blank'>
              A Minkowski space embedding to understand Markov models dynamics
              </a>
            </td>
          <td>
            David Andrieux
          </td>
          <td>2024-04-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Convergence of Q-learning has been the focus of extensive research over the past several decades. Recently, an asymptotic convergence analysis for Q-learning was introduced using a switching system framework. This approach applies the so-called ordinary differential equation (ODE) approach to prove the convergence of the asynchronous Q-learning modeled as a continuous-time switching system, where notions from switching system theory are used to prove its asymptotic stability without using explicit Lyapunov arguments. However, to prove stability, restrictive conditions, such as quasi-monotonicity, must be satisfied for the underlying switching systems, which makes it hard to easily generalize the analysis method to other reinforcement learning algorithms, such as the smooth Q-learning variants. In this paper, we present a more general and unified convergence analysis that improves upon the switching system approach and can analyze Q-learning and its smooth variants. The proposed analysis is motivated by previous work on the convergence of synchronous Q-learning based on $p$-norm serving as a Lyapunov function. However, the proposed analysis addresses more general ODE models that can cover both asynchronous Q-learning and its smooth versions with simpler frameworks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/35b665f654e9258edb7584a57208684c34f749a0" target='_blank'>
              Unified ODE Analysis of Smooth Q-Learning Algorithms
              </a>
            </td>
          <td>
            Donghwan Lee
          </td>
          <td>2024-04-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This paper presents sufficient conditions for the stability and $\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a"lifted"representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7299e4cb34fb84726ea855a21d8c1bd60c6f7b7d" target='_blank'>
              Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks
              </a>
            </td>
          <td>
            Sahel Vahedi Noori, Bin Hu, G. Dullerud, Peter J. Seiler
          </td>
          <td>2024-05-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="In this study, we introduce a novel family of tensor networks, termed constrained matrix product states (MPS), designed to incorporate exactly arbitrary linear constraints into sparse block structures. These tensor networks effectively bridge the gap between U(1) symmetric MPS and traditional, unconstrained MPS. Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario. We further develop canonical forms for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules. Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary cost functions subject to linear constraints. We use this to solve the quadratic knapsack problem and show a superior performance against a leading nonlinear integer programming solver, highlighting the potential of our method in tackling complex constrained combinatorial optimization problems">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fa272e532859613c86b201df2aee87299f44fef7" target='_blank'>
              Cons-training tensor networks
              </a>
            </td>
          <td>
            Javier Lopez-Piqueres, Jing Chen
          </td>
          <td>2024-05-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2024'],
    y: [27],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>