---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-07-22 06:07:00 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Physics-informed machine learning</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="Physics-informed machine learning (PIML) refers to the emerging area of extracting physically relevant solutions to complex multiscale modeling problems lacking sufficient quantity and veracity of data with learning models informed by physically relevant prior information. This work discusses the recent critical advancements in the PIML domain. Novel methods and applications of domain decomposition in physics-informed neural networks (PINNs) in particular are highlighted. Additionally, we explore recent works toward utilizing neural operator learning to intuit relationships in physics systems traditionally modeled by sets of complex governing equations and solved with expensive differentiation techniques. Finally, expansive applications of traditional physics-informed machine learning and potential limitations are discussed. In addition to summarizing recent work, we propose a novel taxonomic structure to catalog physics-informed machine learning based on how the physics-information is derived and injected into the machine learning process. The taxonomy assumes the explicit objectives of facilitating interdisciplinary collaboration in methodology, thereby promoting a wider characterization of what types of physics problems are served by the physics-informed learning machines and assisting in identifying suitable targets for future work. To summarize, the major twofold goal of this work is to summarize recent advancements and introduce a taxonomic catalog for applications of physics-informed machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c02bcf76b42448750eb510a96bfba6c18e6c84ce" target='_blank'>A Taxonomic Survey of Physics-Informed Machine Learning</a></td>
          <td>
            J. Pateras, P. Rana, Preetam Ghosh
          </td>
          <td>2023-06-07</td>
          <td>Applied Sciences</td>
          <td>7</td>
          <td>11</td>
        </tr>
    
        <tr id="Abstract Physics-informed machine learning (PIML) has emerged as a promising new approach for simulating complex physical and biological systems that are governed by complex multiscale processes for which some data are also available. In some instances, the objective is to discover part of the hidden physics from the available data, and PIML has been shown to be particularly effective for such problems for which conventional methods may fail. Unlike commercial machine learning where training of deep neural networks requires big data, in PIML big data are not available. Instead, we can train such networks from additional information obtained by employing the physical laws and evaluating them at random points in the space–time domain. Such PIML integrates multimodality and multifidelity data with mathematical models, and implements them using neural networks or graph networks. Here, we review some of the prevailing trends in embedding physics into machine learning, using physics-informed neural networks (PINNs) based primarily on feed-forward neural networks and automatic differentiation. For more complex systems or systems of systems and unstructured data, graph neural networks (GNNs) present some distinct advantages, and here we review how physics-informed learning can be accomplished with GNNs based on graph exterior calculus to construct differential operators; we refer to these architectures as physics-informed graph networks (PIGNs). We present representative examples for both forward and inverse problems and discuss what advances are needed to scale up PINNs, PIGNs and more broadly GNNs for large-scale engineering problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b30c496f3eae005a948eb2b6dff4db69e39958f0" target='_blank'>Scalable algorithms for physics-informed neural and graph networks</a></td>
          <td>
            K. Shukla, Mengjia Xu, N. Trask, G. Karniadakis
          </td>
          <td>2022-05-16</td>
          <td>Data-Centric Engineering</td>
          <td>26</td>
          <td>127</td>
        </tr>
    
        <tr id="Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator (KNO) family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the KNO, a kind of mesh-independent neural-network-based PDE solvers developed following the dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes, while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high-dimensional, and non-linear PDEs. All variants are validated by mesh-independent and long-term prediction experiments implemented on representative PDEs (e.g., the Navier–Stokes equation and the Bateman–Burgers equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution global-scale climate datasets in earth physics). These demonstrations suggest the potential of KoopmanLab to be a fundamental tool in diverse physics studies related to equations or dynamic systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1ea73ffbdc09eab4c8dd3c4a8f234bf88effdd11" target='_blank'>KoopmanLab: machine learning for solving complex physics equations</a></td>
          <td>
            Wei Xiong, Muyuan Ma, Ziyang Zhang, Pei Sun, Yang Tian
          </td>
          <td>2023-01-03</td>
          <td>ArXiv</td>
          <td>7</td>
          <td>6</td>
        </tr>
    
        <tr id="
 Advancements in computing power have recently made it possible to utilize machine learning and deep learning to push scientific computing forward in a range of disciplines, such as fluid mechanics, solid mechanics, materials science, etc. The incorporation of neural networks is particularly crucial in this hybridization process. Due to their intrinsic architecture, conventional neural networks cannot be successfully trained and scoped when data is sparse, which is the case in many scientific and engineering domains. Nonetheless, neural networks provide a solid foundation to respect physics-driven or knowledge-based constraints during training. Generally speaking, there are three distinct neural network frameworks to enforce the underlying physics: (i) physics-guided neural networks (PgNNs), (ii) physics-informed neural networks (PiNNs), and (iii) physics-encoded neural networks (PeNNs). These methods provide distinct advantages for accelerating the numerical modeling of complex multiscale multi-physics phenomena. In addition, the recent developments in neural operators (NOs) add another dimension to these new simulation paradigms, especially when the real-time prediction of complex multi-physics systems is required. All these models also come with their own unique drawbacks and limitations that call for further fundamental research. This study aims to present a review of the four neural network frameworks (i.e., PgNNs, PiNNs, PeNNs, and NOs) used in scientific computing research. The state-of-the-art architectures and their applications are reviewed, limitations are discussed, and future research opportunities are presented in terms of improving algorithms, considering causalities, expanding applications, and coupling scientific and deep learning solvers.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bc9ae66c84e31839015dd30c0bd76548bf1b7a5c" target='_blank'>Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks and Operators in Scientific Computing: Fluid and Solid Mechanics</a></td>
          <td>
            Salah A. Faroughi, Nikhil M. Pawar, Célio Fernandes, M. Raissi, Subasish Das, N. Kalantari, S. K. Mahjour
          </td>
          <td>2024-01-08</td>
          <td>J. Comput. Inf. Sci. Eng.</td>
          <td>22</td>
          <td>24</td>
        </tr>
    
        <tr id=" Physics-Informed Neural Networks (PINNs) represent a groundbreaking approach wherein neural networks (NNs) integrate model equations, such as Partial Differential Equations (PDEs), within their architecture. This innovation has become instrumental in solving diverse problem sets including PDEs, fractional equations, integral-differential equations, and stochastic PDEs. It's a versatile multi-task learning framework that tasks NNs with fitting observed data while simultaneously minimizing PDE residuals. This paper delves into the landscape of PINNs, aiming to delineate their inherent strengths and weaknesses. Beyond exploring the fundamental characteristics of these networks, this review endeavors to encompass a wider spectrum of collocation-based physics-informed neural networks, extending beyond the core PINN model. Variants like physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN) constitute pivotal aspects of this exploration. The study accentuates a predominant focus in research on tailoring PINNs through diverse strategies: adapting activation functions, refining gradient optimization techniques, innovating neural network structures, and enhancing loss function architectures. Despite the extensive applicability demonstrated by PINNs, surpassing classical numerical methods like Finite Element Method (FEM) in certain contexts, the review highlights ongoing opportunities for advancement. Notably, there are persisting theoretical challenges that demand resolution, ensuring the continued evolution and refinement of this revolutionary approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d724afae8cafd27a64fb7f6418c81436eaf2f6cc" target='_blank'>Machine Learning Through Physics–Informed Neural Networks: Progress and Challenges</a></td>
          <td>
            Klapa Antonion, Xiao Wang, M. Raissi, Laurn Joshie
          </td>
          <td>2024-01-20</td>
          <td>Academic Journal of Science and Technology</td>
          <td>2</td>
          <td>24</td>
        </tr>
    
        <tr id="Partial differential equations (PDEs) play a pivotal role in mathematical analysis and modeling of dynamic processes across various disciplines of science and engineering. Machine learning (ML) techniques have emerged as a promising new approach to solving PDEs. Among them, Physics-Informed Neural Networks (PINNs) have garnered significant attention in numerous scientific and engineering studies. PINNs employ a single deep neural network to assimilate observational data with PDEs across the entire space-time of a physical system, subsequently yielding rapid solutions. However, a PINN may entail intricate analyses or computations and can be cost-intensive, depending on initial or boundary conditions and other input parameters. To address the limitations of the PINN, especially concerning resolution for nonlinear problems, the Physical-Informed Deep Operator Network (DeepONet) is introduced in this paper. The Physics-Informed DeepONet is a deep learning framework crafted to discern solution operators for any given PDEs, even in scenarios lacking paired input/output training data. The proposed framework is able to predict solutions for various types of parameterized PDEs much faster than conventional PDE solvers. Several cases confirm that this approach is effective in establishing previously unexplored paradigms for modeling/simulating nonlinear and non-equilibrium processes in science and engineering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/54aa8e27e0e1c52997a21502817afcb71d9e8ccb" target='_blank'>Understanding on Physics-Informed DeepONet</a></td>
          <td>
            Sang-Min Lee
          </td>
          <td>2024-01-31</td>
          <td>Journal of the Korea Academia-Industrial cooperation Society</td>
          <td>0</td>
          <td>0</td>
        </tr>
    
        <tr id="Recent breakthroughs in computing power have made it feasible to use machine learning and deep learning to advance scientific computing in many fields, including fluid mechanics, solid mechanics, materials science, etc. Neural networks, in particular, play a central role in this hybridization. Due to their intrinsic architecture, conventional neural networks cannot be successfully trained and scoped when data is sparse, which is the case in many scientific and engineering domains. Nonetheless, neural networks provide a solid foundation to respect physics-driven or knowledge-based constraints during training. Generally speaking, there are three distinct neural network frameworks to enforce the underlying physics: (i) physics-guided neural networks (PgNNs), (ii) physics-informed neural networks (PiNNs), and (iii) physics-encoded neural networks (PeNNs). These methods provide distinct advantages for accelerating the numerical modeling of complex multiscale multi-physics phenomena. In addition, the recent developments in neural operators (NOs) add another dimension to these new simulation paradigms, especially when the real-time prediction of complex multi-physics systems is required. All these models also come with their own unique drawbacks and limitations that call for further fundamental research. This study aims to present a review of the four neural network frameworks (i.e., PgNNs, PiNNs, PeNNs, and NOs) used in scientific computing research. The state-of-the-art architectures and their applications are reviewed, limitations are discussed, and future research opportunities in terms of improving algorithms, considering causalities, expanding applications, and coupling scientific and deep learning solvers are presented. This critical review provides researchers and engineers with a solid starting point to comprehend how to integrate different layers of physics into neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/759a38745575ef3ef1a67f7128d5e00fd8a303f0" target='_blank'>Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing</a></td>
          <td>
            Salah A. Faroughi, N. Pawar, C. Fernandes, Subasish Das, N. Kalantari, S. K. Mahjour
          </td>
          <td>2022-11-14</td>
          <td>ArXiv</td>
          <td>31</td>
          <td>29</td>
        </tr>
    
        <tr id="We propose a very general framework for deriving rigorous bounds on the approximation error for physics-informed neural networks (PINNs) and operator learning architectures such as DeepONets and FNOs as well as for physics-informed operator learning. These bounds guarantee that PINNs and (physics-informed) DeepONets or FNOs will efficiently approximate the underlying solution or solution operator of generic partial differential equations (PDEs). Our framework utilizes existing neural network approximation results to obtain bounds on more involved learning architectures for PDEs. We illustrate the general framework by deriving the first rigorous bounds on the approximation error of physics-informed operator learning and by showing that PINNs (and physics-informed DeepONets and FNOs) mitigate the curse of dimensionality in approximating nonlinear parabolic PDEs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a1d9a2ca8002ba984357c156b4a7339eda219e2f" target='_blank'>Generic bounds on the approximation error for physics-informed (and) operator learning</a></td>
          <td>
            Tim De Ryck, Siddhartha Mishra
          </td>
          <td>2022-05-23</td>
          <td>ArXiv</td>
          <td>48</td>
          <td>10</td>
        </tr>
    
        <tr id="Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy. Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks. This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning. The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed. The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed. In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods. In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain. In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5858daa78db0cd950891018505e0e63c41b90ee" target='_blank'>Scientific machine learning for closure models in multiscale problems: a review</a></td>
          <td>
            Benjamin Sanderse, P. Stinis, R. Maulik, Shady E. Ahmed
          </td>
          <td>2024-03-05</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>21</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>