---
hide:
 - navigation
---
<!DOCTYPE html>
#
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2025-12-22 06:13:50 UTC</i>
  </p>
  
  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <p>
  <h3 data-intro='Recommendations for the article'>
    Recommendations for the article <i>Data-driven discovery of coordinates and governing equations</i>
  </h3>
  <table id="table1" class="display wrap" style="width:100%">
  <thead>
    <tr>
        <th data-intro='Click to view the abstract (if available)'>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/ Conference</th>
        <th>Citation count</th>
        <th data-intro='Highest h-index among the authors'>Highest h-index</th>
    </tr>
  </thead>
  <tbody>
    
        <tr id="Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture. Recovery of underlying governing laws or equations describing the evolution of complex systems from data can be challenging if dataset is damaged or incomplete. The authors propose a learning approach which allows to discover governing partial differential equations from scarce and noisy data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e596988b1df3a0bc78bf72c0bfdb21c85eaab6c9" target='_blank'>Physics-informed learning of governing equations from scarce data</a></td>
          <td>
            Zhao Chen, Yang Liu, Hao Sun
          </td>
          <td>2020-05-05</td>
          <td>Nature Communications</td>
          <td>508</td>
          <td>15</td>
        </tr>
    
        <tr id="Identifying governing equations for a dynamical system is a topic of critical interest across an array of disciplines, from mathematics to engineering to biology. Machine learning-specifically deep learning-techniques have shown their capabilities in approximating dynamics from data, but a shortcoming of traditional deep learning is that there is little insight into the underlying mapping beyond its numerical output for a given input. This limits their utility in analysis beyond simple prediction. Simultaneously, a number of strategies exist which identify models based on a fixed dictionary of basis functions, but most either require some intuition or insight about the system, or are susceptible to overfitting or a lack of parsimony. Here, we present a novel approach that combines the flexibility and accuracy of deep learning approaches with the utility of symbolic solutions: a deep neural network that generates a symbolic expression for the governing equations. We first describe the architecture for our model and then show the accuracy of our algorithm across a range of classical dynamical systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eddffa1f2cf18c7ca4edbdaf01fd805f415aa4b8" target='_blank'>Symbolic regression via neural networks.</a></td>
          <td>
            N. Boddupalli, T. Matchen, J. Moehlis
          </td>
          <td>2023-08-01</td>
          <td>Chaos</td>
          <td>13</td>
          <td>37</td>
        </tr>
    
        <tr id="Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/036441fb7af2f04ef5ee8def7e8e0461fb9fdc66" target='_blank'>Machine Learning for Partial Differential Equations</a></td>
          <td>
            S. Brunton, J. Kutz
          </td>
          <td>2023-03-30</td>
          <td>ArXiv</td>
          <td>21</td>
          <td>77</td>
        </tr>
    
        <tr id="The machine learning methods for data-driven identification of partial differential equations (PDEs) are typically defined for a given number of spatial dimensions and a choice of coordinates the data have been collected in. This dependence prevents the learned evolution equation from generalizing to other spaces. In this work, we reformulate the problem in terms of coordinate- and dimension-independent representations, paving the way toward what we call ``spatially liberated"PDE learning. To this end, we employ a machine learning approach to predict the evolution of scalar field systems expressed in the formalism of exterior calculus, which is coordinate-free and immediately generalizes to arbitrary dimensions by construction. We demonstrate the performance of this approach in the FitzHugh-Nagumo and Barkley reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by in-situ chemotactic bacteria observations. We provide extensive numerical experiments that demonstrate that our approach allows for seamless transitions across various spatial contexts. We show that the field dynamics learned in one space can be used to make accurate predictions in other spaces with different dimensions, coordinate systems, boundary conditions, and curvatures.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5a72323dd0994f74cd0229cf910084ec1cc5c74" target='_blank'>Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations</a></td>
          <td>
            Trung V Phan, George A. Kevrekidis, Soledad Villar, Y. Kevrekidis, J. Bello-Rivas
          </td>
          <td>2025-05-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>
    
        <tr id="
 In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring "big data". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9" target='_blank'>Universal Differential Equations for Scientific Machine Learning</a></td>
          <td>
            Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, K. Zubov, R. Supekar, Dominic J. Skinner, A. Ramadhan
          </td>
          <td>2020-01-13</td>
          <td>ArXiv</td>
          <td>689</td>
          <td>27</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d69703a20ce4710f71900130dd935761bbf01fa" target='_blank'>Promising directions of machine learning for partial differential equations</a></td>
          <td>
            Steve Brunton, J. Kutz
          </td>
          <td>2024-06-28</td>
          <td>Nature Computational Science</td>
          <td>86</td>
          <td>4</td>
        </tr>
    
        <tr id="Differential equations have become increasingly integral to the development of neural network architectures, providing a continuous-time framework that enhances both the theoretical understanding and practical application of deep learning models. This review explores the mathematical foundations of integrating ordinary and partial differential equations (ODEs and PDEs) into neural networks, with a focus on Neural Ordinary Differential Equations (Neural ODEs) and Physics-Informed Neural Networks (PINNs). We discuss the theoretical insights gained from this integration, including stability, convergence, and generalization, and examine key applications in time-series forecasting, generative modeling, control systems, and biological systems. Despite their promise, these models face challenges related to computational complexity, stability, scalability, and interpretability. Future research directions include the development of specialized numerical solvers, enhancing model robustness, scaling to high-dimensional problems, and improving interpretability. This paper provides a comprehensive overview of the current state and future prospects of differential equation-based neural networks, highlighting their potential to advance both theoretical and applied machine learning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83ab2fd64c6cd5123b1959b80d876ee5e619c706" target='_blank'>Exploring the Integration of Differential Equations in Neural Networks: Theoretical Foundations, Applications, and Future Directions</a></td>
          <td>
            Lehel Dénes-Fazakas, Kata Sándor-Rokaly, László Szász, Henrik Csuzi, Levente Kovács, László Szilágyi, G. Eigner
          </td>
          <td>2024-11-19</td>
          <td>2024 IEEE 24th International Symposium on Computational Intelligence and Informatics (CINTI)</td>
          <td>0</td>
          <td>12</td>
        </tr>
    
        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d5e16fc64d0555d187b47d03c5698747e15adaa" target='_blank'>Hidden physics models: Machine learning of nonlinear partial differential equations</a></td>
          <td>
            M. Raissi, G. Karniadakis
          </td>
          <td>2017-08-02</td>
          <td>J. Comput. Phys.</td>
          <td>1226</td>
          <td>140</td>
        </tr>
    
        <tr id="The recently introduced class of architectures known as Neural Operators has emerged as highly versatile tools applicable to a wide range of tasks in the field of Scientific Machine Learning (SciML), including data representation and forecasting. In this study, we investigate the capabilities of Neural Implicit Flow (NIF), a recently developed mesh-agnostic neural operator, for representing the latent dynamics of canonical systems such as the Kuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon (SG) equations, as well as for extracting dynamically relevant information from them. Finally we assess the applicability of NIF as a dimensionality reduction algorithm and conduct a comparative analysis with another widely recognized family of neural operators, known as Deep Operator Networks (DeepONets).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bd2ea0dbc681d0e289d33f89b612d08a007f2fc6" target='_blank'>Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems</a></td>
          <td>
            Imran Nasim, Joao Lucas de Sousa Almeida
          </td>
          <td>2024-04-26</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>2</td>
        </tr>
    
  </tbody>
  <tfoot>
    <tr>
        <th>Abstract</th>
        <th>Title</th>
        <th>Authors</th>
        <th>Publication Date</th>
        <th>Journal/Conference</th>
        <th>Citation count</th>
        <th>Highest h-index</th>
    </tr>
  </tfoot>
  </table>
  </p>

</body>

<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: false,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '5%', targets: 0 },
        { width: '25%', targets: 1 },
        { width: '20%', targets: 2 },
        { width: '10%', targets: 3 },
        { width: '20%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  
  var table = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>